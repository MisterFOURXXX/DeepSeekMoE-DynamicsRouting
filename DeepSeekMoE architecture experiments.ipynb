{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install necessary packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (6.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: GPUtil in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.66.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.11/dist-packages (7.352.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (12.0.0)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml) (12.575.51)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.66.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge) (1.16.0)\n",
      "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install GPUtil\n",
    "!pip install rouge_score\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install pynvml\n",
    "!pip install rouge-score\n",
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 13:44:56.254661: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 13:44:56.254764: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 13:44:56.354588: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 13:44:56.547633: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 13:44:58.096520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, losses, metrics, callbacks\n",
    "import psutil\n",
    "import time\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import gc\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import defaultdict\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import nvidia_smi\n",
    "except ImportError:\n",
    "    nvidia_smi = None\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE baseline experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 13:07:30.028772: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 13:07:30.028908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 13:07:30.103505: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 13:07:30.245306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 13:07:31.526568: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training Iteration 1\n",
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 13:07:34.914445: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:07:35.152078: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:07:35.152322: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:07:35.153825: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:07:35.153976: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:07:35.154066: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:07:35.227364: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:07:35.227562: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:07:35.227678: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:07:35.227758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/4428 [00:00<?, ?it/s]2025-07-04 13:08:03.625390: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f1a244cf020 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 13:08:03.625452: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2025-07-04 13:08:03.646323: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 13:08:03.691190: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751634483.811700     680 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 1: 100%|██████████| 4428/4428 [03:02<00:00, 24.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_Deepseekbaseline_moe_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_Deepseekbaseline_moe_model/assets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Iteration 2</th>\n",
       "      <th>Iteration 3</th>\n",
       "      <th>Iteration 4</th>\n",
       "      <th>Iteration 5</th>\n",
       "      <th>Iteration 6</th>\n",
       "      <th>Iteration 7</th>\n",
       "      <th>Iteration 8</th>\n",
       "      <th>Iteration 9</th>\n",
       "      <th>Iteration 10</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Speed (epochs per second)</th>\n",
       "      <td>0.005666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training Time (second/epoch)</th>\n",
       "      <td>176.480350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.648035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Training Time (second/iteration)</th>\n",
       "      <td>183.451998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computational Resource Usage</th>\n",
       "      <td>64.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>25.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>7.495152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.749515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.531799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.453180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPS Estimate (GFLOPS)</th>\n",
       "      <td>2.377917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLU Load Distribution Stability</th>\n",
       "      <td>207.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLG Load Distribution Stability</th>\n",
       "      <td>737.512085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.751209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 2 (percent)</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 3 (percent)</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 4 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 5 (percent)</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 6 (percent)</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 7 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 8 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 2 (percent)</th>\n",
       "      <td>1.432836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 3 (percent)</th>\n",
       "      <td>3.388060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 4 (percent)</th>\n",
       "      <td>5.746269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 5 (percent)</th>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 6 (percent)</th>\n",
       "      <td>3.208955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 7 (percent)</th>\n",
       "      <td>84.208955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.420896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 8 (percent)</th>\n",
       "      <td>1.194030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Entity Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Intent Accuracy</th>\n",
       "      <td>0.750226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Perplexity</th>\n",
       "      <td>18.817522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.881752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Response Cosine Similarity</th>\n",
       "      <td>0.449263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Iteration 1  Iteration 2  \\\n",
       "Training Speed (epochs per second)                0.005666          0.0   \n",
       "Training Time (second/epoch)                    176.480350          0.0   \n",
       "Total Training Time (second/iteration)          183.451998          0.0   \n",
       "Computational Resource Usage                     64.100000          0.0   \n",
       "Average CPU Usage (percent)                      25.100000          0.0   \n",
       "Average GPU Usage (percent)                      39.000000          0.0   \n",
       "Average Memory (GB)                               7.495152          0.0   \n",
       "Average GPU Memory (GB)                          14.531799          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                   2.377917          0.0   \n",
       "Expert NLU Load Distribution Stability          207.250000          0.0   \n",
       "Expert NLG Load Distribution Stability          737.512085          0.0   \n",
       "Average NLU Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 2 (percent)                   33.000000          0.0   \n",
       "Average NLU Expert 3 (percent)                   37.000000          0.0   \n",
       "Average NLU Expert 4 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 5 (percent)                   15.000000          0.0   \n",
       "Average NLU Expert 6 (percent)                   15.000000          0.0   \n",
       "Average NLU Expert 7 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 8 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 2 (percent)                    1.432836          0.0   \n",
       "Average NLG Expert 3 (percent)                    3.388060          0.0   \n",
       "Average NLG Expert 4 (percent)                    5.746269          0.0   \n",
       "Average NLG Expert 5 (percent)                    0.820896          0.0   \n",
       "Average NLG Expert 6 (percent)                    3.208955          0.0   \n",
       "Average NLG Expert 7 (percent)                   84.208955          0.0   \n",
       "Average NLG Expert 8 (percent)                    1.194030          0.0   \n",
       "Average Validation Entity Accuracy                1.000000          0.0   \n",
       "Average Intent Accuracy                           0.750226          0.0   \n",
       "Average Validation Perplexity                    18.817522          0.0   \n",
       "Average Validation Response Cosine Similarity     0.449263          0.0   \n",
       "\n",
       "                                               Iteration 3  Iteration 4  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 5  Iteration 6  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 7  Iteration 8  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 9  Iteration 10  \\\n",
       "Training Speed (epochs per second)                     0.0           0.0   \n",
       "Training Time (second/epoch)                           0.0           0.0   \n",
       "Total Training Time (second/iteration)                 0.0           0.0   \n",
       "Computational Resource Usage                           0.0           0.0   \n",
       "Average CPU Usage (percent)                            0.0           0.0   \n",
       "Average GPU Usage (percent)                            0.0           0.0   \n",
       "Average Memory (GB)                                    0.0           0.0   \n",
       "Average GPU Memory (GB)                                0.0           0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0           0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0           0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0           0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0           0.0   \n",
       "Average Validation Entity Accuracy                     0.0           0.0   \n",
       "Average Intent Accuracy                                0.0           0.0   \n",
       "Average Validation Perplexity                          0.0           0.0   \n",
       "Average Validation Response Cosine Similarity          0.0           0.0   \n",
       "\n",
       "                                                 Average  \n",
       "Training Speed (epochs per second)              0.000567  \n",
       "Training Time (second/epoch)                   17.648035  \n",
       "Total Training Time (second/iteration)         18.345200  \n",
       "Computational Resource Usage                    6.410000  \n",
       "Average CPU Usage (percent)                     2.510000  \n",
       "Average GPU Usage (percent)                     3.900000  \n",
       "Average Memory (GB)                             0.749515  \n",
       "Average GPU Memory (GB)                         1.453180  \n",
       "Average FLOPS Estimate (GFLOPS)                 0.237792  \n",
       "Expert NLU Load Distribution Stability         20.725000  \n",
       "Expert NLG Load Distribution Stability         73.751209  \n",
       "Average NLU Expert 1 (percent)                  0.000000  \n",
       "Average NLU Expert 2 (percent)                  3.300000  \n",
       "Average NLU Expert 3 (percent)                  3.700000  \n",
       "Average NLU Expert 4 (percent)                  0.000000  \n",
       "Average NLU Expert 5 (percent)                  1.500000  \n",
       "Average NLU Expert 6 (percent)                  1.500000  \n",
       "Average NLU Expert 7 (percent)                  0.000000  \n",
       "Average NLU Expert 8 (percent)                  0.000000  \n",
       "Average NLG Expert 1 (percent)                  0.000000  \n",
       "Average NLG Expert 2 (percent)                  0.143284  \n",
       "Average NLG Expert 3 (percent)                  0.338806  \n",
       "Average NLG Expert 4 (percent)                  0.574627  \n",
       "Average NLG Expert 5 (percent)                  0.082090  \n",
       "Average NLG Expert 6 (percent)                  0.320896  \n",
       "Average NLG Expert 7 (percent)                  8.420896  \n",
       "Average NLG Expert 8 (percent)                  0.119403  \n",
       "Average Validation Entity Accuracy              0.100000  \n",
       "Average Intent Accuracy                         0.075023  \n",
       "Average Validation Perplexity                   1.881752  \n",
       "Average Validation Response Cosine Similarity   0.044926  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reset_kernel():\n",
    "    tf.keras.backend.clear_session()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "results_table = {\n",
    "    'Training Speed (epochs per second)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Training Time (second/epoch)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Total Training Time (second/iteration)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Computational Resource Usage': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average CPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average FLOPS Estimate (GFLOPS)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLU Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLG Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Entity Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Intent Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Perplexity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Response Cosine Similarity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0}\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_Deepseekbaseline_moe_model\"\n",
    "\n",
    "for iteration in range(5):\n",
    "    print(f\"\\nStarting Training Iteration {iteration + 1}\")\n",
    "    reset_kernel()\n",
    "    \n",
    "    def load_tf_datasets_from_disk(load_path):\n",
    "        print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "        try:\n",
    "            with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "                loaded_moe_params = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "        element_spec = (\n",
    "            {\n",
    "                'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            },\n",
    "            {\n",
    "                'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "                'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "                'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "            test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "        raw_data = {}\n",
    "        for dataset_name in ['train', 'test']:\n",
    "            path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "            if os.path.exists(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    raw_data[dataset_name] = pickle.load(f)\n",
    "                print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Raw data file {path} not found.\")\n",
    "                raw_data[dataset_name] = []\n",
    "\n",
    "        return {\n",
    "            \"train_dataset\": train_tf_dataset,\n",
    "            \"test_dataset\": test_tf_dataset,\n",
    "            \"moe_params\": loaded_moe_params,\n",
    "            \"raw_data\": raw_data\n",
    "        }\n",
    "\n",
    "    tf_dataset_save_path = \"tf_datasets\"\n",
    "    loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "    train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "    moe_params = loaded_data[\"moe_params\"]\n",
    "    raw_data = loaded_data[\"raw_data\"]\n",
    "    moe_params[\"num_experts\"] = 4\n",
    "\n",
    "    class MoELayer(layers.Layer):\n",
    "        def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, top_k=2, name=None):\n",
    "            super(MoELayer, self).__init__(name=name)\n",
    "            self.m = m\n",
    "            self.k_s = k_s\n",
    "            self.total_experts = num_experts * self.m\n",
    "            self.routed_experts = self.total_experts - self.k_s\n",
    "            self.top_k = top_k\n",
    "            self.top_mk = self.m * self.top_k\n",
    "            self.top_mk = min(self.top_mk, self.routed_experts)\n",
    "            self.alpha = alpha\n",
    "            self.input_dim = input_dim\n",
    "            self.seq_length = None\n",
    "            self.adjusted_expert_dim = expert_dim // self.m\n",
    "            self.shared_experts = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                    layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "                ]) for i in range(self.k_s)\n",
    "            ]\n",
    "            self.routed_experts_list = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                    layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "                ]) for i in range(self.k_s, self.total_experts)\n",
    "            ]\n",
    "            self.experts = self.shared_experts + self.routed_experts_list\n",
    "            self.gate = layers.Dense(self.routed_experts, activation='softmax')\n",
    "            self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            input_rank = inputs.shape.rank\n",
    "            if input_rank == 3:\n",
    "                batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "                flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "                is_3d = True\n",
    "            else:\n",
    "                batch_size = tf.shape(inputs)[0]\n",
    "                seq_length = 1\n",
    "                flat_inputs = inputs\n",
    "                is_3d = False\n",
    "            flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "            shared_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "            for i in range(self.k_s):\n",
    "                shared_output += self.shared_experts[i](flat_inputs)\n",
    "\n",
    "            gate_weights = self.gate(flat_inputs)\n",
    "            top_mk_values, top_mk_indices = tf.nn.top_k(gate_weights, k=self.top_mk, sorted=True)\n",
    "            top_mk_indices = top_mk_indices + self.k_s\n",
    "            top_mk_weights = top_mk_values / (tf.reduce_sum(top_mk_values, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "            routed_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "            \n",
    "            for k in range(self.top_mk):\n",
    "                kth_weights = top_mk_weights[:, k]\n",
    "                kth_indices = top_mk_indices[:, k]\n",
    "                mask = tf.one_hot(kth_indices, depth=self.total_experts, dtype=tf.float32)\n",
    "                expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "                kth_expert_output = tf.reduce_sum(expert_outputs * tf.expand_dims(mask, -1), axis=1)\n",
    "                weighted_kth_output = kth_expert_output * tf.expand_dims(kth_weights, -1)\n",
    "                routed_output += weighted_kth_output\n",
    "\n",
    "            output_flat = shared_output + routed_output + flat_inputs\n",
    "\n",
    "            if is_3d:\n",
    "                output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "                if self.seq_length is not None:\n",
    "                    output.set_shape([None, self.seq_length, self.input_dim])\n",
    "                gate_weights_reshaped = tf.reshape(gate_weights, [batch_size, seq_length, self.routed_experts])\n",
    "                if self.seq_length is not None:\n",
    "                    gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "            else:\n",
    "                output = output_flat\n",
    "                gate_weights_reshaped = gate_weights\n",
    "\n",
    "            f_i = tf.reduce_mean(tf.reduce_sum(tf.one_hot(tf.argmax(gate_weights, axis=-1), depth=self.routed_experts), axis=0), axis=0)\n",
    "            P_i = tf.reduce_mean(gate_weights, axis=0)\n",
    "            load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "            self.add_loss(load_balancing_loss)\n",
    "            self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "\n",
    "            return output, gate_weights_reshaped\n",
    "\n",
    "        def get_metrics(self):\n",
    "            return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'num_experts': self.total_experts // self.m,\n",
    "                'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "                'input_dim': self.input_dim,\n",
    "                'm': self.m,\n",
    "                'k_s': self.k_s,\n",
    "                'alpha': self.alpha,\n",
    "                'top_k': self.top_k,\n",
    "                'name': self.name\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class TransformerDecoderLayer(layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "            super(TransformerDecoderLayer, self).__init__()\n",
    "            self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.ffn = keras.Sequential([\n",
    "                layers.Dense(dff, activation='relu'),\n",
    "                layers.Dense(d_model)\n",
    "            ])\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "            self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "        def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "            if look_ahead_mask is not None:\n",
    "                look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "                look_ahead_mask = 1.0 - look_ahead_mask\n",
    "\n",
    "            attn1_output = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)\n",
    "            attn1 = self.dropout1(attn1_output, training=training)\n",
    "            out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "            attn2_output = self.mha2(query=out1, value=enc_output, key=enc_output, attention_mask=None, training=training)\n",
    "            attn2 = self.dropout2(attn2_output, training=training)\n",
    "            out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "            ffn_output = self.ffn(out2)\n",
    "            ffn_output = self.dropout3(ffn_output, training=training)\n",
    "            out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "            return out3\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            mha1_config = self.mha1.get_config()\n",
    "            config.update({\n",
    "                'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "                'num_heads': mha1_config['num_heads'],\n",
    "                'dff': self.ffn.layers[0].units,\n",
    "                'rate': self.dropout1.rate\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class MoEModel(models.Model):\n",
    "        def __init__(self, moe_params):\n",
    "            super(MoEModel, self).__init__()\n",
    "            self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "            self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "            self.num_domains = moe_params[\"num_domains\"]\n",
    "            self.num_intents = moe_params[\"num_intents\"]\n",
    "            self.vocab_size = moe_params[\"vocab_size\"]\n",
    "            self.num_experts = moe_params[\"num_experts\"]\n",
    "            self.expert_dim = moe_params[\"expert_dim\"]\n",
    "            self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "            self.num_heads = 4\n",
    "            self.dff = self.embedding_dim * 4\n",
    "            self.dropout_rate = 0.1\n",
    "            self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                                 self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "            self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "            self.embedding = layers.Embedding(\n",
    "                self.vocab_size,\n",
    "                self.embedding_dim,\n",
    "                mask_zero=True,\n",
    "                embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05)\n",
    "            )\n",
    "            self.embedding.build((None,))\n",
    "            with tf.init_scope():\n",
    "                embedding_weights = self.embedding.get_weights()\n",
    "                if embedding_weights and len(embedding_weights) > 0:\n",
    "                    embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                    self.embedding.set_weights(embedding_weights)\n",
    "\n",
    "            self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "            self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "            self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "            self.moe_nlu = MoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlu_input_dim, m=2, k_s=1, alpha=0.01, top_k=2, name='moe_nlu')\n",
    "            self.moe_nlg = MoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlg_input_dim, m=2, k_s=1, alpha=0.01, top_k=2, name='moe_nlg')\n",
    "            self.intent_output = layers.Dense(self.num_intents, activation='sigmoid')\n",
    "            self.domain_output = layers.Dense(self.num_domains, activation='softmax')\n",
    "            self.response_output = layers.TimeDistributed(layers.Dense(self.vocab_size, activation='softmax'))\n",
    "            self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "\n",
    "        def create_look_ahead_mask(self, size):\n",
    "            mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "            return mask\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            user_utterance_tokens = inputs.get('user_utterance_tokens')\n",
    "            prev_system_response_tokens = inputs.get('prev_system_response_tokens')\n",
    "            decoder_input_tokens = inputs.get('decoder_input_tokens')\n",
    "            domain_onehot_input = inputs.get('domain_onehot_input')\n",
    "            turn_id_embedding = inputs.get('turn_id_embedding')\n",
    "            ontology_multihot_input = inputs.get('ontology_multihot_input')\n",
    "            if any(x is None for x in [user_utterance_tokens, prev_system_response_tokens, decoder_input_tokens,\n",
    "                                      domain_onehot_input, turn_id_embedding, ontology_multihot_input]):\n",
    "                raise ValueError(\"One or more required inputs are missing or None\")\n",
    "\n",
    "            pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "            user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "            prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "            decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "            user_enc_out = user_embedded\n",
    "            prev_system_enc_out = prev_system_embedded\n",
    "\n",
    "            look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "            dec_output = decoder_embedded\n",
    "            for i, layer in enumerate(self.decoder_layers):\n",
    "                dec_output = layer(dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask)\n",
    "\n",
    "            decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "\n",
    "            user_attn_out = self.attn_layer(query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), \n",
    "                                          value=user_enc_out, key=user_enc_out, training=training)\n",
    "            prev_system_attn_out = self.attn_layer(query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), \n",
    "                                                 value=prev_system_enc_out, key=prev_system_enc_out, training=training)\n",
    "            decoder_attn_out = self.attn_layer(query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), \n",
    "                                             value=decoder_output, key=decoder_output, training=training)\n",
    "\n",
    "            combined_features = layers.Concatenate()([\n",
    "                tf.squeeze(user_attn_out, 1),\n",
    "                tf.squeeze(prev_system_attn_out, 1),\n",
    "                tf.squeeze(decoder_attn_out, 1),\n",
    "                domain_onehot_input,\n",
    "                turn_id_embedding,\n",
    "                ontology_multihot_input\n",
    "            ])\n",
    "            combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "            nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "            nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "            intent_out = self.intent_output(nlu_out)\n",
    "            domain_out = self.domain_output(nlu_out)\n",
    "            intent_out_expanded = tf.expand_dims(intent_out, axis=1) \n",
    "            domain_out_expanded = tf.expand_dims(domain_out, axis=1) \n",
    "            intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1]) \n",
    "            domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "            nlg_input = layers.Concatenate(axis=-1)([\n",
    "                decoder_output, \n",
    "                intent_out_tiled,  \n",
    "                domain_out_tiled  \n",
    "            ])\n",
    "            nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "            nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            response_out = self.response_output(nlg_out)\n",
    "\n",
    "            return {\n",
    "                'intent_output': intent_out,\n",
    "                'domain_output': domain_out,\n",
    "                'response_output': response_out,\n",
    "                'response_embeddings': decoder_output,\n",
    "                'nlu_gate_weights': nlu_gate_weights,\n",
    "                'nlg_gate_weights': nlg_gate_weights\n",
    "            }\n",
    "\n",
    "        def build_graph(self):\n",
    "            inputs = {\n",
    "                'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "                'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32),\n",
    "            }\n",
    "            return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'moe_params': {\n",
    "                    'embedding_dim': self.embedding_dim,\n",
    "                    'max_seq_length': self.max_seq_length,\n",
    "                    'num_domains': self.num_domains,\n",
    "                    'num_intents': self.num_intents,\n",
    "                    'vocab_size': self.vocab_size,\n",
    "                    'num_experts': self.num_experts,\n",
    "                    'expert_dim': self.expert_dim,\n",
    "                    'turn_id_dim': self.turn_id_dim\n",
    "                }\n",
    "            })\n",
    "            return config\n",
    "    \n",
    "    class IntentAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros')\n",
    "            self.total_preds = self.add_weight(name='total_preds', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "            self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "            self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.correct_preds.assign(0.)\n",
    "            self.total_preds.assign(0.)\n",
    "\n",
    "    class IntentPrecision(metrics.Metric):\n",
    "        def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "            self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            predicted_positives = tf.reduce_sum(y_pred)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.predicted_positives.assign(0.)\n",
    "\n",
    "    class IntentRecall(metrics.Metric):\n",
    "        def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "            self.actual_positives = self.add_weight(name='ap', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            actual_positives = tf.reduce_sum(y_true)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.actual_positives.assign(0.)\n",
    "\n",
    "    class IntentF1(metrics.Metric):\n",
    "        def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.precision = IntentPrecision(threshold=threshold)\n",
    "            self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "            self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "        def result(self):\n",
    "            p = self.precision.result()\n",
    "            r = self.recall.result()\n",
    "            return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.precision.reset_state()\n",
    "            self.recall.reset_state()\n",
    "\n",
    "    class DomainAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='domain_accuracy', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.accuracy = self.add_weight(name='acc', initializer='zeros')\n",
    "            self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "            pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "            matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "            self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "            self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.accuracy.assign(0.)\n",
    "            self.count.assign(0.)\n",
    "\n",
    "    class Perplexity(metrics.Metric):\n",
    "        def __init__(self, name='perplexity', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "            self.total_loss = self.add_weight(name='total_loss', initializer='zeros')\n",
    "            self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "            loss = self.cross_entropy(y_true, y_pred)\n",
    "            masked_loss = loss * mask\n",
    "            sum_loss = tf.reduce_sum(masked_loss)\n",
    "            num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "            self.total_loss.assign_add(sum_loss)\n",
    "            self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "        def result(self):\n",
    "            avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "            return tf.exp(avg_loss)\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_loss.assign(0.)\n",
    "            self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "    class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "        def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "            super().__init__(name=name)\n",
    "            self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "            self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "            y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "            y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "            y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "            cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "            non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "            non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "            masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "            num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "            avg_cosine_sim_per_sample = tf.where(\n",
    "                num_non_zero_tokens > 0,\n",
    "                tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "                tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "            )\n",
    "            self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "            self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_cosine_sim.assign(0.)\n",
    "            self.num_samples.assign(0.)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "        flops = 0\n",
    "        functional_model = model.build_graph()\n",
    "\n",
    "        def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "            layer_flops_count = 0\n",
    "\n",
    "            if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "                return 0\n",
    "\n",
    "            if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "                for sub_layer in layer_instance.layers:\n",
    "                    layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "                return layer_flops_count\n",
    "\n",
    "            if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "                return 0\n",
    "\n",
    "            if isinstance(layer_instance, layers.Dense):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                output_dim = layer_instance.output_shape[-1]\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.LSTM):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                hidden_dim = layer_instance.units\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "                layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.Embedding):\n",
    "                pass\n",
    "\n",
    "            elif isinstance(layer_instance, MoELayer):\n",
    "                moe_input_dim = layer_instance.input_dim\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "                layer_flops_count += gate_flops\n",
    "                for expert in layer_instance.experts:\n",
    "                    layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "                config = layer_instance.get_config()\n",
    "                num_heads = config['num_heads']\n",
    "                key_dim = config['key_dim']\n",
    "                d_model = num_heads * key_dim\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "                projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "                attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "                context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "                output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "                layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "            elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "                inner_layer = layer_instance.layer\n",
    "                td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "                layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "                ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "            return layer_flops_count\n",
    "\n",
    "        for layer in functional_model.layers:\n",
    "            flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "        return flops / 1e9\n",
    "\n",
    "    def get_gpu_stats():\n",
    "        if nvidia_smi is None:\n",
    "            return 0, 0\n",
    "        try:\n",
    "            nvidia_smi.nvmlInit()\n",
    "            handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "            gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "            mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "            gpu_memory = mem_info.used / (1024 ** 3)\n",
    "            nvidia_smi.nvmlShutdown()\n",
    "            return gpu_load, gpu_memory\n",
    "        except Exception:\n",
    "            return 0, 0\n",
    "\n",
    "    class ResourceMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, validation_data):\n",
    "            super().__init__()\n",
    "            self.resource_stats = []\n",
    "            self.expert_load_history = []\n",
    "            self.start_time = None\n",
    "            self.epoch_start_time = None\n",
    "            self.flops = None\n",
    "            self.validation_data = validation_data\n",
    "            self.nlu_expert_usage_counts = None\n",
    "            self.nlg_expert_usage_counts = None\n",
    "            self.nlu_gate_weights_history = []\n",
    "            self.nlg_gate_weights_history = []\n",
    "\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = time.time()\n",
    "            try:\n",
    "                self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "                self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "                self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "            except Exception:\n",
    "                self.flops = 0\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.epoch_start_time = time.time()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.resource_stats.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_time': epoch_time,\n",
    "                'cpu_usage': cpu_usage,\n",
    "                'memory_used': memory.used / (1024 ** 3),\n",
    "                'memory_total': memory.total / (1024 ** 3),\n",
    "                'gpu_load': gpu_load,\n",
    "                'gpu_memory': gpu_memory,\n",
    "                'loss': logs.get('loss', 0),\n",
    "                'val_loss': logs.get('val_loss', 0),\n",
    "                'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "                'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "                'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "                'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "                'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "                'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "                'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "                'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "                'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "                'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "                'perplexity': logs.get('response_output_perplexity', 0),\n",
    "                'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "                'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "                'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "            })\n",
    "            if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "                self.expert_load_history.append({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                    'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "                })\n",
    "\n",
    "            sample_size = 100\n",
    "            for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "                inputs, _ = batch\n",
    "                outputs = self.model(inputs, training=False)\n",
    "                nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "                reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "                self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "                nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "                reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "                self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            self.total_time = time.time() - self.start_time\n",
    "\n",
    "            if len(self.nlu_gate_weights_history) > 0:\n",
    "                all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "                nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "                for expert_id in nlu_top_indices:\n",
    "                    if expert_id < self.model.moe_nlu.total_experts:\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "            if len(self.nlg_gate_weights_history) > 0:\n",
    "                all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "                nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "                for expert_id in nlg_top_indices:\n",
    "                    if expert_id < self.model.moe_nlg.total_experts:\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        def visualize_expert_load_distribution(self):\n",
    "            total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "            total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "            nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "            nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "            nlu_stability = np.var(nlu_percentages)\n",
    "            nlg_stability = np.var(nlg_percentages)\n",
    "            return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "    class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super().__init__()\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "    model = MoEModel(moe_params)\n",
    "    embedding_layer = model.embedding\n",
    "\n",
    "    def create_validation_split(dataset, embedding_layer, validation_split=0.2, batch_size=moe_params[\"batch_size\"]):\n",
    "        element_spec = dataset.element_spec\n",
    "        data_list = [(features, targets) for features, targets in dataset.unbatch().as_numpy_iterator()]\n",
    "        if not data_list:\n",
    "            raise ValueError(\"Dataset is empty.\")\n",
    "        if len(data_list) < 2:\n",
    "            raise ValueError(\"Dataset too small to split.\")\n",
    "        train_data, val_data = train_test_split(data_list, test_size=validation_split, shuffle=True)\n",
    "\n",
    "        def create_dataset_from_list(data):\n",
    "            if not data:\n",
    "                raise ValueError(\"Empty data list provided.\")\n",
    "            features = {\n",
    "                'user_utterance_tokens': np.array([d[0]['user_utterance_tokens'] for d in data], dtype=np.int32),\n",
    "                'prev_system_response_tokens': np.array([d[0]['prev_system_response_tokens'] for d in data], dtype=np.int32),\n",
    "                'decoder_input_tokens': np.array([d[0]['decoder_input_tokens'] for d in data], dtype=np.int32),\n",
    "                'domain_onehot_input': np.array([d[0]['domain_onehot_input'] for d in data], dtype=np.float32),\n",
    "                'turn_id_embedding': np.array([d[0]['turn_id_embedding'] for d in data], dtype=np.float32),\n",
    "                'ontology_multihot_input': np.array([d[0]['ontology_multihot_input'] for d in data], dtype=np.float32),\n",
    "            }\n",
    "            response_output = np.array([d[1]['response_output'] for d in data], dtype=np.int32)\n",
    "            response_embeddings = embedding_layer(response_output).numpy()\n",
    "            targets = {\n",
    "                'domain_output': np.array([d[1]['domain_output'] for d in data], dtype=np.int32),\n",
    "                'intent_output': np.array([d[1]['intent_output'] for d in data], dtype=np.float32),\n",
    "                'response_output': response_output,\n",
    "                'response_embeddings': response_embeddings\n",
    "            }\n",
    "            return tf.data.Dataset.from_tensor_slices((features, targets))\n",
    "\n",
    "        train_dataset = create_dataset_from_list(train_data).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        val_dataset = create_dataset_from_list(val_data).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    train_tf_dataset, val_tf_dataset = create_validation_split(train_tf_dataset, embedding_layer)\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': losses.BinaryCrossentropy(),\n",
    "        'domain_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={'intent_output': 0.5, 'domain_output': 0.5, 'response_output': 1.0, 'response_embeddings': 1.0}\n",
    "    )\n",
    "\n",
    "    sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "    model(sample_input[0])\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=8, mode='min', restore_best_weights=True)\n",
    "    resource_monitor = ResourceMonitor(val_tf_dataset)\n",
    "    class TQDMProgressBar(callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.progress_bar = tqdm(total=len(train_tf_dataset), desc=f'Epoch {epoch + 1}')\n",
    "\n",
    "        def on_batch_end(self, batch, logs=None):\n",
    "            self.progress_bar.update(1)\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            self.progress_bar.close()\n",
    "\n",
    "    tqdm_callback = TQDMProgressBar()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_tf_dataset,\n",
    "        epochs=100,\n",
    "        validation_data=val_tf_dataset,\n",
    "        callbacks=[early_stopping, resource_monitor, tqdm_callback],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    expert_stats = resource_monitor.visualize_expert_load_distribution()\n",
    "    stats = resource_monitor.resource_stats\n",
    "    avg_epoch_duration = np.mean([s['epoch_time'] for s in stats]) if stats else 0\n",
    "    total_training_time = resource_monitor.total_time\n",
    "    avg_cpu_usage = np.mean([s['cpu_usage'] for s in stats]) if stats else 0\n",
    "    avg_memory = np.mean([s['memory_used'] for s in stats]) if stats else 0\n",
    "    avg_gpu_load = np.mean([s['gpu_load'] for s in stats]) if stats else 0\n",
    "    avg_gpu_memory = np.mean([s['gpu_memory'] for s in stats]) if stats else 0\n",
    "    avg_flops = resource_monitor.flops if resource_monitor.flops else 0\n",
    "    nlu_counts = resource_monitor.nlu_expert_usage_counts\n",
    "    nlg_counts = resource_monitor.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) or 1\n",
    "    total_nlg = np.sum(nlg_counts) or 1\n",
    "    expert_nlu_percentages = [(nlu_counts[i] / total_nlu) * 100 for i in range(8)]\n",
    "    expert_nlg_percentages = [(nlg_counts[i] / total_nlg) * 100 for i in range(8)]\n",
    "    val_intent_accuracy = np.mean([s['val_intent_accuracy'] for s in stats if s['val_intent_accuracy'] > 0]) if stats else 0\n",
    "    val_entity_accuracy = np.mean([s['val_domain_accuracy'] for s in stats if s['val_domain_accuracy'] > 0]) if stats else 0\n",
    "    val_perplexity = np.mean([s['val_perplexity'] for s in stats if s['val_perplexity'] > 0]) if stats else 0\n",
    "    val_cosine_similarity = np.mean([s['val_cosine_similarity'] for s in stats if s['val_cosine_similarity'] > 0]) if stats and any(s['val_cosine_similarity'] > 0 for s in stats) else 0\n",
    "    \n",
    "    results_table['Average Validation Response Cosine Similarity'][f'Iteration {iteration + 1}'] = val_cosine_similarity\n",
    "    results_table['Training Speed (epochs per second)'][f'Iteration {iteration + 1}'] = 1 / avg_epoch_duration if avg_epoch_duration > 0 else 0\n",
    "    results_table['Training Time (second/epoch)'][f'Iteration {iteration + 1}'] = avg_epoch_duration\n",
    "    results_table['Total Training Time (second/iteration)'][f'Iteration {iteration + 1}'] = total_training_time\n",
    "    results_table['Computational Resource Usage'][f'Iteration {iteration + 1}'] = avg_cpu_usage + avg_gpu_load\n",
    "    results_table['Average CPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_cpu_usage\n",
    "    results_table['Average GPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_gpu_load\n",
    "    results_table['Average Memory (GB)'][f'Iteration {iteration + 1}'] = avg_memory\n",
    "    results_table['Average GPU Memory (GB)'][f'Iteration {iteration + 1}'] = avg_gpu_memory\n",
    "    results_table['Average FLOPS Estimate (GFLOPS)'][f'Iteration {iteration + 1}'] = avg_flops\n",
    "    results_table['Expert NLU Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlu_stability']\n",
    "    results_table['Expert NLG Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlg_stability']\n",
    "    for i in range(8):\n",
    "        if i < len(expert_stats['nlu_percentages']):\n",
    "            results_table[f'Average NLU Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlu_percentages'][i]\n",
    "        if i < len(expert_stats['nlg_percentages']):\n",
    "            results_table[f'Average NLG Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlg_percentages'][i]\n",
    "    results_table['Average Validation Entity Accuracy'][f'Iteration {iteration + 1}'] = val_entity_accuracy\n",
    "    results_table['Average Intent Accuracy'][f'Iteration {iteration + 1}'] = val_intent_accuracy\n",
    "    results_table['Average Validation Perplexity'][f'Iteration {iteration + 1}'] = val_perplexity\n",
    "    results_table['Average Validation Response Cosine Similarity'][f'Iteration {iteration + 1}'] = val_cosine_similarity\n",
    "    val_loss = min([s['val_loss'] for s in stats]) if stats else float('inf')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        try:\n",
    "            model.save(best_model_path, save_format='tf', include_optimizer=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to save best model to {best_model_path}: {str(e)}\")\n",
    "\n",
    "for key in results_table:\n",
    "    results_table[key]['Average'] = np.mean([results_table[key][f'Iteration {i+1}'] for i in range(10)])\n",
    "\n",
    "final_result = pd.DataFrame(results_table)\n",
    "final_result = final_result.T\n",
    "final_result.to_excel('training_Deepseekbaseline_result.xlsx', index=True)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE baseline evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 13:11:25.337236: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 13:11:25.337365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 13:11:25.372273: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 13:11:25.481770: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 13:11:26.690291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 13:11:29.952269: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:11:30.151026: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:11:30.151340: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:11:30.153439: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:11:30.153702: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:11:30.153784: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:11:30.224724: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:11:30.224905: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:11:30.225011: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:11:30.225524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n",
      "Loaded vocabulary from preprocessor_models/preprocessor_params.json\n",
      "\n",
      "Loading model from: best_deepseekbaseline_moe_model\n",
      "\n",
      "Model loaded and compiled successfully!\n",
      "Model: \"mo_e_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  784896    \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     multiple                  8576      \n",
      "                                                                 \n",
      " layer_normalization (Layer  multiple                  256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_layer   multiple                  264576    \n",
      " (TransformerDecoderLayer)                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " moe_nlu (MoELayer)          multiple                  245318    \n",
      "                                                                 \n",
      " moe_nlg (MoELayer)          multiple                  87745     \n",
      "                                                                 \n",
      " dense_36 (Dense)            multiple                  14446     \n",
      "                                                                 \n",
      " dense_37 (Dense)            multiple                  3262      \n",
      "                                                                 \n",
      " time_distributed (TimeDist  multiple                  1024044   \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " multi_head_attention_2 (Mu  multiple                  66048     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2499167 (9.53 MB)\n",
      "Trainable params: 2499167 (9.53 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting Iteration 1\n",
      "\n",
      "Evaluating model on test set...\n",
      "1397/1397 [==============================] - 23s 12ms/step - loss: 3.2242 - domain_output_loss: 0.0196 - intent_output_loss: 0.0666 - response_embeddings_loss: 0.7965 - response_output_loss: 2.1813 - domain_output_domain_accuracy: 0.9911 - intent_output_intent_accuracy: 0.4936 - intent_output_intent_precision: 1.0000 - intent_output_intent_recall: 0.1173 - intent_output_intent_f1: 0.2099 - response_embeddings_response_embedding_cosine_similarity: 0.2035 - response_output_perplexity: 41.9524 - moe_nlu_load_balancing_loss: 0.0029 - moe_nlg_load_balancing_loss: 0.1914\n",
      "\n",
      "Training results saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evaluation Time (seconds)</th>\n",
       "      <td>25.593836</td>\n",
       "      <td>25.593836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction Speed (ms/token)</th>\n",
       "      <td>3.054597</td>\n",
       "      <td>3.054597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>25.551396</td>\n",
       "      <td>25.551396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>1.942019</td>\n",
       "      <td>1.942019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>6.975417</td>\n",
       "      <td>6.975417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.504456</td>\n",
       "      <td>14.504456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPs Estimate (GFLOPs)</th>\n",
       "      <td>2.377917</td>\n",
       "      <td>2.377917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 2 (percent)</th>\n",
       "      <td>6.818182</td>\n",
       "      <td>6.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 3 (percent)</th>\n",
       "      <td>11.864710</td>\n",
       "      <td>11.864710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 4 (percent)</th>\n",
       "      <td>6.030780</td>\n",
       "      <td>6.030780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 5 (percent)</th>\n",
       "      <td>16.159628</td>\n",
       "      <td>16.159628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 6 (percent)</th>\n",
       "      <td>24.606299</td>\n",
       "      <td>24.606299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 7 (percent)</th>\n",
       "      <td>33.965641</td>\n",
       "      <td>33.965641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 8 (percent)</th>\n",
       "      <td>0.554760</td>\n",
       "      <td>0.554760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 2 (percent)</th>\n",
       "      <td>17.015940</td>\n",
       "      <td>17.015940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 3 (percent)</th>\n",
       "      <td>5.658447</td>\n",
       "      <td>5.658447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 4 (percent)</th>\n",
       "      <td>9.699623</td>\n",
       "      <td>9.699623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 5 (percent)</th>\n",
       "      <td>13.887435</td>\n",
       "      <td>13.887435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 6 (percent)</th>\n",
       "      <td>42.408840</td>\n",
       "      <td>42.408840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 7 (percent)</th>\n",
       "      <td>5.176337</td>\n",
       "      <td>5.176337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 8 (percent)</th>\n",
       "      <td>6.153378</td>\n",
       "      <td>6.153378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Micro)</th>\n",
       "      <td>0.209894</td>\n",
       "      <td>0.209894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Macro)</th>\n",
       "      <td>0.125643</td>\n",
       "      <td>0.125643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Weighted)</th>\n",
       "      <td>0.173698</td>\n",
       "      <td>0.173698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent Accuracy</th>\n",
       "      <td>0.493558</td>\n",
       "      <td>0.493558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain Accuracy</th>\n",
       "      <td>0.991052</td>\n",
       "      <td>0.991052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain F1-score (Macro)</th>\n",
       "      <td>0.925126</td>\n",
       "      <td>0.925126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU Score</th>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.000916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1 F1</th>\n",
       "      <td>0.166440</td>\n",
       "      <td>0.166440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-2 F1</th>\n",
       "      <td>0.008979</td>\n",
       "      <td>0.008979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-L F1</th>\n",
       "      <td>0.128524</td>\n",
       "      <td>0.128524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METEOR Score</th>\n",
       "      <td>0.120057</td>\n",
       "      <td>0.120057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>41.952354</td>\n",
       "      <td>41.952354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert Load Stability (Variance)</th>\n",
       "      <td>124.275662</td>\n",
       "      <td>124.275662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert Load Stability (Variance)</th>\n",
       "      <td>152.709000</td>\n",
       "      <td>152.709000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Iteration 1     Average\n",
       "Evaluation Time (seconds)               25.593836   25.593836\n",
       "Prediction Speed (ms/token)              3.054597    3.054597\n",
       "Average CPU Usage (percent)             25.551396   25.551396\n",
       "Average GPU Usage (percent)              1.942019    1.942019\n",
       "Average Memory (GB)                      6.975417    6.975417\n",
       "Average GPU Memory (GB)                 14.504456   14.504456\n",
       "Average FLOPs Estimate (GFLOPs)          2.377917    2.377917\n",
       "NLU Expert 1 (percent)                   0.000000    0.000000\n",
       "NLU Expert 2 (percent)                   6.818182    6.818182\n",
       "NLU Expert 3 (percent)                  11.864710   11.864710\n",
       "NLU Expert 4 (percent)                   6.030780    6.030780\n",
       "NLU Expert 5 (percent)                  16.159628   16.159628\n",
       "NLU Expert 6 (percent)                  24.606299   24.606299\n",
       "NLU Expert 7 (percent)                  33.965641   33.965641\n",
       "NLU Expert 8 (percent)                   0.554760    0.554760\n",
       "NLG Expert 1 (percent)                   0.000000    0.000000\n",
       "NLG Expert 2 (percent)                  17.015940   17.015940\n",
       "NLG Expert 3 (percent)                   5.658447    5.658447\n",
       "NLG Expert 4 (percent)                   9.699623    9.699623\n",
       "NLG Expert 5 (percent)                  13.887435   13.887435\n",
       "NLG Expert 6 (percent)                  42.408840   42.408840\n",
       "NLG Expert 7 (percent)                   5.176337    5.176337\n",
       "NLG Expert 8 (percent)                   6.153378    6.153378\n",
       "Intent F1-score (Micro)                  0.209894    0.209894\n",
       "Intent F1-score (Macro)                  0.125643    0.125643\n",
       "Intent F1-score (Weighted)               0.173698    0.173698\n",
       "Intent Accuracy                          0.493558    0.493558\n",
       "Domain Accuracy                          0.991052    0.991052\n",
       "Domain F1-score (Macro)                  0.925126    0.925126\n",
       "BLEU Score                               0.000916    0.000916\n",
       "ROUGE-1 F1                               0.166440    0.166440\n",
       "ROUGE-2 F1                               0.008979    0.008979\n",
       "ROUGE-L F1                               0.128524    0.128524\n",
       "METEOR Score                             0.120057    0.120057\n",
       "Perplexity                              41.952354   41.952354\n",
       "NLU Expert Load Stability (Variance)   124.275662  124.275662\n",
       "NLG Expert Load Stability (Variance)   152.709000  152.709000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tf_datasets_from_disk(load_path):\n",
    "    print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "    try:\n",
    "        with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "            loaded_moe_params = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "    element_spec = (\n",
    "        {\n",
    "            'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "        },\n",
    "        {\n",
    "            'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "            'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "        test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "    raw_data = {}\n",
    "    for dataset_name in ['train', 'test']:\n",
    "        path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                raw_data[dataset_name] = pickle.load(f)\n",
    "            print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Raw data file {path} not found.\")\n",
    "            raw_data[dataset_name] = []\n",
    "\n",
    "    return {\n",
    "        \"train_dataset\": train_tf_dataset,\n",
    "        \"test_dataset\": test_tf_dataset,\n",
    "        \"moe_params\": loaded_moe_params,\n",
    "        \"raw_data\": raw_data\n",
    "    }\n",
    "\n",
    "tf_dataset_save_path = \"tf_datasets\"\n",
    "loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "test_tf_dataset = loaded_data[\"test_dataset\"]\n",
    "moe_params = loaded_data[\"moe_params\"]\n",
    "raw_data = loaded_data[\"raw_data\"]\n",
    "moe_params[\"num_experts\"] = 4\n",
    "\n",
    "class MoELayer(layers.Layer):\n",
    "    def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, top_k=2, name=None):\n",
    "        super(MoELayer, self).__init__(name=name)\n",
    "        self.m = m\n",
    "        self.k_s = k_s\n",
    "        self.total_experts = num_experts * self.m\n",
    "        self.routed_experts = self.total_experts - self.k_s\n",
    "        self.top_k = top_k\n",
    "        self.top_mk = self.m * self.top_k\n",
    "        self.top_mk = min(self.top_mk, self.routed_experts)\n",
    "        self.alpha = alpha\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = None\n",
    "        self.adjusted_expert_dim = expert_dim // self.m\n",
    "        self.shared_experts = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "            ]) for i in range(self.k_s)\n",
    "        ]\n",
    "        self.routed_experts_list = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "            ]) for i in range(self.k_s, self.total_experts)\n",
    "        ]\n",
    "        self.experts = self.shared_experts + self.routed_experts_list\n",
    "        self.gate = layers.Dense(self.routed_experts, activation='softmax')\n",
    "        self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_rank = inputs.shape.rank\n",
    "        if input_rank == 3:\n",
    "            batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "            flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "            is_3d = True\n",
    "        else:\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            seq_length = 1\n",
    "            flat_inputs = inputs\n",
    "            is_3d = False\n",
    "\n",
    "        flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "        shared_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "        \n",
    "        for i in range(self.k_s):\n",
    "            shared_output += self.shared_experts[i](flat_inputs)\n",
    "\n",
    "        gate_weights = self.gate(flat_inputs)\n",
    "        top_mk_values, top_mk_indices = tf.nn.top_k(gate_weights, k=self.top_mk, sorted=True)\n",
    "        top_mk_indices = top_mk_indices + self.k_s\n",
    "        top_mk_weights = top_mk_values / (tf.reduce_sum(top_mk_values, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "        routed_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "        \n",
    "        for k in range(self.top_mk):\n",
    "            kth_weights = top_mk_weights[:, k]\n",
    "            kth_indices = top_mk_indices[:, k]\n",
    "            mask = tf.one_hot(kth_indices, depth=self.total_experts, dtype=tf.float32)\n",
    "            expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "            kth_expert_output = tf.reduce_sum(expert_outputs * tf.expand_dims(mask, -1), axis=1)\n",
    "            weighted_kth_output = kth_expert_output * tf.expand_dims(kth_weights, -1)\n",
    "            routed_output += weighted_kth_output\n",
    "        output_flat = shared_output + routed_output + flat_inputs\n",
    "\n",
    "        if is_3d:\n",
    "            output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "            if self.seq_length is not None:\n",
    "                output.set_shape([None, self.seq_length, self.input_dim])\n",
    "            gate_weights_reshaped = tf.reshape(gate_weights, [batch_size, seq_length, self.routed_experts])\n",
    "            if self.seq_length is not None:\n",
    "                gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "        else:\n",
    "            output = output_flat\n",
    "            gate_weights_reshaped = gate_weights\n",
    "\n",
    "        f_i = tf.reduce_mean(tf.reduce_sum(tf.one_hot(tf.argmax(gate_weights, axis=-1), depth=self.routed_experts), axis=0), axis=0)\n",
    "        P_i = tf.reduce_mean(gate_weights, axis=0)\n",
    "        load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "        self.add_loss(load_balancing_loss)\n",
    "        self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "\n",
    "        return output, gate_weights_reshaped\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_experts': self.total_experts // self.m,\n",
    "            'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "            'input_dim': self.input_dim,\n",
    "            'm': self.m,\n",
    "            'k_s': self.k_s,\n",
    "            'alpha': self.alpha,\n",
    "            'top_k': self.top_k,\n",
    "            'name': self.name\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "        if look_ahead_mask is not None:\n",
    "            look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "            look_ahead_mask = 1.0 - look_ahead_mask\n",
    "\n",
    "        attn1_output = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)\n",
    "        attn1 = self.dropout1(attn1_output, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2_output = self.mha2(query=out1, value=enc_output, key=enc_output, attention_mask=None, training=training)\n",
    "        attn2 = self.dropout2(attn2_output, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        mha1_config = self.mha1.get_config()\n",
    "        config.update({\n",
    "            'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "            'num_heads': mha1_config['num_heads'],\n",
    "            'dff': self.ffn.layers[0].units,\n",
    "            'rate': self.dropout1.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MoEModel(models.Model):\n",
    "    def __init__(self, moe_params):\n",
    "        super(MoEModel, self).__init__()\n",
    "        self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "        self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "        self.num_domains = moe_params[\"num_domains\"]\n",
    "        self.num_intents = moe_params[\"num_intents\"]\n",
    "        self.vocab_size = moe_params[\"vocab_size\"]\n",
    "        self.num_experts = moe_params[\"num_experts\"]\n",
    "        self.expert_dim = moe_params[\"expert_dim\"]\n",
    "        self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "        self.num_heads = 4\n",
    "        self.dff = self.embedding_dim * 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                             self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "        self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "        self.embedding = layers.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05)\n",
    "        )\n",
    "        self.embedding.build((None,))\n",
    "        with tf.init_scope():\n",
    "            embedding_weights = self.embedding.get_weights()\n",
    "            if embedding_weights and len(embedding_weights) > 0:\n",
    "                embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                self.embedding.set_weights(embedding_weights)\n",
    "\n",
    "        self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "        self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "        self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "        self.moe_nlu = MoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlu_input_dim, m=2, k_s=1, alpha=0.01, top_k=2, name='moe_nlu')\n",
    "        self.moe_nlg = MoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlg_input_dim, m=2, k_s=1, alpha=0.01, top_k=2, name='moe_nlg')\n",
    "        self.intent_output = layers.Dense(self.num_intents, activation='sigmoid')\n",
    "        self.domain_output = layers.Dense(self.num_domains, activation='softmax')\n",
    "        self.response_output = layers.TimeDistributed(layers.Dense(self.vocab_size, activation='softmax'))\n",
    "        self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        user_utterance_tokens = inputs.get('user_utterance_tokens')\n",
    "        prev_system_response_tokens = inputs.get('prev_system_response_tokens')\n",
    "        decoder_input_tokens = inputs.get('decoder_input_tokens')\n",
    "        domain_onehot_input = inputs.get('domain_onehot_input')\n",
    "        turn_id_embedding = inputs.get('turn_id_embedding')\n",
    "        ontology_multihot_input = inputs.get('ontology_multihot_input')\n",
    "        if any(x is None for x in [user_utterance_tokens, prev_system_response_tokens, decoder_input_tokens,\n",
    "                                  domain_onehot_input, turn_id_embedding, ontology_multihot_input]):\n",
    "            raise ValueError(\"One or more required inputs are missing or None\")\n",
    "\n",
    "        pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "        user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "        prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "        decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "\n",
    "        user_enc_out = user_embedded\n",
    "        prev_system_enc_out = prev_system_embedded\n",
    "        look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "        dec_output = decoder_embedded\n",
    "        \n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            dec_output = layer(dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask)\n",
    "\n",
    "        decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "\n",
    "        user_attn_out = self.attn_layer(query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), \n",
    "                                      value=user_enc_out, key=user_enc_out, training=training)\n",
    "        prev_system_attn_out = self.attn_layer(query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), \n",
    "                                             value=prev_system_enc_out, key=prev_system_enc_out, training=training)\n",
    "        decoder_attn_out = self.attn_layer(query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), \n",
    "                                         value=decoder_output, key=decoder_output, training=training)\n",
    "\n",
    "        combined_features = layers.Concatenate()([\n",
    "            tf.squeeze(user_attn_out, 1),\n",
    "            tf.squeeze(prev_system_attn_out, 1),\n",
    "            tf.squeeze(decoder_attn_out, 1),\n",
    "            domain_onehot_input,\n",
    "            turn_id_embedding,\n",
    "            ontology_multihot_input\n",
    "        ])\n",
    "        combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "        nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "        nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "        intent_out = self.intent_output(nlu_out)\n",
    "        domain_out = self.domain_output(nlu_out)\n",
    "        intent_out_expanded = tf.expand_dims(intent_out, axis=1) \n",
    "        domain_out_expanded = tf.expand_dims(domain_out, axis=1) \n",
    "        intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1]) \n",
    "        domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "        nlg_input = layers.Concatenate(axis=-1)([\n",
    "            decoder_output, \n",
    "            intent_out_tiled,  \n",
    "            domain_out_tiled  \n",
    "        ])\n",
    "        nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "        nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        response_out = self.response_output(nlg_out)\n",
    "\n",
    "        return {\n",
    "            'intent_output': intent_out,\n",
    "            'domain_output': domain_out,\n",
    "            'response_output': response_out,\n",
    "            'response_embeddings': decoder_output,\n",
    "            'nlu_gate_weights': nlu_gate_weights,\n",
    "            'nlg_gate_weights': nlg_gate_weights\n",
    "        }\n",
    "\n",
    "    def build_graph(self):\n",
    "        inputs = {\n",
    "            'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32),\n",
    "        }\n",
    "        return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'moe_params': {\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'max_seq_length': self.max_seq_length,\n",
    "                'num_domains': self.num_domains,\n",
    "                'num_intents': self.num_intents,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'num_experts': self.num_experts,\n",
    "                'expert_dim': self.expert_dim,\n",
    "                'turn_id_dim': self.turn_id_dim\n",
    "            }\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class IntentAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros')\n",
    "        self.total_preds = self.add_weight(name='total_preds', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "        self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "        self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.correct_preds.assign(0.)\n",
    "        self.total_preds.assign(0.)\n",
    "\n",
    "class IntentPrecision(metrics.Metric):\n",
    "    def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        predicted_positives = tf.reduce_sum(y_pred)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.predicted_positives.assign(0.)\n",
    "\n",
    "class IntentRecall(metrics.Metric):\n",
    "    def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.actual_positives = self.add_weight(name='ap', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        actual_positives = tf.reduce_sum(y_true)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.actual_positives.assign(0.)\n",
    "\n",
    "class IntentF1(metrics.Metric):\n",
    "    def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = IntentPrecision(threshold=threshold)\n",
    "        self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()\n",
    "\n",
    "class DomainAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='domain_accuracy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.accuracy = self.add_weight(name='acc', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "        pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "        matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "        self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.accuracy.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "class Perplexity(metrics.Metric):\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "        self.total_loss = self.add_weight(name='total_loss', initializer='zeros')\n",
    "        self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "        loss = self.cross_entropy(y_true, y_pred)\n",
    "        masked_loss = loss * mask\n",
    "        sum_loss = tf.reduce_sum(masked_loss)\n",
    "        num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "        self.total_loss.assign_add(sum_loss)\n",
    "        self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "    def result(self):\n",
    "        avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "        return tf.exp(avg_loss)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_loss.assign(0.)\n",
    "        self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "    def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "        y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "        cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "        non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "        non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "        masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "        num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "        avg_cosine_sim_per_sample = tf.where(\n",
    "            num_non_zero_tokens > 0,\n",
    "            tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "            tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "        )\n",
    "        self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_cosine_sim.assign(0.)\n",
    "        self.num_samples.assign(0.)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "    y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "    y_true = y_true / (y_true_norm + epsilon)\n",
    "    y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "    cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    positive_loss = 1.0 - cosine_sim\n",
    "    mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "    masked_loss = positive_loss * mask\n",
    "    total_loss = tf.reduce_sum(masked_loss)\n",
    "    num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "    return total_loss / num_tokens\n",
    "\n",
    "def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "    flops = 0\n",
    "    functional_model = model.build_graph()\n",
    "\n",
    "    def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "        layer_flops_count = 0\n",
    "\n",
    "        if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "            return 0\n",
    "\n",
    "        if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "            for sub_layer in layer_instance.layers:\n",
    "                layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "            return layer_flops_count\n",
    "\n",
    "        if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "            return 0\n",
    "\n",
    "        if isinstance(layer_instance, layers.Dense):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            output_dim = layer_instance.output_shape[-1]\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "        elif isinstance(layer_instance, layers.LSTM):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            hidden_dim = layer_instance.units\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "            layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "        elif isinstance(layer_instance, layers.Embedding):\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer_instance, MoELayer):\n",
    "            moe_input_dim = layer_instance.input_dim\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "            layer_flops_count += gate_flops\n",
    "            for expert in layer_instance.experts:\n",
    "                layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "        elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "            config = layer_instance.get_config()\n",
    "            num_heads = config['num_heads']\n",
    "            key_dim = config['key_dim']\n",
    "            d_model = num_heads * key_dim\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "            projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "            attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "            context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "            output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "            layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "        elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "            inner_layer = layer_instance.layer\n",
    "            td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "            layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "        elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "            ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "        return layer_flops_count\n",
    "\n",
    "    for layer in functional_model.layers:\n",
    "        flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "    return flops / 1e9\n",
    "\n",
    "def get_gpu_stats():\n",
    "    if nvidia_smi is None:\n",
    "        return 0, 0\n",
    "    try:\n",
    "        nvidia_smi.nvmlInit()\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "        mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_memory = mem_info.used / (1024 ** 3)\n",
    "        nvidia_smi.nvmlShutdown()\n",
    "        return gpu_load, gpu_memory\n",
    "    except Exception:\n",
    "        return 0, 0\n",
    "\n",
    "class ResourceMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.resource_stats = []\n",
    "        self.expert_load_history = []\n",
    "        self.start_time = None\n",
    "        self.epoch_start_time = None\n",
    "        self.flops = None\n",
    "        self.validation_data = validation_data\n",
    "        self.nlu_expert_usage_counts = None\n",
    "        self.nlg_expert_usage_counts = None\n",
    "        self.nlu_gate_weights_history = []\n",
    "        self.nlg_gate_weights_history = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        try:\n",
    "            self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "            self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "            self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "        except Exception:\n",
    "            self.flops = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory()\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.resource_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'epoch_time': epoch_time,\n",
    "            'cpu_usage': cpu_usage,\n",
    "            'memory_used': memory.used / (1024 ** 3),\n",
    "            'memory_total': memory.total / (1024 ** 3),\n",
    "            'gpu_load': gpu_load,\n",
    "            'gpu_memory': gpu_memory,\n",
    "            'loss': logs.get('loss', 0),\n",
    "            'val_loss': logs.get('val_loss', 0),\n",
    "            'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "            'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "            'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "            'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "            'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "            'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "            'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "            'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "            'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "            'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "            'perplexity': logs.get('response_output_perplexity', 0),\n",
    "            'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "            'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "            'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "        })\n",
    "        if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "            self.expert_load_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "            })\n",
    "\n",
    "        sample_size = 100\n",
    "        for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "            inputs, _ = batch\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "            reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "            self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "            nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "            reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "            self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.total_time = time.time() - self.start_time\n",
    "\n",
    "        if len(self.nlu_gate_weights_history) > 0:\n",
    "            all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "            nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "            for expert_id in nlu_top_indices:\n",
    "                if expert_id < self.model.moe_nlu.total_experts:\n",
    "                    self.nlu_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        if len(self.nlg_gate_weights_history) > 0:\n",
    "            all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "            nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "            for expert_id in nlg_top_indices:\n",
    "                if expert_id < self.model.moe_nlg.total_experts:\n",
    "                    self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "    def visualize_expert_load_distribution(self):\n",
    "        total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "        total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "        nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "        nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "        nlu_stability = np.var(nlu_percentages)\n",
    "        nlg_stability = np.var(nlg_percentages)\n",
    "        return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "try:\n",
    "    possible_paths = [\n",
    "        os.path.join(\"preprocessor_models\", \"preprocessor_params.json\"),\n",
    "        os.path.join(\"tf_datasets\", \"preprocessor_params.json\"),\n",
    "        \"preprocessor_params.json\"\n",
    "    ]\n",
    "    \n",
    "    vocab_loaded = False\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                preprocessor_params = json.load(f)\n",
    "            word_to_id = {k: int(v) for k, v in preprocessor_params['word_to_id'].items()}\n",
    "            id_to_word = {int(k): v for k, v in preprocessor_params['id_to_word'].items()}\n",
    "            print(f\"Loaded vocabulary from {path}\")\n",
    "            vocab_loaded = True\n",
    "            break\n",
    "    \n",
    "    if not vocab_loaded:\n",
    "        raise FileNotFoundError(\"Vocabulary file not found in any location\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load vocabulary from preprocessor: {str(e)}\")\n",
    "    word_to_id = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    id_to_word = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
    "\n",
    "\n",
    "model_save_path = \"best_deepseekbaseline_moe_model\"\n",
    "print(f\"\\nLoading model from: {model_save_path}\")\n",
    "custom_objects = {\n",
    "    \"MoEModel\":MoEModel,\n",
    "    \"CustomLearningRateSchedule\": CustomLearningRateSchedule,\n",
    "    \"MoELayer\": MoELayer,\n",
    "    \"TransformerDecoderLayer\": TransformerDecoderLayer,\n",
    "    \"IntentAccuracy\": IntentAccuracy,\n",
    "    \"IntentPrecision\": IntentPrecision,\n",
    "    \"IntentRecall\": IntentRecall,\n",
    "    \"IntentF1\": IntentF1,\n",
    "    \"DomainAccuracy\": DomainAccuracy,\n",
    "    \"Perplexity\": Perplexity,\n",
    "    \"ResponseEmbeddingCosineSimilarity\": ResponseEmbeddingCosineSimilarity\n",
    "}\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model from {model_save_path}: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "def compile_model(model, moe_params):\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'domain_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss,\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={\n",
    "            'intent_output': 0.5,\n",
    "            'domain_output': 0.5,\n",
    "            'response_output': 1.0,\n",
    "            'response_embeddings': 1.0\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = compile_model(model, moe_params)\n",
    "print(\"\\nModel loaded and compiled successfully!\")\n",
    "sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "model(sample_input[0])\n",
    "model.summary()\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self, model, test_dataset, moe_params, raw_data, word_to_id, id_to_word):\n",
    "        self.model = model\n",
    "        self.test_dataset = test_dataset\n",
    "        self.moe_params = moe_params\n",
    "        self.raw_data = raw_data\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.nlu_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.nlg_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.resource_stats = {\n",
    "            'cpu_usage': [],\n",
    "            'memory_used': [],\n",
    "            'gpu_load': [],\n",
    "            'gpu_memory': [],\n",
    "            'batch_times': []\n",
    "        }\n",
    "        self.special_tokens = {'<pad>', '<sos>', '<eos>', '<unk>'}\n",
    "        self.nlu_top_k = model.get_layer('moe_nlu').get_config().get('top_k', 2)\n",
    "        self.nlg_top_k = model.get_layer('moe_nlg').get_config().get('top_k', 2)\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(\"\\nEvaluating model on test set...\")\n",
    "        \n",
    "        def add_response_embeddings(features, targets):\n",
    "            response_tokens = targets['response_output']\n",
    "            response_embeddings = self.model.embedding(response_tokens)\n",
    "            targets['response_embeddings'] = response_embeddings\n",
    "            return features, targets\n",
    "        \n",
    "        test_dataset_with_embeddings = self.test_dataset.map(add_response_embeddings, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        start_time = time.time()\n",
    "        test_results = self.model.evaluate(test_dataset_with_embeddings, verbose=1)\n",
    "        eval_time = time.time() - start_time\n",
    "        \n",
    "        metric_names = self.model.metrics_names\n",
    "        \n",
    "        perplexity_value = None\n",
    "        for name, value in zip(metric_names, test_results):\n",
    "            if name == 'response_output_perplexity':\n",
    "                perplexity_value = value\n",
    "                break\n",
    "        if perplexity_value is None:\n",
    "            print(\"Warning: Could not find perplexity in standard evaluation results.\")\n",
    "            perplexity_value = 0.0\n",
    "        \n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory().used / (1024 ** 3)\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "\n",
    "        total_tokens = 0\n",
    "        total_time = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        test_dataset_small = test_dataset_with_embeddings.unbatch().batch(moe_params[\"batch_size\"])\n",
    "        \n",
    "        all_true_intents = []\n",
    "        all_pred_intents = []\n",
    "        all_true_domains = []\n",
    "        all_pred_domains = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(test_dataset_small):\n",
    "            inputs, targets = batch\n",
    "            response_tokens = targets['response_output'].numpy()\n",
    "            non_padding_mask = response_tokens != 0\n",
    "            total_tokens += np.sum(non_padding_mask)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            total_time += batch_time\n",
    "            num_batches += tf.shape(inputs['user_utterance_tokens'])[0]\n",
    "            \n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            \n",
    "            self.resource_stats['cpu_usage'].append(cpu_usage)\n",
    "            self.resource_stats['memory_used'].append(memory.used / (1024 ** 3))\n",
    "            self.resource_stats['gpu_load'].append(gpu_load)\n",
    "            self.resource_stats['gpu_memory'].append(gpu_memory)\n",
    "            self.resource_stats['batch_times'].append(batch_time)\n",
    "            \n",
    "            nlu_gate_weights = outputs.get('nlu_gate_weights', tf.zeros([0, self.model.moe_nlu.routed_experts])).numpy()\n",
    "            nlu_top_indices = np.argsort(-nlu_gate_weights, axis=-1)[:, :self.nlu_top_k] + self.model.moe_nlu.k_s\n",
    "            for idx in range(self.nlu_top_k):\n",
    "                expert_ids = nlu_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlu_expert_usage_counts):\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "            \n",
    "            nlg_gate_weights = outputs.get('nlg_gate_weights', tf.zeros([0, self.model.moe_nlg.routed_experts])).numpy()\n",
    "            if len(nlg_gate_weights.shape) == 3:\n",
    "                nlg_gate_weights = nlg_gate_weights.reshape(-1, nlg_gate_weights.shape[-1])\n",
    "            nlg_top_indices = np.argsort(-nlg_gate_weights, axis=-1)[:, :self.nlg_top_k] + self.model.moe_nlg.k_s\n",
    "            for idx in range(self.nlg_top_k):\n",
    "                expert_ids = nlg_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlg_expert_usage_counts):\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "            \n",
    "            true_intents = targets.get('intent_output', np.zeros((moe_params[\"batch_size\"], self.model.num_intents))).numpy()\n",
    "            pred_intents = (outputs.get('intent_output', np.zeros_like(true_intents)).numpy() > 0.5).astype(np.int32)\n",
    "            all_true_intents.append(true_intents)\n",
    "            all_pred_intents.append(pred_intents)\n",
    "            \n",
    "            true_domains = targets.get('domain_output', np.zeros((moe_params[\"batch_size\"], 1))).numpy().flatten()\n",
    "            pred_domains = np.argmax(outputs.get('domain_output', np.zeros((moe_params[\"batch_size\"], self.model.num_domains))).numpy(), axis=-1)\n",
    "            all_true_domains.append(true_domains)\n",
    "            all_pred_domains.append(pred_domains)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                gc.collect()\n",
    "                tf.keras.backend.clear_session()\n",
    "        \n",
    "        avg_time_per_token = (total_time / total_tokens) * 1000 if total_tokens > 0 else 0\n",
    "        \n",
    "        avg_cpu = np.mean(self.resource_stats['cpu_usage']) if self.resource_stats['cpu_usage'] else 0\n",
    "        avg_memory = np.mean(self.resource_stats['memory_used']) if self.resource_stats['memory_used'] else 0\n",
    "        avg_gpu_load = np.mean(self.resource_stats['gpu_load']) if self.resource_stats['gpu_load'] else 0\n",
    "        avg_gpu_memory = np.mean(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        peak_gpu_memory = np.max(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        \n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "        \n",
    "        all_true_intents = np.vstack(all_true_intents)\n",
    "        all_pred_intents = np.vstack(all_pred_intents)\n",
    "        all_true_domains = np.concatenate(all_true_domains)\n",
    "        all_pred_domains = np.concatenate(all_pred_domains)\n",
    "        \n",
    "        intent_f1_score = f1_score(all_true_intents, all_pred_intents, average='micro')\n",
    "        intent_f1_score_macro = f1_score(all_true_intents, all_pred_intents, average='macro')\n",
    "        intent_f1_score_weighted = f1_score(all_true_intents, all_pred_intents, average='weighted')\n",
    "        intent_accuracy_score = accuracy_score(all_true_intents, all_pred_intents)\n",
    "        \n",
    "        domain_accuracy_score = accuracy_score(all_true_domains, all_pred_domains)\n",
    "        domain_f1_score_macro = f1_score(all_true_domains, all_pred_domains, average='macro')\n",
    "\n",
    "        hypotheses = []\n",
    "        references = []\n",
    "        meteor_scores = []\n",
    "        rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "        domain_metrics = defaultdict(lambda: {\n",
    "            'bleu': [],\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'meteor': [],\n",
    "            'count': 0\n",
    "        })\n",
    "        \n",
    "        if not self.raw_data.get('test'):\n",
    "            print(\"Warning: raw_data['test'] is empty. Skipping NLG metrics calculation.\")\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "            domain_metrics = {}\n",
    "        else:\n",
    "            for i, batch in enumerate(test_dataset_small):\n",
    "                if i >= len(self.raw_data.get('test', [])):\n",
    "                    print(f\"Warning: raw_data['test'] has fewer items ({len(self.raw_data.get('test', []))}) than test dataset. Stopping NLG metrics calculation.\")\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    inputs, targets = batch\n",
    "                    with tf.device('/CPU:0'):\n",
    "                        outputs = self.model(inputs, training=False)\n",
    "                    \n",
    "                    domain_id = targets['domain_output'].numpy()[0][0]\n",
    "                    domain_name = f\"domain_{domain_id}\"\n",
    "                    \n",
    "                    response_probs = outputs['response_output'].numpy()\n",
    "                    generated_tokens = np.argmax(response_probs, axis=-1)[0]\n",
    "                    true_tokens = targets['response_output'].numpy()[0]\n",
    "                    \n",
    "                    raw_item = self.raw_data['test'][i]\n",
    "                    ref_text = raw_item.get('raw_next_system_response', '')\n",
    "                    if not ref_text:\n",
    "                        continue\n",
    "                    \n",
    "                    gen_text = self.tokens_to_text(generated_tokens)\n",
    "                    ref_tokens = self.tokens_to_text(true_tokens)\n",
    "                    \n",
    "                    if not gen_text.strip() or not ref_text.strip():\n",
    "                        continue\n",
    "                    \n",
    "                    hypotheses.append(gen_text)\n",
    "                    references.append([ref_text])\n",
    "                    \n",
    "                    ref_tokens = word_tokenize(ref_text.lower())\n",
    "                    gen_tokens = word_tokenize(gen_text.lower())\n",
    "                    \n",
    "                    try:\n",
    "                        meteor = meteor_score([ref_tokens], gen_tokens)\n",
    "                        meteor_scores.append(meteor)\n",
    "                        domain_metrics[domain_name]['meteor'].append(meteor)\n",
    "                    except Exception as e:\n",
    "                        print(f\"METEOR calculation error for example {i}: {str(e)}\")\n",
    "                        meteor_scores.append(0)\n",
    "                        domain_metrics[domain_name]['meteor'].append(0)\n",
    "                    \n",
    "                    try:\n",
    "                        rouge_scores = rouge_scorer_instance.score(ref_text, gen_text)\n",
    "                        domain_metrics[domain_name]['rouge1'].append(rouge_scores['rouge1'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rouge2'].append(rouge_scores['rouge2'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rougeL'].append(rouge_scores['rougeL'].fmeasure)\n",
    "                    except Exception as e:\n",
    "                        print(f\"ROUGE calculation error for example {i}: {str(e)}\")\n",
    "                    \n",
    "                    domain_metrics[domain_name]['count'] += 1\n",
    "                    \n",
    "                    if i % 50 == 0:\n",
    "                        gc.collect()\n",
    "                        tf.keras.backend.clear_session()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing example {i}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if hypotheses and references:\n",
    "            hypotheses_tok = []\n",
    "            references_tok = []\n",
    "            \n",
    "            for hyp, ref_list in zip(hypotheses, references):\n",
    "                if not hyp or not ref_list[0]:\n",
    "                    continue\n",
    "                hyp_tokens = word_tokenize(hyp.lower())\n",
    "                ref_tokens = [word_tokenize(ref.lower()) for ref in ref_list]\n",
    "                hypotheses_tok.append(hyp_tokens)\n",
    "                references_tok.append(ref_tokens)\n",
    "            \n",
    "            chencherry = SmoothingFunction()\n",
    "            try:\n",
    "                bleu_score = corpus_bleu(references_tok, hypotheses_tok, smoothing_function=chencherry.method1)\n",
    "            except Exception as e:\n",
    "                print(f\"BLEU calculation error: {str(e)}\")\n",
    "                bleu_score = 0.0\n",
    "            \n",
    "            rouge_scores = []\n",
    "            for hyp, ref in zip(hypotheses, references):\n",
    "                if hyp and ref[0]:\n",
    "                    try:\n",
    "                        rouge_scores.append(rouge_scorer_instance.score(ref[0], hyp))\n",
    "                    except Exception as e:\n",
    "                        print(f\"ROUGE scoring error: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            avg_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_meteor = np.mean(meteor_scores) if meteor_scores else 0.0\n",
    "        else:\n",
    "            print(\"Warning: No valid hypotheses or references for NLG metrics. Setting to 0.0.\")\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "\n",
    "        return {\n",
    "            'intent_f1_micro': intent_f1_score,\n",
    "            'intent_f1_macro': intent_f1_score_macro,\n",
    "            'intent_f1_weighted': intent_f1_score_weighted,\n",
    "            'intent_accuracy': intent_accuracy_score,\n",
    "            'domain_accuracy': domain_accuracy_score,\n",
    "            'domain_f1_macro': domain_f1_score_macro,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge1': avg_rouge1,\n",
    "            'rouge2': avg_rouge2,\n",
    "            'rougeL': avg_rougeL,\n",
    "            'meteor': avg_meteor,\n",
    "            'perplexity': perplexity_value,\n",
    "            'pred_speed': avg_time_per_token,\n",
    "            'domain_metrics': domain_metrics,\n",
    "            'eval_time': eval_time\n",
    "        }\n",
    "    \n",
    "    def tokens_to_text(self, tokens):\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            if token == 0:\n",
    "                continue\n",
    "            word = self.id_to_word.get(token, '<unk>')\n",
    "            if word not in self.special_tokens:\n",
    "                words.append(word)\n",
    "        return \" \".join(words).strip()\n",
    "\n",
    "results_table = {\n",
    "    'Evaluation Time (seconds)': [],\n",
    "    'Prediction Speed (ms/token)': [],\n",
    "    'Average CPU Usage (percent)': [],\n",
    "    'Average GPU Usage (percent)': [],\n",
    "    'Average Memory (GB)': [],\n",
    "    'Average GPU Memory (GB)': [],\n",
    "    'Average FLOPs Estimate (GFLOPs)': [],\n",
    "    'NLU Expert 1 (percent)': [],\n",
    "    'NLU Expert 2 (percent)': [],\n",
    "    'NLU Expert 3 (percent)': [],\n",
    "    'NLU Expert 4 (percent)': [],\n",
    "    'NLU Expert 5 (percent)': [],\n",
    "    'NLU Expert 6 (percent)': [],\n",
    "    'NLU Expert 7 (percent)': [],\n",
    "    'NLU Expert 8 (percent)': [],\n",
    "    'NLG Expert 1 (percent)': [],\n",
    "    'NLG Expert 2 (percent)': [],\n",
    "    'NLG Expert 3 (percent)': [],\n",
    "    'NLG Expert 4 (percent)': [],\n",
    "    'NLG Expert 5 (percent)': [],\n",
    "    'NLG Expert 6 (percent)': [],\n",
    "    'NLG Expert 7 (percent)': [],\n",
    "    'NLG Expert 8 (percent)': [],\n",
    "    'Intent F1-score (Micro)': [],\n",
    "    'Intent F1-score (Macro)': [],\n",
    "    'Intent F1-score (Weighted)': [],\n",
    "    'Intent Accuracy': [],\n",
    "    'Domain Accuracy': [],\n",
    "    'Domain F1-score (Macro)': [],\n",
    "    'BLEU Score': [],\n",
    "    'ROUGE-1 F1': [],\n",
    "    'ROUGE-2 F1': [],\n",
    "    'ROUGE-L F1': [],\n",
    "    'METEOR Score': [],\n",
    "    'Perplexity': [],\n",
    "    'NLU Expert Load Stability (Variance)': [],\n",
    "    'NLG Expert Load Stability (Variance)': []\n",
    "}\n",
    "\n",
    "num_iterations = 5\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    print(f\"\\nStarting Iteration {iteration}\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "    model = compile_model(model, moe_params)\n",
    "    \n",
    "    evaluator = EvaluationMetrics(model, test_tf_dataset, moe_params, raw_data, word_to_id, id_to_word)\n",
    "\n",
    "    evaluation_results = evaluator.evaluate()\n",
    "    \n",
    "    results_table['Evaluation Time (seconds)'].append(evaluation_results['eval_time'])\n",
    "    results_table['Prediction Speed (ms/token)'].append(evaluation_results['pred_speed'])\n",
    "    results_table['Average CPU Usage (percent)'].append(np.mean(evaluator.resource_stats['cpu_usage']) if evaluator.resource_stats['cpu_usage'] else 0)\n",
    "    results_table['Average GPU Usage (percent)'].append(np.mean(evaluator.resource_stats['gpu_load']) if evaluator.resource_stats['gpu_load'] else 0)\n",
    "    results_table['Average Memory (GB)'].append(np.mean(evaluator.resource_stats['memory_used']) if evaluator.resource_stats['memory_used'] else 0)\n",
    "    results_table['Average GPU Memory (GB)'].append(np.mean(evaluator.resource_stats['gpu_memory']) if evaluator.resource_stats['gpu_memory'] else 0)\n",
    "    results_table['Average FLOPs Estimate (GFLOPs)'].append(calculate_flops(model))\n",
    "    nlu_counts = evaluator.nlu_expert_usage_counts\n",
    "    nlg_counts = evaluator.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) if np.sum(nlu_counts) > 0 else 1\n",
    "    total_nlg = np.sum(nlg_counts) if np.sum(nlg_counts) > 0 else 1\n",
    "    results_table['NLU Expert 1 (percent)'].append((nlu_counts[0] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 2 (percent)'].append((nlu_counts[1] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 3 (percent)'].append((nlu_counts[2] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 4 (percent)'].append((nlu_counts[3] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 5 (percent)'].append((nlu_counts[4] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 6 (percent)'].append((nlu_counts[5] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 7 (percent)'].append((nlu_counts[6] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 8 (percent)'].append((nlu_counts[7] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLG Expert 1 (percent)'].append((nlg_counts[0] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 2 (percent)'].append((nlg_counts[1] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 3 (percent)'].append((nlg_counts[2] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 4 (percent)'].append((nlg_counts[3] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 5 (percent)'].append((nlg_counts[4] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 6 (percent)'].append((nlg_counts[5] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 7 (percent)'].append((nlg_counts[6] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 8 (percent)'].append((nlg_counts[7] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['Intent F1-score (Micro)'].append(evaluation_results['intent_f1_micro'])\n",
    "    results_table['Intent F1-score (Macro)'].append(evaluation_results['intent_f1_macro'])\n",
    "    results_table['Intent F1-score (Weighted)'].append(evaluation_results['intent_f1_weighted'])\n",
    "    results_table['Intent Accuracy'].append(evaluation_results['intent_accuracy'])\n",
    "    results_table['Domain Accuracy'].append(evaluation_results['domain_accuracy'])\n",
    "    results_table['Domain F1-score (Macro)'].append(evaluation_results['domain_f1_macro'])\n",
    "    results_table['BLEU Score'].append(evaluation_results['bleu'])\n",
    "    results_table['ROUGE-1 F1'].append(evaluation_results['rouge1'])\n",
    "    results_table['ROUGE-2 F1'].append(evaluation_results['rouge2'])\n",
    "    results_table['ROUGE-L F1'].append(evaluation_results['rougeL'])\n",
    "    results_table['METEOR Score'].append(evaluation_results['meteor'])\n",
    "    results_table['Perplexity'].append(evaluation_results['perplexity'])\n",
    "    results_table['NLU Expert Load Stability (Variance)'].append(np.var(nlu_counts / total_nlu * 100) if total_nlu > 0 else 0)\n",
    "    results_table['NLG Expert Load Stability (Variance)'].append(np.var(nlg_counts / total_nlg * 100) if total_nlg > 0 else 0)\n",
    "\n",
    "for key in results_table:\n",
    "    if results_table[key]:\n",
    "        results_table[key].append(np.mean(results_table[key]))\n",
    "    else:\n",
    "        results_table[key].append(0) \n",
    "        \n",
    "print(\"\\nTraining results saved\")\n",
    "final_df = pd.DataFrame(results_table)\n",
    "final_df = final_df.T\n",
    "final_df.columns = [f'Iteration {i+1}' for i in range(num_iterations)] + ['Average']\n",
    "final_df.to_excel('prediction_deepseekbaseline_results.xlsx', index=False) \n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE Top-P experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training Iteration 1\n",
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 13:45:07.222166: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:45:07.591959: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:45:07.592158: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:45:07.593084: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:45:07.593232: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:45:07.593322: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:45:07.668027: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:45:07.668186: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:45:07.668291: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 13:45:07.668371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n",
      "Warning: Layer multi_head_attention still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer multi_head_attention_1 still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer nlg_projection still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer dense_2 still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/4428 [00:00<?, ?it/s]2025-07-04 13:45:34.515705: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f7a0011f8d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 13:45:34.515755: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2025-07-04 13:45:34.536714: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 13:45:34.582022: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751636734.671252    4748 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 1: 100%|██████████| 4428/4428 [02:39<00:00, 27.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deepseektopp_moe_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deepseektopp_moe_model/assets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Iteration 2</th>\n",
       "      <th>Iteration 3</th>\n",
       "      <th>Iteration 4</th>\n",
       "      <th>Iteration 5</th>\n",
       "      <th>Iteration 6</th>\n",
       "      <th>Iteration 7</th>\n",
       "      <th>Iteration 8</th>\n",
       "      <th>Iteration 9</th>\n",
       "      <th>Iteration 10</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Speed (epochs per second)</th>\n",
       "      <td>0.006431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training Time (second/epoch)</th>\n",
       "      <td>155.506772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.550677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Training Time (second/iteration)</th>\n",
       "      <td>161.162587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.116259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computational Resource Usage</th>\n",
       "      <td>45.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>27.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>7.522411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.752241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.531799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.453180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPS Estimate (GFLOPS)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLU Load Distribution Stability</th>\n",
       "      <td>1044.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLG Load Distribution Stability</th>\n",
       "      <td>552.731288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.273129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 3 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 4 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 5 (percent)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 6 (percent)</th>\n",
       "      <td>98.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 7 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 8 (percent)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 3 (percent)</th>\n",
       "      <td>6.731343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.673134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 4 (percent)</th>\n",
       "      <td>3.955224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 5 (percent)</th>\n",
       "      <td>2.283582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 6 (percent)</th>\n",
       "      <td>74.268657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.426866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 7 (percent)</th>\n",
       "      <td>4.328358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.432836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 8 (percent)</th>\n",
       "      <td>8.432836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.843284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Entity Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Intent Accuracy</th>\n",
       "      <td>0.893857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Perplexity</th>\n",
       "      <td>18.137508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.813751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Response Cosine Similarity</th>\n",
       "      <td>0.331646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Iteration 1  Iteration 2  \\\n",
       "Training Speed (epochs per second)                0.006431          0.0   \n",
       "Training Time (second/epoch)                    155.506772          0.0   \n",
       "Total Training Time (second/iteration)          161.162587          0.0   \n",
       "Computational Resource Usage                     45.400000          0.0   \n",
       "Average CPU Usage (percent)                      27.400000          0.0   \n",
       "Average GPU Usage (percent)                      18.000000          0.0   \n",
       "Average Memory (GB)                               7.522411          0.0   \n",
       "Average GPU Memory (GB)                          14.531799          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                   2.377871          0.0   \n",
       "Expert NLU Load Distribution Stability         1044.500000          0.0   \n",
       "Expert NLG Load Distribution Stability          552.731288          0.0   \n",
       "Average NLU Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 3 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 4 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 5 (percent)                    1.000000          0.0   \n",
       "Average NLU Expert 6 (percent)                   98.000000          0.0   \n",
       "Average NLU Expert 7 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 8 (percent)                    1.000000          0.0   \n",
       "Average NLG Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 3 (percent)                    6.731343          0.0   \n",
       "Average NLG Expert 4 (percent)                    3.955224          0.0   \n",
       "Average NLG Expert 5 (percent)                    2.283582          0.0   \n",
       "Average NLG Expert 6 (percent)                   74.268657          0.0   \n",
       "Average NLG Expert 7 (percent)                    4.328358          0.0   \n",
       "Average NLG Expert 8 (percent)                    8.432836          0.0   \n",
       "Average Validation Entity Accuracy                1.000000          0.0   \n",
       "Average Intent Accuracy                           0.893857          0.0   \n",
       "Average Validation Perplexity                    18.137508          0.0   \n",
       "Average Validation Response Cosine Similarity     0.331646          0.0   \n",
       "\n",
       "                                               Iteration 3  Iteration 4  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 5  Iteration 6  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 7  Iteration 8  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 9  Iteration 10  \\\n",
       "Training Speed (epochs per second)                     0.0           0.0   \n",
       "Training Time (second/epoch)                           0.0           0.0   \n",
       "Total Training Time (second/iteration)                 0.0           0.0   \n",
       "Computational Resource Usage                           0.0           0.0   \n",
       "Average CPU Usage (percent)                            0.0           0.0   \n",
       "Average GPU Usage (percent)                            0.0           0.0   \n",
       "Average Memory (GB)                                    0.0           0.0   \n",
       "Average GPU Memory (GB)                                0.0           0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0           0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0           0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0           0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0           0.0   \n",
       "Average Validation Entity Accuracy                     0.0           0.0   \n",
       "Average Intent Accuracy                                0.0           0.0   \n",
       "Average Validation Perplexity                          0.0           0.0   \n",
       "Average Validation Response Cosine Similarity          0.0           0.0   \n",
       "\n",
       "                                                  Average  \n",
       "Training Speed (epochs per second)               0.000643  \n",
       "Training Time (second/epoch)                    15.550677  \n",
       "Total Training Time (second/iteration)          16.116259  \n",
       "Computational Resource Usage                     4.540000  \n",
       "Average CPU Usage (percent)                      2.740000  \n",
       "Average GPU Usage (percent)                      1.800000  \n",
       "Average Memory (GB)                              0.752241  \n",
       "Average GPU Memory (GB)                          1.453180  \n",
       "Average FLOPS Estimate (GFLOPS)                  0.237787  \n",
       "Expert NLU Load Distribution Stability         104.450000  \n",
       "Expert NLG Load Distribution Stability          55.273129  \n",
       "Average NLU Expert 1 (percent)                   0.000000  \n",
       "Average NLU Expert 2 (percent)                   0.000000  \n",
       "Average NLU Expert 3 (percent)                   0.000000  \n",
       "Average NLU Expert 4 (percent)                   0.000000  \n",
       "Average NLU Expert 5 (percent)                   0.100000  \n",
       "Average NLU Expert 6 (percent)                   9.800000  \n",
       "Average NLU Expert 7 (percent)                   0.000000  \n",
       "Average NLU Expert 8 (percent)                   0.100000  \n",
       "Average NLG Expert 1 (percent)                   0.000000  \n",
       "Average NLG Expert 2 (percent)                   0.000000  \n",
       "Average NLG Expert 3 (percent)                   0.673134  \n",
       "Average NLG Expert 4 (percent)                   0.395522  \n",
       "Average NLG Expert 5 (percent)                   0.228358  \n",
       "Average NLG Expert 6 (percent)                   7.426866  \n",
       "Average NLG Expert 7 (percent)                   0.432836  \n",
       "Average NLG Expert 8 (percent)                   0.843284  \n",
       "Average Validation Entity Accuracy               0.100000  \n",
       "Average Intent Accuracy                          0.089386  \n",
       "Average Validation Perplexity                    1.813751  \n",
       "Average Validation Response Cosine Similarity    0.033165  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reset_kernel():\n",
    "    tf.keras.backend.clear_session()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "results_table = {\n",
    "    'Training Speed (epochs per second)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Training Time (second/epoch)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Total Training Time (second/iteration)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Computational Resource Usage': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average CPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average FLOPS Estimate (GFLOPS)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLU Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLG Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Entity Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Intent Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Perplexity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Response Cosine Similarity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0}\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_deepseektopp_moe_model\"\n",
    "\n",
    "for iteration in range(5):\n",
    "    print(f\"\\nStarting Training Iteration {iteration + 1}\")\n",
    "    reset_kernel()\n",
    "\n",
    "    def load_tf_datasets_from_disk(load_path):\n",
    "        print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "        try:\n",
    "            with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "                loaded_moe_params = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "        element_spec = (\n",
    "            {\n",
    "                'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            },\n",
    "            {\n",
    "                'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "                'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "                'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "            test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "        raw_data = {}\n",
    "        for dataset_name in ['train', 'test']:\n",
    "            path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "            if os.path.exists(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    raw_data[dataset_name] = pickle.load(f)\n",
    "                print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Raw data file {path} not found.\")\n",
    "                raw_data[dataset_name] = []\n",
    "\n",
    "        return {\n",
    "            \"train_dataset\": train_tf_dataset,\n",
    "            \"test_dataset\": test_tf_dataset,\n",
    "            \"moe_params\": loaded_moe_params,\n",
    "            \"raw_data\": raw_data\n",
    "        }\n",
    "\n",
    "    tf_dataset_save_path = \"tf_datasets\"\n",
    "    loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "    train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "    moe_params = loaded_data[\"moe_params\"]\n",
    "    raw_data = loaded_data[\"raw_data\"]\n",
    "    moe_params[\"num_experts\"] = 4\n",
    "\n",
    "    class TopPRouting:\n",
    "        def __init__(self, p=0.9):\n",
    "            self.p = p \n",
    "\n",
    "        def __call__(self, gate_logits):\n",
    "            probs = tf.nn.softmax(gate_logits, axis=-1)\n",
    "            sorted_probs, sorted_indices = tf.math.top_k(probs, k=tf.shape(probs)[-1])\n",
    "            cum_probs = tf.cumsum(sorted_probs, axis=-1)\n",
    "            p_mask = cum_probs >= self.p\n",
    "            first_over_p = tf.argmax(tf.cast(p_mask, tf.int32), axis=-1, output_type=tf.int32)\n",
    "            first_over_p = tf.maximum(first_over_p, 0)\n",
    "            batch_size = tf.shape(gate_logits)[0]\n",
    "            num_experts = tf.shape(gate_logits)[1]\n",
    "            batch_range = tf.range(batch_size)\n",
    "            batch_range = tf.expand_dims(batch_range, -1)\n",
    "            selection_mask = tf.less_equal(tf.range(num_experts), tf.expand_dims(first_over_p, -1))\n",
    "            selected_indices = tf.boolean_mask(sorted_indices, selection_mask)\n",
    "            selected_probs = tf.boolean_mask(sorted_probs, selection_mask)\n",
    "            num_selected = first_over_p + 1\n",
    "            max_selected = tf.reduce_max(num_selected)\n",
    "            selected_indices = tf.RaggedTensor.from_row_lengths(selected_indices, num_selected).to_tensor(shape=[batch_size, max_selected])\n",
    "            selected_probs = tf.RaggedTensor.from_row_lengths(selected_probs, num_selected).to_tensor(shape=[batch_size, max_selected])\n",
    "            weights = selected_probs / tf.reduce_sum(selected_probs, axis=-1, keepdims=True)\n",
    "\n",
    "            return selected_indices, weights, num_selected\n",
    "\n",
    "    class MoELayer(layers.Layer):\n",
    "        def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, top_p=0.9, name=None):\n",
    "            super(MoELayer, self).__init__(name=name)\n",
    "            self.m = m \n",
    "            self.k_s = k_s \n",
    "            self.total_experts = num_experts * self.m  \n",
    "            self.routed_experts = self.total_experts - self.k_s  \n",
    "            self.top_p = top_p  \n",
    "            self.alpha = alpha\n",
    "            self.input_dim = input_dim\n",
    "            self.seq_length = None\n",
    "            self.adjusted_expert_dim = expert_dim // self.m\n",
    "            self.shared_experts = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', \n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(1e-4), \n",
    "                                name=f'shared_expert_{i}_dense1'),\n",
    "                    layers.Dense(input_dim, activation='relu', \n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(1e-4), \n",
    "                                name=f'shared_expert_{i}_dense2')\n",
    "                ], name=f'shared_expert_{i}') for i in range(self.k_s)\n",
    "            ]\n",
    "\n",
    "            self.routed_experts_list = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', \n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(1e-4), \n",
    "                                name=f'routed_expert_{i}_dense1'),\n",
    "                    layers.Dense(input_dim, activation='relu', \n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(1e-4), \n",
    "                                name=f'routed_expert_{i}_dense2')\n",
    "                ], name=f'routed_expert_{i}') for i in range(self.k_s, self.total_experts)\n",
    "            ]\n",
    "\n",
    "            self.experts = self.shared_experts + self.routed_experts_list\n",
    "            self.gate = layers.Dense(self.routed_experts, activation='softmax', name='gate_weights')\n",
    "            self.top_p_router = TopPRouting(p=self.top_p)\n",
    "            self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            input_rank = inputs.shape.rank\n",
    "            if input_rank == 3:\n",
    "                batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "                flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "                is_3d = True\n",
    "            else:\n",
    "                batch_size = tf.shape(inputs)[0]\n",
    "                seq_length = 1\n",
    "                flat_inputs = inputs\n",
    "                is_3d = False\n",
    "\n",
    "            flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "            shared_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "            for i in range(self.k_s):\n",
    "                shared_output += self.shared_experts[i](flat_inputs)\n",
    "            gate_probs = self.gate(flat_inputs)\n",
    "            expert_indices, expert_weights, num_selected = self.top_p_router(gate_probs)\n",
    "            expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "            expert_indices_adjusted = expert_indices + self.k_s\n",
    "            effective_batch_size = tf.shape(flat_inputs)[0]\n",
    "            batch_indices = tf.tile(\n",
    "                tf.expand_dims(tf.range(effective_batch_size), 1),\n",
    "                [1, tf.shape(expert_indices)[1]]\n",
    "            )\n",
    "            gather_indices = tf.stack([batch_indices, expert_indices_adjusted], axis=-1)\n",
    "            selected_outputs = tf.gather_nd(expert_outputs, gather_indices)\n",
    "            weighted_outputs = selected_outputs * tf.expand_dims(expert_weights, -1)\n",
    "            routed_output = tf.reduce_sum(weighted_outputs, axis=1)\n",
    "            output_flat = shared_output + routed_output + flat_inputs\n",
    "\n",
    "            if is_3d:\n",
    "                output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "                if self.seq_length is not None:\n",
    "                    output.set_shape([None, self.seq_length, self.input_dim])\n",
    "                gate_weights_reshaped = tf.reshape(gate_probs, [batch_size, seq_length, self.routed_experts])\n",
    "                if self.seq_length is not None:\n",
    "                    gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "            else:\n",
    "                output = output_flat\n",
    "                gate_weights_reshaped = gate_probs\n",
    "            expert_selection = tf.one_hot(expert_indices, depth=self.routed_experts, dtype=tf.float32)\n",
    "            expert_selection_sum = tf.reduce_sum(expert_selection, axis=1)  \n",
    "            f_i = tf.reduce_mean(expert_selection_sum, axis=0)  \n",
    "            P_i = tf.reduce_mean(gate_probs, axis=0)  \n",
    "            load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "            self.add_loss(load_balancing_loss)\n",
    "            self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "\n",
    "            return output, gate_weights_reshaped\n",
    "\n",
    "        def get_metrics(self):\n",
    "            return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'num_experts': self.total_experts // self.m,\n",
    "                'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "                'input_dim': self.input_dim,\n",
    "                'm': self.m,\n",
    "                'k_s': self.k_s,\n",
    "                'alpha': self.alpha,\n",
    "                'top_p': self.top_p,\n",
    "                'name': self.name\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class TransformerDecoderLayer(layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "            super(TransformerDecoderLayer, self).__init__()\n",
    "            self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.ffn = keras.Sequential([\n",
    "                layers.Dense(dff, activation='relu'),\n",
    "                layers.Dense(d_model)\n",
    "            ])\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "            self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "        def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "            if look_ahead_mask is not None:\n",
    "                look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "                look_ahead_mask = 1.0 - look_ahead_mask\n",
    "\n",
    "            attn1_output = self.mha1(\n",
    "                query=x,\n",
    "                value=x,\n",
    "                key=x,\n",
    "                attention_mask=look_ahead_mask,\n",
    "                training=training,\n",
    "                return_attention_scores=True\n",
    "            )\n",
    "            if isinstance(attn1_output, tuple):\n",
    "                attn1, attn_weights1 = attn1_output\n",
    "            else:\n",
    "                attn1, attn_weights1 = attn1_output, None\n",
    "\n",
    "            attn1 = self.dropout1(attn1, training=training)\n",
    "            out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "            attn2_output = self.mha2(\n",
    "                query=out1,\n",
    "                value=enc_output,\n",
    "                key=enc_output,\n",
    "                attention_mask=None,\n",
    "                training=training,\n",
    "                return_attention_scores=True\n",
    "            )\n",
    "            if isinstance(attn2_output, tuple):\n",
    "                attn2, attn_weights2 = attn2_output\n",
    "            else:\n",
    "                attn2, attn_weights2 = attn2_output, None\n",
    "\n",
    "            attn2 = self.dropout2(attn2, training=training)\n",
    "            out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "            ffn_output = self.ffn(out2)\n",
    "            ffn_output = self.dropout3(ffn_output, training=training)\n",
    "            out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "            return out3, attn_weights1, attn_weights2\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            mha1_config = self.mha1.get_config()\n",
    "            config.update({\n",
    "                'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "                'num_heads': mha1_config['num_heads'],\n",
    "                'dff': self.ffn.layers[0].units,\n",
    "                'rate': self.dropout1.rate\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class MoEModel(models.Model):\n",
    "        def __init__(self, moe_params):\n",
    "            super(MoEModel, self).__init__()\n",
    "            self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "            self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "            self.num_domains = moe_params[\"num_domains\"]\n",
    "            self.num_intents = moe_params[\"num_intents\"]\n",
    "            self.vocab_size = moe_params[\"vocab_size\"]\n",
    "            self.num_experts = moe_params[\"num_experts\"]\n",
    "            self.expert_dim = moe_params[\"expert_dim\"]\n",
    "            self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "            self.num_heads = 4\n",
    "            self.dff = self.embedding_dim * 4\n",
    "            self.dropout_rate = 0.1\n",
    "            self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                                 self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "            self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "            self.embedding = layers.Embedding(\n",
    "                self.vocab_size,\n",
    "                self.embedding_dim,\n",
    "                mask_zero=True,\n",
    "                embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "                name=\"embedding\"\n",
    "            )\n",
    "            self.embedding.build((None,))\n",
    "            with tf.init_scope():\n",
    "                embedding_weights = self.embedding.get_weights()\n",
    "                if embedding_weights and len(embedding_weights) > 0:\n",
    "                    embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                    self.embedding.set_weights(embedding_weights)\n",
    "\n",
    "            self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "            self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "            self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "            self.moe_nlu = MoELayer(\n",
    "                num_experts=self.num_experts,\n",
    "                expert_dim=self.expert_dim,\n",
    "                input_dim=self.nlu_input_dim,\n",
    "                m=2,\n",
    "                k_s=2,\n",
    "                alpha=0.01,\n",
    "                top_p=0.9,\n",
    "                name='moe_nlu'\n",
    "            )\n",
    "            self.moe_nlg = MoELayer(\n",
    "                num_experts=self.num_experts,\n",
    "                expert_dim=self.expert_dim,\n",
    "                input_dim=self.nlg_input_dim,\n",
    "                m=2,\n",
    "                k_s=2,\n",
    "                alpha=0.01,\n",
    "                top_p=0.9,\n",
    "                name='moe_nlg'\n",
    "            )\n",
    "            self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "            self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "            self.response_output = layers.TimeDistributed(\n",
    "                layers.Dense(self.vocab_size, activation='softmax'), name='response_output'\n",
    "            )\n",
    "            self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "            self.nlg_projection = layers.TimeDistributed(\n",
    "                layers.Dense(self.embedding_dim, activation='relu', name='nlg_projection')\n",
    "            )\n",
    "\n",
    "        def create_look_ahead_mask(self, size):\n",
    "            mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "            return mask\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "            prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "            decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "            domain_onehot_input = inputs['domain_onehot_input']\n",
    "            turn_id_embedding = inputs['turn_id_embedding']\n",
    "            ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "\n",
    "            if any(x is None for x in [user_utterance_tokens, prev_system_response_tokens, decoder_input_tokens,\n",
    "                                      domain_onehot_input, turn_id_embedding, ontology_multihot_input]):\n",
    "                raise ValueError(\"One or more required inputs are missing or None\")\n",
    "\n",
    "            pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "            user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "            prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "            decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "\n",
    "            user_enc_out = user_embedded\n",
    "            prev_system_enc_out = prev_system_embedded\n",
    "            look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "            dec_output = decoder_embedded\n",
    "            attn_weights = {}\n",
    "            \n",
    "            for i, layer in enumerate(self.decoder_layers):\n",
    "                dec_output, attn_w1, attn_w2 = layer(\n",
    "                    dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask\n",
    "                )\n",
    "                attn_weights[f'decoder_layer{i+1}_attn1'] = attn_w1\n",
    "                attn_weights[f'decoder_layer{i+1}_attn2'] = attn_w2\n",
    "\n",
    "            decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "\n",
    "            user_attn_out = self.attn_layer(\n",
    "                query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), \n",
    "                value=user_enc_out, key=user_enc_out, training=training\n",
    "            )\n",
    "            prev_system_attn_out = self.attn_layer(\n",
    "                query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), \n",
    "                value=prev_system_enc_out, key=prev_system_enc_out, training=training\n",
    "            )\n",
    "            decoder_attn_out = self.attn_layer(\n",
    "                query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), \n",
    "                value=decoder_output, key=decoder_output, training=training\n",
    "            )\n",
    "\n",
    "            combined_features = layers.Concatenate()([\n",
    "                tf.squeeze(user_attn_out, 1),\n",
    "                tf.squeeze(prev_system_attn_out, 1),\n",
    "                tf.squeeze(decoder_attn_out, 1),\n",
    "                domain_onehot_input,\n",
    "                turn_id_embedding,\n",
    "                ontology_multihot_input\n",
    "            ])\n",
    "            combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "            nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "            nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "            intent_out = self.intent_output(nlu_out)\n",
    "            domain_out = self.domain_output(nlu_out)\n",
    "\n",
    "            intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "            domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "            intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "            domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "\n",
    "            nlg_input = layers.Concatenate(axis=-1)([\n",
    "                decoder_output,\n",
    "                intent_out_tiled,\n",
    "                domain_out_tiled\n",
    "            ])\n",
    "            nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "\n",
    "            nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "            nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            response_embeddings = self.nlg_projection(nlg_out)\n",
    "            response_embeddings = tf.ensure_shape(response_embeddings, [None, self.max_seq_length, self.embedding_dim])\n",
    "            response_out = self.response_output(nlg_out)\n",
    "\n",
    "            return {\n",
    "                'intent_output': intent_out,\n",
    "                'domain_output': domain_out,\n",
    "                'response_output': response_out,\n",
    "                'response_embeddings': response_embeddings,\n",
    "                'nlu_gate_weights': nlu_gate_weights,\n",
    "                'nlg_gate_weights': nlg_gate_weights\n",
    "            }\n",
    "\n",
    "        def build_graph(self):\n",
    "            inputs = {\n",
    "                'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "                'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='prev_system_response_tokens'),\n",
    "                'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='decoder_input_tokens'),\n",
    "                'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32, name='domain_onehot_input'),\n",
    "                'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32, name='turn_id_embedding'),\n",
    "                'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32, name='ontology_multihot_input'),\n",
    "            }\n",
    "            return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'moe_params': {\n",
    "                    'embedding_dim': self.embedding_dim,\n",
    "                    'max_seq_length': self.max_seq_length,\n",
    "                    'num_domains': self.num_domains,\n",
    "                    'num_intents': self.num_intents,\n",
    "                    'vocab_size': self.vocab_size,\n",
    "                    'num_experts': self.num_experts,\n",
    "                    'expert_dim': self.expert_dim,\n",
    "                    'turn_id_dim': self.turn_id_dim\n",
    "                }\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class IntentAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros', dtype=tf.float32)\n",
    "            self.total_preds = self.add_weight(name='total_preds', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "            self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "            self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.correct_preds.assign(0.)\n",
    "            self.total_preds.assign(0.)\n",
    "\n",
    "    class IntentPrecision(metrics.Metric):\n",
    "        def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "            self.predicted_positives = self.add_weight(name='pp', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            predicted_positives = tf.reduce_sum(y_pred)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.predicted_positives.assign(0.)\n",
    "\n",
    "    class IntentRecall(metrics.Metric):\n",
    "        def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "            self.actual_positives = self.add_weight(name='ap', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            actual_positives = tf.reduce_sum(y_true)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.actual_positives.assign(0.)\n",
    "\n",
    "    class IntentF1(metrics.Metric):\n",
    "        def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.precision = IntentPrecision(threshold=threshold)\n",
    "            self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "            self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "        def result(self):\n",
    "            p = self.precision.result()\n",
    "            r = self.recall.result()\n",
    "            return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.precision.reset_state()\n",
    "            self.recall.reset_state()\n",
    "\n",
    "    class DomainAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='domain_accuracy', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.accuracy = self.add_weight(name='acc', initializer='zeros', dtype=tf.float32)\n",
    "            self.count = self.add_weight(name='count', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "            pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "            matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "            self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "            self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.accuracy.assign(0.)\n",
    "            self.count.assign(0.)\n",
    "\n",
    "    class Perplexity(metrics.Metric):\n",
    "        def __init__(self, name='perplexity', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "            self.total_loss = self.add_weight(name='total_loss', initializer='zeros', dtype=tf.float32)\n",
    "            self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "            loss = self.cross_entropy(y_true, y_pred)\n",
    "            masked_loss = loss * mask\n",
    "            sum_loss = tf.reduce_sum(masked_loss)\n",
    "            num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "            self.total_loss.assign_add(sum_loss)\n",
    "            self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "        def result(self):\n",
    "            avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "            return tf.exp(avg_loss)\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_loss.assign(0.)\n",
    "            self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "    class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "        def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "            super().__init__(name=name)\n",
    "            self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "            self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "            y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "            y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "            y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "            cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "            non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "            non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "            masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "            num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "            avg_cosine_sim_per_sample = tf.where(\n",
    "                num_non_zero_tokens > 0,\n",
    "                tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "                tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "            )\n",
    "            self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "            self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_cosine_sim.assign(0.)\n",
    "            self.num_samples.assign(0.)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "        flops = 0\n",
    "        functional_model = model.build_graph()\n",
    "\n",
    "        def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "            layer_flops_count = 0\n",
    "\n",
    "            if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "                return 0\n",
    "\n",
    "            if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "                for sub_layer in layer_instance.layers:\n",
    "                    layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "                return layer_flops_count\n",
    "\n",
    "            if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "                print(f\"Warning: Layer {layer_instance.name} still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\")\n",
    "                return 0\n",
    "\n",
    "            if isinstance(layer_instance, layers.Dense):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                output_dim = layer_instance.output_shape[-1]\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.LSTM):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                hidden_dim = layer_instance.units\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "                layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.Embedding):\n",
    "                pass\n",
    "\n",
    "            elif isinstance(layer_instance, MoELayer):\n",
    "                moe_input_dim = layer_instance.input_dim\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "                layer_flops_count += gate_flops\n",
    "                for expert in layer_instance.experts:\n",
    "                    layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "                config = layer_instance.get_config()\n",
    "                num_heads = config['num_heads']\n",
    "                key_dim = config['key_dim']\n",
    "                d_model = num_heads * key_dim\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "                projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "                attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "                context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "                output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "                layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "            elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "                inner_layer = layer_instance.layer\n",
    "                td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "                layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "                ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "            return layer_flops_count\n",
    "\n",
    "        for layer in functional_model.layers:\n",
    "            flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "        return flops / 1e9\n",
    "\n",
    "    def get_gpu_stats():\n",
    "        if nvidia_smi is None:\n",
    "            print(\"NVIDIA SMI not available. GPU stats will be reported as zero.\")\n",
    "            return 0, 0\n",
    "        try:\n",
    "            nvidia_smi.nvmlInit()\n",
    "            handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "            gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "            mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "            gpu_memory = mem_info.used / (1024 ** 2)\n",
    "            nvidia_smi.nvmlShutdown()\n",
    "            return gpu_load, gpu_memory / 1024\n",
    "        except Exception as e:\n",
    "            print(f\"GPU monitoring failed: {str(e)}. GPU stats will be reported as zero.\")\n",
    "            return 0, 0\n",
    "\n",
    "    class ResourceMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, validation_data):\n",
    "            super().__init__()\n",
    "            self.resource_stats = []\n",
    "            self.expert_load_history = []\n",
    "            self.start_time = None\n",
    "            self.epoch_start_time = None\n",
    "            self.flops = None\n",
    "            self.validation_data = validation_data\n",
    "            self.nlu_expert_usage_counts = None\n",
    "            self.nlg_expert_usage_counts = None\n",
    "            self.nlu_gate_weights_history = []\n",
    "            self.nlg_gate_weights_history = []\n",
    "\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = time.time()\n",
    "            try:\n",
    "                self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "                self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "                self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "            except Exception:\n",
    "                self.flops = 0\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.epoch_start_time = time.time()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.resource_stats.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_time': epoch_time,\n",
    "                'cpu_usage': cpu_usage,\n",
    "                'memory_used': memory.used / (1024 ** 3),\n",
    "                'memory_total': memory.total / (1024 ** 3),\n",
    "                'gpu_load': gpu_load,\n",
    "                'gpu_memory': gpu_memory,\n",
    "                'loss': logs.get('loss', 0),\n",
    "                'val_loss': logs.get('val_loss', 0),\n",
    "                'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "                'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "                'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "                'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "                'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "                'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "                'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "                'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "                'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "                'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "                'perplexity': logs.get('response_output_perplexity', 0),\n",
    "                'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "                'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "                'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "            })\n",
    "            if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "                self.expert_load_history.append({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                    'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "                })\n",
    "\n",
    "            sample_size = 100\n",
    "            for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "                inputs, _ = batch\n",
    "                outputs = self.model(inputs, training=False)\n",
    "                nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "                reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "                self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "                nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "                reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "                self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            self.total_time = time.time() - self.start_time\n",
    "\n",
    "            if len(self.nlu_gate_weights_history) > 0:\n",
    "                all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "                nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "                for expert_id in nlu_top_indices:\n",
    "                    if expert_id < self.model.moe_nlu.total_experts:\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "            if len(self.nlg_gate_weights_history) > 0:\n",
    "                all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "                nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "                for expert_id in nlg_top_indices:\n",
    "                    if expert_id < self.model.moe_nlg.total_experts:\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        def visualize_expert_load_distribution(self):\n",
    "            total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "            total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "            nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "            nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "            nlu_stability = np.var(nlu_percentages)\n",
    "            nlg_stability = np.var(nlg_percentages)\n",
    "            return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "    class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super().__init__()\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\n",
    "                \"d_model\": self.d_model.numpy(),\n",
    "                \"warmup_steps\": self.warmup_steps\n",
    "            }\n",
    "\n",
    "    model = MoEModel(moe_params)\n",
    "    embedding_layer = model.embedding\n",
    "\n",
    "    def create_validation_split(dataset, embedding_layer, validation_split=0.2, batch_size=moe_params[\"batch_size\"]):\n",
    "        element_spec = dataset.element_spec\n",
    "        data_list = [(features, targets) for features, targets in dataset.unbatch().as_numpy_iterator()]\n",
    "        if not data_list:\n",
    "            raise ValueError(\"Dataset is empty.\")\n",
    "        if len(data_list) < 2:\n",
    "            raise ValueError(\"Dataset too small to split.\")\n",
    "        train_data, val_data = train_test_split(data_list, test_size=validation_split, shuffle=True)\n",
    "\n",
    "        def create_dataset_from_list(data):\n",
    "            if not data:\n",
    "                raise ValueError(\"Empty data list provided.\")\n",
    "            features = {\n",
    "                'user_utterance_tokens': np.array([d[0]['user_utterance_tokens'] for d in data], dtype=np.int32),\n",
    "                'prev_system_response_tokens': np.array([d[0]['prev_system_response_tokens'] for d in data], dtype=np.int32),\n",
    "                'decoder_input_tokens': np.array([d[0]['decoder_input_tokens'] for d in data], dtype=np.int32),\n",
    "                'domain_onehot_input': np.array([d[0]['domain_onehot_input'] for d in data], dtype=np.float32),\n",
    "                'turn_id_embedding': np.array([d[0]['turn_id_embedding'] for d in data], dtype=np.float32),\n",
    "                'ontology_multihot_input': np.array([d[0]['ontology_multihot_input'] for d in data], dtype=np.float32),\n",
    "            }\n",
    "            response_output = np.array([d[1]['response_output'] for d in data], dtype=np.int32)\n",
    "            response_embeddings = embedding_layer(response_output).numpy()\n",
    "            targets = {\n",
    "                'domain_output': np.array([d[1]['domain_output'] for d in data], dtype=np.int32),\n",
    "                'intent_output': np.array([d[1]['intent_output'] for d in data], dtype=np.float32),\n",
    "                'response_output': response_output,\n",
    "                'response_embeddings': response_embeddings\n",
    "            }\n",
    "            return tf.data.Dataset.from_tensor_slices((features, targets))\n",
    "\n",
    "        train_dataset = create_dataset_from_list(train_data).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        val_dataset = create_dataset_from_list(val_data).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    train_tf_dataset, val_tf_dataset = create_validation_split(train_tf_dataset, embedding_layer)\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': losses.BinaryCrossentropy(),\n",
    "        'domain_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={'intent_output': 0.5, 'domain_output': 0.5, 'response_output': 1.0, 'response_embeddings': 1.0}\n",
    "    )\n",
    "\n",
    "    sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "    model(sample_input[0])\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=8, mode='min', restore_best_weights=True)\n",
    "    resource_monitor = ResourceMonitor(val_tf_dataset)\n",
    "    class TQDMProgressBar(callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.progress_bar = tqdm(total=len(train_tf_dataset), desc=f'Epoch {epoch + 1}')\n",
    "\n",
    "        def on_batch_end(self, batch, logs=None):\n",
    "            self.progress_bar.update(1)\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            self.progress_bar.close()\n",
    "\n",
    "    tqdm_callback = TQDMProgressBar()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_tf_dataset,\n",
    "        epochs=100,\n",
    "        validation_data=val_tf_dataset,\n",
    "        callbacks=[early_stopping, resource_monitor, tqdm_callback],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    expert_stats = resource_monitor.visualize_expert_load_distribution()\n",
    "    stats = resource_monitor.resource_stats\n",
    "    avg_epoch_duration = np.mean([s['epoch_time'] for s in stats]) if stats else 0\n",
    "    total_training_time = resource_monitor.total_time\n",
    "    avg_cpu_usage = np.mean([s['cpu_usage'] for s in stats]) if stats else 0\n",
    "    avg_memory = np.mean([s['memory_used'] for s in stats]) if stats else 0\n",
    "    avg_gpu_load = np.mean([s['gpu_load'] for s in stats]) if stats else 0\n",
    "    avg_gpu_memory = np.mean([s['gpu_memory'] for s in stats]) if stats else 0\n",
    "    avg_flops = resource_monitor.flops if resource_monitor.flops else 0\n",
    "    nlu_counts = resource_monitor.nlu_expert_usage_counts\n",
    "    nlg_counts = resource_monitor.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) or 1\n",
    "    total_nlg = np.sum(nlg_counts) or 1\n",
    "    expert_nlu_percentages = [(nlu_counts[i] / total_nlu) * 100 for i in range(8)]\n",
    "    expert_nlg_percentages = [(nlg_counts[i] / total_nlg) * 100 for i in range(8)]\n",
    "    val_intent_accuracy = np.mean([s['val_intent_accuracy'] for s in stats if s['val_intent_accuracy'] > 0]) if stats else 0\n",
    "    val_entity_accuracy = np.mean([s['val_domain_accuracy'] for s in stats if s['val_domain_accuracy'] > 0]) if stats else 0\n",
    "    val_perplexity = np.mean([s['val_perplexity'] for s in stats if s['val_perplexity'] > 0]) if stats else 0\n",
    "    val_cosine_similarity = np.mean([s['val_cosine_similarity'] for s in stats if s['val_cosine_similarity'] > 0]) if stats else 0\n",
    "\n",
    "    results_table['Training Speed (epochs per second)'][f'Iteration {iteration + 1}'] = 1 / avg_epoch_duration if avg_epoch_duration > 0 else 0\n",
    "    results_table['Training Time (second/epoch)'][f'Iteration {iteration + 1}'] = avg_epoch_duration\n",
    "    results_table['Total Training Time (second/iteration)'][f'Iteration {iteration + 1}'] = total_training_time\n",
    "    results_table['Computational Resource Usage'][f'Iteration {iteration + 1}'] = avg_cpu_usage + avg_gpu_load\n",
    "    results_table['Average CPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_cpu_usage\n",
    "    results_table['Average GPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_gpu_load\n",
    "    results_table['Average Memory (GB)'][f'Iteration {iteration + 1}'] = avg_memory\n",
    "    results_table['Average GPU Memory (GB)'][f'Iteration {iteration + 1}'] = avg_gpu_memory\n",
    "    results_table['Average FLOPS Estimate (GFLOPS)'][f'Iteration {iteration + 1}'] = avg_flops\n",
    "    results_table['Expert NLU Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlu_stability']\n",
    "    results_table['Expert NLG Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlg_stability']\n",
    "    for i in range(8):\n",
    "        if i < len(expert_stats['nlu_percentages']):\n",
    "            results_table[f'Average NLU Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlu_percentages'][i]\n",
    "        if i < len(expert_stats['nlg_percentages']):\n",
    "            results_table[f'Average NLG Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlg_percentages'][i]\n",
    "    results_table['Average Validation Entity Accuracy'][f'Iteration {iteration + 1}'] = val_entity_accuracy\n",
    "    results_table['Average Intent Accuracy'][f'Iteration {iteration + 1}'] = val_intent_accuracy\n",
    "    results_table['Average Validation Perplexity'][f'Iteration {iteration + 1}'] = val_perplexity\n",
    "    results_table['Average Validation Response Cosine Similarity'][f'Iteration {iteration + 1}'] = val_cosine_similarity\n",
    "\n",
    "    val_loss = min([s['val_loss'] for s in stats]) if stats else float('inf')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        try:\n",
    "            model.save(best_model_path, save_format='tf', include_optimizer=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to save best model to {best_model_path}: {str(e)}\")\n",
    "\n",
    "for key in results_table:\n",
    "    results_table[key]['Average'] = np.mean([results_table[key][f'Iteration {i+1}'] for i in range(10)])\n",
    "\n",
    "final_result = pd.DataFrame(results_table)\n",
    "final_result = final_result.T\n",
    "final_result.to_excel('training_deepseektopp_result.xlsx', index=True)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE Top-P evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 06:34:34.785026: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:34:35.143446: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:34:35.143669: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:34:35.145213: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:34:35.145391: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:34:35.145479: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:34:35.231656: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:34:35.231831: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:34:35.231948: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:34:35.232027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n",
      "Loaded vocabulary from preprocessor_models/preprocessor_params.json\n",
      "\n",
      "Loading model from: best_deepseektopp_moe_model\n",
      "\n",
      "Model loaded and compiled successfully!\n",
      "Model: \"mo_e_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  784896    \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  8576      \n",
      "                                                                 \n",
      " layer_normalization (Layer  multiple                  256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_layer   multiple                  264576    \n",
      " (TransformerDecoderLayer)                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " moe_nlu (MoELayer)          multiple                  244852    \n",
      "                                                                 \n",
      " moe_nlg (MoELayer)          multiple                  87578     \n",
      "                                                                 \n",
      " intent_output (Dense)       multiple                  14446     \n",
      "                                                                 \n",
      " domain_output (Dense)       multiple                  3262      \n",
      "                                                                 \n",
      " response_output (TimeDistr  multiple                  1024044   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " multi_head_attention_2 (Mu  multiple                  66048     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      " time_distributed (TimeDist  multiple                  21376     \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2519910 (9.61 MB)\n",
      "Trainable params: 2519910 (9.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting Iteration 1\n",
      "\n",
      "Evaluating model on test set...\n",
      "1397/1397 [==============================] - 22s 12ms/step - loss: 2.8838 - domain_output_loss: 0.0020 - intent_output_loss: 0.0386 - response_embeddings_loss: 0.9457 - response_output_loss: 1.8770 - domain_output_domain_accuracy: 1.0000 - intent_output_intent_accuracy: 0.6682 - intent_output_intent_precision: 1.0000 - intent_output_intent_recall: 0.4879 - intent_output_intent_f1: 0.6558 - response_embeddings_response_embedding_cosine_similarity: 0.0543 - response_output_perplexity: 29.6595 - moe_nlu_load_balancing_loss: 0.0100 - moe_nlg_load_balancing_loss: 0.0100\n",
      "Warning: Layer multi_head_attention still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer multi_head_attention_1 still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer nlg_projection still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer dense_2 still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer multi_head_attention still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer multi_head_attention_1 still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer nlg_projection still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer dense_2 still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer multi_head_attention still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer multi_head_attention_1 still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer nlg_projection still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer dense_2 still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "\n",
      "Training results saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evaluation Time (seconds)</th>\n",
       "      <td>22.119367</td>\n",
       "      <td>22.119367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction Speed (ms/token)</th>\n",
       "      <td>2.530680</td>\n",
       "      <td>2.530680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>20.531353</td>\n",
       "      <td>20.531353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>1.837509</td>\n",
       "      <td>1.837509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>5.678094</td>\n",
       "      <td>5.678094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.504456</td>\n",
       "      <td>14.504456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPs Estimate (GFLOPs)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>2.377871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 3 (percent)</th>\n",
       "      <td>9.824624</td>\n",
       "      <td>9.824624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 4 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 5 (percent)</th>\n",
       "      <td>22.852541</td>\n",
       "      <td>22.852541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 6 (percent)</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 7 (percent)</th>\n",
       "      <td>17.287044</td>\n",
       "      <td>17.287044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 8 (percent)</th>\n",
       "      <td>0.035791</td>\n",
       "      <td>0.035791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 3 (percent)</th>\n",
       "      <td>7.232716</td>\n",
       "      <td>7.232716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 4 (percent)</th>\n",
       "      <td>16.643073</td>\n",
       "      <td>16.643073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 5 (percent)</th>\n",
       "      <td>33.812327</td>\n",
       "      <td>33.812327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 6 (percent)</th>\n",
       "      <td>5.376927</td>\n",
       "      <td>5.376927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 7 (percent)</th>\n",
       "      <td>28.351532</td>\n",
       "      <td>28.351532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 8 (percent)</th>\n",
       "      <td>8.583425</td>\n",
       "      <td>8.583425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Micro)</th>\n",
       "      <td>0.655787</td>\n",
       "      <td>0.655787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Macro)</th>\n",
       "      <td>0.411181</td>\n",
       "      <td>0.411181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Weighted)</th>\n",
       "      <td>0.553021</td>\n",
       "      <td>0.553021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent Accuracy</th>\n",
       "      <td>0.668218</td>\n",
       "      <td>0.668218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain F1-score (Macro)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU Score</th>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.002708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1 F1</th>\n",
       "      <td>0.154428</td>\n",
       "      <td>0.154428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-2 F1</th>\n",
       "      <td>0.013482</td>\n",
       "      <td>0.013482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-L F1</th>\n",
       "      <td>0.126043</td>\n",
       "      <td>0.126043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METEOR Score</th>\n",
       "      <td>0.128286</td>\n",
       "      <td>0.128286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>29.659519</td>\n",
       "      <td>29.659519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert Load Stability (Variance)</th>\n",
       "      <td>270.950630</td>\n",
       "      <td>270.950630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert Load Stability (Variance)</th>\n",
       "      <td>141.121675</td>\n",
       "      <td>141.121675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Iteration 1     Average\n",
       "Evaluation Time (seconds)               22.119367   22.119367\n",
       "Prediction Speed (ms/token)              2.530680    2.530680\n",
       "Average CPU Usage (percent)             20.531353   20.531353\n",
       "Average GPU Usage (percent)              1.837509    1.837509\n",
       "Average Memory (GB)                      5.678094    5.678094\n",
       "Average GPU Memory (GB)                 14.504456   14.504456\n",
       "Average FLOPs Estimate (GFLOPs)          2.377871    2.377871\n",
       "NLU Expert 1 (percent)                   0.000000    0.000000\n",
       "NLU Expert 2 (percent)                   0.000000    0.000000\n",
       "NLU Expert 3 (percent)                   9.824624    9.824624\n",
       "NLU Expert 4 (percent)                   0.000000    0.000000\n",
       "NLU Expert 5 (percent)                  22.852541   22.852541\n",
       "NLU Expert 6 (percent)                  50.000000   50.000000\n",
       "NLU Expert 7 (percent)                  17.287044   17.287044\n",
       "NLU Expert 8 (percent)                   0.035791    0.035791\n",
       "NLG Expert 1 (percent)                   0.000000    0.000000\n",
       "NLG Expert 2 (percent)                   0.000000    0.000000\n",
       "NLG Expert 3 (percent)                   7.232716    7.232716\n",
       "NLG Expert 4 (percent)                  16.643073   16.643073\n",
       "NLG Expert 5 (percent)                  33.812327   33.812327\n",
       "NLG Expert 6 (percent)                   5.376927    5.376927\n",
       "NLG Expert 7 (percent)                  28.351532   28.351532\n",
       "NLG Expert 8 (percent)                   8.583425    8.583425\n",
       "Intent F1-score (Micro)                  0.655787    0.655787\n",
       "Intent F1-score (Macro)                  0.411181    0.411181\n",
       "Intent F1-score (Weighted)               0.553021    0.553021\n",
       "Intent Accuracy                          0.668218    0.668218\n",
       "Domain Accuracy                          1.000000    1.000000\n",
       "Domain F1-score (Macro)                  1.000000    1.000000\n",
       "BLEU Score                               0.002708    0.002708\n",
       "ROUGE-1 F1                               0.154428    0.154428\n",
       "ROUGE-2 F1                               0.013482    0.013482\n",
       "ROUGE-L F1                               0.126043    0.126043\n",
       "METEOR Score                             0.128286    0.128286\n",
       "Perplexity                              29.659519   29.659519\n",
       "NLU Expert Load Stability (Variance)   270.950630  270.950630\n",
       "NLG Expert Load Stability (Variance)   141.121675  141.121675"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tf_datasets_from_disk(load_path):\n",
    "    print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "    try:\n",
    "        with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "            loaded_moe_params = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "    element_spec = (\n",
    "        {\n",
    "            'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "        },\n",
    "        {\n",
    "            'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "            'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "        test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "    raw_data = {}\n",
    "    for dataset_name in ['train', 'test']:\n",
    "        path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                raw_data[dataset_name] = pickle.load(f)\n",
    "            print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Raw data file {path} not found.\")\n",
    "            raw_data[dataset_name] = []\n",
    "\n",
    "    return {\n",
    "        \"train_dataset\": train_tf_dataset,\n",
    "        \"test_dataset\": test_tf_dataset,\n",
    "        \"moe_params\": loaded_moe_params,\n",
    "        \"raw_data\": raw_data\n",
    "    }\n",
    "\n",
    "tf_dataset_save_path = \"tf_datasets\"\n",
    "loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "test_tf_dataset = loaded_data[\"test_dataset\"]\n",
    "moe_params = loaded_data[\"moe_params\"]\n",
    "raw_data = loaded_data[\"raw_data\"]\n",
    "moe_params[\"num_experts\"] = 4\n",
    "\n",
    "class TopPRouting:\n",
    "    def __init__(self, p=0.9):\n",
    "        self.p = p \n",
    "\n",
    "    def __call__(self, gate_logits):\n",
    "        probs = tf.nn.softmax(gate_logits, axis=-1)\n",
    "        sorted_probs, sorted_indices = tf.math.top_k(probs, k=tf.shape(probs)[-1])\n",
    "        cum_probs = tf.cumsum(sorted_probs, axis=-1)\n",
    "        p_mask = cum_probs >= self.p\n",
    "        first_over_p = tf.argmax(tf.cast(p_mask, tf.int32), axis=-1, output_type=tf.int32)\n",
    "        first_over_p = tf.maximum(first_over_p, 0)\n",
    "        batch_size = tf.shape(gate_logits)[0]\n",
    "        num_experts = tf.shape(gate_logits)[1]\n",
    "        batch_range = tf.range(batch_size)\n",
    "        batch_range = tf.expand_dims(batch_range, -1)\n",
    "        selection_mask = tf.less_equal(tf.range(num_experts), tf.expand_dims(first_over_p, -1))\n",
    "        selected_indices = tf.boolean_mask(sorted_indices, selection_mask)\n",
    "        selected_probs = tf.boolean_mask(sorted_probs, selection_mask)\n",
    "        num_selected = first_over_p + 1\n",
    "        max_selected = tf.reduce_max(num_selected)\n",
    "        selected_indices = tf.RaggedTensor.from_row_lengths(selected_indices, num_selected).to_tensor(shape=[batch_size, max_selected])\n",
    "        selected_probs = tf.RaggedTensor.from_row_lengths(selected_probs, num_selected).to_tensor(shape=[batch_size, max_selected])\n",
    "        weights = selected_probs / tf.reduce_sum(selected_probs, axis=-1, keepdims=True)\n",
    "\n",
    "        return selected_indices, weights, num_selected\n",
    "\n",
    "class MoELayer(layers.Layer):\n",
    "    def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, top_p=0.9, name=None):\n",
    "        super(MoELayer, self).__init__(name=name)\n",
    "        self.m = m \n",
    "        self.k_s = k_s \n",
    "        self.total_experts = num_experts * self.m  \n",
    "        self.routed_experts = self.total_experts - self.k_s  \n",
    "        self.top_p = top_p  \n",
    "        self.alpha = alpha\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = None\n",
    "        self.adjusted_expert_dim = expert_dim // self.m\n",
    "        self.shared_experts = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', \n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(1e-4), \n",
    "                            name=f'shared_expert_{i}_dense1'),\n",
    "                layers.Dense(input_dim, activation='relu', \n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(1e-4), \n",
    "                            name=f'shared_expert_{i}_dense2')\n",
    "            ], name=f'shared_expert_{i}') for i in range(self.k_s)\n",
    "        ]\n",
    "\n",
    "        self.routed_experts_list = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', \n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(1e-4), \n",
    "                            name=f'routed_expert_{i}_dense1'),\n",
    "                layers.Dense(input_dim, activation='relu', \n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(1e-4), \n",
    "                            name=f'routed_expert_{i}_dense2')\n",
    "            ], name=f'routed_expert_{i}') for i in range(self.k_s, self.total_experts)\n",
    "        ]\n",
    "\n",
    "        self.experts = self.shared_experts + self.routed_experts_list\n",
    "        self.gate = layers.Dense(self.routed_experts, activation='softmax', name='gate_weights')\n",
    "        self.top_p_router = TopPRouting(p=self.top_p)\n",
    "        self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_rank = inputs.shape.rank\n",
    "        if input_rank == 3:\n",
    "            batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "            flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "            is_3d = True\n",
    "        else:\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            seq_length = 1\n",
    "            flat_inputs = inputs\n",
    "            is_3d = False\n",
    "\n",
    "        flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "        shared_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "        for i in range(self.k_s):\n",
    "            shared_output += self.shared_experts[i](flat_inputs)\n",
    "        gate_probs = self.gate(flat_inputs)\n",
    "        expert_indices, expert_weights, num_selected = self.top_p_router(gate_probs)\n",
    "        expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "        expert_indices_adjusted = expert_indices + self.k_s\n",
    "        effective_batch_size = tf.shape(flat_inputs)[0]\n",
    "        batch_indices = tf.tile(\n",
    "            tf.expand_dims(tf.range(effective_batch_size), 1),\n",
    "            [1, tf.shape(expert_indices)[1]]\n",
    "        )\n",
    "        gather_indices = tf.stack([batch_indices, expert_indices_adjusted], axis=-1)\n",
    "        selected_outputs = tf.gather_nd(expert_outputs, gather_indices)\n",
    "        weighted_outputs = selected_outputs * tf.expand_dims(expert_weights, -1)\n",
    "        routed_output = tf.reduce_sum(weighted_outputs, axis=1)\n",
    "        output_flat = shared_output + routed_output + flat_inputs\n",
    "\n",
    "        if is_3d:\n",
    "            output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "            if self.seq_length is not None:\n",
    "                output.set_shape([None, self.seq_length, self.input_dim])\n",
    "            gate_weights_reshaped = tf.reshape(gate_probs, [batch_size, seq_length, self.routed_experts])\n",
    "            if self.seq_length is not None:\n",
    "                gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "        else:\n",
    "            output = output_flat\n",
    "            gate_weights_reshaped = gate_probs\n",
    "        expert_selection = tf.one_hot(expert_indices, depth=self.routed_experts, dtype=tf.float32)\n",
    "        expert_selection_sum = tf.reduce_sum(expert_selection, axis=1)  \n",
    "        f_i = tf.reduce_mean(expert_selection_sum, axis=0)  \n",
    "        P_i = tf.reduce_mean(gate_probs, axis=0)  \n",
    "        load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "        self.add_loss(load_balancing_loss)\n",
    "        self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "\n",
    "        return output, gate_weights_reshaped\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_experts': self.total_experts // self.m,\n",
    "            'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "            'input_dim': self.input_dim,\n",
    "            'm': self.m,\n",
    "            'k_s': self.k_s,\n",
    "            'alpha': self.alpha,\n",
    "            'top_p': self.top_p,\n",
    "            'name': self.name\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "        if look_ahead_mask is not None:\n",
    "            look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "            look_ahead_mask = 1.0 - look_ahead_mask\n",
    "\n",
    "        attn1_output = self.mha1(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            attention_mask=look_ahead_mask,\n",
    "            training=training,\n",
    "            return_attention_scores=True\n",
    "        )\n",
    "        if isinstance(attn1_output, tuple):\n",
    "            attn1, attn_weights1 = attn1_output\n",
    "        else:\n",
    "            attn1, attn_weights1 = attn1_output, None\n",
    "\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2_output = self.mha2(\n",
    "            query=out1,\n",
    "            value=enc_output,\n",
    "            key=enc_output,\n",
    "            attention_mask=None,\n",
    "            training=training,\n",
    "            return_attention_scores=True\n",
    "        )\n",
    "        if isinstance(attn2_output, tuple):\n",
    "            attn2, attn_weights2 = attn2_output\n",
    "        else:\n",
    "            attn2, attn_weights2 = attn2_output, None\n",
    "\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights1, attn_weights2\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        mha1_config = self.mha1.get_config()\n",
    "        config.update({\n",
    "            'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "            'num_heads': mha1_config['num_heads'],\n",
    "            'dff': self.ffn.layers[0].units,\n",
    "            'rate': self.dropout1.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MoEModel(models.Model):\n",
    "    def __init__(self, moe_params):\n",
    "        super(MoEModel, self).__init__()\n",
    "        self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "        self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "        self.num_domains = moe_params[\"num_domains\"]\n",
    "        self.num_intents = moe_params[\"num_intents\"]\n",
    "        self.vocab_size = moe_params[\"vocab_size\"]\n",
    "        self.num_experts = moe_params[\"num_experts\"]\n",
    "        self.expert_dim = moe_params[\"expert_dim\"]\n",
    "        self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "        self.num_heads = 4\n",
    "        self.dff = self.embedding_dim * 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                             self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "        self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "        self.embedding = layers.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "            name=\"embedding\"\n",
    "        )\n",
    "        self.embedding.build((None,))\n",
    "        with tf.init_scope():\n",
    "            embedding_weights = self.embedding.get_weights()\n",
    "            if embedding_weights and len(embedding_weights) > 0:\n",
    "                embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                self.embedding.set_weights(embedding_weights)\n",
    "\n",
    "        self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "        self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "        self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "        self.moe_nlu = MoELayer(\n",
    "            num_experts=self.num_experts,\n",
    "            expert_dim=self.expert_dim,\n",
    "            input_dim=self.nlu_input_dim,\n",
    "            m=2,\n",
    "            k_s=2,\n",
    "            alpha=0.01,\n",
    "            top_p=0.9,\n",
    "            name='moe_nlu'\n",
    "        )\n",
    "        self.moe_nlg = MoELayer(\n",
    "            num_experts=self.num_experts,\n",
    "            expert_dim=self.expert_dim,\n",
    "            input_dim=self.nlg_input_dim,\n",
    "            m=2,\n",
    "            k_s=2,\n",
    "            alpha=0.01,\n",
    "            top_p=0.9,\n",
    "            name='moe_nlg'\n",
    "        )\n",
    "        self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "        self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "        self.response_output = layers.TimeDistributed(\n",
    "            layers.Dense(self.vocab_size, activation='softmax'), name='response_output'\n",
    "        )\n",
    "        self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "        self.nlg_projection = layers.TimeDistributed(\n",
    "            layers.Dense(self.embedding_dim, activation='relu', name='nlg_projection')\n",
    "        )\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "        prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "        decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "        domain_onehot_input = inputs['domain_onehot_input']\n",
    "        turn_id_embedding = inputs['turn_id_embedding']\n",
    "        ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "\n",
    "        if any(x is None for x in [user_utterance_tokens, prev_system_response_tokens, decoder_input_tokens,\n",
    "                                  domain_onehot_input, turn_id_embedding, ontology_multihot_input]):\n",
    "            raise ValueError(\"One or more required inputs are missing or None\")\n",
    "\n",
    "        pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "        user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "        prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "        decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "\n",
    "        user_enc_out = user_embedded\n",
    "        prev_system_enc_out = prev_system_embedded\n",
    "        look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "        dec_output = decoder_embedded\n",
    "        attn_weights = {}\n",
    "\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            dec_output, attn_w1, attn_w2 = layer(\n",
    "                dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask\n",
    "            )\n",
    "            attn_weights[f'decoder_layer{i+1}_attn1'] = attn_w1\n",
    "            attn_weights[f'decoder_layer{i+1}_attn2'] = attn_w2\n",
    "\n",
    "        decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "\n",
    "        user_attn_out = self.attn_layer(\n",
    "            query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), \n",
    "            value=user_enc_out, key=user_enc_out, training=training\n",
    "        )\n",
    "        prev_system_attn_out = self.attn_layer(\n",
    "            query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), \n",
    "            value=prev_system_enc_out, key=prev_system_enc_out, training=training\n",
    "        )\n",
    "        decoder_attn_out = self.attn_layer(\n",
    "            query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), \n",
    "            value=decoder_output, key=decoder_output, training=training\n",
    "        )\n",
    "\n",
    "        combined_features = layers.Concatenate()([\n",
    "            tf.squeeze(user_attn_out, 1),\n",
    "            tf.squeeze(prev_system_attn_out, 1),\n",
    "            tf.squeeze(decoder_attn_out, 1),\n",
    "            domain_onehot_input,\n",
    "            turn_id_embedding,\n",
    "            ontology_multihot_input\n",
    "        ])\n",
    "        combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "        nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "        nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "        intent_out = self.intent_output(nlu_out)\n",
    "        domain_out = self.domain_output(nlu_out)\n",
    "\n",
    "        intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "        domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "        intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "        domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "\n",
    "        nlg_input = layers.Concatenate(axis=-1)([\n",
    "            decoder_output,\n",
    "            intent_out_tiled,\n",
    "            domain_out_tiled\n",
    "        ])\n",
    "        nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "\n",
    "        nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "        nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        response_embeddings = self.nlg_projection(nlg_out)\n",
    "        response_embeddings = tf.ensure_shape(response_embeddings, [None, self.max_seq_length, self.embedding_dim])\n",
    "        response_out = self.response_output(nlg_out)\n",
    "\n",
    "        return {\n",
    "            'intent_output': intent_out,\n",
    "            'domain_output': domain_out,\n",
    "            'response_output': response_out,\n",
    "            'response_embeddings': response_embeddings,\n",
    "            'nlu_gate_weights': nlu_gate_weights,\n",
    "            'nlg_gate_weights': nlg_gate_weights\n",
    "        }\n",
    "\n",
    "    def build_graph(self):\n",
    "        inputs = {\n",
    "            'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "            'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='prev_system_response_tokens'),\n",
    "            'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='decoder_input_tokens'),\n",
    "            'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32, name='domain_onehot_input'),\n",
    "            'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32, name='turn_id_embedding'),\n",
    "            'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32, name='ontology_multihot_input'),\n",
    "        }\n",
    "        return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'moe_params': {\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'max_seq_length': self.max_seq_length,\n",
    "                'num_domains': self.num_domains,\n",
    "                'num_intents': self.num_intents,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'num_experts': self.num_experts,\n",
    "                'expert_dim': self.expert_dim,\n",
    "                'turn_id_dim': self.turn_id_dim\n",
    "            }\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class IntentAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros', dtype=tf.float32)\n",
    "        self.total_preds = self.add_weight(name='total_preds', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "        self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "        self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.correct_preds.assign(0.)\n",
    "        self.total_preds.assign(0.)\n",
    "\n",
    "class IntentPrecision(metrics.Metric):\n",
    "    def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "        self.predicted_positives = self.add_weight(name='pp', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        predicted_positives = tf.reduce_sum(y_pred)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.predicted_positives.assign(0.)\n",
    "\n",
    "class IntentRecall(metrics.Metric):\n",
    "    def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "        self.actual_positives = self.add_weight(name='ap', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        actual_positives = tf.reduce_sum(y_true)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.actual_positives.assign(0.)\n",
    "\n",
    "class IntentF1(metrics.Metric):\n",
    "    def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = IntentPrecision(threshold=threshold)\n",
    "        self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()\n",
    "\n",
    "class DomainAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='domain_accuracy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.accuracy = self.add_weight(name='acc', initializer='zeros', dtype=tf.float32)\n",
    "        self.count = self.add_weight(name='count', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "        pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "        matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "        self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.accuracy.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "class Perplexity(metrics.Metric):\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "        self.total_loss = self.add_weight(name='total_loss', initializer='zeros', dtype=tf.float32)\n",
    "        self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "        loss = self.cross_entropy(y_true, y_pred)\n",
    "        masked_loss = loss * mask\n",
    "        sum_loss = tf.reduce_sum(masked_loss)\n",
    "        num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "        self.total_loss.assign_add(sum_loss)\n",
    "        self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "    def result(self):\n",
    "        avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "        return tf.exp(avg_loss)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_loss.assign(0.)\n",
    "        self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "    def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "        y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "        cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "        non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "        non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "        masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "        num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "        avg_cosine_sim_per_sample = tf.where(\n",
    "            num_non_zero_tokens > 0,\n",
    "            tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "            tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "        )\n",
    "        self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_cosine_sim.assign(0.)\n",
    "        self.num_samples.assign(0.)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "    y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "    y_true = y_true / (y_true_norm + epsilon)\n",
    "    y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "    cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    positive_loss = 1.0 - cosine_sim\n",
    "    mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "    masked_loss = positive_loss * mask\n",
    "    total_loss = tf.reduce_sum(masked_loss)\n",
    "    num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "    return total_loss / num_tokens\n",
    "\n",
    "def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "    flops = 0\n",
    "    functional_model = model.build_graph()\n",
    "\n",
    "    def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "        layer_flops_count = 0\n",
    "\n",
    "        if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "            return 0\n",
    "\n",
    "        if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "            for sub_layer in layer_instance.layers:\n",
    "                layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "            return layer_flops_count\n",
    "\n",
    "        if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "            print(f\"Warning: Layer {layer_instance.name} still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\")\n",
    "            return 0\n",
    "\n",
    "        if isinstance(layer_instance, layers.Dense):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            output_dim = layer_instance.output_shape[-1]\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "        elif isinstance(layer_instance, layers.LSTM):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            hidden_dim = layer_instance.units\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "            layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "        elif isinstance(layer_instance, layers.Embedding):\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer_instance, MoELayer):\n",
    "            moe_input_dim = layer_instance.input_dim\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "            layer_flops_count += gate_flops\n",
    "            for expert in layer_instance.experts:\n",
    "                layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "        elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "            config = layer_instance.get_config()\n",
    "            num_heads = config['num_heads']\n",
    "            key_dim = config['key_dim']\n",
    "            d_model = num_heads * key_dim\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "            projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "            attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "            context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "            output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "            layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "        elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "            inner_layer = layer_instance.layer\n",
    "            td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "            layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "        elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "            ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "        return layer_flops_count\n",
    "\n",
    "    for layer in functional_model.layers:\n",
    "        flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "    return flops / 1e9\n",
    "\n",
    "def get_gpu_stats():\n",
    "    if nvidia_smi is None:\n",
    "        print(\"NVIDIA SMI not available. GPU stats will be reported as zero.\")\n",
    "        return 0, 0\n",
    "    try:\n",
    "        nvidia_smi.nvmlInit()\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "        mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_memory = mem_info.used / (1024 ** 2)\n",
    "        nvidia_smi.nvmlShutdown()\n",
    "        return gpu_load, gpu_memory / 1024\n",
    "    except Exception as e:\n",
    "        print(f\"GPU monitoring failed: {str(e)}. GPU stats will be reported as zero.\")\n",
    "        return 0, 0\n",
    "\n",
    "class ResourceMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.resource_stats = []\n",
    "        self.expert_load_history = []\n",
    "        self.start_time = None\n",
    "        self.epoch_start_time = None\n",
    "        self.flops = None\n",
    "        self.validation_data = validation_data\n",
    "        self.nlu_expert_usage_counts = None\n",
    "        self.nlg_expert_usage_counts = None\n",
    "        self.nlu_gate_weights_history = []\n",
    "        self.nlg_gate_weights_history = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        try:\n",
    "            self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "            self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "            self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "        except Exception:\n",
    "            self.flops = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory()\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.resource_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'epoch_time': epoch_time,\n",
    "            'cpu_usage': cpu_usage,\n",
    "            'memory_used': memory.used / (1024 ** 3),\n",
    "            'memory_total': memory.total / (1024 ** 3),\n",
    "            'gpu_load': gpu_load,\n",
    "            'gpu_memory': gpu_memory,\n",
    "            'loss': logs.get('loss', 0),\n",
    "            'val_loss': logs.get('val_loss', 0),\n",
    "            'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "            'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "            'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "            'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "            'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "            'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "            'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "            'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "            'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "            'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "            'perplexity': logs.get('response_output_perplexity', 0),\n",
    "            'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "            'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "            'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "        })\n",
    "        if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "            self.expert_load_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "            })\n",
    "\n",
    "        sample_size = 100\n",
    "        for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "            inputs, _ = batch\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "            reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "            self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "            nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "            reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "            self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.total_time = time.time() - self.start_time\n",
    "\n",
    "        if len(self.nlu_gate_weights_history) > 0:\n",
    "            all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "            nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "            for expert_id in nlu_top_indices:\n",
    "                if expert_id < self.model.moe_nlu.total_experts:\n",
    "                    self.nlu_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        if len(self.nlg_gate_weights_history) > 0:\n",
    "            all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "            nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "            for expert_id in nlg_top_indices:\n",
    "                if expert_id < self.model.moe_nlg.total_experts:\n",
    "                    self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "    def visualize_expert_load_distribution(self):\n",
    "        total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "        total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "        nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "        nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "        nlu_stability = np.var(nlu_percentages)\n",
    "        nlg_stability = np.var(nlg_percentages)\n",
    "        return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\": self.d_model.numpy(),\n",
    "            \"warmup_steps\": self.warmup_steps\n",
    "        }\n",
    "\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "try:\n",
    "    possible_paths = [\n",
    "        os.path.join(\"preprocessor_models\", \"preprocessor_params.json\"),\n",
    "        os.path.join(\"tf_datasets\", \"preprocessor_params.json\"),\n",
    "        \"preprocessor_params.json\"\n",
    "    ]\n",
    "    \n",
    "    vocab_loaded = False\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                preprocessor_params = json.load(f)\n",
    "            word_to_id = {k: int(v) for k, v in preprocessor_params['word_to_id'].items()}\n",
    "            id_to_word = {int(k): v for k, v in preprocessor_params['id_to_word'].items()}\n",
    "            print(f\"Loaded vocabulary from {path}\")\n",
    "            vocab_loaded = True\n",
    "            break\n",
    "    \n",
    "    if not vocab_loaded:\n",
    "        raise FileNotFoundError(\"Vocabulary file not found in any location\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load vocabulary from preprocessor: {str(e)}\")\n",
    "    word_to_id = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    id_to_word = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
    "\n",
    "\n",
    "model_save_path = \"best_deepseektopp_moe_model\"\n",
    "print(f\"\\nLoading model from: {model_save_path}\")\n",
    "custom_objects = {\n",
    "    \"MoEModel\":MoEModel,\n",
    "    \"CustomLearningRateSchedule\": CustomLearningRateSchedule,\n",
    "    \"MoELayer\": MoELayer,\n",
    "    \"TransformerDecoderLayer\": TransformerDecoderLayer,\n",
    "    \"IntentAccuracy\": IntentAccuracy,\n",
    "    \"IntentPrecision\": IntentPrecision,\n",
    "    \"IntentRecall\": IntentRecall,\n",
    "    \"IntentF1\": IntentF1,\n",
    "    \"DomainAccuracy\": DomainAccuracy,\n",
    "    \"Perplexity\": Perplexity,\n",
    "    \"ResponseEmbeddingCosineSimilarity\": ResponseEmbeddingCosineSimilarity\n",
    "}\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model from {model_save_path}: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "def compile_model(model, moe_params):\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'domain_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss,\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={\n",
    "            'intent_output': 0.5,\n",
    "            'domain_output': 0.5,\n",
    "            'response_output': 1.0,\n",
    "            'response_embeddings': 1.0\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = compile_model(model, moe_params)\n",
    "print(\"\\nModel loaded and compiled successfully!\")\n",
    "sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "model(sample_input[0])\n",
    "model.summary()\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self, model, test_dataset, moe_params, raw_data, word_to_id, id_to_word):\n",
    "        self.model = model\n",
    "        self.test_dataset = test_dataset\n",
    "        self.moe_params = moe_params\n",
    "        self.raw_data = raw_data\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.nlu_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.nlg_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.resource_stats = {\n",
    "            'cpu_usage': [],\n",
    "            'memory_used': [],\n",
    "            'gpu_load': [],\n",
    "            'gpu_memory': [],\n",
    "            'batch_times': []\n",
    "        }\n",
    "        self.special_tokens = {'<pad>', '<sos>', '<eos>', '<unk>'}\n",
    "        self.nlu_top_k = model.get_layer('moe_nlu').get_config().get('top_k', 2)\n",
    "        self.nlg_top_k = model.get_layer('moe_nlg').get_config().get('top_k', 2)\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(\"\\nEvaluating model on test set...\")\n",
    "        \n",
    "        def add_response_embeddings(features, targets):\n",
    "            response_tokens = targets['response_output']\n",
    "            response_embeddings = self.model.embedding(response_tokens)\n",
    "            targets['response_embeddings'] = response_embeddings\n",
    "            return features, targets\n",
    "        \n",
    "        test_dataset_with_embeddings = self.test_dataset.map(add_response_embeddings, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        start_time = time.time()\n",
    "        test_results = self.model.evaluate(test_dataset_with_embeddings, verbose=1)\n",
    "        eval_time = time.time() - start_time\n",
    "        \n",
    "        metric_names = self.model.metrics_names\n",
    "        \n",
    "        perplexity_value = None\n",
    "        for name, value in zip(metric_names, test_results):\n",
    "            if name == 'response_output_perplexity':\n",
    "                perplexity_value = value\n",
    "                break\n",
    "        if perplexity_value is None:\n",
    "            print(\"Warning: Could not find perplexity in standard evaluation results.\")\n",
    "            perplexity_value = 0.0\n",
    "        \n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory().used / (1024 ** 3)\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "\n",
    "        total_tokens = 0\n",
    "        total_time = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        test_dataset_small = test_dataset_with_embeddings.unbatch().batch(moe_params[\"batch_size\"])\n",
    "        \n",
    "        all_true_intents = []\n",
    "        all_pred_intents = []\n",
    "        all_true_domains = []\n",
    "        all_pred_domains = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(test_dataset_small):\n",
    "            inputs, targets = batch\n",
    "            response_tokens = targets['response_output'].numpy()\n",
    "            non_padding_mask = response_tokens != 0\n",
    "            total_tokens += np.sum(non_padding_mask)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            total_time += batch_time\n",
    "            num_batches += tf.shape(inputs['user_utterance_tokens'])[0]\n",
    "            \n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            \n",
    "            self.resource_stats['cpu_usage'].append(cpu_usage)\n",
    "            self.resource_stats['memory_used'].append(memory.used / (1024 ** 3))\n",
    "            self.resource_stats['gpu_load'].append(gpu_load)\n",
    "            self.resource_stats['gpu_memory'].append(gpu_memory)\n",
    "            self.resource_stats['batch_times'].append(batch_time)\n",
    "            \n",
    "            nlu_gate_weights = outputs.get('nlu_gate_weights', tf.zeros([0, self.model.moe_nlu.routed_experts])).numpy()\n",
    "            nlu_top_indices = np.argsort(-nlu_gate_weights, axis=-1)[:, :self.nlu_top_k] + self.model.moe_nlu.k_s\n",
    "            for idx in range(self.nlu_top_k):\n",
    "                expert_ids = nlu_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlu_expert_usage_counts):\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "            \n",
    "            nlg_gate_weights = outputs.get('nlg_gate_weights', tf.zeros([0, self.model.moe_nlg.routed_experts])).numpy()\n",
    "            if len(nlg_gate_weights.shape) == 3:\n",
    "                nlg_gate_weights = nlg_gate_weights.reshape(-1, nlg_gate_weights.shape[-1])\n",
    "            nlg_top_indices = np.argsort(-nlg_gate_weights, axis=-1)[:, :self.nlg_top_k] + self.model.moe_nlg.k_s\n",
    "            for idx in range(self.nlg_top_k):\n",
    "                expert_ids = nlg_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlg_expert_usage_counts):\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "            \n",
    "            true_intents = targets.get('intent_output', np.zeros((moe_params[\"batch_size\"], self.model.num_intents))).numpy()\n",
    "            pred_intents = (outputs.get('intent_output', np.zeros_like(true_intents)).numpy() > 0.5).astype(np.int32)\n",
    "            all_true_intents.append(true_intents)\n",
    "            all_pred_intents.append(pred_intents)\n",
    "            \n",
    "            true_domains = targets.get('domain_output', np.zeros((moe_params[\"batch_size\"], 1))).numpy().flatten()\n",
    "            pred_domains = np.argmax(outputs.get('domain_output', np.zeros((moe_params[\"batch_size\"], self.model.num_domains))).numpy(), axis=-1)\n",
    "            all_true_domains.append(true_domains)\n",
    "            all_pred_domains.append(pred_domains)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                gc.collect()\n",
    "                tf.keras.backend.clear_session()\n",
    "        \n",
    "        avg_time_per_token = (total_time / total_tokens) * 1000 if total_tokens > 0 else 0\n",
    "        \n",
    "        avg_cpu = np.mean(self.resource_stats['cpu_usage']) if self.resource_stats['cpu_usage'] else 0\n",
    "        avg_memory = np.mean(self.resource_stats['memory_used']) if self.resource_stats['memory_used'] else 0\n",
    "        avg_gpu_load = np.mean(self.resource_stats['gpu_load']) if self.resource_stats['gpu_load'] else 0\n",
    "        avg_gpu_memory = np.mean(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        peak_gpu_memory = np.max(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        \n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "        \n",
    "        all_true_intents = np.vstack(all_true_intents)\n",
    "        all_pred_intents = np.vstack(all_pred_intents)\n",
    "        all_true_domains = np.concatenate(all_true_domains)\n",
    "        all_pred_domains = np.concatenate(all_pred_domains)\n",
    "        \n",
    "        intent_f1_score = f1_score(all_true_intents, all_pred_intents, average='micro')\n",
    "        intent_f1_score_macro = f1_score(all_true_intents, all_pred_intents, average='macro')\n",
    "        intent_f1_score_weighted = f1_score(all_true_intents, all_pred_intents, average='weighted')\n",
    "        intent_accuracy_score = accuracy_score(all_true_intents, all_pred_intents)\n",
    "        \n",
    "        domain_accuracy_score = accuracy_score(all_true_domains, all_pred_domains)\n",
    "        domain_f1_score_macro = f1_score(all_true_domains, all_pred_domains, average='macro')\n",
    "\n",
    "        hypotheses = []\n",
    "        references = []\n",
    "        meteor_scores = []\n",
    "        rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "        domain_metrics = defaultdict(lambda: {\n",
    "            'bleu': [],\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'meteor': [],\n",
    "            'count': 0\n",
    "        })\n",
    "        \n",
    "        if not self.raw_data.get('test'):\n",
    "            print(\"Warning: raw_data['test'] is empty. Skipping NLG metrics calculation.\")\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "            domain_metrics = {}\n",
    "        else:\n",
    "            for i, batch in enumerate(test_dataset_small):\n",
    "                if i >= len(self.raw_data.get('test', [])):\n",
    "                    print(f\"Warning: raw_data['test'] has fewer items ({len(self.raw_data.get('test', []))}) than test dataset. Stopping NLG metrics calculation.\")\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    inputs, targets = batch\n",
    "                    with tf.device('/CPU:0'):\n",
    "                        outputs = self.model(inputs, training=False)\n",
    "                    \n",
    "                    domain_id = targets['domain_output'].numpy()[0][0]\n",
    "                    domain_name = f\"domain_{domain_id}\"\n",
    "                    \n",
    "                    response_probs = outputs['response_output'].numpy()\n",
    "                    generated_tokens = np.argmax(response_probs, axis=-1)[0]\n",
    "                    true_tokens = targets['response_output'].numpy()[0]\n",
    "                    \n",
    "                    raw_item = self.raw_data['test'][i]\n",
    "                    ref_text = raw_item.get('raw_next_system_response', '')\n",
    "                    if not ref_text:\n",
    "                        continue\n",
    "                    \n",
    "                    gen_text = self.tokens_to_text(generated_tokens)\n",
    "                    ref_tokens = self.tokens_to_text(true_tokens)\n",
    "                    \n",
    "                    if not gen_text.strip() or not ref_text.strip():\n",
    "                        continue\n",
    "                    \n",
    "                    hypotheses.append(gen_text)\n",
    "                    references.append([ref_text])\n",
    "                    \n",
    "                    ref_tokens = word_tokenize(ref_text.lower())\n",
    "                    gen_tokens = word_tokenize(gen_text.lower())\n",
    "                    \n",
    "                    try:\n",
    "                        meteor = meteor_score([ref_tokens], gen_tokens)\n",
    "                        meteor_scores.append(meteor)\n",
    "                        domain_metrics[domain_name]['meteor'].append(meteor)\n",
    "                    except Exception as e:\n",
    "                        print(f\"METEOR calculation error for example {i}: {str(e)}\")\n",
    "                        meteor_scores.append(0)\n",
    "                        domain_metrics[domain_name]['meteor'].append(0)\n",
    "                    \n",
    "                    try:\n",
    "                        rouge_scores = rouge_scorer_instance.score(ref_text, gen_text)\n",
    "                        domain_metrics[domain_name]['rouge1'].append(rouge_scores['rouge1'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rouge2'].append(rouge_scores['rouge2'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rougeL'].append(rouge_scores['rougeL'].fmeasure)\n",
    "                    except Exception as e:\n",
    "                        print(f\"ROUGE calculation error for example {i}: {str(e)}\")\n",
    "                    \n",
    "                    domain_metrics[domain_name]['count'] += 1\n",
    "                    \n",
    "                    if i % 50 == 0:\n",
    "                        gc.collect()\n",
    "                        tf.keras.backend.clear_session()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing example {i}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if hypotheses and references:\n",
    "            hypotheses_tok = []\n",
    "            references_tok = []\n",
    "            \n",
    "            for hyp, ref_list in zip(hypotheses, references):\n",
    "                if not hyp or not ref_list[0]:\n",
    "                    continue\n",
    "                hyp_tokens = word_tokenize(hyp.lower())\n",
    "                ref_tokens = [word_tokenize(ref.lower()) for ref in ref_list]\n",
    "                hypotheses_tok.append(hyp_tokens)\n",
    "                references_tok.append(ref_tokens)\n",
    "            \n",
    "            chencherry = SmoothingFunction()\n",
    "            try:\n",
    "                bleu_score = corpus_bleu(references_tok, hypotheses_tok, smoothing_function=chencherry.method1)\n",
    "            except Exception as e:\n",
    "                print(f\"BLEU calculation error: {str(e)}\")\n",
    "                bleu_score = 0.0\n",
    "            \n",
    "            rouge_scores = []\n",
    "            for hyp, ref in zip(hypotheses, references):\n",
    "                if hyp and ref[0]:\n",
    "                    try:\n",
    "                        rouge_scores.append(rouge_scorer_instance.score(ref[0], hyp))\n",
    "                    except Exception as e:\n",
    "                        print(f\"ROUGE scoring error: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            avg_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_meteor = np.mean(meteor_scores) if meteor_scores else 0.0\n",
    "        else:\n",
    "            print(\"Warning: No valid hypotheses or references for NLG metrics. Setting to 0.0.\")\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "\n",
    "        return {\n",
    "            'intent_f1_micro': intent_f1_score,\n",
    "            'intent_f1_macro': intent_f1_score_macro,\n",
    "            'intent_f1_weighted': intent_f1_score_weighted,\n",
    "            'intent_accuracy': intent_accuracy_score,\n",
    "            'domain_accuracy': domain_accuracy_score,\n",
    "            'domain_f1_macro': domain_f1_score_macro,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge1': avg_rouge1,\n",
    "            'rouge2': avg_rouge2,\n",
    "            'rougeL': avg_rougeL,\n",
    "            'meteor': avg_meteor,\n",
    "            'perplexity': perplexity_value,\n",
    "            'pred_speed': avg_time_per_token,\n",
    "            'domain_metrics': domain_metrics,\n",
    "            'eval_time': eval_time\n",
    "        }\n",
    "    \n",
    "    def tokens_to_text(self, tokens):\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            if token == 0:\n",
    "                continue\n",
    "            word = self.id_to_word.get(token, '<unk>')\n",
    "            if word not in self.special_tokens:\n",
    "                words.append(word)\n",
    "        return \" \".join(words).strip()\n",
    "\n",
    "results_table = {\n",
    "    'Evaluation Time (seconds)': [],\n",
    "    'Prediction Speed (ms/token)': [],\n",
    "    'Average CPU Usage (percent)': [],\n",
    "    'Average GPU Usage (percent)': [],\n",
    "    'Average Memory (GB)': [],\n",
    "    'Average GPU Memory (GB)': [],\n",
    "    'Average FLOPs Estimate (GFLOPs)': [],\n",
    "    'NLU Expert 1 (percent)': [],\n",
    "    'NLU Expert 2 (percent)': [],\n",
    "    'NLU Expert 3 (percent)': [],\n",
    "    'NLU Expert 4 (percent)': [],\n",
    "    'NLU Expert 5 (percent)': [],\n",
    "    'NLU Expert 6 (percent)': [],\n",
    "    'NLU Expert 7 (percent)': [],\n",
    "    'NLU Expert 8 (percent)': [],\n",
    "    'NLG Expert 1 (percent)': [],\n",
    "    'NLG Expert 2 (percent)': [],\n",
    "    'NLG Expert 3 (percent)': [],\n",
    "    'NLG Expert 4 (percent)': [],\n",
    "    'NLG Expert 5 (percent)': [],\n",
    "    'NLG Expert 6 (percent)': [],\n",
    "    'NLG Expert 7 (percent)': [],\n",
    "    'NLG Expert 8 (percent)': [],\n",
    "    'Intent F1-score (Micro)': [],\n",
    "    'Intent F1-score (Macro)': [],\n",
    "    'Intent F1-score (Weighted)': [],\n",
    "    'Intent Accuracy': [],\n",
    "    'Domain Accuracy': [],\n",
    "    'Domain F1-score (Macro)': [],\n",
    "    'BLEU Score': [],\n",
    "    'ROUGE-1 F1': [],\n",
    "    'ROUGE-2 F1': [],\n",
    "    'ROUGE-L F1': [],\n",
    "    'METEOR Score': [],\n",
    "    'Perplexity': [],\n",
    "    'NLU Expert Load Stability (Variance)': [],\n",
    "    'NLG Expert Load Stability (Variance)': []\n",
    "}\n",
    "\n",
    "num_iterations = 5\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    print(f\"\\nStarting Iteration {iteration}\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "    model = compile_model(model, moe_params)\n",
    "    \n",
    "    evaluator = EvaluationMetrics(model, test_tf_dataset, moe_params, raw_data, word_to_id, id_to_word)\n",
    "\n",
    "    evaluation_results = evaluator.evaluate()\n",
    "    results_table['Evaluation Time (seconds)'].append(evaluation_results['eval_time'])\n",
    "    results_table['Prediction Speed (ms/token)'].append(evaluation_results['pred_speed'])\n",
    "    results_table['Average CPU Usage (percent)'].append(np.mean(evaluator.resource_stats['cpu_usage']) if evaluator.resource_stats['cpu_usage'] else 0)\n",
    "    results_table['Average GPU Usage (percent)'].append(np.mean(evaluator.resource_stats['gpu_load']) if evaluator.resource_stats['gpu_load'] else 0)\n",
    "    results_table['Average Memory (GB)'].append(np.mean(evaluator.resource_stats['memory_used']) if evaluator.resource_stats['memory_used'] else 0)\n",
    "    results_table['Average GPU Memory (GB)'].append(np.mean(evaluator.resource_stats['gpu_memory']) if evaluator.resource_stats['gpu_memory'] else 0)\n",
    "    results_table['Average FLOPs Estimate (GFLOPs)'].append(calculate_flops(model))\n",
    "    nlu_counts = evaluator.nlu_expert_usage_counts\n",
    "    nlg_counts = evaluator.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) if np.sum(nlu_counts) > 0 else 1\n",
    "    total_nlg = np.sum(nlg_counts) if np.sum(nlg_counts) > 0 else 1\n",
    "    results_table['NLU Expert 1 (percent)'].append((nlu_counts[0] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 2 (percent)'].append((nlu_counts[1] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 3 (percent)'].append((nlu_counts[2] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 4 (percent)'].append((nlu_counts[3] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 5 (percent)'].append((nlu_counts[4] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 6 (percent)'].append((nlu_counts[5] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 7 (percent)'].append((nlu_counts[6] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 8 (percent)'].append((nlu_counts[7] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLG Expert 1 (percent)'].append((nlg_counts[0] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 2 (percent)'].append((nlg_counts[1] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 3 (percent)'].append((nlg_counts[2] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 4 (percent)'].append((nlg_counts[3] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 5 (percent)'].append((nlg_counts[4] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 6 (percent)'].append((nlg_counts[5] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 7 (percent)'].append((nlg_counts[6] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 8 (percent)'].append((nlg_counts[7] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['Intent F1-score (Micro)'].append(evaluation_results['intent_f1_micro'])\n",
    "    results_table['Intent F1-score (Macro)'].append(evaluation_results['intent_f1_macro'])\n",
    "    results_table['Intent F1-score (Weighted)'].append(evaluation_results['intent_f1_weighted'])\n",
    "    results_table['Intent Accuracy'].append(evaluation_results['intent_accuracy'])\n",
    "    results_table['Domain Accuracy'].append(evaluation_results['domain_accuracy'])\n",
    "    results_table['Domain F1-score (Macro)'].append(evaluation_results['domain_f1_macro'])\n",
    "    results_table['BLEU Score'].append(evaluation_results['bleu'])\n",
    "    results_table['ROUGE-1 F1'].append(evaluation_results['rouge1'])\n",
    "    results_table['ROUGE-2 F1'].append(evaluation_results['rouge2'])\n",
    "    results_table['ROUGE-L F1'].append(evaluation_results['rougeL'])\n",
    "    results_table['METEOR Score'].append(evaluation_results['meteor'])\n",
    "    results_table['Perplexity'].append(evaluation_results['perplexity'])\n",
    "    results_table['NLU Expert Load Stability (Variance)'].append(np.var(nlu_counts / total_nlu * 100) if total_nlu > 0 else 0)\n",
    "    results_table['NLG Expert Load Stability (Variance)'].append(np.var(nlg_counts / total_nlg * 100) if total_nlg > 0 else 0)\n",
    "\n",
    "for key in results_table:\n",
    "    if results_table[key]:\n",
    "        results_table[key].append(np.mean(results_table[key]))\n",
    "    else:\n",
    "        results_table[key].append(0)\n",
    "\n",
    "print(\"\\nTraining results saved\")\n",
    "#final_df\n",
    "final_df = pd.DataFrame(results_table)\n",
    "final_df = final_df.T\n",
    "final_df.columns = [f'Iteration {i+1}' for i in range(num_iterations)] + ['Average']\n",
    "final_df.to_excel('prediction_deepseektopp_results.xlsx', index=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE Noisy Top-K Gating experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 07:04:44.232945: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 07:04:44.726116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 07:04:44.726256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 07:04:44.807736: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 07:04:44.982292: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 07:04:46.408232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training Iteration 1\n",
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 07:04:49.941350: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:04:50.273054: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:04:50.273485: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:04:50.276331: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:04:50.276721: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:04:50.277008: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:04:50.381225: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:04:50.381384: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:04:50.381496: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:04:50.381576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n",
      "Warning: Layer multi_head_attention still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer multi_head_attention_1 still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer nlg_projection still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n",
      "Warning: Layer dense_34 still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/4428 [00:00<?, ?it/s]2025-07-04 07:05:15.520267: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd0dc12f160 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 07:05:15.520351: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2025-07-04 07:05:15.539822: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 07:05:15.587014: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751612715.682121    7873 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 1: 100%|██████████| 4428/4428 [03:20<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_noisytop-K_moe_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_noisytop-K_moe_model/assets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Iteration 2</th>\n",
       "      <th>Iteration 3</th>\n",
       "      <th>Iteration 4</th>\n",
       "      <th>Iteration 5</th>\n",
       "      <th>Iteration 6</th>\n",
       "      <th>Iteration 7</th>\n",
       "      <th>Iteration 8</th>\n",
       "      <th>Iteration 9</th>\n",
       "      <th>Iteration 10</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Speed (epochs per second)</th>\n",
       "      <td>0.005138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training Time (second/epoch)</th>\n",
       "      <td>194.615082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.461508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Training Time (second/iteration)</th>\n",
       "      <td>201.887702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.188770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computational Resource Usage</th>\n",
       "      <td>65.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>23.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>6.599609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.659961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.531799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.453180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPS Estimate (GFLOPS)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLU Load Distribution Stability</th>\n",
       "      <td>148.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLG Load Distribution Stability</th>\n",
       "      <td>64.752450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.475245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 3 (percent)</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 4 (percent)</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 5 (percent)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 6 (percent)</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 7 (percent)</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 8 (percent)</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 3 (percent)</th>\n",
       "      <td>20.164179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.016418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 4 (percent)</th>\n",
       "      <td>18.164179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.816418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 5 (percent)</th>\n",
       "      <td>21.253731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.125373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 6 (percent)</th>\n",
       "      <td>16.223881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.622388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 7 (percent)</th>\n",
       "      <td>15.552239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.555224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 8 (percent)</th>\n",
       "      <td>8.641791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.864179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Entity Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Intent Accuracy</th>\n",
       "      <td>0.985547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Perplexity</th>\n",
       "      <td>19.501957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.950196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Response Cosine Similarity</th>\n",
       "      <td>0.332306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Iteration 1  Iteration 2  \\\n",
       "Training Speed (epochs per second)                0.005138          0.0   \n",
       "Training Time (second/epoch)                    194.615082          0.0   \n",
       "Total Training Time (second/iteration)          201.887702          0.0   \n",
       "Computational Resource Usage                     65.500000          0.0   \n",
       "Average CPU Usage (percent)                      23.500000          0.0   \n",
       "Average GPU Usage (percent)                      42.000000          0.0   \n",
       "Average Memory (GB)                               6.599609          0.0   \n",
       "Average GPU Memory (GB)                          14.531799          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                   2.377871          0.0   \n",
       "Expert NLU Load Distribution Stability          148.750000          0.0   \n",
       "Expert NLG Load Distribution Stability           64.752450          0.0   \n",
       "Average NLU Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 3 (percent)                   12.000000          0.0   \n",
       "Average NLU Expert 4 (percent)                    7.000000          0.0   \n",
       "Average NLU Expert 5 (percent)                    1.000000          0.0   \n",
       "Average NLU Expert 6 (percent)                   18.000000          0.0   \n",
       "Average NLU Expert 7 (percent)                   31.000000          0.0   \n",
       "Average NLU Expert 8 (percent)                   31.000000          0.0   \n",
       "Average NLG Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 3 (percent)                   20.164179          0.0   \n",
       "Average NLG Expert 4 (percent)                   18.164179          0.0   \n",
       "Average NLG Expert 5 (percent)                   21.253731          0.0   \n",
       "Average NLG Expert 6 (percent)                   16.223881          0.0   \n",
       "Average NLG Expert 7 (percent)                   15.552239          0.0   \n",
       "Average NLG Expert 8 (percent)                    8.641791          0.0   \n",
       "Average Validation Entity Accuracy                1.000000          0.0   \n",
       "Average Intent Accuracy                           0.985547          0.0   \n",
       "Average Validation Perplexity                    19.501957          0.0   \n",
       "Average Validation Response Cosine Similarity     0.332306          0.0   \n",
       "\n",
       "                                               Iteration 3  Iteration 4  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 5  Iteration 6  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 7  Iteration 8  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 9  Iteration 10  \\\n",
       "Training Speed (epochs per second)                     0.0           0.0   \n",
       "Training Time (second/epoch)                           0.0           0.0   \n",
       "Total Training Time (second/iteration)                 0.0           0.0   \n",
       "Computational Resource Usage                           0.0           0.0   \n",
       "Average CPU Usage (percent)                            0.0           0.0   \n",
       "Average GPU Usage (percent)                            0.0           0.0   \n",
       "Average Memory (GB)                                    0.0           0.0   \n",
       "Average GPU Memory (GB)                                0.0           0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0           0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0           0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0           0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0           0.0   \n",
       "Average Validation Entity Accuracy                     0.0           0.0   \n",
       "Average Intent Accuracy                                0.0           0.0   \n",
       "Average Validation Perplexity                          0.0           0.0   \n",
       "Average Validation Response Cosine Similarity          0.0           0.0   \n",
       "\n",
       "                                                 Average  \n",
       "Training Speed (epochs per second)              0.000514  \n",
       "Training Time (second/epoch)                   19.461508  \n",
       "Total Training Time (second/iteration)         20.188770  \n",
       "Computational Resource Usage                    6.550000  \n",
       "Average CPU Usage (percent)                     2.350000  \n",
       "Average GPU Usage (percent)                     4.200000  \n",
       "Average Memory (GB)                             0.659961  \n",
       "Average GPU Memory (GB)                         1.453180  \n",
       "Average FLOPS Estimate (GFLOPS)                 0.237787  \n",
       "Expert NLU Load Distribution Stability         14.875000  \n",
       "Expert NLG Load Distribution Stability          6.475245  \n",
       "Average NLU Expert 1 (percent)                  0.000000  \n",
       "Average NLU Expert 2 (percent)                  0.000000  \n",
       "Average NLU Expert 3 (percent)                  1.200000  \n",
       "Average NLU Expert 4 (percent)                  0.700000  \n",
       "Average NLU Expert 5 (percent)                  0.100000  \n",
       "Average NLU Expert 6 (percent)                  1.800000  \n",
       "Average NLU Expert 7 (percent)                  3.100000  \n",
       "Average NLU Expert 8 (percent)                  3.100000  \n",
       "Average NLG Expert 1 (percent)                  0.000000  \n",
       "Average NLG Expert 2 (percent)                  0.000000  \n",
       "Average NLG Expert 3 (percent)                  2.016418  \n",
       "Average NLG Expert 4 (percent)                  1.816418  \n",
       "Average NLG Expert 5 (percent)                  2.125373  \n",
       "Average NLG Expert 6 (percent)                  1.622388  \n",
       "Average NLG Expert 7 (percent)                  1.555224  \n",
       "Average NLG Expert 8 (percent)                  0.864179  \n",
       "Average Validation Entity Accuracy              0.100000  \n",
       "Average Intent Accuracy                         0.098555  \n",
       "Average Validation Perplexity                   1.950196  \n",
       "Average Validation Response Cosine Similarity   0.033231  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reset_kernel():\n",
    "    tf.keras.backend.clear_session()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "results_table = {\n",
    "    'Training Speed (epochs per second)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Training Time (second/epoch)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Total Training Time (second/iteration)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Computational Resource Usage': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average CPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average FLOPS Estimate (GFLOPS)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLU Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLG Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Entity Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Intent Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Perplexity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Response Cosine Similarity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0}\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_noisytop-K_moe_model\"\n",
    "\n",
    "for iteration in range(5):\n",
    "    print(f\"\\nStarting Training Iteration {iteration + 1}\")\n",
    "    reset_kernel()\n",
    "\n",
    "    def load_tf_datasets_from_disk(load_path):\n",
    "        print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "        try:\n",
    "            with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "                loaded_moe_params = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "        element_spec = (\n",
    "            {\n",
    "                'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            },\n",
    "            {\n",
    "                'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "                'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "                'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "            test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "        raw_data = {}\n",
    "        for dataset_name in ['train', 'test']:\n",
    "            path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "            if os.path.exists(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    raw_data[dataset_name] = pickle.load(f)\n",
    "                print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Raw data file {path} not found.\")\n",
    "                raw_data[dataset_name] = []\n",
    "\n",
    "        return {\n",
    "            \"train_dataset\": train_tf_dataset,\n",
    "            \"test_dataset\": test_tf_dataset,\n",
    "            \"moe_params\": loaded_moe_params,\n",
    "            \"raw_data\": raw_data\n",
    "        }\n",
    "\n",
    "    tf_dataset_save_path = \"tf_datasets\"\n",
    "    loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "    train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "    moe_params = loaded_data[\"moe_params\"]\n",
    "    raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "    moe_params[\"num_experts\"] = 4\n",
    "\n",
    "    class NoisyTopKGating(layers.Layer):\n",
    "        def __init__(self, num_experts, input_dim, k=2, noise_epsilon=1.0, name=None):\n",
    "            super(NoisyTopKGating, self).__init__(name=name)\n",
    "            self.num_experts = num_experts\n",
    "            self.input_dim = input_dim\n",
    "            self.k = k\n",
    "            self.noise_epsilon = noise_epsilon\n",
    "            self.w_g = self.add_weight(\n",
    "                name='w_g',\n",
    "                shape=(input_dim, num_experts),\n",
    "                initializer='glorot_uniform',\n",
    "                trainable=True\n",
    "            )\n",
    "            self.w_noise = self.add_weight(\n",
    "                name='w_noise',\n",
    "                shape=(input_dim, num_experts),\n",
    "                initializer=keras.initializers.Constant(value=noise_epsilon),\n",
    "                trainable=True\n",
    "            )\n",
    "            self.softmax = layers.Softmax(axis=-1)\n",
    "\n",
    "        def call(self, inputs, training=False, usage_stats=None):\n",
    "            gate_logits = tf.matmul(inputs, self.w_g)\n",
    "            if training and usage_stats is not None:\n",
    "                usage_mean = tf.reduce_mean(usage_stats)\n",
    "                usage_std = tf.math.reduce_std(usage_stats)\n",
    "                imbalance_factor = tf.clip_by_value(usage_std / (usage_mean + 1e-6), 0.0, 1.0)\n",
    "                adjusted_k = tf.cast(\n",
    "                    self.k * (1.0 + imbalance_factor * 2),\n",
    "                    tf.int32\n",
    "                )\n",
    "                adjusted_k = tf.minimum(tf.maximum(adjusted_k, self.k), self.num_experts)\n",
    "            else:\n",
    "                adjusted_k = self.k\n",
    "\n",
    "            if training:\n",
    "                raw_noise = tf.random.normal(\n",
    "                    shape=(tf.shape(inputs)[0], self.num_experts),\n",
    "                    mean=0.0,\n",
    "                    stddev=1.0\n",
    "                )\n",
    "                gate_std = tf.math.reduce_std(gate_logits, axis=-1, keepdims=True)\n",
    "                noise_scale = tf.nn.softplus(tf.matmul(inputs, self.w_noise)) * (1.0 + gate_std)\n",
    "                noisy_gate_logits = gate_logits + (raw_noise * noise_scale)\n",
    "            else:\n",
    "                noisy_gate_logits = gate_logits\n",
    "            top_k_values, _ = tf.math.top_k(noisy_gate_logits, k=adjusted_k)\n",
    "            min_values = tf.reduce_min(top_k_values, axis=-1, keepdims=True)\n",
    "            top_k_mask = tf.cast(\n",
    "                noisy_gate_logits >= min_values,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            keep_top_k_logits = tf.where(\n",
    "                top_k_mask > 0,\n",
    "                noisy_gate_logits,\n",
    "                tf.ones_like(noisy_gate_logits) * -float('inf')\n",
    "            )\n",
    "            gate_outputs = self.softmax(keep_top_k_logits)\n",
    "            return gate_outputs, gate_logits\n",
    "\n",
    "    class MoELayer(layers.Layer):\n",
    "        def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.5, top_k=2, name=None):\n",
    "            super(MoELayer, self).__init__(name=name)\n",
    "            self.m = m\n",
    "            self.k_s = k_s\n",
    "            self.total_experts = num_experts * self.m\n",
    "            self.routed_experts = self.total_experts - self.k_s\n",
    "            self.top_k = top_k\n",
    "            self.top_mk = self.m * self.top_k\n",
    "            self.top_mk = min(self.top_mk, self.routed_experts)\n",
    "            self.alpha = alpha\n",
    "            self.input_dim = input_dim\n",
    "            self.seq_length = None\n",
    "            self.usage_stats = tf.Variable(\n",
    "                tf.ones(self.routed_experts) / self.routed_experts,\n",
    "                trainable=False,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            self.adjusted_expert_dim = expert_dim // self.m\n",
    "            self.shared_experts = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu'),\n",
    "                    layers.Dense(input_dim, activation=None)\n",
    "                ], name=f'shared_expert_{i}') for i in range(self.k_s)\n",
    "            ]\n",
    "            self.routed_experts_list = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu'),\n",
    "                    layers.Dense(input_dim, activation=None)\n",
    "                ], name=f'routed_expert_{i}') for i in range(self.k_s, self.total_experts)\n",
    "            ]\n",
    "            self.experts = self.shared_experts + self.routed_experts_list\n",
    "            self.gate = NoisyTopKGating(\n",
    "                num_experts=self.routed_experts,\n",
    "                input_dim=self.input_dim,\n",
    "                k=self.top_mk,\n",
    "                noise_epsilon=1.0,\n",
    "                name='noisy_topk_gate'\n",
    "            )\n",
    "            self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "            self.expert_usage_metric = tf.keras.metrics.Mean(name=f'{name}_expert_usage')\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            input_rank = inputs.shape.rank\n",
    "            if input_rank == 3:\n",
    "                batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "                flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "                is_3d = True\n",
    "            else:\n",
    "                batch_size = tf.shape(inputs)[0]\n",
    "                seq_length = 1\n",
    "                flat_inputs = inputs\n",
    "                is_3d = False\n",
    "\n",
    "            flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "            shared_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "            for i in range(self.k_s):\n",
    "                shared_output += self.shared_experts[i](flat_inputs)\n",
    "            gate_weights, clean_logits = self.gate(flat_inputs, training=training, usage_stats=self.usage_stats)\n",
    "            top_mk_values, top_mk_indices = tf.nn.top_k(gate_weights, k=self.top_mk, sorted=True)\n",
    "            top_mk_indices = top_mk_indices + self.k_s\n",
    "            top_mk_weights = top_mk_values / (tf.reduce_sum(top_mk_values, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "            routed_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "\n",
    "            for k in range(self.top_mk):\n",
    "                kth_weights = top_mk_weights[:, k]\n",
    "                kth_indices = top_mk_indices[:, k]\n",
    "                mask = tf.one_hot(kth_indices, depth=self.total_experts, dtype=tf.float32)\n",
    "                expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "                kth_expert_output = tf.reduce_sum(expert_outputs * tf.expand_dims(mask, -1), axis=1)\n",
    "                weighted_kth_output = kth_expert_output * tf.expand_dims(kth_weights, -1)\n",
    "                routed_output += weighted_kth_output\n",
    "\n",
    "            output_flat = shared_output + routed_output + flat_inputs\n",
    "\n",
    "            if is_3d:\n",
    "                output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "                gate_weights_reshaped = tf.reshape(gate_weights, [batch_size, seq_length, self.routed_experts])\n",
    "            else:\n",
    "                output = output_flat\n",
    "                gate_weights_reshaped = gate_weights\n",
    "            expert_assignments = tf.argmax(clean_logits, axis=-1)\n",
    "            f_i = tf.reduce_mean(\n",
    "                tf.one_hot(expert_assignments, depth=self.routed_experts, dtype=tf.float32),\n",
    "                axis=0\n",
    "            )\n",
    "            P_i = tf.reduce_mean(gate_weights, axis=0)\n",
    "            cov = tf.reduce_sum((f_i - tf.reduce_mean(f_i)) * (P_i - tf.reduce_mean(P_i)))\n",
    "            load_balancing_loss = self.alpha * (tf.reduce_sum(f_i * P_i) + 0.1 * cov)\n",
    "            self.add_loss(load_balancing_loss)\n",
    "            self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "            if training:\n",
    "                current_usage = tf.reduce_mean(\n",
    "                    tf.one_hot(expert_assignments, depth=self.routed_experts, dtype=tf.float32),\n",
    "                    axis=0\n",
    "                )\n",
    "                decay_rate = tf.minimum(0.1, 1.0 / (tf.reduce_sum(self.usage_stats) + 1e-6))\n",
    "                self.usage_stats.assign(\n",
    "                    self.usage_stats * (1.0 - decay_rate) + current_usage * decay_rate\n",
    "                )\n",
    "            expert_usage = tf.reduce_mean(tf.reduce_sum(tf.cast(gate_weights > 0, tf.float32), axis=0))\n",
    "            self.expert_usage_metric.update_state(expert_usage)\n",
    "            return output, gate_weights_reshaped\n",
    "\n",
    "        def get_metrics(self):\n",
    "            return {\n",
    "                f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric,\n",
    "                f'{self.name}_expert_usage': self.expert_usage_metric\n",
    "            }\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'num_experts': self.total_experts // self.m,\n",
    "                'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "                'input_dim': self.input_dim,\n",
    "                'm': self.m,\n",
    "                'k_s': self.k_s,\n",
    "                'alpha': self.alpha,\n",
    "                'top_k': self.top_k,\n",
    "                'name': self.name\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class TransformerDecoderLayer(layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "            super(TransformerDecoderLayer, self).__init__()\n",
    "            self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.ffn = keras.Sequential([\n",
    "                layers.Dense(dff, activation='relu'),\n",
    "                layers.Dense(d_model)\n",
    "            ])\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "            self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "        def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "            if look_ahead_mask is not None:\n",
    "                look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "                look_ahead_mask = 1.0 - look_ahead_mask\n",
    "            attn1_output = self.mha1(\n",
    "                query=x,\n",
    "                value=x,\n",
    "                key=x,\n",
    "                attention_mask=look_ahead_mask,\n",
    "                training=training,\n",
    "                return_attention_scores=True\n",
    "            )\n",
    "            if isinstance(attn1_output, tuple):\n",
    "                attn1, attn_weights1 = attn1_output\n",
    "            else:\n",
    "                attn1, attn_weights1 = attn1_output, None\n",
    "            attn1 = self.dropout1(attn1, training=training)\n",
    "            out1 = self.layernorm1(attn1 + x)\n",
    "            attn2_output = self.mha2(\n",
    "                query=out1,\n",
    "                value=enc_output,\n",
    "                key=enc_output,\n",
    "                attention_mask=None,\n",
    "                training=training,\n",
    "                return_attention_scores=True\n",
    "            )\n",
    "            if isinstance(attn2_output, tuple):\n",
    "                attn2, attn_weights2 = attn2_output\n",
    "            else:\n",
    "                attn2, attn_weights2 = attn2_output, None\n",
    "            attn2 = self.dropout2(attn2, training=training)\n",
    "            out2 = self.layernorm2(attn2 + out1)\n",
    "            ffn_output = self.ffn(out2)\n",
    "            ffn_output = self.dropout3(ffn_output, training=training)\n",
    "            out3 = self.layernorm3(ffn_output + out2)\n",
    "            return out3, attn_weights1, attn_weights2\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            mha1_config = self.mha1.get_config()\n",
    "            config.update({\n",
    "                'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "                'num_heads': mha1_config['num_heads'],\n",
    "                'dff': self.ffn.layers[0].units,\n",
    "                'rate': self.dropout1.rate\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class MoEModel(models.Model):\n",
    "        def __init__(self, moe_params):\n",
    "            super(MoEModel, self).__init__()\n",
    "            self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "            self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "            self.num_domains = moe_params[\"num_domains\"]\n",
    "            self.num_intents = moe_params[\"num_intents\"]\n",
    "            self.vocab_size = moe_params[\"vocab_size\"]\n",
    "            self.num_experts = moe_params[\"num_experts\"]\n",
    "            self.expert_dim = moe_params[\"expert_dim\"]\n",
    "            self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "            self.num_heads = 4\n",
    "            self.dff = self.embedding_dim * 4\n",
    "            self.dropout_rate = 0.1\n",
    "            self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                                 self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "            self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "            self.embedding = layers.Embedding(\n",
    "                self.vocab_size,\n",
    "                self.embedding_dim,\n",
    "                mask_zero=True,\n",
    "                embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "                name=\"embedding\"\n",
    "            )\n",
    "            self.embedding.build((None,))\n",
    "            with tf.init_scope():\n",
    "                embedding_weights = self.embedding.get_weights()\n",
    "                if embedding_weights and len(embedding_weights) > 0:\n",
    "                    embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                    self.embedding.set_weights(embedding_weights)\n",
    "            self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "            self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "            self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "            self.moe_nlu = MoELayer(\n",
    "                num_experts=self.num_experts,\n",
    "                expert_dim=self.expert_dim,\n",
    "                input_dim=self.nlu_input_dim,\n",
    "                m=2,\n",
    "                k_s=2,\n",
    "                alpha=0.01,\n",
    "                top_k=2,\n",
    "                name='moe_nlu'\n",
    "            )\n",
    "            self.moe_nlg = MoELayer(\n",
    "                num_experts=self.num_experts,\n",
    "                expert_dim=self.expert_dim,\n",
    "                input_dim=self.nlg_input_dim,\n",
    "                m=2,\n",
    "                k_s=2,\n",
    "                alpha=0.01,\n",
    "                top_k=2,\n",
    "                name='moe_nlg'\n",
    "            )\n",
    "            self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "            self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "            self.response_output = layers.TimeDistributed(\n",
    "                layers.Dense(self.vocab_size, activation='softmax'), name='response_output'\n",
    "            )\n",
    "            self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "            self.nlg_projection = layers.TimeDistributed(\n",
    "                layers.Dense(self.embedding_dim, activation='relu', name='nlg_projection')\n",
    "            )\n",
    "\n",
    "        def create_look_ahead_mask(self, size):\n",
    "            mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "            return mask\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "            prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "            decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "            domain_onehot_input = inputs['domain_onehot_input']\n",
    "            turn_id_embedding = inputs['turn_id_embedding']\n",
    "            ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "            pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "            user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "            prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "            decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "            user_enc_out = user_embedded\n",
    "            prev_system_enc_out = prev_system_embedded\n",
    "            look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "            dec_output = decoder_embedded\n",
    "            attn_weights = {}\n",
    "            for i, layer in enumerate(self.decoder_layers):\n",
    "                dec_output, attn_w1, attn_w2 = layer(\n",
    "                    dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask\n",
    "                )\n",
    "                attn_weights[f'decoder_layer{i+1}_attn1'] = attn_w1\n",
    "                attn_weights[f'decoder_layer{i+1}_attn2'] = attn_w2\n",
    "            decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "            user_attn_out = self.attn_layer(\n",
    "                query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), \n",
    "                value=user_enc_out, key=user_enc_out, training=training\n",
    "            )\n",
    "            prev_system_attn_out = self.attn_layer(\n",
    "                query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), \n",
    "                value=prev_system_enc_out, key=prev_system_enc_out, training=training\n",
    "            )\n",
    "            decoder_attn_out = self.attn_layer(\n",
    "                query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), \n",
    "                value=decoder_output, key=decoder_output, training=training\n",
    "            )\n",
    "            combined_features = layers.Concatenate()([\n",
    "                tf.squeeze(user_attn_out, 1),\n",
    "                tf.squeeze(prev_system_attn_out, 1),\n",
    "                tf.squeeze(decoder_attn_out, 1),\n",
    "                domain_onehot_input,\n",
    "                turn_id_embedding,\n",
    "                ontology_multihot_input\n",
    "            ])\n",
    "            combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "            nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "            nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "            intent_out = self.intent_output(nlu_out)\n",
    "            domain_out = self.domain_output(nlu_out)\n",
    "            intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "            domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "            intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "            domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "            nlg_input = layers.Concatenate(axis=-1)([\n",
    "                decoder_output,\n",
    "                intent_out_tiled,\n",
    "                domain_out_tiled\n",
    "            ])\n",
    "            nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "            nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            response_embeddings = self.nlg_projection(nlg_out)\n",
    "            response_embeddings = tf.ensure_shape(response_embeddings, [None, self.max_seq_length, self.embedding_dim])\n",
    "            response_out = self.response_output(nlg_out)\n",
    "            return {\n",
    "                'intent_output': intent_out,\n",
    "                'domain_output': domain_out,\n",
    "                'response_output': response_out,\n",
    "                'response_embeddings': response_embeddings,\n",
    "                'nlu_gate_weights': nlu_gate_weights,\n",
    "                'nlg_gate_weights': nlg_gate_weights\n",
    "            }\n",
    "\n",
    "        def build_graph(self):\n",
    "            inputs = {\n",
    "                'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "                'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='prev_system_response_tokens'),\n",
    "                'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='decoder_input_tokens'),\n",
    "                'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32, name='domain_onehot_input'),\n",
    "                'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32, name='turn_id_embedding'),\n",
    "                'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32, name='ontology_multihot_input'),\n",
    "            }\n",
    "            return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'moe_params': {\n",
    "                    'embedding_dim': self.embedding_dim,\n",
    "                    'max_seq_length': self.max_seq_length,\n",
    "                    'num_domains': self.num_domains,\n",
    "                    'num_intents': self.num_intents,\n",
    "                    'vocab_size': self.vocab_size,\n",
    "                    'num_experts': self.num_experts,\n",
    "                    'expert_dim': self.expert_dim,\n",
    "                    'turn_id_dim': self.turn_id_dim\n",
    "                }\n",
    "            })\n",
    "            return config\n",
    "\n",
    "        def compile(self, **kwargs):\n",
    "            super().compile(**kwargs)\n",
    "            self.val_moe_nlu_load_balancing_loss_metric = tf.keras.metrics.Mean(name='val_moe_nlu_load_balancing_loss')\n",
    "            self.val_moe_nlg_load_balancing_loss_metric = tf.keras.metrics.Mean(name='val_moe_nlg_load_balancing_loss')\n",
    "\n",
    "        def test_step(self, data):\n",
    "            x, y = data\n",
    "            y_pred = self(x, training=False)\n",
    "            self.compiled_loss(y, y_pred)\n",
    "            self.compiled_metrics.update_state(y, y_pred)\n",
    "            nlu_gate_weights = y_pred['nlu_gate_weights']\n",
    "            nlg_gate_weights = y_pred['nlg_gate_weights']\n",
    "            nlu_expert_assignments = tf.argmax(nlu_gate_weights, axis=-1)\n",
    "            nlu_f_i = tf.reduce_mean(\n",
    "                tf.one_hot(nlu_expert_assignments, depth=self.moe_nlu.routed_experts, dtype=tf.float32),\n",
    "                axis=0\n",
    "            )\n",
    "            nlu_P_i = tf.reduce_mean(nlu_gate_weights, axis=0)\n",
    "            nlu_load_balancing_loss = self.moe_nlu.alpha * tf.reduce_sum(nlu_f_i * nlu_P_i)\n",
    "            self.val_moe_nlu_load_balancing_loss_metric.update_state(nlu_load_balancing_loss)\n",
    "            nlg_expert_assignments = tf.argmax(nlg_gate_weights, axis=-1)\n",
    "            nlg_f_i = tf.reduce_mean(\n",
    "                tf.one_hot(nlg_expert_assignments, depth=self.moe_nlg.routed_experts, dtype=tf.float32),\n",
    "                axis=0\n",
    "            )\n",
    "            nlg_P_i = tf.reduce_mean(nlg_gate_weights, axis=0)\n",
    "            nlg_load_balancing_loss = self.moe_nlg.alpha * tf.reduce_sum(nlg_f_i * nlg_P_i)\n",
    "            self.val_moe_nlg_load_balancing_loss_metric.update_state(nlg_load_balancing_loss)\n",
    "            return {m.name: m.result() for m in self.metrics + [self.val_moe_nlu_load_balancing_loss_metric, self.val_moe_nlg_load_balancing_loss_metric]}\n",
    "\n",
    "    class IntentAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros', dtype=tf.float32)\n",
    "            self.total_preds = self.add_weight(name='total_preds', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "            self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "            self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.correct_preds.assign(0.)\n",
    "            self.total_preds.assign(0.)\n",
    "\n",
    "    class IntentPrecision(metrics.Metric):\n",
    "        def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "            self.predicted_positives = self.add_weight(name='pp', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            predicted_positives = tf.reduce_sum(y_pred)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.predicted_positives.assign(0.)\n",
    "\n",
    "    class IntentRecall(metrics.Metric):\n",
    "        def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "            self.actual_positives = self.add_weight(name='ap', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            actual_positives = tf.reduce_sum(y_true)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.actual_positives.assign(0.)\n",
    "\n",
    "    class IntentF1(metrics.Metric):\n",
    "        def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.precision = IntentPrecision(threshold=threshold)\n",
    "            self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "            self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "        def result(self):\n",
    "            p = self.precision.result()\n",
    "            r = self.recall.result()\n",
    "            return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.precision.reset_state()\n",
    "            self.recall.reset_state()\n",
    "\n",
    "    class DomainAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='domain_accuracy', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.accuracy = self.add_weight(name='acc', initializer='zeros', dtype=tf.float32)\n",
    "            self.count = self.add_weight(name='count', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "            pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "            matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "            self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "            self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.accuracy.assign(0.)\n",
    "            self.count.assign(0.)\n",
    "\n",
    "    class Perplexity(metrics.Metric):\n",
    "        def __init__(self, name='perplexity', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "            self.total_loss = self.add_weight(name='total_loss', initializer='zeros', dtype=tf.float32)\n",
    "            self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "            loss = self.cross_entropy(y_true, y_pred)\n",
    "            masked_loss = loss * mask\n",
    "            sum_loss = tf.reduce_sum(masked_loss)\n",
    "            num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "            self.total_loss.assign_add(sum_loss)\n",
    "            self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "        def result(self):\n",
    "            avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "            return tf.exp(avg_loss)\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_loss.assign(0.)\n",
    "            self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "    class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "        def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "            super().__init__(name=name)\n",
    "            self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "            self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "            y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "            y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "            y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "            cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "            non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "            non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "            masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "            num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "            avg_cosine_sim_per_sample = tf.where(\n",
    "                num_non_zero_tokens > 0,\n",
    "                tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "                tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "            )\n",
    "            self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "            self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_cosine_sim.assign(0.)\n",
    "            self.num_samples.assign(0.)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "        flops = 0\n",
    "        functional_model = model.build_graph()\n",
    "        def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "            layer_flops_count = 0\n",
    "            if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "                return 0\n",
    "            if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "                for sub_layer in layer_instance.layers:\n",
    "                    layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "                return layer_flops_count\n",
    "            if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "                print(f\"Warning: Layer {layer_instance.name} still has no defined input shape after forward pass. Skipping FLOPs calculation for it.\")\n",
    "                return 0\n",
    "            if isinstance(layer_instance, layers.Dense):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                output_dim = layer_instance.output_shape[-1]\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "            elif isinstance(layer_instance, layers.LSTM):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                hidden_dim = layer_instance.units\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "                layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "            elif isinstance(layer_instance, layers.Embedding):\n",
    "                pass\n",
    "            elif isinstance(layer_instance, MoELayer):\n",
    "                moe_input_dim = layer_instance.input_dim\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "                layer_flops_count += gate_flops\n",
    "                for expert in layer_instance.experts:\n",
    "                    layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "            elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "                config = layer_instance.get_config()\n",
    "                num_heads = config['num_heads']\n",
    "                key_dim = config['key_dim']\n",
    "                d_model = num_heads * key_dim\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "                projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "                attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "                context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "                output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "                layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "            elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "                inner_layer = layer_instance.layer\n",
    "                td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "                layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "            elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "                ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "            return layer_flops_count\n",
    "        for layer in functional_model.layers:\n",
    "            flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "        return flops / 1e9\n",
    "\n",
    "    def get_gpu_stats():\n",
    "        if nvidia_smi is None:\n",
    "            print(\"NVIDIA SMI not available. GPU stats will be reported as zero.\")\n",
    "            return 0, 0\n",
    "        try:\n",
    "            nvidia_smi.nvmlInit()\n",
    "            handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "            gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "            mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "            gpu_memory = mem_info.used / (1024 ** 2)\n",
    "            nvidia_smi.nvmlShutdown()\n",
    "            return gpu_load, gpu_memory / 1024\n",
    "        except Exception as e:\n",
    "            print(f\"GPU monitoring failed: {str(e)}. GPU stats will be reported as zero.\")\n",
    "            return 0, 0\n",
    "\n",
    "    class ResourceMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, validation_data):\n",
    "            super().__init__()\n",
    "            self.resource_stats = []\n",
    "            self.expert_load_history = []\n",
    "            self.start_time = None\n",
    "            self.epoch_start_time = None\n",
    "            self.flops = None\n",
    "            self.validation_data = validation_data\n",
    "            self.nlu_expert_usage_counts = None\n",
    "            self.nlg_expert_usage_counts = None\n",
    "            self.nlu_gate_weights_history = []\n",
    "            self.nlg_gate_weights_history = []\n",
    "\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = time.time()\n",
    "            try:\n",
    "                self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "                self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "                self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "            except Exception:\n",
    "                self.flops = 0\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.epoch_start_time = time.time()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.resource_stats.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_time': epoch_time,\n",
    "                'cpu_usage': cpu_usage,\n",
    "                'memory_used': memory.used / (1024 ** 3),\n",
    "                'memory_total': memory.total / (1024 ** 3),\n",
    "                'gpu_load': gpu_load,\n",
    "                'gpu_memory': gpu_memory,\n",
    "                'loss': logs.get('loss', 0),\n",
    "                'val_loss': logs.get('val_loss', 0),\n",
    "                'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "                'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "                'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "                'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "                'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "                'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "                'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "                'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "                'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "                'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "                'perplexity': logs.get('response_output_perplexity', 0),\n",
    "                'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "                'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "                'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "            })\n",
    "            if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "                self.expert_load_history.append({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                    'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "                })\n",
    "            sample_size = 100\n",
    "            for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "                inputs, _ = batch\n",
    "                outputs = self.model(inputs, training=False)\n",
    "                nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "                reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "                self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "                nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "                reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "                self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            self.total_time = time.time() - self.start_time\n",
    "            if len(self.nlu_gate_weights_history) > 0:\n",
    "                all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "                nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "                for expert_id in nlu_top_indices:\n",
    "                    if expert_id < self.model.moe_nlu.total_experts:\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "            if len(self.nlg_gate_weights_history) > 0:\n",
    "                all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "                nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "                for expert_id in nlg_top_indices:\n",
    "                    if expert_id < self.model.moe_nlg.total_experts:\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        def visualize_expert_load_distribution(self):\n",
    "            total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "            total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "            nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "            nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "            nlu_stability = np.var(nlu_percentages)\n",
    "            nlg_stability = np.var(nlg_percentages)\n",
    "            return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "    class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super().__init__()\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\n",
    "                \"d_model\": self.d_model.numpy(),\n",
    "                \"warmup_steps\": self.warmup_steps\n",
    "            }\n",
    "\n",
    "    model = MoEModel(moe_params)\n",
    "    embedding_layer = model.embedding\n",
    "\n",
    "    def create_validation_split(dataset, embedding_layer, validation_split=0.2, batch_size=moe_params[\"batch_size\"]):\n",
    "        element_spec = dataset.element_spec\n",
    "        data_list = [(features, targets) for features, targets in dataset.unbatch().as_numpy_iterator()]\n",
    "        if not data_list:\n",
    "            raise ValueError(\"Dataset is empty.\")\n",
    "        if len(data_list) < 2:\n",
    "            raise ValueError(\"Dataset too small to split.\")\n",
    "        train_data, val_data = train_test_split(data_list, test_size=validation_split, shuffle=True)\n",
    "        def create_dataset_from_list(data):\n",
    "            if not data:\n",
    "                raise ValueError(\"Empty data list provided.\")\n",
    "            features = {\n",
    "                'user_utterance_tokens': np.array([d[0]['user_utterance_tokens'] for d in data], dtype=np.int32),\n",
    "                'prev_system_response_tokens': np.array([d[0]['prev_system_response_tokens'] for d in data], dtype=np.int32),\n",
    "                'decoder_input_tokens': np.array([d[0]['decoder_input_tokens'] for d in data], dtype=np.int32),\n",
    "                'domain_onehot_input': np.array([d[0]['domain_onehot_input'] for d in data], dtype=np.float32),\n",
    "                'turn_id_embedding': np.array([d[0]['turn_id_embedding'] for d in data], dtype=np.float32),\n",
    "                'ontology_multihot_input': np.array([d[0]['ontology_multihot_input'] for d in data], dtype=np.float32),\n",
    "            }\n",
    "            response_output = np.array([d[1]['response_output'] for d in data], dtype=np.int32)\n",
    "            response_embeddings = embedding_layer(response_output).numpy()\n",
    "            targets = {\n",
    "                'domain_output': np.array([d[1]['domain_output'] for d in data], dtype=np.int32),\n",
    "                'intent_output': np.array([d[1]['intent_output'] for d in data], dtype=np.float32),\n",
    "                'response_output': response_output,\n",
    "                'response_embeddings': response_embeddings\n",
    "            }\n",
    "            return tf.data.Dataset.from_tensor_slices((features, targets))\n",
    "        train_dataset = create_dataset_from_list(train_data).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        val_dataset = create_dataset_from_list(val_data).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    train_tf_dataset, val_tf_dataset = create_validation_split(train_tf_dataset, embedding_layer)\n",
    "    losses_dict = {\n",
    "        'intent_output': losses.BinaryCrossentropy(),\n",
    "        'domain_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss\n",
    "    }\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={'intent_output': 0.5, 'domain_output': 0.5, 'response_output': 1.0, 'response_embeddings': 1.0}\n",
    "    )\n",
    "    model.metrics.extend([\n",
    "        model.moe_nlu.load_balancing_loss_metric,\n",
    "        model.moe_nlg.load_balancing_loss_metric,\n",
    "        model.val_moe_nlu_load_balancing_loss_metric,\n",
    "        model.val_moe_nlg_load_balancing_loss_metric\n",
    "    ])\n",
    "    sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "    model(sample_input[0])\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=8, mode='min', restore_best_weights=True)\n",
    "    resource_monitor = ResourceMonitor(val_tf_dataset)\n",
    "    class TQDMProgressBar(callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.progress_bar = tqdm(total=len(train_tf_dataset), desc=f'Epoch {epoch + 1}')\n",
    "        def on_batch_end(self, batch, logs=None):\n",
    "            self.progress_bar.update(1)\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            self.progress_bar.close()\n",
    "    tqdm_callback = TQDMProgressBar()\n",
    "    history = model.fit(\n",
    "        train_tf_dataset,\n",
    "        epochs=100,\n",
    "        validation_data=val_tf_dataset,\n",
    "        callbacks=[early_stopping, resource_monitor, tqdm_callback],\n",
    "        verbose=0\n",
    "    )\n",
    "    expert_stats = resource_monitor.visualize_expert_load_distribution()\n",
    "    stats = resource_monitor.resource_stats\n",
    "    avg_epoch_duration = np.mean([s['epoch_time'] for s in stats]) if stats else 0\n",
    "    total_training_time = resource_monitor.total_time\n",
    "    avg_cpu_usage = np.mean([s['cpu_usage'] for s in stats]) if stats else 0\n",
    "    avg_memory = np.mean([s['memory_used'] for s in stats]) if stats else 0\n",
    "    avg_gpu_load = np.mean([s['gpu_load'] for s in stats]) if stats else 0\n",
    "    avg_gpu_memory = np.mean([s['gpu_memory'] for s in stats]) if stats else 0\n",
    "    avg_flops = resource_monitor.flops if resource_monitor.flops else 0\n",
    "    nlu_counts = resource_monitor.nlu_expert_usage_counts\n",
    "    nlg_counts = resource_monitor.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) or 1\n",
    "    total_nlg = np.sum(nlg_counts) or 1\n",
    "    expert_nlu_percentages = [(nlu_counts[i] / total_nlu) * 100 for i in range(8)]\n",
    "    expert_nlg_percentages = [(nlg_counts[i] / total_nlg) * 100 for i in range(8)]\n",
    "    val_intent_accuracy = np.mean([s['val_intent_accuracy'] for s in stats if s['val_intent_accuracy'] > 0]) if stats and any(s['val_intent_accuracy'] > 0 for s in stats) else 0\n",
    "    val_entity_accuracy = np.mean([s['val_domain_accuracy'] for s in stats if s['val_domain_accuracy'] > 0]) if stats and any(s['val_domain_accuracy'] > 0 for s in stats) else 0\n",
    "    val_perplexity = np.mean([s['val_perplexity'] for s in stats if s['val_perplexity'] > 0]) if stats and any(s['val_perplexity'] > 0 for s in stats) else 0\n",
    "    val_cosine_similarity = np.mean([s['val_cosine_similarity'] for s in stats if s['val_cosine_similarity'] > 0]) if stats and any(s['val_cosine_similarity'] > 0 for s in stats) else 0\n",
    "    results_table['Training Speed (epochs per second)'][f'Iteration {iteration + 1}'] = 1 / avg_epoch_duration if avg_epoch_duration > 0 else 0\n",
    "    results_table['Training Time (second/epoch)'][f'Iteration {iteration + 1}'] = avg_epoch_duration\n",
    "    results_table['Total Training Time (second/iteration)'][f'Iteration {iteration + 1}'] = total_training_time\n",
    "    results_table['Computational Resource Usage'][f'Iteration {iteration + 1}'] = avg_cpu_usage + avg_gpu_load\n",
    "    results_table['Average CPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_cpu_usage\n",
    "    results_table['Average GPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_gpu_load\n",
    "    results_table['Average Memory (GB)'][f'Iteration {iteration + 1}'] = avg_memory\n",
    "    results_table['Average GPU Memory (GB)'][f'Iteration {iteration + 1}'] = avg_gpu_memory\n",
    "    results_table['Average FLOPS Estimate (GFLOPS)'][f'Iteration {iteration + 1}'] = avg_flops\n",
    "    results_table['Expert NLU Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlu_stability']\n",
    "    results_table['Expert NLG Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlg_stability']\n",
    "    for i in range(8):\n",
    "        if i < len(expert_stats['nlu_percentages']):\n",
    "            results_table[f'Average NLU Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlu_percentages'][i]\n",
    "        if i < len(expert_stats['nlg_percentages']):\n",
    "            results_table[f'Average NLG Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlg_percentages'][i]\n",
    "    results_table['Average Validation Entity Accuracy'][f'Iteration {iteration + 1}'] = val_entity_accuracy\n",
    "    results_table['Average Intent Accuracy'][f'Iteration {iteration + 1}'] = val_intent_accuracy\n",
    "    results_table['Average Validation Perplexity'][f'Iteration {iteration + 1}'] = val_perplexity\n",
    "    results_table['Average Validation Response Cosine Similarity'][f'Iteration {iteration + 1}'] = val_cosine_similarity\n",
    "    val_loss = min([s['val_loss'] for s in stats]) if stats else float('inf')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        try:\n",
    "            model.save(best_model_path, save_format='tf', include_optimizer=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to save best model to {best_model_path}: {str(e)}\")\n",
    "\n",
    "for key in results_table:\n",
    "    results_table[key]['Average'] = np.mean([results_table[key][f'Iteration {i+1}'] for i in range(10)])\n",
    "\n",
    "final_result = pd.DataFrame(results_table)\n",
    "final_result = final_result.T\n",
    "final_result.to_excel('training_NoisyTop-K_result.xlsx', index=True)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE Noisy Top-K Gating evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 06:46:12.446085: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:46:12.753432: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:46:12.753627: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:46:12.754966: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:46:12.755176: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:46:12.755295: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:46:12.828296: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:46:12.828452: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:46:12.828558: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:46:12.828636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mo_e_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  784896    \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  8576      \n",
      "                                                                 \n",
      " layer_normalization (Layer  multiple                  256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_layer   multiple                  264576    \n",
      " (TransformerDecoderLayer)                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " moe_nlu (MoELayer)          multiple                  247642    \n",
      "                                                                 \n",
      " moe_nlg (MoELayer)          multiple                  88574     \n",
      "                                                                 \n",
      " intent_output (Dense)       multiple                  14446     \n",
      "                                                                 \n",
      " domain_output (Dense)       multiple                  3262      \n",
      "                                                                 \n",
      " response_output (TimeDistr  multiple                  1024044   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " multi_head_attention_2 (Mu  multiple                  66048     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      " time_distributed (TimeDist  multiple                  21376     \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2523700 (9.63 MB)\n",
      "Trainable params: 2523684 (9.63 MB)\n",
      "Non-trainable params: 16 (64.00 Byte)\n",
      "_________________________________________________________________\n",
      "1397/1397 [==============================] - 24s 14ms/step - loss: 2.9783 - domain_output_loss: 1.0534e-04 - intent_output_loss: 0.0022 - response_embeddings_loss: 0.9597 - response_output_loss: 2.0174 - domain_output_domain_accuracy: 1.0000 - intent_output_intent_accuracy: 0.9817 - intent_output_intent_precision: 0.9936 - intent_output_intent_recall: 0.9885 - intent_output_intent_f1: 0.9910 - response_embeddings_response_embedding_cosine_similarity: 0.0403 - response_output_perplexity: 33.2971 - val_moe_nlu_load_balancing_loss: 0.0025 - val_moe_nlg_load_balancing_loss: 0.4125 - moe_nlu_load_balancing_loss: 0.0026 - moe_nlu_expert_usage: 1.3333 - moe_nlg_load_balancing_loss: 0.0028 - moe_nlg_expert_usage: 89.3343\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evaluation Time (seconds)</th>\n",
       "      <td>23.712171</td>\n",
       "      <td>23.712171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction Speed (ms/token)</th>\n",
       "      <td>3.400129</td>\n",
       "      <td>3.400129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>22.791625</td>\n",
       "      <td>22.791625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>1.934860</td>\n",
       "      <td>1.934860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>5.661268</td>\n",
       "      <td>5.661268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.504456</td>\n",
       "      <td>14.504456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPs Estimate (GFLOPs)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>2.377871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 3 (percent)</th>\n",
       "      <td>6.460272</td>\n",
       "      <td>6.460272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 4 (percent)</th>\n",
       "      <td>30.762348</td>\n",
       "      <td>30.762348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 5 (percent)</th>\n",
       "      <td>15.479599</td>\n",
       "      <td>15.479599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 6 (percent)</th>\n",
       "      <td>1.556908</td>\n",
       "      <td>1.556908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 7 (percent)</th>\n",
       "      <td>18.647101</td>\n",
       "      <td>18.647101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 8 (percent)</th>\n",
       "      <td>27.093772</td>\n",
       "      <td>27.093772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 3 (percent)</th>\n",
       "      <td>17.272888</td>\n",
       "      <td>17.272888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 4 (percent)</th>\n",
       "      <td>15.876772</td>\n",
       "      <td>15.876772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 5 (percent)</th>\n",
       "      <td>31.747935</td>\n",
       "      <td>31.747935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 6 (percent)</th>\n",
       "      <td>4.524087</td>\n",
       "      <td>4.524087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 7 (percent)</th>\n",
       "      <td>13.059434</td>\n",
       "      <td>13.059434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 8 (percent)</th>\n",
       "      <td>17.518884</td>\n",
       "      <td>17.518884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Micro)</th>\n",
       "      <td>0.991031</td>\n",
       "      <td>0.991031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Macro)</th>\n",
       "      <td>0.929733</td>\n",
       "      <td>0.929733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Weighted)</th>\n",
       "      <td>0.987940</td>\n",
       "      <td>0.987940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent Accuracy</th>\n",
       "      <td>0.981747</td>\n",
       "      <td>0.981747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain F1-score (Macro)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU Score</th>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.001968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1 F1</th>\n",
       "      <td>0.167277</td>\n",
       "      <td>0.167277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-2 F1</th>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-L F1</th>\n",
       "      <td>0.140349</td>\n",
       "      <td>0.140349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METEOR Score</th>\n",
       "      <td>0.119921</td>\n",
       "      <td>0.119921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>33.297123</td>\n",
       "      <td>33.297123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert Load Stability (Variance)</th>\n",
       "      <td>132.735748</td>\n",
       "      <td>132.735748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert Load Stability (Variance)</th>\n",
       "      <td>100.785427</td>\n",
       "      <td>100.785427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Iteration 1     Average\n",
       "Evaluation Time (seconds)               23.712171   23.712171\n",
       "Prediction Speed (ms/token)              3.400129    3.400129\n",
       "Average CPU Usage (percent)             22.791625   22.791625\n",
       "Average GPU Usage (percent)              1.934860    1.934860\n",
       "Average Memory (GB)                      5.661268    5.661268\n",
       "Average GPU Memory (GB)                 14.504456   14.504456\n",
       "Average FLOPs Estimate (GFLOPs)          2.377871    2.377871\n",
       "NLU Expert 1 (percent)                   0.000000    0.000000\n",
       "NLU Expert 2 (percent)                   0.000000    0.000000\n",
       "NLU Expert 3 (percent)                   6.460272    6.460272\n",
       "NLU Expert 4 (percent)                  30.762348   30.762348\n",
       "NLU Expert 5 (percent)                  15.479599   15.479599\n",
       "NLU Expert 6 (percent)                   1.556908    1.556908\n",
       "NLU Expert 7 (percent)                  18.647101   18.647101\n",
       "NLU Expert 8 (percent)                  27.093772   27.093772\n",
       "NLG Expert 1 (percent)                   0.000000    0.000000\n",
       "NLG Expert 2 (percent)                   0.000000    0.000000\n",
       "NLG Expert 3 (percent)                  17.272888   17.272888\n",
       "NLG Expert 4 (percent)                  15.876772   15.876772\n",
       "NLG Expert 5 (percent)                  31.747935   31.747935\n",
       "NLG Expert 6 (percent)                   4.524087    4.524087\n",
       "NLG Expert 7 (percent)                  13.059434   13.059434\n",
       "NLG Expert 8 (percent)                  17.518884   17.518884\n",
       "Intent F1-score (Micro)                  0.991031    0.991031\n",
       "Intent F1-score (Macro)                  0.929733    0.929733\n",
       "Intent F1-score (Weighted)               0.987940    0.987940\n",
       "Intent Accuracy                          0.981747    0.981747\n",
       "Domain Accuracy                          1.000000    1.000000\n",
       "Domain F1-score (Macro)                  1.000000    1.000000\n",
       "BLEU Score                               0.001968    0.001968\n",
       "ROUGE-1 F1                               0.167277    0.167277\n",
       "ROUGE-2 F1                               0.014000    0.014000\n",
       "ROUGE-L F1                               0.140349    0.140349\n",
       "METEOR Score                             0.119921    0.119921\n",
       "Perplexity                              33.297123   33.297123\n",
       "NLU Expert Load Stability (Variance)   132.735748  132.735748\n",
       "NLG Expert Load Stability (Variance)   100.785427  100.785427"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tf_datasets_from_disk(load_path):\n",
    "    try:\n",
    "        with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "            loaded_moe_params = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "    element_spec = (\n",
    "        {\n",
    "            'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "        },\n",
    "        {\n",
    "            'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "            'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "        test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "    raw_data = {}\n",
    "    for dataset_name in ['train', 'test']:\n",
    "        path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                raw_data[dataset_name] = pickle.load(f)\n",
    "        else:\n",
    "            raw_data[dataset_name] = []\n",
    "\n",
    "    return {\n",
    "        \"train_dataset\": train_tf_dataset,\n",
    "        \"test_dataset\": test_tf_dataset,\n",
    "        \"moe_params\": loaded_moe_params,\n",
    "        \"raw_data\": raw_data\n",
    "    }\n",
    "\n",
    "tf_dataset_save_path = \"tf_datasets\"\n",
    "loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "test_tf_dataset = loaded_data[\"test_dataset\"]\n",
    "moe_params = loaded_data[\"moe_params\"]\n",
    "raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "moe_params[\"num_experts\"] = 4\n",
    "\n",
    "class NoisyTopKGating(layers.Layer):\n",
    "    def __init__(self, num_experts, input_dim, k=2, noise_epsilon=1.0, name=None):\n",
    "        super(NoisyTopKGating, self).__init__(name=name)\n",
    "        self.num_experts = num_experts\n",
    "        self.input_dim = input_dim\n",
    "        self.k = k\n",
    "        self.noise_epsilon = noise_epsilon\n",
    "        self.w_g = self.add_weight(\n",
    "            name='w_g',\n",
    "            shape=(input_dim, num_experts),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.w_noise = self.add_weight(\n",
    "            name='w_noise',\n",
    "            shape=(input_dim, num_experts),\n",
    "            initializer=keras.initializers.Constant(value=noise_epsilon),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.softmax = layers.Softmax(axis=-1)\n",
    "\n",
    "    def call(self, inputs, training=False, usage_stats=None):\n",
    "        gate_logits = tf.matmul(inputs, self.w_g)\n",
    "        if training and usage_stats is not None:\n",
    "            usage_mean = tf.reduce_mean(usage_stats)\n",
    "            usage_std = tf.math.reduce_std(usage_stats)\n",
    "            imbalance_factor = tf.clip_by_value(usage_std / (usage_mean + 1e-6), 0.0, 1.0)\n",
    "            adjusted_k = tf.cast(\n",
    "                self.k * (1.0 + imbalance_factor * 2),\n",
    "                tf.int32\n",
    "            )\n",
    "            adjusted_k = tf.minimum(tf.maximum(adjusted_k, self.k), self.num_experts)\n",
    "        else:\n",
    "            adjusted_k = self.k\n",
    "        if training:\n",
    "            raw_noise = tf.random.normal(\n",
    "                shape=(tf.shape(inputs)[0], self.num_experts),\n",
    "                mean=0.0,\n",
    "                stddev=1.0\n",
    "            )\n",
    "            gate_std = tf.math.reduce_std(gate_logits, axis=-1, keepdims=True)\n",
    "            noise_scale = tf.nn.softplus(tf.matmul(inputs, self.w_noise)) * (1.0 + gate_std)\n",
    "            noisy_gate_logits = gate_logits + (raw_noise * noise_scale)\n",
    "        else:\n",
    "            noisy_gate_logits = gate_logits\n",
    "        top_k_values, _ = tf.math.top_k(noisy_gate_logits, k=adjusted_k)\n",
    "        min_values = tf.reduce_min(top_k_values, axis=-1, keepdims=True)\n",
    "        top_k_mask = tf.cast(\n",
    "            noisy_gate_logits >= min_values,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        keep_top_k_logits = tf.where(\n",
    "            top_k_mask > 0,\n",
    "            noisy_gate_logits,\n",
    "            tf.ones_like(noisy_gate_logits) * -float('inf')\n",
    "        )\n",
    "        gate_outputs = self.softmax(keep_top_k_logits)\n",
    "        return gate_outputs, gate_logits\n",
    "\n",
    "class MoELayer(layers.Layer):\n",
    "    def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.5, top_k=2, name=None):\n",
    "        super(MoELayer, self).__init__(name=name)\n",
    "        self.m = m\n",
    "        self.k_s = k_s\n",
    "        self.total_experts = num_experts * self.m\n",
    "        self.routed_experts = self.total_experts - self.k_s\n",
    "        self.top_k = top_k\n",
    "        self.top_mk = self.m * self.top_k\n",
    "        self.top_mk = min(self.top_mk, self.routed_experts)\n",
    "        self.alpha = alpha\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = None\n",
    "        self.usage_stats = tf.Variable(\n",
    "            tf.ones(self.routed_experts) / self.routed_experts,\n",
    "            trainable=False,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        self.adjusted_expert_dim = expert_dim // self.m\n",
    "        self.shared_experts = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu'),\n",
    "                layers.Dense(input_dim, activation=None)\n",
    "            ], name=f'shared_expert_{i}') for i in range(self.k_s)\n",
    "        ]\n",
    "        self.routed_experts_list = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu'),\n",
    "                layers.Dense(input_dim, activation=None)\n",
    "            ], name=f'routed_expert_{i}') for i in range(self.k_s, self.total_experts)\n",
    "        ]\n",
    "        self.experts = self.shared_experts + self.routed_experts_list\n",
    "        self.gate = NoisyTopKGating(\n",
    "            num_experts=self.routed_experts,\n",
    "            input_dim=self.input_dim,\n",
    "            k=self.top_mk,\n",
    "            noise_epsilon=1.0,\n",
    "            name='noisy_topk_gate'\n",
    "        )\n",
    "        self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "        self.expert_usage_metric = tf.keras.metrics.Mean(name=f'{name}_expert_usage')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_rank = inputs.shape.rank\n",
    "        if input_rank == 3:\n",
    "            batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "            flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "            is_3d = True\n",
    "        else:\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            seq_length = 1\n",
    "            flat_inputs = inputs\n",
    "            is_3d = False\n",
    "        flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "        shared_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "        for i in range(self.k_s):\n",
    "            shared_output += self.shared_experts[i](flat_inputs)\n",
    "        gate_weights, clean_logits = self.gate(flat_inputs, training=training, usage_stats=self.usage_stats)\n",
    "        top_mk_values, top_mk_indices = tf.nn.top_k(gate_weights, k=self.top_mk, sorted=True)\n",
    "        top_mk_indices = top_mk_indices + self.k_s\n",
    "        top_mk_weights = top_mk_values / (tf.reduce_sum(top_mk_values, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "        routed_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "        for k in range(self.top_mk):\n",
    "            kth_weights = top_mk_weights[:, k]\n",
    "            kth_indices = top_mk_indices[:, k]\n",
    "            mask = tf.one_hot(kth_indices, depth=self.total_experts, dtype=tf.float32)\n",
    "            expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "            kth_expert_output = tf.reduce_sum(expert_outputs * tf.expand_dims(mask, -1), axis=1)\n",
    "            weighted_kth_output = kth_expert_output * tf.expand_dims(kth_weights, -1)\n",
    "            routed_output += weighted_kth_output\n",
    "        output_flat = shared_output + routed_output + flat_inputs\n",
    "        if is_3d:\n",
    "            output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "            gate_weights_reshaped = tf.reshape(gate_weights, [batch_size, seq_length, self.routed_experts])\n",
    "        else:\n",
    "            output = output_flat\n",
    "            gate_weights_reshaped = gate_weights\n",
    "        expert_assignments = tf.argmax(clean_logits, axis=-1)\n",
    "        f_i = tf.reduce_mean(\n",
    "            tf.one_hot(expert_assignments, depth=self.routed_experts, dtype=tf.float32),\n",
    "            axis=0\n",
    "        )\n",
    "        P_i = tf.reduce_mean(gate_weights, axis=0)\n",
    "        cov = tf.reduce_sum((f_i - tf.reduce_mean(f_i)) * (P_i - tf.reduce_mean(P_i)))\n",
    "        load_balancing_loss = self.alpha * (tf.reduce_sum(f_i * P_i) + 0.1 * cov)\n",
    "        self.add_loss(load_balancing_loss)\n",
    "        self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "        if training:\n",
    "            current_usage = tf.reduce_mean(\n",
    "                tf.one_hot(expert_assignments, depth=self.routed_experts, dtype=tf.float32),\n",
    "                axis=0\n",
    "            )\n",
    "            decay_rate = tf.minimum(0.1, 1.0 / (tf.reduce_sum(self.usage_stats) + 1e-6))\n",
    "            self.usage_stats.assign(\n",
    "                self.usage_stats * (1.0 - decay_rate) + current_usage * decay_rate\n",
    "            )\n",
    "        expert_usage = tf.reduce_mean(tf.reduce_sum(tf.cast(gate_weights > 0, tf.float32), axis=0))\n",
    "        self.expert_usage_metric.update_state(expert_usage)\n",
    "        return output, gate_weights_reshaped\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return {\n",
    "            f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric,\n",
    "            f'{self.name}_expert_usage': self.expert_usage_metric\n",
    "        }\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_experts': self.total_experts // self.m,\n",
    "            'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "            'input_dim': self.input_dim,\n",
    "            'm': self.m,\n",
    "            'k_s': self.k_s,\n",
    "            'alpha': self.alpha,\n",
    "            'top_k': self.top_k,\n",
    "            'name': self.name\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "        if look_ahead_mask is not None:\n",
    "            look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "            look_ahead_mask = 1.0 - look_ahead_mask\n",
    "        attn1_output = self.mha1(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            attention_mask=look_ahead_mask,\n",
    "            training=training,\n",
    "            return_attention_scores=True\n",
    "        )\n",
    "        if isinstance(attn1_output, tuple):\n",
    "            attn1, attn_weights1 = attn1_output\n",
    "        else:\n",
    "            attn1, attn_weights1 = attn1_output, None\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        attn2_output = self.mha2(\n",
    "            query=out1,\n",
    "            value=enc_output,\n",
    "            key=enc_output,\n",
    "            attention_mask=None,\n",
    "            training=training,\n",
    "            return_attention_scores=True\n",
    "        )\n",
    "        if isinstance(attn2_output, tuple):\n",
    "            attn2, attn_weights2 = attn2_output\n",
    "        else:\n",
    "            attn2, attn_weights2 = attn2_output, None\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        return out3, attn_weights1, attn_weights2\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        mha1_config = self.mha1.get_config()\n",
    "        config.update({\n",
    "            'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "            'num_heads': mha1_config['num_heads'],\n",
    "            'dff': self.ffn.layers[0].units,\n",
    "            'rate': self.dropout1.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MoEModel(models.Model):\n",
    "    def __init__(self, moe_params):\n",
    "        super(MoEModel, self).__init__()\n",
    "        self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "        self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "        self.num_domains = moe_params[\"num_domains\"]\n",
    "        self.num_intents = moe_params[\"num_intents\"]\n",
    "        self.vocab_size = moe_params[\"vocab_size\"]\n",
    "        self.num_experts = moe_params[\"num_experts\"]\n",
    "        self.expert_dim = moe_params[\"expert_dim\"]\n",
    "        self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "        self.num_heads = 4\n",
    "        self.dff = self.embedding_dim * 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                             self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "        self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "        self.embedding = layers.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "            name=\"embedding\"\n",
    "        )\n",
    "        self.embedding.build((None,))\n",
    "        with tf.init_scope():\n",
    "            embedding_weights = self.embedding.get_weights()\n",
    "            if embedding_weights and len(embedding_weights) > 0:\n",
    "                embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                self.embedding.set_weights(embedding_weights)\n",
    "        self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "        self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "        self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "        self.moe_nlu = MoELayer(\n",
    "            num_experts=self.num_experts,\n",
    "            expert_dim=self.expert_dim,\n",
    "            input_dim=self.nlu_input_dim,\n",
    "            m=2,\n",
    "            k_s=2,\n",
    "            alpha=0.01,\n",
    "            top_k=2,\n",
    "            name='moe_nlu'\n",
    "        )\n",
    "        self.moe_nlg = MoELayer(\n",
    "            num_experts=self.num_experts,\n",
    "            expert_dim=self.expert_dim,\n",
    "            input_dim=self.nlg_input_dim,\n",
    "            m=2,\n",
    "            k_s=2,\n",
    "            alpha=0.01,\n",
    "            top_k=2,\n",
    "            name='moe_nlg'\n",
    "        )\n",
    "        self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "        self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "        self.response_output = layers.TimeDistributed(\n",
    "            layers.Dense(self.vocab_size, activation='softmax'), name='response_output'\n",
    "        )\n",
    "        self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "        self.nlg_projection = layers.TimeDistributed(\n",
    "            layers.Dense(self.embedding_dim, activation='relu', name='nlg_projection')\n",
    "        )\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "        prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "        decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "        domain_onehot_input = inputs['domain_onehot_input']\n",
    "        turn_id_embedding = inputs['turn_id_embedding']\n",
    "        ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "        pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "        user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "        prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "        decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "        user_enc_out = user_embedded\n",
    "        prev_system_enc_out = prev_system_embedded\n",
    "        look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "        dec_output = decoder_embedded\n",
    "        attn_weights = {}\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            dec_output, attn_w1, attn_w2 = layer(\n",
    "                dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask\n",
    "            )\n",
    "            attn_weights[f'decoder_layer{i+1}_attn1'] = attn_w1\n",
    "            attn_weights[f'decoder_layer{i+1}_attn2'] = attn_w2\n",
    "        decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "        user_attn_out = self.attn_layer(\n",
    "            query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), \n",
    "            value=user_enc_out, key=user_enc_out, training=training\n",
    "        )\n",
    "        prev_system_attn_out = self.attn_layer(\n",
    "            query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), \n",
    "            value=prev_system_enc_out, key=prev_system_enc_out, training=training\n",
    "        )\n",
    "        decoder_attn_out = self.attn_layer(\n",
    "            query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), \n",
    "            value=decoder_output, key=decoder_output, training=training\n",
    "        )\n",
    "        combined_features = layers.Concatenate()([\n",
    "            tf.squeeze(user_attn_out, 1),\n",
    "            tf.squeeze(prev_system_attn_out, 1),\n",
    "            tf.squeeze(decoder_attn_out, 1),\n",
    "            domain_onehot_input,\n",
    "            turn_id_embedding,\n",
    "            ontology_multihot_input\n",
    "        ])\n",
    "        combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "        nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "        nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "        intent_out = self.intent_output(nlu_out)\n",
    "        domain_out = self.domain_output(nlu_out)\n",
    "        intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "        domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "        intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "        domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "        nlg_input = layers.Concatenate(axis=-1)([\n",
    "            decoder_output,\n",
    "            intent_out_tiled,\n",
    "            domain_out_tiled\n",
    "        ])\n",
    "        nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "        nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        response_embeddings = self.nlg_projection(nlg_out)\n",
    "        response_embeddings = tf.ensure_shape(response_embeddings, [None, self.max_seq_length, self.embedding_dim])\n",
    "        response_out = self.response_output(nlg_out)\n",
    "        return {\n",
    "            'intent_output': intent_out,\n",
    "            'domain_output': domain_out,\n",
    "            'response_output': response_out,\n",
    "            'response_embeddings': response_embeddings,\n",
    "            'nlu_gate_weights': nlu_gate_weights,\n",
    "            'nlg_gate_weights': nlg_gate_weights\n",
    "        }\n",
    "\n",
    "    def build_graph(self):\n",
    "        inputs = {\n",
    "            'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "            'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='prev_system_response_tokens'),\n",
    "            'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='decoder_input_tokens'),\n",
    "            'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32, name='domain_onehot_input'),\n",
    "            'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32, name='turn_id_embedding'),\n",
    "            'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32, name='ontology_multihot_input'),\n",
    "        }\n",
    "        return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'moe_params': {\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'max_seq_length': self.max_seq_length,\n",
    "                'num_domains': self.num_domains,\n",
    "                'num_intents': self.num_intents,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'num_experts': self.num_experts,\n",
    "                'expert_dim': self.expert_dim,\n",
    "                'turn_id_dim': self.turn_id_dim\n",
    "            }\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.val_moe_nlu_load_balancing_loss_metric = tf.keras.metrics.Mean(name='val_moe_nlu_load_balancing_loss')\n",
    "        self.val_moe_nlg_load_balancing_loss_metric = tf.keras.metrics.Mean(name='val_moe_nlg_load_balancing_loss')\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        y_pred = self(x, training=False)\n",
    "        self.compiled_loss(y, y_pred)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        nlu_gate_weights = y_pred['nlu_gate_weights']\n",
    "        nlg_gate_weights = y_pred['nlg_gate_weights']\n",
    "        nlu_expert_assignments = tf.argmax(nlu_gate_weights, axis=-1)\n",
    "        nlu_f_i = tf.reduce_mean(\n",
    "            tf.one_hot(nlu_expert_assignments, depth=self.moe_nlu.routed_experts, dtype=tf.float32),\n",
    "            axis=0\n",
    "        )\n",
    "        nlu_P_i = tf.reduce_mean(nlu_gate_weights, axis=0)\n",
    "        nlu_load_balancing_loss = self.moe_nlu.alpha * tf.reduce_sum(nlu_f_i * nlu_P_i)\n",
    "        self.val_moe_nlu_load_balancing_loss_metric.update_state(nlu_load_balancing_loss)\n",
    "        nlg_expert_assignments = tf.argmax(nlg_gate_weights, axis=-1)\n",
    "        nlg_f_i = tf.reduce_mean(\n",
    "            tf.one_hot(nlg_expert_assignments, depth=self.moe_nlg.routed_experts, dtype=tf.float32),\n",
    "            axis=0\n",
    "        )\n",
    "        nlg_P_i = tf.reduce_mean(nlg_gate_weights, axis=0)\n",
    "        nlg_load_balancing_loss = self.moe_nlg.alpha * tf.reduce_sum(nlg_f_i * nlg_P_i)\n",
    "        self.val_moe_nlg_load_balancing_loss_metric.update_state(nlg_load_balancing_loss)\n",
    "        return {m.name: m.result() for m in self.metrics + [self.val_moe_nlu_load_balancing_loss_metric, self.val_moe_nlg_load_balancing_loss_metric]}\n",
    "\n",
    "class IntentAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros', dtype=tf.float32)\n",
    "        self.total_preds = self.add_weight(name='total_preds', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "        self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "        self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.correct_preds.assign(0.)\n",
    "        self.total_preds.assign(0.)\n",
    "\n",
    "class IntentPrecision(metrics.Metric):\n",
    "    def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "        self.predicted_positives = self.add_weight(name='pp', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        predicted_positives = tf.reduce_sum(y_pred)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.predicted_positives.assign(0.)\n",
    "\n",
    "class IntentRecall(metrics.Metric):\n",
    "    def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "        self.actual_positives = self.add_weight(name='ap', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        actual_positives = tf.reduce_sum(y_true)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.actual_positives.assign(0.)\n",
    "\n",
    "class IntentF1(metrics.Metric):\n",
    "    def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = IntentPrecision(threshold=threshold)\n",
    "        self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()\n",
    "\n",
    "class DomainAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='domain_accuracy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.accuracy = self.add_weight(name='acc', initializer='zeros', dtype=tf.float32)\n",
    "        self.count = self.add_weight(name='count', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "        pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "        matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "        self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.accuracy.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "class Perplexity(metrics.Metric):\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "        self.total_loss = self.add_weight(name='total_loss', initializer='zeros', dtype=tf.float32)\n",
    "        self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "        loss = self.cross_entropy(y_true, y_pred)\n",
    "        masked_loss = loss * mask\n",
    "        sum_loss = tf.reduce_sum(masked_loss)\n",
    "        num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "        self.total_loss.assign_add(sum_loss)\n",
    "        self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "    def result(self):\n",
    "        avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "        return tf.exp(avg_loss)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_loss.assign(0.)\n",
    "        self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "    def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "        y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "        cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "        non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "        non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "        masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "        num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "        avg_cosine_sim_per_sample = tf.where(\n",
    "            num_non_zero_tokens > 0,\n",
    "            tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "            tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "        )\n",
    "        self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_cosine_sim.assign(0.)\n",
    "        self.num_samples.assign(0.)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "    y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "    y_true = y_true / (y_true_norm + epsilon)\n",
    "    y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "    cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    positive_loss = 1.0 - cosine_sim\n",
    "    mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "    masked_loss = positive_loss * mask\n",
    "    total_loss = tf.reduce_sum(masked_loss)\n",
    "    num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "    return total_loss / num_tokens\n",
    "\n",
    "def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "    flops = 0\n",
    "    functional_model = model.build_graph()\n",
    "    def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "        layer_flops_count = 0\n",
    "        if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "            return 0\n",
    "        if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "            for sub_layer in layer_instance.layers:\n",
    "                layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "            return layer_flops_count\n",
    "        if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "            return 0\n",
    "        if isinstance(layer_instance, layers.Dense):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            output_dim = layer_instance.output_shape[-1]\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "        elif isinstance(layer_instance, layers.LSTM):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            hidden_dim = layer_instance.units\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "            layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "        elif isinstance(layer_instance, layers.Embedding):\n",
    "            pass\n",
    "        elif isinstance(layer_instance, MoELayer):\n",
    "            moe_input_dim = layer_instance.input_dim\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "            layer_flops_count += gate_flops\n",
    "            for expert in layer_instance.experts:\n",
    "                layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "        elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "            config = layer_instance.get_config()\n",
    "            num_heads = config['num_heads']\n",
    "            key_dim = config['key_dim']\n",
    "            d_model = num_heads * key_dim\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "            projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "            attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "            context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "            output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "            layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "        elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "            inner_layer = layer_instance.layer\n",
    "            td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "            layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "        elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "            ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "        return layer_flops_count\n",
    "    for layer in functional_model.layers:\n",
    "        flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "    return flops / 1e9\n",
    "\n",
    "def get_gpu_stats():\n",
    "    if nvidia_smi is None:\n",
    "        return 0, 0\n",
    "    try:\n",
    "        nvidia_smi.nvmlInit()\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "        mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_memory = mem_info.used / (1024 ** 2)\n",
    "        nvidia_smi.nvmlShutdown()\n",
    "        return gpu_load, gpu_memory / 1024\n",
    "    except Exception as e:\n",
    "        return 0, 0\n",
    "\n",
    "class ResourceMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.resource_stats = []\n",
    "        self.expert_load_history = []\n",
    "        self.start_time = None\n",
    "        self.epoch_start_time = None\n",
    "        self.flops = None\n",
    "        self.validation_data = validation_data\n",
    "        self.nlu_expert_usage_counts = None\n",
    "        self.nlg_expert_usage_counts = None\n",
    "        self.nlu_gate_weights_history = []\n",
    "        self.nlg_gate_weights_history = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        try:\n",
    "            self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "            self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "            self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "        except Exception:\n",
    "            self.flops = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory()\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.resource_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'epoch_time': epoch_time,\n",
    "            'cpu_usage': cpu_usage,\n",
    "            'memory_used': memory.used / (1024 ** 3),\n",
    "            'memory_total': memory.total / (1024 ** 3),\n",
    "            'gpu_load': gpu_load,\n",
    "            'gpu_memory': gpu_memory,\n",
    "            'loss': logs.get('loss', 0),\n",
    "            'val_loss': logs.get('val_loss', 0),\n",
    "            'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "            'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "            'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "            'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "            'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "            'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "            'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "            'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "            'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "            'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "            'perplexity': logs.get('response_output_perplexity', 0),\n",
    "            'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "            'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "            'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "        })\n",
    "        if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "            self.expert_load_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "            })\n",
    "        sample_size = 100\n",
    "        for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "            inputs, _ = batch\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "            reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "            self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "            nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "            reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "            self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.total_time = time.time() - self.start_time\n",
    "        if len(self.nlu_gate_weights_history) > 0:\n",
    "            all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "            nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "            for expert_id in nlu_top_indices:\n",
    "                if expert_id < self.model.moe_nlu.total_experts:\n",
    "                    self.nlu_expert_usage_counts[expert_id] += 1\n",
    "        if len(self.nlg_gate_weights_history) > 0:\n",
    "            all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "            nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "            for expert_id in nlg_top_indices:\n",
    "                if expert_id < self.model.moe_nlg.total_experts:\n",
    "                    self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "    def visualize_expert_load_distribution(self):\n",
    "        total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "        total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "        nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "        nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "        nlu_stability = np.var(nlu_percentages)\n",
    "        nlg_stability = np.var(nlg_percentages)\n",
    "        return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\": self.d_model.numpy(),\n",
    "            \"warmup_steps\": self.warmup_steps\n",
    "        }\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "try:\n",
    "    possible_paths = [\n",
    "        os.path.join(\"preprocessor_models\", \"preprocessor_params.json\"),\n",
    "        os.path.join(\"tf_datasets\", \"preprocessor_params.json\"),\n",
    "        \"preprocessor_params.json\"\n",
    "    ]\n",
    "    vocab_loaded = False\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                preprocessor_params = json.load(f)\n",
    "            word_to_id = {k: int(v) for k, v in preprocessor_params['word_to_id'].items()}\n",
    "            id_to_word = {int(k): v for k, v in preprocessor_params['id_to_word'].items()}\n",
    "            vocab_loaded = True\n",
    "            break\n",
    "    if not vocab_loaded:\n",
    "        raise FileNotFoundError(\"Vocabulary file not found in any location\")\n",
    "except Exception as e:\n",
    "    word_to_id = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    id_to_word = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
    "\n",
    "model_save_path = \"best_noisytop-K_moe_model\"\n",
    "custom_objects = {\n",
    "    \"NoisyTopKGating\":NoisyTopKGating,\n",
    "    \"MoEModel\":MoEModel,\n",
    "    \"CustomLearningRateSchedule\": CustomLearningRateSchedule,\n",
    "    \"MoELayer\": MoELayer,\n",
    "    \"TransformerDecoderLayer\": TransformerDecoderLayer,\n",
    "    \"IntentAccuracy\": IntentAccuracy,\n",
    "    \"IntentPrecision\": IntentPrecision,\n",
    "    \"IntentRecall\": IntentRecall,\n",
    "    \"IntentF1\": IntentF1,\n",
    "    \"DomainAccuracy\": DomainAccuracy,\n",
    "    \"Perplexity\": Perplexity,\n",
    "    \"ResponseEmbeddingCosineSimilarity\": ResponseEmbeddingCosineSimilarity\n",
    "}\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "except Exception as e:\n",
    "    raise\n",
    "\n",
    "def compile_model(model, moe_params):\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "    losses_dict = {\n",
    "        'intent_output': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'domain_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss,\n",
    "    }\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={\n",
    "            'intent_output': 0.5,\n",
    "            'domain_output': 0.5,\n",
    "            'response_output': 1.0,\n",
    "            'response_embeddings': 1.0\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = compile_model(model, moe_params)\n",
    "sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "model(sample_input[0])\n",
    "model.summary()\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self, model, test_dataset, moe_params, raw_data, word_to_id, id_to_word):\n",
    "        self.model = model\n",
    "        self.test_dataset = test_dataset\n",
    "        self.moe_params = moe_params\n",
    "        self.raw_data = raw_data\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.nlu_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.nlg_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.resource_stats = {\n",
    "            'cpu_usage': [],\n",
    "            'memory_used': [],\n",
    "            'gpu_load': [],\n",
    "            'gpu_memory': [],\n",
    "            'batch_times': []\n",
    "        }\n",
    "        self.special_tokens = {'<pad>', '<sos>', '<eos>', '<unk>'}\n",
    "        self.nlu_top_k = model.get_layer('moe_nlu').get_config().get('top_k', 2)\n",
    "        self.nlg_top_k = model.get_layer('moe_nlg').get_config().get('top_k', 2)\n",
    "\n",
    "    def evaluate(self):\n",
    "        def add_response_embeddings(features, targets):\n",
    "            response_tokens = targets['response_output']\n",
    "            response_embeddings = self.model.embedding(response_tokens)\n",
    "            targets['response_embeddings'] = response_embeddings\n",
    "            return features, targets\n",
    "        test_dataset_with_embeddings = self.test_dataset.map(add_response_embeddings, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        start_time = time.time()\n",
    "        test_results = self.model.evaluate(test_dataset_with_embeddings, verbose=1)\n",
    "        eval_time = time.time() - start_time\n",
    "        metric_names = self.model.metrics_names\n",
    "        perplexity_value = None\n",
    "        for name, value in zip(metric_names, test_results):\n",
    "            if name == 'response_output_perplexity':\n",
    "                perplexity_value = value\n",
    "                break\n",
    "        if perplexity_value is None:\n",
    "            perplexity_value = 0.0\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory().used / (1024 ** 3)\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "        total_tokens = 0\n",
    "        total_time = 0\n",
    "        num_batches = 0\n",
    "        test_dataset_small = test_dataset_with_embeddings.unbatch().batch(moe_params[\"batch_size\"])\n",
    "        all_true_intents = []\n",
    "        all_pred_intents = []\n",
    "        all_true_domains = []\n",
    "        all_pred_domains = []\n",
    "        for batch_idx, batch in enumerate(test_dataset_small):\n",
    "            inputs, targets = batch\n",
    "            response_tokens = targets['response_output'].numpy()\n",
    "            non_padding_mask = response_tokens != 0\n",
    "            total_tokens += np.sum(non_padding_mask)\n",
    "            start_time = time.time()\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            batch_time = time.time() - start_time\n",
    "            total_time += batch_time\n",
    "            num_batches += tf.shape(inputs['user_utterance_tokens'])[0]\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            self.resource_stats['cpu_usage'].append(cpu_usage)\n",
    "            self.resource_stats['memory_used'].append(memory.used / (1024 ** 3))\n",
    "            self.resource_stats['gpu_load'].append(gpu_load)\n",
    "            self.resource_stats['gpu_memory'].append(gpu_memory)\n",
    "            self.resource_stats['batch_times'].append(batch_time)\n",
    "            nlu_gate_weights = outputs.get('nlu_gate_weights', tf.zeros([0, self.model.moe_nlu.routed_experts])).numpy()\n",
    "            nlu_top_indices = np.argsort(-nlu_gate_weights, axis=-1)[:, :self.nlu_top_k] + self.model.moe_nlu.k_s\n",
    "            for idx in range(self.nlu_top_k):\n",
    "                expert_ids = nlu_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlu_expert_usage_counts):\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "            nlg_gate_weights = outputs.get('nlg_gate_weights', tf.zeros([0, self.model.moe_nlg.routed_experts])).numpy()\n",
    "            if len(nlg_gate_weights.shape) == 3:\n",
    "                nlg_gate_weights = nlg_gate_weights.reshape(-1, nlg_gate_weights.shape[-1])\n",
    "            nlg_top_indices = np.argsort(-nlg_gate_weights, axis=-1)[:, :self.nlg_top_k] + self.model.moe_nlg.k_s\n",
    "            for idx in range(self.nlg_top_k):\n",
    "                expert_ids = nlg_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlg_expert_usage_counts):\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "            true_intents = targets.get('intent_output', np.zeros((moe_params[\"batch_size\"], self.model.num_intents))).numpy()\n",
    "            pred_intents = (outputs.get('intent_output', np.zeros_like(true_intents)).numpy() > 0.5).astype(np.int32)\n",
    "            all_true_intents.append(true_intents)\n",
    "            all_pred_intents.append(pred_intents)\n",
    "            true_domains = targets.get('domain_output', np.zeros((moe_params[\"batch_size\"], 1))).numpy().flatten()\n",
    "            pred_domains = np.argmax(outputs.get('domain_output', np.zeros((moe_params[\"batch_size\"], self.model.num_domains))).numpy(), axis=-1)\n",
    "            all_true_domains.append(true_domains)\n",
    "            all_pred_domains.append(pred_domains)\n",
    "            if batch_idx % 50 == 0:\n",
    "                gc.collect()\n",
    "                tf.keras.backend.clear_session()\n",
    "        avg_time_per_token = (total_time / total_tokens) * 1000 if total_tokens > 0 else 0\n",
    "        avg_cpu = np.mean(self.resource_stats['cpu_usage']) if self.resource_stats['cpu_usage'] else 0\n",
    "        avg_memory = np.mean(self.resource_stats['memory_used']) if self.resource_stats['memory_used'] else 0\n",
    "        avg_gpu_load = np.mean(self.resource_stats['gpu_load']) if self.resource_stats['gpu_load'] else 0\n",
    "        avg_gpu_memory = np.mean(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        peak_gpu_memory = np.max(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "        all_true_intents = np.vstack(all_true_intents)\n",
    "        all_pred_intents = np.vstack(all_pred_intents)\n",
    "        all_true_domains = np.concatenate(all_true_domains)\n",
    "        all_pred_domains = np.concatenate(all_pred_domains)\n",
    "        intent_f1_score = f1_score(all_true_intents, all_pred_intents, average='micro')\n",
    "        intent_f1_score_macro = f1_score(all_true_intents, all_pred_intents, average='macro')\n",
    "        intent_f1_score_weighted = f1_score(all_true_intents, all_pred_intents, average='weighted')\n",
    "        intent_accuracy_score = accuracy_score(all_true_intents, all_pred_intents)\n",
    "        domain_accuracy_score = accuracy_score(all_true_domains, all_pred_domains)\n",
    "        domain_f1_score_macro = f1_score(all_true_domains, all_pred_domains, average='macro')\n",
    "        hypotheses = []\n",
    "        references = []\n",
    "        meteor_scores = []\n",
    "        rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        domain_metrics = defaultdict(lambda: {\n",
    "            'bleu': [],\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'meteor': [],\n",
    "            'count': 0\n",
    "        })\n",
    "        if not self.raw_data.get('test'):\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "            domain_metrics = {}\n",
    "        else:\n",
    "            for i, batch in enumerate(test_dataset_small):\n",
    "                if i >= len(self.raw_data.get('test', [])):\n",
    "                    break\n",
    "                try:\n",
    "                    inputs, targets = batch\n",
    "                    with tf.device('/CPU:0'):\n",
    "                        outputs = self.model(inputs, training=False)\n",
    "                    domain_id = targets['domain_output'].numpy()[0][0]\n",
    "                    domain_name = f\"domain_{domain_id}\"\n",
    "                    response_probs = outputs['response_output'].numpy()\n",
    "                    generated_tokens = np.argmax(response_probs, axis=-1)[0]\n",
    "                    true_tokens = targets['response_output'].numpy()[0]\n",
    "                    raw_item = self.raw_data['test'][i]\n",
    "                    ref_text = raw_item.get('raw_next_system_response', '')\n",
    "                    if not ref_text:\n",
    "                        continue\n",
    "                    gen_text = self.tokens_to_text(generated_tokens)\n",
    "                    ref_tokens = self.tokens_to_text(true_tokens)\n",
    "                    if not gen_text.strip() or not ref_text.strip():\n",
    "                        continue\n",
    "                    hypotheses.append(gen_text)\n",
    "                    references.append([ref_text])\n",
    "                    ref_tokens = word_tokenize(ref_text.lower())\n",
    "                    gen_tokens = word_tokenize(gen_text.lower())\n",
    "                    try:\n",
    "                        meteor = meteor_score([ref_tokens], gen_tokens)\n",
    "                        meteor_scores.append(meteor)\n",
    "                        domain_metrics[domain_name]['meteor'].append(meteor)\n",
    "                    except Exception as e:\n",
    "                        meteor_scores.append(0)\n",
    "                        domain_metrics[domain_name]['meteor'].append(0)\n",
    "                    try:\n",
    "                        rouge_scores = rouge_scorer_instance.score(ref_text, gen_text)\n",
    "                        domain_metrics[domain_name]['rouge1'].append(rouge_scores['rouge1'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rouge2'].append(rouge_scores['rouge2'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rougeL'].append(rouge_scores['rougeL'].fmeasure)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                    domain_metrics[domain_name]['count'] += 1\n",
    "                    if i % 50 == 0:\n",
    "                        gc.collect()\n",
    "                        tf.keras.backend.clear_session()\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        if hypotheses and references:\n",
    "            hypotheses_tok = []\n",
    "            references_tok = []\n",
    "            for hyp, ref_list in zip(hypotheses, references):\n",
    "                if not hyp or not ref_list[0]:\n",
    "                    continue\n",
    "                hyp_tokens = word_tokenize(hyp.lower())\n",
    "                ref_tokens = [word_tokenize(ref.lower()) for ref in ref_list]\n",
    "                hypotheses_tok.append(hyp_tokens)\n",
    "                references_tok.append(ref_tokens)\n",
    "            chencherry = SmoothingFunction()\n",
    "            try:\n",
    "                bleu_score = corpus_bleu(references_tok, hypotheses_tok, smoothing_function=chencherry.method1)\n",
    "            except Exception as e:\n",
    "                bleu_score = 0.0\n",
    "            rouge_scores = []\n",
    "            for hyp, ref in zip(hypotheses, references):\n",
    "                if hyp and ref[0]:\n",
    "                    try:\n",
    "                        rouge_scores.append(rouge_scorer_instance.score(ref[0], hyp))\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "            avg_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_meteor = np.mean(meteor_scores) if meteor_scores else 0.0\n",
    "        else:\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "        return {\n",
    "            'intent_f1_micro': intent_f1_score,\n",
    "            'intent_f1_macro': intent_f1_score_macro,\n",
    "            'intent_f1_weighted': intent_f1_score_weighted,\n",
    "            'intent_accuracy': intent_accuracy_score,\n",
    "            'domain_accuracy': domain_accuracy_score,\n",
    "            'domain_f1_macro': domain_f1_score_macro,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge1': avg_rouge1,\n",
    "            'rouge2': avg_rouge2,\n",
    "            'rougeL': avg_rougeL,\n",
    "            'meteor': avg_meteor,\n",
    "            'perplexity': perplexity_value,\n",
    "            'pred_speed': avg_time_per_token,\n",
    "            'domain_metrics': domain_metrics,\n",
    "            'eval_time': eval_time\n",
    "        }\n",
    "    \n",
    "    def tokens_to_text(self, tokens):\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            if token == 0:\n",
    "                continue\n",
    "            word = self.id_to_word.get(token, '<unk>')\n",
    "            if word not in self.special_tokens:\n",
    "                words.append(word)\n",
    "        return \" \".join(words).strip()\n",
    "\n",
    "results_table = {\n",
    "    'Evaluation Time (seconds)': [],\n",
    "    'Prediction Speed (ms/token)': [],\n",
    "    'Average CPU Usage (percent)': [],\n",
    "    'Average GPU Usage (percent)': [],\n",
    "    'Average Memory (GB)': [],\n",
    "    'Average GPU Memory (GB)': [],\n",
    "    'Average FLOPs Estimate (GFLOPs)': [],\n",
    "    'NLU Expert 1 (percent)': [],\n",
    "    'NLU Expert 2 (percent)': [],\n",
    "    'NLU Expert 3 (percent)': [],\n",
    "    'NLU Expert 4 (percent)': [],\n",
    "    'NLU Expert 5 (percent)': [],\n",
    "    'NLU Expert 6 (percent)': [],\n",
    "    'NLU Expert 7 (percent)': [],\n",
    "    'NLU Expert 8 (percent)': [],\n",
    "    'NLG Expert 1 (percent)': [],\n",
    "    'NLG Expert 2 (percent)': [],\n",
    "    'NLG Expert 3 (percent)': [],\n",
    "    'NLG Expert 4 (percent)': [],\n",
    "    'NLG Expert 5 (percent)': [],\n",
    "    'NLG Expert 6 (percent)': [],\n",
    "    'NLG Expert 7 (percent)': [],\n",
    "    'NLG Expert 8 (percent)': [],\n",
    "    'Intent F1-score (Micro)': [],\n",
    "    'Intent F1-score (Macro)': [],\n",
    "    'Intent F1-score (Weighted)': [],\n",
    "    'Intent Accuracy': [],\n",
    "    'Domain Accuracy': [],\n",
    "    'Domain F1-score (Macro)': [],\n",
    "    'BLEU Score': [],\n",
    "    'ROUGE-1 F1': [],\n",
    "    'ROUGE-2 F1': [],\n",
    "    'ROUGE-L F1': [],\n",
    "    'METEOR Score': [],\n",
    "    'Perplexity': [],\n",
    "    'NLU Expert Load Stability (Variance)': [],\n",
    "    'NLG Expert Load Stability (Variance)': []\n",
    "}\n",
    "\n",
    "num_iterations = 5\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "    model = compile_model(model, moe_params)\n",
    "    evaluator = EvaluationMetrics(model, test_tf_dataset, moe_params, raw_data, word_to_id, id_to_word)\n",
    "    evaluation_results = evaluator.evaluate()\n",
    "    results_table['Evaluation Time (seconds)'].append(evaluation_results['eval_time'])\n",
    "    results_table['Prediction Speed (ms/token)'].append(evaluation_results['pred_speed'])\n",
    "    results_table['Average CPU Usage (percent)'].append(np.mean(evaluator.resource_stats['cpu_usage']) if evaluator.resource_stats['cpu_usage'] else 0)\n",
    "    results_table['Average GPU Usage (percent)'].append(np.mean(evaluator.resource_stats['gpu_load']) if evaluator.resource_stats['gpu_load'] else 0)\n",
    "    results_table['Average Memory (GB)'].append(np.mean(evaluator.resource_stats['memory_used']) if evaluator.resource_stats['memory_used'] else 0)\n",
    "    results_table['Average GPU Memory (GB)'].append(np.mean(evaluator.resource_stats['gpu_memory']) if evaluator.resource_stats['gpu_memory'] else 0)\n",
    "    results_table['Average FLOPs Estimate (GFLOPs)'].append(calculate_flops(model))\n",
    "    nlu_counts = evaluator.nlu_expert_usage_counts\n",
    "    nlg_counts = evaluator.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) if np.sum(nlu_counts) > 0 else 1\n",
    "    total_nlg = np.sum(nlg_counts) if np.sum(nlg_counts) > 0 else 1\n",
    "    results_table['NLU Expert 1 (percent)'].append((nlu_counts[0] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 2 (percent)'].append((nlu_counts[1] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 3 (percent)'].append((nlu_counts[2] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 4 (percent)'].append((nlu_counts[3] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 5 (percent)'].append((nlu_counts[4] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 6 (percent)'].append((nlu_counts[5] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 7 (percent)'].append((nlu_counts[6] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 8 (percent)'].append((nlu_counts[7] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLG Expert 1 (percent)'].append((nlg_counts[0] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 2 (percent)'].append((nlg_counts[1] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 3 (percent)'].append((nlg_counts[2] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 4 (percent)'].append((nlg_counts[3] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 5 (percent)'].append((nlg_counts[4] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 6 (percent)'].append((nlg_counts[5] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 7 (percent)'].append((nlg_counts[6] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 8 (percent)'].append((nlg_counts[7] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['Intent F1-score (Micro)'].append(evaluation_results['intent_f1_micro'])\n",
    "    results_table['Intent F1-score (Macro)'].append(evaluation_results['intent_f1_macro'])\n",
    "    results_table['Intent F1-score (Weighted)'].append(evaluation_results['intent_f1_weighted'])\n",
    "    results_table['Intent Accuracy'].append(evaluation_results['intent_accuracy'])\n",
    "    results_table['Domain Accuracy'].append(evaluation_results['domain_accuracy'])\n",
    "    results_table['Domain F1-score (Macro)'].append(evaluation_results['domain_f1_macro'])\n",
    "    results_table['BLEU Score'].append(evaluation_results['bleu'])\n",
    "    results_table['ROUGE-1 F1'].append(evaluation_results['rouge1'])\n",
    "    results_table['ROUGE-2 F1'].append(evaluation_results['rouge2'])\n",
    "    results_table['ROUGE-L F1'].append(evaluation_results['rougeL'])\n",
    "    results_table['METEOR Score'].append(evaluation_results['meteor'])\n",
    "    results_table['Perplexity'].append(evaluation_results['perplexity'])\n",
    "    results_table['NLU Expert Load Stability (Variance)'].append(np.var(nlu_counts / total_nlu * 100) if total_nlu > 0 else 0)\n",
    "    results_table['NLG Expert Load Stability (Variance)'].append(np.var(nlg_counts / total_nlg * 100) if total_nlg > 0 else 0)\n",
    "\n",
    "for key in results_table:\n",
    "    if results_table[key]:\n",
    "        results_table[key].append(np.mean(results_table[key]))\n",
    "    else:\n",
    "        results_table[key].append(0)\n",
    "\n",
    "final_df = pd.DataFrame(results_table)\n",
    "final_df = final_df.T\n",
    "final_df.columns = [f'Iteration {i+1}' for i in range(num_iterations)] + ['Average']\n",
    "final_df.to_excel('prediction_noisytop-kgating_results.xlsx', index=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE with Efficient Distributed Sparse Routing experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 07:17:03.965878: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 07:17:04.381808: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 07:17:04.381961: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 07:17:04.466307: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 07:17:04.613763: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 07:17:05.906511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training Iteration 1\n",
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 07:17:09.674440: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:17:09.970538: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:17:09.970824: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:17:09.972196: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:17:09.972400: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:17:09.972536: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:17:10.059915: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:17:10.060163: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:17:10.060313: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:17:10.060424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/4428 [00:00<?, ?it/s]2025-07-04 07:17:35.240570: I external/local_xla/xla/service/service.cc:168] XLA service 0xb54c840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 07:17:35.240648: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2025-07-04 07:17:35.258485: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 07:17:35.297694: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751613455.416348    9833 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 1: 100%|██████████| 4428/4428 [03:23<00:00, 21.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deepseekefficientdis_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deepseekefficientdis_model/assets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Iteration 2</th>\n",
       "      <th>Iteration 3</th>\n",
       "      <th>Iteration 4</th>\n",
       "      <th>Iteration 5</th>\n",
       "      <th>Iteration 6</th>\n",
       "      <th>Iteration 7</th>\n",
       "      <th>Iteration 8</th>\n",
       "      <th>Iteration 9</th>\n",
       "      <th>Iteration 10</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Speed (epochs per second)</th>\n",
       "      <td>0.005058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training Time (second/epoch)</th>\n",
       "      <td>197.697094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.769709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Training Time (second/iteration)</th>\n",
       "      <td>205.429722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.542972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computational Resource Usage</th>\n",
       "      <td>57.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>26.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>6.583912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.658391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.531799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.453180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPS Estimate (GFLOPS)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLU Load Distribution Stability</th>\n",
       "      <td>1093.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLG Load Distribution Stability</th>\n",
       "      <td>546.529517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.652952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 3 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 4 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 5 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 6 (percent)</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 7 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 8 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 3 (percent)</th>\n",
       "      <td>9.895522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.989552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 4 (percent)</th>\n",
       "      <td>2.358209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 5 (percent)</th>\n",
       "      <td>3.134328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 6 (percent)</th>\n",
       "      <td>73.507463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.350746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 7 (percent)</th>\n",
       "      <td>10.238806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.023881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 8 (percent)</th>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Entity Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Intent Accuracy</th>\n",
       "      <td>0.794038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Perplexity</th>\n",
       "      <td>17.734283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.773428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Response Cosine Similarity</th>\n",
       "      <td>0.454426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Iteration 1  Iteration 2  \\\n",
       "Training Speed (epochs per second)                0.005058          0.0   \n",
       "Training Time (second/epoch)                    197.697094          0.0   \n",
       "Total Training Time (second/iteration)          205.429722          0.0   \n",
       "Computational Resource Usage                     57.600000          0.0   \n",
       "Average CPU Usage (percent)                      26.600000          0.0   \n",
       "Average GPU Usage (percent)                      31.000000          0.0   \n",
       "Average Memory (GB)                               6.583912          0.0   \n",
       "Average GPU Memory (GB)                          14.531799          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                   2.377871          0.0   \n",
       "Expert NLU Load Distribution Stability         1093.750000          0.0   \n",
       "Expert NLG Load Distribution Stability          546.529517          0.0   \n",
       "Average NLU Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 3 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 4 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 5 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 6 (percent)                  100.000000          0.0   \n",
       "Average NLU Expert 7 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 8 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 3 (percent)                    9.895522          0.0   \n",
       "Average NLG Expert 4 (percent)                    2.358209          0.0   \n",
       "Average NLG Expert 5 (percent)                    3.134328          0.0   \n",
       "Average NLG Expert 6 (percent)                   73.507463          0.0   \n",
       "Average NLG Expert 7 (percent)                   10.238806          0.0   \n",
       "Average NLG Expert 8 (percent)                    0.865672          0.0   \n",
       "Average Validation Entity Accuracy                1.000000          0.0   \n",
       "Average Intent Accuracy                           0.794038          0.0   \n",
       "Average Validation Perplexity                    17.734283          0.0   \n",
       "Average Validation Response Cosine Similarity     0.454426          0.0   \n",
       "\n",
       "                                               Iteration 3  Iteration 4  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 5  Iteration 6  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 7  Iteration 8  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 9  Iteration 10  \\\n",
       "Training Speed (epochs per second)                     0.0           0.0   \n",
       "Training Time (second/epoch)                           0.0           0.0   \n",
       "Total Training Time (second/iteration)                 0.0           0.0   \n",
       "Computational Resource Usage                           0.0           0.0   \n",
       "Average CPU Usage (percent)                            0.0           0.0   \n",
       "Average GPU Usage (percent)                            0.0           0.0   \n",
       "Average Memory (GB)                                    0.0           0.0   \n",
       "Average GPU Memory (GB)                                0.0           0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0           0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0           0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0           0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0           0.0   \n",
       "Average Validation Entity Accuracy                     0.0           0.0   \n",
       "Average Intent Accuracy                                0.0           0.0   \n",
       "Average Validation Perplexity                          0.0           0.0   \n",
       "Average Validation Response Cosine Similarity          0.0           0.0   \n",
       "\n",
       "                                                  Average  \n",
       "Training Speed (epochs per second)               0.000506  \n",
       "Training Time (second/epoch)                    19.769709  \n",
       "Total Training Time (second/iteration)          20.542972  \n",
       "Computational Resource Usage                     5.760000  \n",
       "Average CPU Usage (percent)                      2.660000  \n",
       "Average GPU Usage (percent)                      3.100000  \n",
       "Average Memory (GB)                              0.658391  \n",
       "Average GPU Memory (GB)                          1.453180  \n",
       "Average FLOPS Estimate (GFLOPS)                  0.237787  \n",
       "Expert NLU Load Distribution Stability         109.375000  \n",
       "Expert NLG Load Distribution Stability          54.652952  \n",
       "Average NLU Expert 1 (percent)                   0.000000  \n",
       "Average NLU Expert 2 (percent)                   0.000000  \n",
       "Average NLU Expert 3 (percent)                   0.000000  \n",
       "Average NLU Expert 4 (percent)                   0.000000  \n",
       "Average NLU Expert 5 (percent)                   0.000000  \n",
       "Average NLU Expert 6 (percent)                  10.000000  \n",
       "Average NLU Expert 7 (percent)                   0.000000  \n",
       "Average NLU Expert 8 (percent)                   0.000000  \n",
       "Average NLG Expert 1 (percent)                   0.000000  \n",
       "Average NLG Expert 2 (percent)                   0.000000  \n",
       "Average NLG Expert 3 (percent)                   0.989552  \n",
       "Average NLG Expert 4 (percent)                   0.235821  \n",
       "Average NLG Expert 5 (percent)                   0.313433  \n",
       "Average NLG Expert 6 (percent)                   7.350746  \n",
       "Average NLG Expert 7 (percent)                   1.023881  \n",
       "Average NLG Expert 8 (percent)                   0.086567  \n",
       "Average Validation Entity Accuracy               0.100000  \n",
       "Average Intent Accuracy                          0.079404  \n",
       "Average Validation Perplexity                    1.773428  \n",
       "Average Validation Response Cosine Similarity    0.045443  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reset_kernel():\n",
    "    tf.keras.backend.clear_session()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "results_table = {\n",
    "    'Training Speed (epochs per second)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Training Time (second/epoch)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Total Training Time (second/iteration)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Computational Resource Usage': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average CPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average FLOPS Estimate (GFLOPS)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLU Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLG Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Entity Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Intent Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Perplexity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Response Cosine Similarity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0}\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_deepseekefficientdis_model\"\n",
    "\n",
    "for iteration in range(5):\n",
    "    print(f\"\\nStarting Training Iteration {iteration + 1}\")\n",
    "    reset_kernel()\n",
    "\n",
    "    def load_tf_datasets_from_disk(load_path):\n",
    "        print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "        try:\n",
    "            with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "                loaded_moe_params = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "        element_spec = (\n",
    "            {\n",
    "                'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            },\n",
    "            {\n",
    "                'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "                'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "                'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "            test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "        raw_data = {}\n",
    "        for dataset_name in ['train', 'test']:\n",
    "            path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "            if os.path.exists(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    raw_data[dataset_name] = pickle.load(f)\n",
    "                print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Raw data file {path} not found.\")\n",
    "                raw_data[dataset_name] = []\n",
    "\n",
    "        return {\n",
    "            \"train_dataset\": train_tf_dataset,\n",
    "            \"test_dataset\": test_tf_dataset,\n",
    "            \"moe_params\": loaded_moe_params,\n",
    "            \"raw_data\": raw_data\n",
    "        }\n",
    "\n",
    "    tf_dataset_save_path = \"tf_datasets\"\n",
    "    loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "    train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "    moe_params = loaded_data[\"moe_params\"]\n",
    "    raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "    class DeepSeekMoELayer(layers.Layer):\n",
    "        def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, top_k=2, name=None):\n",
    "            super(DeepSeekMoELayer, self).__init__(name=name)\n",
    "            self.m = m\n",
    "            self.k_s = k_s\n",
    "            self.total_experts = num_experts * self.m\n",
    "            self.routed_experts = self.total_experts - self.k_s\n",
    "            self.top_k = top_k\n",
    "            self.top_mk = self.m * self.top_k\n",
    "            self.top_mk = min(self.top_mk, self.routed_experts)\n",
    "            self.alpha = alpha\n",
    "            self.input_dim = input_dim\n",
    "            self.seq_length = None\n",
    "            self.adjusted_expert_dim = expert_dim // self.m\n",
    "            self.shared_experts = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                    layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "                ]) for _ in range(self.k_s)\n",
    "            ]\n",
    "            self.routed_experts_list = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                    layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "                ]) for _ in range(self.routed_experts)\n",
    "            ]\n",
    "            self.experts = self.shared_experts + self.routed_experts_list\n",
    "            self.gate = layers.Dense(self.routed_experts, activation='softmax')\n",
    "            self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            input_rank = inputs.shape.rank\n",
    "            if input_rank == 3:\n",
    "                batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "                flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "                is_3d = True\n",
    "            else:\n",
    "                batch_size = tf.shape(inputs)[0]\n",
    "                seq_length = 1\n",
    "                flat_inputs = inputs\n",
    "                is_3d = False\n",
    "\n",
    "            flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "            shared_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "            for i in range(self.k_s):\n",
    "                shared_output += self.shared_experts[i](flat_inputs)\n",
    "            gate_weights = self.gate(flat_inputs)\n",
    "            top_mk_values, top_mk_indices = tf.nn.top_k(gate_weights, k=self.top_mk, sorted=True)\n",
    "            top_mk_indices = top_mk_indices + self.k_s\n",
    "            top_mk_weights = top_mk_values / (tf.reduce_sum(top_mk_values, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "            routed_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "            for k in range(self.top_mk):\n",
    "                kth_weights = top_mk_weights[:, k]\n",
    "                kth_indices = top_mk_indices[:, k]\n",
    "                mask = tf.one_hot(kth_indices, depth=self.total_experts, dtype=tf.float32)\n",
    "                expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "                kth_expert_output = tf.reduce_sum(expert_outputs * tf.expand_dims(mask, -1), axis=1)\n",
    "                weighted_kth_output = kth_expert_output * tf.expand_dims(kth_weights, -1)\n",
    "                routed_output += weighted_kth_output\n",
    "            output_flat = shared_output + routed_output + flat_inputs\n",
    "            if is_3d:\n",
    "                output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "                if self.seq_length is not None:\n",
    "                    output.set_shape([None, self.seq_length, self.input_dim])\n",
    "                gate_weights_reshaped = tf.reshape(gate_weights, [batch_size, seq_length, self.routed_experts])\n",
    "                if self.seq_length is not None:\n",
    "                    gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "            else:\n",
    "                output = output_flat\n",
    "                gate_weights_reshaped = gate_weights\n",
    "            f_i = tf.reduce_mean(tf.reduce_sum(tf.one_hot(tf.argmax(gate_weights, axis=-1), depth=self.routed_experts), axis=0), axis=0)\n",
    "            P_i = tf.reduce_mean(gate_weights, axis=0)\n",
    "            load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "            self.add_loss(load_balancing_loss)\n",
    "            self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "            return output, gate_weights_reshaped\n",
    "\n",
    "        def get_metrics(self):\n",
    "            return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'num_experts': self.total_experts // self.m,\n",
    "                'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "                'input_dim': self.input_dim,\n",
    "                'm': self.m,\n",
    "                'k_s': self.k_s,\n",
    "                'alpha': self.alpha,\n",
    "                'top_k': self.top_k,\n",
    "                'name': self.name\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class TransformerDecoderLayer(layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "            super(TransformerDecoderLayer, self).__init__()\n",
    "            self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.ffn = keras.Sequential([\n",
    "                layers.Dense(dff, activation='relu'),\n",
    "                layers.Dense(d_model)\n",
    "            ])\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "            self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "        def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "            if look_ahead_mask is not None:\n",
    "                look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "                look_ahead_mask = 1.0 - look_ahead_mask\n",
    "            attn1_output = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)\n",
    "            attn1 = self.dropout1(attn1_output, training=training)\n",
    "            out1 = self.layernorm1(attn1 + x)\n",
    "            attn2_output = self.mha2(query=out1, value=enc_output, key=enc_output, training=training)\n",
    "            attn2 = self.dropout2(attn2_output, training=training)\n",
    "            out2 = self.layernorm2(attn2 + out1)\n",
    "            ffn_output = self.ffn(out2)\n",
    "            ffn_output = self.dropout3(ffn_output, training=training)\n",
    "            out3 = self.layernorm3(ffn_output + out2)\n",
    "            return out3\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            mha1_config = self.mha1.get_config()\n",
    "            config.update({\n",
    "                'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "                'num_heads': mha1_config['num_heads'],\n",
    "                'dff': self.ffn.layers[0].units,\n",
    "                'rate': self.dropout1.rate\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class MoEModel(models.Model):\n",
    "        def __init__(self, moe_params):\n",
    "            super(MoEModel, self).__init__()\n",
    "            self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "            self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "            self.num_domains = moe_params[\"num_domains\"]\n",
    "            self.num_intents = moe_params[\"num_intents\"]\n",
    "            self.vocab_size = moe_params[\"vocab_size\"]\n",
    "            self.num_experts = moe_params[\"num_experts\"]\n",
    "            self.expert_dim = moe_params[\"expert_dim\"]\n",
    "            self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "            self.num_heads = 4\n",
    "            self.dff = self.embedding_dim * 4\n",
    "            self.dropout_rate = 0.1\n",
    "            self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                                 self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "            self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "            self.embedding = layers.Embedding(\n",
    "                self.vocab_size,\n",
    "                self.embedding_dim,\n",
    "                mask_zero=True,\n",
    "                embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05)\n",
    "            )\n",
    "            self.embedding.build((None,))\n",
    "            with tf.init_scope():\n",
    "                embedding_weights = self.embedding.get_weights()\n",
    "                if embedding_weights and len(embedding_weights) > 0:\n",
    "                    embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                    self.embedding.set_weights(embedding_weights)\n",
    "            self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "            self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "            self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "            self.moe_nlu = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlu_input_dim, m=2, k_s=2, alpha=0.01, top_k=2, name='moe_nlu')\n",
    "            self.moe_nlg = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlg_input_dim, m=2, k_s=2, alpha=0.01, top_k=2, name='moe_nlg')\n",
    "            self.intent_output = layers.Dense(self.num_intents, activation='sigmoid')\n",
    "            self.domain_output = layers.Dense(self.num_domains, activation='softmax')\n",
    "            self.response_output = layers.TimeDistributed(layers.Dense(self.vocab_size, activation='softmax'))\n",
    "            self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "\n",
    "        def create_look_ahead_mask(self, size):\n",
    "            mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "            return mask\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "            prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "            decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "            domain_onehot_input = inputs['domain_onehot_input']\n",
    "            turn_id_embedding = inputs['turn_id_embedding']\n",
    "            ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "            pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "            user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "            prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "            decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "            user_enc_out = user_embedded\n",
    "            prev_system_enc_out = prev_system_embedded\n",
    "            look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "            dec_output = decoder_embedded\n",
    "            for i, layer in enumerate(self.decoder_layers):\n",
    "                dec_output = layer(dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask)\n",
    "            decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "            user_attn_out = self.attn_layer(query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), value=user_enc_out, key=user_enc_out, training=training)\n",
    "            prev_system_attn_out = self.attn_layer(query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), value=prev_system_enc_out, key=prev_system_enc_out, training=training)\n",
    "            decoder_attn_out = self.attn_layer(query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), value=decoder_output, key=decoder_output, training=training)\n",
    "            combined_features = layers.Concatenate()([\n",
    "                tf.squeeze(user_attn_out, 1),\n",
    "                tf.squeeze(prev_system_attn_out, 1),\n",
    "                tf.squeeze(decoder_attn_out, 1),\n",
    "                domain_onehot_input,\n",
    "                turn_id_embedding,\n",
    "                ontology_multihot_input\n",
    "            ])\n",
    "            combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "            nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "            nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "            intent_out = self.intent_output(nlu_out)\n",
    "            domain_out = self.domain_output(nlu_out)\n",
    "            intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "            domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "            intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "            domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "            nlg_input = layers.Concatenate(axis=-1)([\n",
    "                decoder_output,\n",
    "                intent_out_tiled,\n",
    "                domain_out_tiled\n",
    "            ])\n",
    "            nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "            nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            response_out = self.response_output(nlg_out)\n",
    "            return {\n",
    "                'intent_output': intent_out,\n",
    "                'domain_output': domain_out,\n",
    "                'response_output': response_out,\n",
    "                'response_embeddings': decoder_output,\n",
    "                'nlu_gate_weights': nlu_gate_weights,\n",
    "                'nlg_gate_weights': nlg_gate_weights\n",
    "            }\n",
    "\n",
    "        def build_graph(self):\n",
    "            inputs = {\n",
    "                'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "                'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32),\n",
    "            }\n",
    "            return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'moe_params': {\n",
    "                    'embedding_dim': self.embedding_dim,\n",
    "                    'max_seq_length': self.max_seq_length,\n",
    "                    'num_domains': self.num_domains,\n",
    "                    'num_intents': self.num_intents,\n",
    "                    'vocab_size': self.vocab_size,\n",
    "                    'num_experts': self.num_experts,\n",
    "                    'expert_dim': self.expert_dim,\n",
    "                    'turn_id_dim': self.turn_id_dim\n",
    "                }\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class IntentAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros')\n",
    "            self.total_preds = self.add_weight(name='total_preds', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "            self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "            self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.correct_preds.assign(0.)\n",
    "            self.total_preds.assign(0.)\n",
    "\n",
    "    class IntentPrecision(metrics.Metric):\n",
    "        def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "            self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            predicted_positives = tf.reduce_sum(y_pred)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.predicted_positives.assign(0.)\n",
    "\n",
    "    class IntentRecall(metrics.Metric):\n",
    "        def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "            self.actual_positives = self.add_weight(name='ap', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            actual_positives = tf.reduce_sum(y_true)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.actual_positives.assign(0.)\n",
    "\n",
    "    class IntentF1(metrics.Metric):\n",
    "        def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.precision = IntentPrecision(threshold=threshold)\n",
    "            self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "            self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "        def result(self):\n",
    "            p = self.precision.result()\n",
    "            r = self.recall.result()\n",
    "            return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.precision.reset_state()\n",
    "            self.recall.reset_state()\n",
    "\n",
    "    class DomainAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='domain_accuracy', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.accuracy = self.add_weight(name='acc', initializer='zeros')\n",
    "            self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "            pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "            matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "            self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "            self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.accuracy.assign(0.)\n",
    "            self.count.assign(0.)\n",
    "\n",
    "    class Perplexity(metrics.Metric):\n",
    "        def __init__(self, name='perplexity', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "            self.total_loss = self.add_weight(name='total_loss', initializer='zeros')\n",
    "            self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "            loss = self.cross_entropy(y_true, y_pred)\n",
    "            masked_loss = loss * mask\n",
    "            sum_loss = tf.reduce_sum(masked_loss)\n",
    "            num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "            self.total_loss.assign_add(sum_loss)\n",
    "            self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "        def result(self):\n",
    "            avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "            return tf.exp(avg_loss)\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_loss.assign(0.)\n",
    "            self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "    class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "        def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "            super().__init__(name=name)\n",
    "            self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "            self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "            y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "            y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "            y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "            cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "            non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "            non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "            masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "            num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "            avg_cosine_sim_per_sample = tf.where(\n",
    "                num_non_zero_tokens > 0,\n",
    "                tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "                tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "            )\n",
    "            self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "            self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_cosine_sim.assign(0.)\n",
    "            self.num_samples.assign(0.)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "        flops = 0\n",
    "        functional_model = model.build_graph()\n",
    "        def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "            layer_flops_count = 0\n",
    "            if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "                return 0\n",
    "            if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "                for sub_layer in layer_instance.layers:\n",
    "                    layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "                return layer_flops_count\n",
    "            if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "                return 0\n",
    "            if isinstance(layer_instance, layers.Dense):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                output_dim = layer_instance.output_shape[-1]\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "            elif isinstance(layer_instance, layers.LSTM):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                hidden_dim = layer_instance.units\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "                layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "            elif isinstance(layer_instance, layers.Embedding):\n",
    "                pass\n",
    "            elif isinstance(layer_instance, DeepSeekMoELayer):\n",
    "                moe_input_dim = layer_instance.input_dim\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "                layer_flops_count += gate_flops\n",
    "                for expert in layer_instance.experts:\n",
    "                    layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "            elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "                config = layer_instance.get_config()\n",
    "                num_heads = config['num_heads']\n",
    "                key_dim = config['key_dim']\n",
    "                d_model = num_heads * key_dim\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "                projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "                attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "                context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "                output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "                layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "            elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "                inner_layer = layer_instance.layer\n",
    "                td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "                layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "            elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "                ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "            return layer_flops_count\n",
    "        for layer in functional_model.layers:\n",
    "            flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "        return flops / 1e9\n",
    "\n",
    "    def get_gpu_stats():\n",
    "        if nvidia_smi is None:\n",
    "            return 0, 0\n",
    "        try:\n",
    "            nvidia_smi.nvmlInit()\n",
    "            handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "            gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "            mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "            gpu_memory = mem_info.used / (1024 ** 3)\n",
    "            nvidia_smi.nvmlShutdown()\n",
    "            return gpu_load, gpu_memory\n",
    "        except Exception:\n",
    "            return 0, 0\n",
    "\n",
    "    class ResourceMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, validation_data):\n",
    "            super().__init__()\n",
    "            self.resource_stats = []\n",
    "            self.expert_load_history = []\n",
    "            self.start_time = None\n",
    "            self.epoch_start_time = None\n",
    "            self.flops = None\n",
    "            self.validation_data = validation_data\n",
    "            self.nlu_expert_usage_counts = None\n",
    "            self.nlg_expert_usage_counts = None\n",
    "            self.nlu_gate_weights_history = []\n",
    "            self.nlg_gate_weights_history = []\n",
    "\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = time.time()\n",
    "            try:\n",
    "                self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "                self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "                self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "            except Exception:\n",
    "                self.flops = 0\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.epoch_start_time = time.time()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.resource_stats.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_time': epoch_time,\n",
    "                'cpu_usage': cpu_usage,\n",
    "                'memory_used': memory.used / (1024 ** 3),\n",
    "                'memory_total': memory.total / (1024 ** 3),\n",
    "                'gpu_load': gpu_load,\n",
    "                'gpu_memory': gpu_memory,\n",
    "                'loss': logs.get('loss', 0),\n",
    "                'val_loss': logs.get('val_loss', 0),\n",
    "                'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "                'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "                'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "                'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "                'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "                'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "                'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "                'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "                'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "                'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "                'perplexity': logs.get('response_output_perplexity', 0),\n",
    "                'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "                'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "                'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "            })\n",
    "            if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "                self.expert_load_history.append({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                    'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "                })\n",
    "            sample_size = 100\n",
    "            for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "                inputs, _ = batch\n",
    "                outputs = self.model(inputs, training=False)\n",
    "                nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "                reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "                self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "                nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "                reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "                self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            self.total_time = time.time() - self.start_time\n",
    "            if len(self.nlu_gate_weights_history) > 0:\n",
    "                all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "                nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "                for expert_id in nlu_top_indices:\n",
    "                    if expert_id < self.model.moe_nlu.total_experts:\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "            if len(self.nlg_gate_weights_history) > 0:\n",
    "                all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "                nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "                for expert_id in nlg_top_indices:\n",
    "                    if expert_id < self.model.moe_nlg.total_experts:\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        def visualize_expert_load_distribution(self):\n",
    "            total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "            total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "            nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "            nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "            nlu_stability = np.var(nlu_percentages)\n",
    "            nlg_stability = np.var(nlg_percentages)\n",
    "            return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "    class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super().__init__()\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "    model = MoEModel(moe_params)\n",
    "    embedding_layer = model.embedding\n",
    "\n",
    "    def create_validation_split(dataset, embedding_layer, validation_split=0.2, batch_size=moe_params[\"batch_size\"]):\n",
    "        element_spec = dataset.element_spec\n",
    "        data_list = [(features, targets) for features, targets in dataset.unbatch().as_numpy_iterator()]\n",
    "        if not data_list:\n",
    "            raise ValueError(\"Dataset is empty.\")\n",
    "        if len(data_list) < 2:\n",
    "            raise ValueError(\"Dataset too small to split.\")\n",
    "        train_data, val_data = train_test_split(data_list, test_size=validation_split, shuffle=True)\n",
    "        def create_dataset_from_list(data):\n",
    "            if not data:\n",
    "                raise ValueError(\"Empty data list provided.\")\n",
    "            features = {\n",
    "                'user_utterance_tokens': np.array([d[0]['user_utterance_tokens'] for d in data], dtype=np.int32),\n",
    "                'prev_system_response_tokens': np.array([d[0]['prev_system_response_tokens'] for d in data], dtype=np.int32),\n",
    "                'decoder_input_tokens': np.array([d[0]['decoder_input_tokens'] for d in data], dtype=np.int32),\n",
    "                'domain_onehot_input': np.array([d[0]['domain_onehot_input'] for d in data], dtype=np.float32),\n",
    "                'turn_id_embedding': np.array([d[0]['turn_id_embedding'] for d in data], dtype=np.float32),\n",
    "                'ontology_multihot_input': np.array([d[0]['ontology_multihot_input'] for d in data], dtype=np.float32),\n",
    "            }\n",
    "            response_output = np.array([d[1]['response_output'] for d in data], dtype=np.int32)\n",
    "            response_embeddings = embedding_layer(response_output).numpy()\n",
    "            targets = {\n",
    "                'domain_output': np.array([d[1]['domain_output'] for d in data], dtype=np.int32),\n",
    "                'intent_output': np.array([d[1]['intent_output'] for d in data], dtype=np.float32),\n",
    "                'response_output': response_output,\n",
    "                'response_embeddings': response_embeddings\n",
    "            }\n",
    "            return tf.data.Dataset.from_tensor_slices((features, targets))\n",
    "        train_dataset = create_dataset_from_list(train_data).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        val_dataset = create_dataset_from_list(val_data).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    train_tf_dataset, val_tf_dataset = create_validation_split(train_tf_dataset, embedding_layer)\n",
    "    losses_dict = {\n",
    "        'intent_output': losses.BinaryCrossentropy(),\n",
    "        'domain_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss\n",
    "    }\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={'intent_output': 0.5, 'domain_output': 0.5, 'response_output': 1.0, 'response_embeddings': 1.0}\n",
    "    )\n",
    "    sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "    model(sample_input[0])\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=8, mode='min', restore_best_weights=True)\n",
    "    resource_monitor = ResourceMonitor(val_tf_dataset)\n",
    "    class TQDMProgressBar(callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.progress_bar = tqdm(total=len(train_tf_dataset), desc=f'Epoch {epoch + 1}')\n",
    "        def on_batch_end(self, batch, logs=None):\n",
    "            self.progress_bar.update(1)\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            self.progress_bar.close()\n",
    "    tqdm_callback = TQDMProgressBar()\n",
    "    history = model.fit(\n",
    "        train_tf_dataset,\n",
    "        epochs=100,\n",
    "        validation_data=val_tf_dataset,\n",
    "        callbacks=[early_stopping, resource_monitor, tqdm_callback],\n",
    "        verbose=0\n",
    "    )\n",
    "    expert_stats = resource_monitor.visualize_expert_load_distribution()\n",
    "    stats = resource_monitor.resource_stats\n",
    "    avg_epoch_duration = np.mean([s['epoch_time'] for s in stats]) if stats else 0\n",
    "    total_training_time = resource_monitor.total_time\n",
    "    avg_cpu_usage = np.mean([s['cpu_usage'] for s in stats]) if stats else 0\n",
    "    avg_memory = np.mean([s['memory_used'] for s in stats]) if stats else 0\n",
    "    avg_gpu_load = np.mean([s['gpu_load'] for s in stats]) if stats else 0\n",
    "    avg_gpu_memory = np.mean([s['gpu_memory'] for s in stats]) if stats else 0\n",
    "    avg_flops = resource_monitor.flops if resource_monitor.flops else 0\n",
    "    nlu_counts = resource_monitor.nlu_expert_usage_counts\n",
    "    nlg_counts = resource_monitor.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) or 1\n",
    "    total_nlg = np.sum(nlg_counts) or 1\n",
    "    expert_nlu_percentages = [(nlu_counts[i] / total_nlu) * 100 for i in range(8)]\n",
    "    expert_nlg_percentages = [(nlg_counts[i] / total_nlg) * 100 for i in range(8)]\n",
    "    val_intent_accuracy = np.mean([s['val_intent_accuracy'] for s in stats if s['val_intent_accuracy'] > 0]) if stats else 0\n",
    "    val_entity_accuracy = np.mean([s['val_domain_accuracy'] for s in stats if s['val_domain_accuracy'] > 0]) if stats else 0\n",
    "    val_perplexity = np.mean([s['val_perplexity'] for s in stats if s['val_perplexity'] > 0]) if stats else 0\n",
    "    val_cosine_similarity = np.mean([s['val_cosine_similarity'] for s in stats if s['val_cosine_similarity'] > 0]) if stats and any(s['val_cosine_similarity'] > 0 for s in stats) else 0\n",
    "    results_table['Average Validation Response Cosine Similarity'][f'Iteration {iteration + 1}'] = val_cosine_similarity\n",
    "    results_table['Training Speed (epochs per second)'][f'Iteration {iteration + 1}'] = 1 / avg_epoch_duration if avg_epoch_duration > 0 else 0\n",
    "    results_table['Training Time (second/epoch)'][f'Iteration {iteration + 1}'] = avg_epoch_duration\n",
    "    results_table['Total Training Time (second/iteration)'][f'Iteration {iteration + 1}'] = total_training_time\n",
    "    results_table['Computational Resource Usage'][f'Iteration {iteration + 1}'] = avg_cpu_usage + avg_gpu_load\n",
    "    results_table['Average CPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_cpu_usage\n",
    "    results_table['Average GPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_gpu_load\n",
    "    results_table['Average Memory (GB)'][f'Iteration {iteration + 1}'] = avg_memory\n",
    "    results_table['Average GPU Memory (GB)'][f'Iteration {iteration + 1}'] = avg_gpu_memory\n",
    "    results_table['Average FLOPS Estimate (GFLOPS)'][f'Iteration {iteration + 1}'] = avg_flops\n",
    "    results_table['Expert NLU Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlu_stability']\n",
    "    results_table['Expert NLG Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlg_stability']\n",
    "    for i in range(8):\n",
    "        if i < len(expert_stats['nlu_percentages']):\n",
    "            results_table[f'Average NLU Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlu_percentages'][i]\n",
    "        if i < len(expert_stats['nlg_percentages']):\n",
    "            results_table[f'Average NLG Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlg_percentages'][i]\n",
    "    results_table['Average Validation Entity Accuracy'][f'Iteration {iteration + 1}'] = val_entity_accuracy\n",
    "    results_table['Average Intent Accuracy'][f'Iteration {iteration + 1}'] = val_intent_accuracy\n",
    "    results_table['Average Validation Perplexity'][f'Iteration {iteration + 1}'] = val_perplexity\n",
    "    results_table['Average Validation Response Cosine Similarity'][f'Iteration {iteration + 1}'] = val_cosine_similarity\n",
    "    val_loss = min([s['val_loss'] for s in stats]) if stats else float('inf')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        try:\n",
    "            model.save(best_model_path, save_format='tf', include_optimizer=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to save best model to {best_model_path}: {str(e)}\")\n",
    "\n",
    "for key in results_table:\n",
    "    results_table[key]['Average'] = np.mean([results_table[key][f'Iteration {i+1}'] for i in range(10)])\n",
    "\n",
    "final_result = pd.DataFrame(results_table)\n",
    "final_result = final_result.T\n",
    "final_result.to_excel('training_deepseekefficientdis_result.xlsx', index=True)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE with Efficient Distributed Sparse Routing evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 07:25:14.806654: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 07:25:15.101313: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 07:25:15.101438: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 07:25:15.155743: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 07:25:15.263202: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 07:25:16.461733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 07:25:19.628719: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:25:19.768384: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:25:19.768674: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:25:19.770482: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:25:19.770609: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:25:19.770696: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:25:19.852997: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:25:19.853187: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:25:19.853357: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:25:19.853436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n",
      "Loaded vocabulary from preprocessor_models/preprocessor_params.json\n",
      "\n",
      "Loading model from: best_deepseekefficientdis_model\n",
      "\n",
      "Model loaded and compiled successfully!\n",
      "Model: \"mo_e_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  784896    \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     multiple                  8576      \n",
      "                                                                 \n",
      " layer_normalization (Layer  multiple                  256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_layer   multiple                  264576    \n",
      " (TransformerDecoderLayer)                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " moe_nlu (DeepSeekMoELayer)  multiple                  244852    \n",
      "                                                                 \n",
      " moe_nlg (DeepSeekMoELayer)  multiple                  87578     \n",
      "                                                                 \n",
      " dense_36 (Dense)            multiple                  14446     \n",
      "                                                                 \n",
      " dense_37 (Dense)            multiple                  3262      \n",
      "                                                                 \n",
      " time_distributed (TimeDist  multiple                  1024044   \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " multi_head_attention_2 (Mu  multiple                  66048     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2498534 (9.53 MB)\n",
      "Trainable params: 2498534 (9.53 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting Iteration 1\n",
      "\n",
      "Evaluating model on test set...\n",
      "1397/1397 [==============================] - 23s 13ms/step - loss: 3.5710 - domain_output_loss: 0.0016 - intent_output_loss: 0.0266 - response_embeddings_loss: 0.8222 - response_output_loss: 2.4869 - domain_output_domain_accuracy: 1.0000 - intent_output_intent_accuracy: 0.7190 - intent_output_intent_precision: 1.0000 - intent_output_intent_recall: 0.6530 - intent_output_intent_f1: 0.7901 - response_embeddings_response_embedding_cosine_similarity: 0.1779 - response_output_perplexity: 30.6571 - moe_nlu_load_balancing_loss: 0.0033 - moe_nlg_load_balancing_loss: 0.2233\n",
      "\n",
      "Training results saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evaluation Time (seconds)</th>\n",
       "      <td>24.118228</td>\n",
       "      <td>24.118228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction Speed (ms/token)</th>\n",
       "      <td>3.358257</td>\n",
       "      <td>3.358257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>23.522763</td>\n",
       "      <td>23.522763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>1.901933</td>\n",
       "      <td>1.901933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>5.702466</td>\n",
       "      <td>5.702466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.504456</td>\n",
       "      <td>14.504456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPs Estimate (GFLOPs)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>2.377871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 3 (percent)</th>\n",
       "      <td>1.252684</td>\n",
       "      <td>1.252684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 4 (percent)</th>\n",
       "      <td>5.243379</td>\n",
       "      <td>5.243379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 5 (percent)</th>\n",
       "      <td>1.950608</td>\n",
       "      <td>1.950608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 6 (percent)</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 7 (percent)</th>\n",
       "      <td>14.030064</td>\n",
       "      <td>14.030064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 8 (percent)</th>\n",
       "      <td>27.523264</td>\n",
       "      <td>27.523264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 3 (percent)</th>\n",
       "      <td>28.735884</td>\n",
       "      <td>28.735884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 4 (percent)</th>\n",
       "      <td>15.025267</td>\n",
       "      <td>15.025267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 5 (percent)</th>\n",
       "      <td>3.159489</td>\n",
       "      <td>3.159489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 6 (percent)</th>\n",
       "      <td>33.125621</td>\n",
       "      <td>33.125621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 7 (percent)</th>\n",
       "      <td>16.470796</td>\n",
       "      <td>16.470796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 8 (percent)</th>\n",
       "      <td>3.482943</td>\n",
       "      <td>3.482943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Micro)</th>\n",
       "      <td>0.790104</td>\n",
       "      <td>0.790104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Macro)</th>\n",
       "      <td>0.559098</td>\n",
       "      <td>0.559098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Weighted)</th>\n",
       "      <td>0.703489</td>\n",
       "      <td>0.703489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent Accuracy</th>\n",
       "      <td>0.719041</td>\n",
       "      <td>0.719041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain F1-score (Macro)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU Score</th>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1 F1</th>\n",
       "      <td>0.171168</td>\n",
       "      <td>0.171168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-2 F1</th>\n",
       "      <td>0.012351</td>\n",
       "      <td>0.012351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-L F1</th>\n",
       "      <td>0.130032</td>\n",
       "      <td>0.130032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METEOR Score</th>\n",
       "      <td>0.139432</td>\n",
       "      <td>0.139432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>30.657061</td>\n",
       "      <td>30.657061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert Load Stability (Variance)</th>\n",
       "      <td>279.654986</td>\n",
       "      <td>279.654986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert Load Stability (Variance)</th>\n",
       "      <td>149.027103</td>\n",
       "      <td>149.027103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Iteration 1     Average\n",
       "Evaluation Time (seconds)               24.118228   24.118228\n",
       "Prediction Speed (ms/token)              3.358257    3.358257\n",
       "Average CPU Usage (percent)             23.522763   23.522763\n",
       "Average GPU Usage (percent)              1.901933    1.901933\n",
       "Average Memory (GB)                      5.702466    5.702466\n",
       "Average GPU Memory (GB)                 14.504456   14.504456\n",
       "Average FLOPs Estimate (GFLOPs)          2.377871    2.377871\n",
       "NLU Expert 1 (percent)                   0.000000    0.000000\n",
       "NLU Expert 2 (percent)                   0.000000    0.000000\n",
       "NLU Expert 3 (percent)                   1.252684    1.252684\n",
       "NLU Expert 4 (percent)                   5.243379    5.243379\n",
       "NLU Expert 5 (percent)                   1.950608    1.950608\n",
       "NLU Expert 6 (percent)                  50.000000   50.000000\n",
       "NLU Expert 7 (percent)                  14.030064   14.030064\n",
       "NLU Expert 8 (percent)                  27.523264   27.523264\n",
       "NLG Expert 1 (percent)                   0.000000    0.000000\n",
       "NLG Expert 2 (percent)                   0.000000    0.000000\n",
       "NLG Expert 3 (percent)                  28.735884   28.735884\n",
       "NLG Expert 4 (percent)                  15.025267   15.025267\n",
       "NLG Expert 5 (percent)                   3.159489    3.159489\n",
       "NLG Expert 6 (percent)                  33.125621   33.125621\n",
       "NLG Expert 7 (percent)                  16.470796   16.470796\n",
       "NLG Expert 8 (percent)                   3.482943    3.482943\n",
       "Intent F1-score (Micro)                  0.790104    0.790104\n",
       "Intent F1-score (Macro)                  0.559098    0.559098\n",
       "Intent F1-score (Weighted)               0.703489    0.703489\n",
       "Intent Accuracy                          0.719041    0.719041\n",
       "Domain Accuracy                          1.000000    1.000000\n",
       "Domain F1-score (Macro)                  1.000000    1.000000\n",
       "BLEU Score                               0.001860    0.001860\n",
       "ROUGE-1 F1                               0.171168    0.171168\n",
       "ROUGE-2 F1                               0.012351    0.012351\n",
       "ROUGE-L F1                               0.130032    0.130032\n",
       "METEOR Score                             0.139432    0.139432\n",
       "Perplexity                              30.657061   30.657061\n",
       "NLU Expert Load Stability (Variance)   279.654986  279.654986\n",
       "NLG Expert Load Stability (Variance)   149.027103  149.027103"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tf_datasets_from_disk(load_path):\n",
    "    print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "    try:\n",
    "        with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "            loaded_moe_params = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "    element_spec = (\n",
    "        {\n",
    "            'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "        },\n",
    "        {\n",
    "            'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "            'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "        test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "    raw_data = {}\n",
    "    for dataset_name in ['train', 'test']:\n",
    "        path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                raw_data[dataset_name] = pickle.load(f)\n",
    "            print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Raw data file {path} not found.\")\n",
    "            raw_data[dataset_name] = []\n",
    "\n",
    "    return {\n",
    "        \"train_dataset\": train_tf_dataset,\n",
    "        \"test_dataset\": test_tf_dataset,\n",
    "        \"moe_params\": loaded_moe_params,\n",
    "        \"raw_data\": raw_data\n",
    "    }\n",
    "\n",
    "tf_dataset_save_path = \"tf_datasets\"\n",
    "loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "test_tf_dataset = loaded_data[\"test_dataset\"]\n",
    "moe_params = loaded_data[\"moe_params\"]\n",
    "raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "moe_params[\"num_experts\"] = 4\n",
    "\n",
    "class DeepSeekMoELayer(layers.Layer):\n",
    "    def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, top_k=2, name=None):\n",
    "        super(DeepSeekMoELayer, self).__init__(name=name)\n",
    "        self.m = m\n",
    "        self.k_s = k_s\n",
    "        self.total_experts = num_experts * self.m\n",
    "        self.routed_experts = self.total_experts - self.k_s\n",
    "        self.top_k = top_k\n",
    "        self.top_mk = self.m * self.top_k\n",
    "        self.top_mk = min(self.top_mk, self.routed_experts)\n",
    "        self.alpha = alpha\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = None\n",
    "        self.adjusted_expert_dim = expert_dim // self.m\n",
    "        self.shared_experts = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "            ]) for _ in range(self.k_s)\n",
    "        ]\n",
    "        self.routed_experts_list = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "            ]) for _ in range(self.routed_experts)\n",
    "        ]\n",
    "        self.experts = self.shared_experts + self.routed_experts_list\n",
    "        self.gate = layers.Dense(self.routed_experts, activation='softmax')\n",
    "        self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_rank = inputs.shape.rank\n",
    "        if input_rank == 3:\n",
    "            batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "            flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "            is_3d = True\n",
    "        else:\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            seq_length = 1\n",
    "            flat_inputs = inputs\n",
    "            is_3d = False\n",
    "\n",
    "        flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "        shared_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "        for i in range(self.k_s):\n",
    "            shared_output += self.shared_experts[i](flat_inputs)\n",
    "        gate_weights = self.gate(flat_inputs)\n",
    "        top_mk_values, top_mk_indices = tf.nn.top_k(gate_weights, k=self.top_mk, sorted=True)\n",
    "        top_mk_indices = top_mk_indices + self.k_s\n",
    "        top_mk_weights = top_mk_values / (tf.reduce_sum(top_mk_values, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "        routed_output = tf.zeros([tf.shape(flat_inputs)[0], self.input_dim], dtype=tf.float32)\n",
    "        for k in range(self.top_mk):\n",
    "            kth_weights = top_mk_weights[:, k]\n",
    "            kth_indices = top_mk_indices[:, k]\n",
    "            mask = tf.one_hot(kth_indices, depth=self.total_experts, dtype=tf.float32)\n",
    "            expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "            kth_expert_output = tf.reduce_sum(expert_outputs * tf.expand_dims(mask, -1), axis=1)\n",
    "            weighted_kth_output = kth_expert_output * tf.expand_dims(kth_weights, -1)\n",
    "            routed_output += weighted_kth_output\n",
    "        output_flat = shared_output + routed_output + flat_inputs\n",
    "        if is_3d:\n",
    "            output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "            if self.seq_length is not None:\n",
    "                output.set_shape([None, self.seq_length, self.input_dim])\n",
    "            gate_weights_reshaped = tf.reshape(gate_weights, [batch_size, seq_length, self.routed_experts])\n",
    "            if self.seq_length is not None:\n",
    "                gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "        else:\n",
    "            output = output_flat\n",
    "            gate_weights_reshaped = gate_weights\n",
    "        f_i = tf.reduce_mean(tf.reduce_sum(tf.one_hot(tf.argmax(gate_weights, axis=-1), depth=self.routed_experts), axis=0), axis=0)\n",
    "        P_i = tf.reduce_mean(gate_weights, axis=0)\n",
    "        load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "        self.add_loss(load_balancing_loss)\n",
    "        self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "        return output, gate_weights_reshaped\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_experts': self.total_experts // self.m,\n",
    "            'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "            'input_dim': self.input_dim,\n",
    "            'm': self.m,\n",
    "            'k_s': self.k_s,\n",
    "            'alpha': self.alpha,\n",
    "            'top_k': self.top_k,\n",
    "            'name': self.name\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "        if look_ahead_mask is not None:\n",
    "            look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "            look_ahead_mask = 1.0 - look_ahead_mask\n",
    "        attn1_output = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)\n",
    "        attn1 = self.dropout1(attn1_output, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        attn2_output = self.mha2(query=out1, value=enc_output, key=enc_output, training=training)\n",
    "        attn2 = self.dropout2(attn2_output, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        return out3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        mha1_config = self.mha1.get_config()\n",
    "        config.update({\n",
    "            'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "            'num_heads': mha1_config['num_heads'],\n",
    "            'dff': self.ffn.layers[0].units,\n",
    "            'rate': self.dropout1.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MoEModel(models.Model):\n",
    "    def __init__(self, moe_params):\n",
    "        super(MoEModel, self).__init__()\n",
    "        self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "        self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "        self.num_domains = moe_params[\"num_domains\"]\n",
    "        self.num_intents = moe_params[\"num_intents\"]\n",
    "        self.vocab_size = moe_params[\"vocab_size\"]\n",
    "        self.num_experts = moe_params[\"num_experts\"]\n",
    "        self.expert_dim = moe_params[\"expert_dim\"]\n",
    "        self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "        self.num_heads = 4\n",
    "        self.dff = self.embedding_dim * 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                             self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "        self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "        self.embedding = layers.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05)\n",
    "        )\n",
    "        self.embedding.build((None,))\n",
    "        with tf.init_scope():\n",
    "            embedding_weights = self.embedding.get_weights()\n",
    "            if embedding_weights and len(embedding_weights) > 0:\n",
    "                embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                self.embedding.set_weights(embedding_weights)\n",
    "        self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "        self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "        self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "        self.moe_nlu = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlu_input_dim, m=2, k_s=2, alpha=0.01, top_k=2, name='moe_nlu')\n",
    "        self.moe_nlg = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlg_input_dim, m=2, k_s=2, alpha=0.01, top_k=2, name='moe_nlg')\n",
    "        self.intent_output = layers.Dense(self.num_intents, activation='sigmoid')\n",
    "        self.domain_output = layers.Dense(self.num_domains, activation='softmax')\n",
    "        self.response_output = layers.TimeDistributed(layers.Dense(self.vocab_size, activation='softmax'))\n",
    "        self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "        prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "        decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "        domain_onehot_input = inputs['domain_onehot_input']\n",
    "        turn_id_embedding = inputs['turn_id_embedding']\n",
    "        ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "        pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "        user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "        prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "        decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "        user_enc_out = user_embedded\n",
    "        prev_system_enc_out = prev_system_embedded\n",
    "        look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "        dec_output = decoder_embedded\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            dec_output = layer(dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask)\n",
    "        decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "        user_attn_out = self.attn_layer(query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), value=user_enc_out, key=user_enc_out, training=training)\n",
    "        prev_system_attn_out = self.attn_layer(query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), value=prev_system_enc_out, key=prev_system_enc_out, training=training)\n",
    "        decoder_attn_out = self.attn_layer(query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), value=decoder_output, key=decoder_output, training=training)\n",
    "        combined_features = layers.Concatenate()([\n",
    "            tf.squeeze(user_attn_out, 1),\n",
    "            tf.squeeze(prev_system_attn_out, 1),\n",
    "            tf.squeeze(decoder_attn_out, 1),\n",
    "            domain_onehot_input,\n",
    "            turn_id_embedding,\n",
    "            ontology_multihot_input\n",
    "        ])\n",
    "        combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "        nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "        nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "        intent_out = self.intent_output(nlu_out)\n",
    "        domain_out = self.domain_output(nlu_out)\n",
    "        intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "        domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "        intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "        domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "        nlg_input = layers.Concatenate(axis=-1)([\n",
    "            decoder_output,\n",
    "            intent_out_tiled,\n",
    "            domain_out_tiled\n",
    "        ])\n",
    "        nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "        nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        response_out = self.response_output(nlg_out)\n",
    "        return {\n",
    "            'intent_output': intent_out,\n",
    "            'domain_output': domain_out,\n",
    "            'response_output': response_out,\n",
    "            'response_embeddings': decoder_output,\n",
    "            'nlu_gate_weights': nlu_gate_weights,\n",
    "            'nlg_gate_weights': nlg_gate_weights\n",
    "        }\n",
    "\n",
    "    def build_graph(self):\n",
    "        inputs = {\n",
    "            'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32),\n",
    "        }\n",
    "        return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'moe_params': {\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'max_seq_length': self.max_seq_length,\n",
    "                'num_domains': self.num_domains,\n",
    "                'num_intents': self.num_intents,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'num_experts': self.num_experts,\n",
    "                'expert_dim': self.expert_dim,\n",
    "                'turn_id_dim': self.turn_id_dim\n",
    "            }\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class IntentAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros')\n",
    "        self.total_preds = self.add_weight(name='total_preds', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "        self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "        self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.correct_preds.assign(0.)\n",
    "        self.total_preds.assign(0.)\n",
    "\n",
    "class IntentPrecision(metrics.Metric):\n",
    "    def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        predicted_positives = tf.reduce_sum(y_pred)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.predicted_positives.assign(0.)\n",
    "\n",
    "class IntentRecall(metrics.Metric):\n",
    "    def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.actual_positives = self.add_weight(name='ap', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        actual_positives = tf.reduce_sum(y_true)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.actual_positives.assign(0.)\n",
    "\n",
    "class IntentF1(metrics.Metric):\n",
    "    def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = IntentPrecision(threshold=threshold)\n",
    "        self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()\n",
    "\n",
    "class DomainAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='domain_accuracy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.accuracy = self.add_weight(name='acc', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "        pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "        matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "        self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.accuracy.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "class Perplexity(metrics.Metric):\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "        self.total_loss = self.add_weight(name='total_loss', initializer='zeros')\n",
    "        self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "        loss = self.cross_entropy(y_true, y_pred)\n",
    "        masked_loss = loss * mask\n",
    "        sum_loss = tf.reduce_sum(masked_loss)\n",
    "        num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "        self.total_loss.assign_add(sum_loss)\n",
    "        self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "    def result(self):\n",
    "        avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "        return tf.exp(avg_loss)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_loss.assign(0.)\n",
    "        self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "    def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "        y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "        cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "        non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "        non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "        masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "        num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "        avg_cosine_sim_per_sample = tf.where(\n",
    "            num_non_zero_tokens > 0,\n",
    "            tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "            tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "        )\n",
    "        self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_cosine_sim.assign(0.)\n",
    "        self.num_samples.assign(0.)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "    y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "    y_true = y_true / (y_true_norm + epsilon)\n",
    "    y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "    cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    positive_loss = 1.0 - cosine_sim\n",
    "    mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "    masked_loss = positive_loss * mask\n",
    "    total_loss = tf.reduce_sum(masked_loss)\n",
    "    num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "    return total_loss / num_tokens\n",
    "\n",
    "def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "    flops = 0\n",
    "    functional_model = model.build_graph()\n",
    "    def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "        layer_flops_count = 0\n",
    "        if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "            return 0\n",
    "        if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "            for sub_layer in layer_instance.layers:\n",
    "                layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "            return layer_flops_count\n",
    "        if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "            return 0\n",
    "        if isinstance(layer_instance, layers.Dense):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            output_dim = layer_instance.output_shape[-1]\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "        elif isinstance(layer_instance, layers.LSTM):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            hidden_dim = layer_instance.units\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "            layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "        elif isinstance(layer_instance, layers.Embedding):\n",
    "            pass\n",
    "        elif isinstance(layer_instance, DeepSeekMoELayer):\n",
    "            moe_input_dim = layer_instance.input_dim\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "            layer_flops_count += gate_flops\n",
    "            for expert in layer_instance.experts:\n",
    "                layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "        elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "            config = layer_instance.get_config()\n",
    "            num_heads = config['num_heads']\n",
    "            key_dim = config['key_dim']\n",
    "            d_model = num_heads * key_dim\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "            projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "            attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "            context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "            output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "            layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "        elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "            inner_layer = layer_instance.layer\n",
    "            td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "            layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "        elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "            ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "        return layer_flops_count\n",
    "    for layer in functional_model.layers:\n",
    "        flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "    return flops / 1e9\n",
    "\n",
    "def get_gpu_stats():\n",
    "    if nvidia_smi is None:\n",
    "        return 0, 0\n",
    "    try:\n",
    "        nvidia_smi.nvmlInit()\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "        mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_memory = mem_info.used / (1024 ** 3)\n",
    "        nvidia_smi.nvmlShutdown()\n",
    "        return gpu_load, gpu_memory\n",
    "    except Exception:\n",
    "        return 0, 0\n",
    "\n",
    "class ResourceMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.resource_stats = []\n",
    "        self.expert_load_history = []\n",
    "        self.start_time = None\n",
    "        self.epoch_start_time = None\n",
    "        self.flops = None\n",
    "        self.validation_data = validation_data\n",
    "        self.nlu_expert_usage_counts = None\n",
    "        self.nlg_expert_usage_counts = None\n",
    "        self.nlu_gate_weights_history = []\n",
    "        self.nlg_gate_weights_history = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        try:\n",
    "            self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "            self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "            self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "        except Exception:\n",
    "            self.flops = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory()\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.resource_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'epoch_time': epoch_time,\n",
    "            'cpu_usage': cpu_usage,\n",
    "            'memory_used': memory.used / (1024 ** 3),\n",
    "            'memory_total': memory.total / (1024 ** 3),\n",
    "            'gpu_load': gpu_load,\n",
    "            'gpu_memory': gpu_memory,\n",
    "            'loss': logs.get('loss', 0),\n",
    "            'val_loss': logs.get('val_loss', 0),\n",
    "            'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "            'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "            'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "            'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "            'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "            'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "            'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "            'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "            'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "            'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "            'perplexity': logs.get('response_output_perplexity', 0),\n",
    "            'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "            'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "            'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "        })\n",
    "        if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "            self.expert_load_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "            })\n",
    "        sample_size = 100\n",
    "        for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "            inputs, _ = batch\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "            reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "            self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "            nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "            reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "            self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.total_time = time.time() - self.start_time\n",
    "        if len(self.nlu_gate_weights_history) > 0:\n",
    "            all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "            nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "            for expert_id in nlu_top_indices:\n",
    "                if expert_id < self.model.moe_nlu.total_experts:\n",
    "                    self.nlu_expert_usage_counts[expert_id] += 1\n",
    "        if len(self.nlg_gate_weights_history) > 0:\n",
    "            all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "            nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "            for expert_id in nlg_top_indices:\n",
    "                if expert_id < self.model.moe_nlg.total_experts:\n",
    "                    self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "    def visualize_expert_load_distribution(self):\n",
    "        total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "        total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "        nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "        nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "        nlu_stability = np.var(nlu_percentages)\n",
    "        nlg_stability = np.var(nlg_percentages)\n",
    "        return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "try:\n",
    "    possible_paths = [\n",
    "        os.path.join(\"preprocessor_models\", \"preprocessor_params.json\"),\n",
    "        os.path.join(\"tf_datasets\", \"preprocessor_params.json\"),\n",
    "        \"preprocessor_params.json\"\n",
    "    ]\n",
    "    vocab_loaded = False\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                preprocessor_params = json.load(f)\n",
    "            word_to_id = {k: int(v) for k, v in preprocessor_params['word_to_id'].items()}\n",
    "            id_to_word = {int(k): v for k, v in preprocessor_params['id_to_word'].items()}\n",
    "            print(f\"Loaded vocabulary from {path}\")\n",
    "            vocab_loaded = True\n",
    "            break\n",
    "    if not vocab_loaded:\n",
    "        raise FileNotFoundError(\"Vocabulary file not found in any location\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load vocabulary from preprocessor: {str(e)}\")\n",
    "    word_to_id = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    id_to_word = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
    "\n",
    "model_save_path = \"best_deepseekefficientdis_model\"\n",
    "print(f\"\\nLoading model from: {model_save_path}\")\n",
    "custom_objects = {\n",
    "    \"MoEModel\":MoEModel,\n",
    "    \"CustomLearningRateSchedule\": CustomLearningRateSchedule,\n",
    "    \"DeepSeekMoELayer\": DeepSeekMoELayer,\n",
    "    \"TransformerDecoderLayer\": TransformerDecoderLayer,\n",
    "    \"IntentAccuracy\": IntentAccuracy,\n",
    "    \"IntentPrecision\": IntentPrecision,\n",
    "    \"IntentRecall\": IntentRecall,\n",
    "    \"IntentF1\": IntentF1,\n",
    "    \"DomainAccuracy\": DomainAccuracy,\n",
    "    \"Perplexity\": Perplexity,\n",
    "    \"ResponseEmbeddingCosineSimilarity\": ResponseEmbeddingCosineSimilarity\n",
    "}\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model from {model_save_path}: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "def compile_model(model, moe_params):\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "    losses_dict = {\n",
    "        'intent_output': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'domain_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss,\n",
    "    }\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={\n",
    "            'intent_output': 0.5,\n",
    "            'domain_output': 0.5,\n",
    "            'response_output': 1.0,\n",
    "            'response_embeddings': 1.0\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = compile_model(model, moe_params)\n",
    "print(\"\\nModel loaded and compiled successfully!\")\n",
    "sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "model(sample_input[0])\n",
    "model.summary()\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self, model, test_dataset, moe_params, raw_data, word_to_id, id_to_word):\n",
    "        self.model = model\n",
    "        self.test_dataset = test_dataset\n",
    "        self.moe_params = moe_params\n",
    "        self.raw_data = raw_data\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.nlu_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.nlg_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.resource_stats = {\n",
    "            'cpu_usage': [],\n",
    "            'memory_used': [],\n",
    "            'gpu_load': [],\n",
    "            'gpu_memory': [],\n",
    "            'batch_times': []\n",
    "        }\n",
    "        self.special_tokens = {'<pad>', '<sos>', '<eos>', '<unk>'}\n",
    "        self.nlu_top_k = model.get_layer('moe_nlu').get_config().get('top_k', 2)\n",
    "        self.nlg_top_k = model.get_layer('moe_nlg').get_config().get('top_k', 2)\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(\"\\nEvaluating model on test set...\")\n",
    "        def add_response_embeddings(features, targets):\n",
    "            response_tokens = targets['response_output']\n",
    "            response_embeddings = self.model.embedding(response_tokens)\n",
    "            targets['response_embeddings'] = response_embeddings\n",
    "            return features, targets\n",
    "        test_dataset_with_embeddings = self.test_dataset.map(add_response_embeddings, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        start_time = time.time()\n",
    "        test_results = self.model.evaluate(test_dataset_with_embeddings, verbose=1)\n",
    "        eval_time = time.time() - start_time\n",
    "        metric_names = self.model.metrics_names\n",
    "        perplexity_value = None\n",
    "        for name, value in zip(metric_names, test_results):\n",
    "            if name == 'response_output_perplexity':\n",
    "                perplexity_value = value\n",
    "                break\n",
    "        if perplexity_value is None:\n",
    "            print(\"Warning: Could not find perplexity in standard evaluation results.\")\n",
    "            perplexity_value = 0.0\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory().used / (1024 ** 3)\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "        total_tokens = 0\n",
    "        total_time = 0\n",
    "        num_batches = 0\n",
    "        test_dataset_small = test_dataset_with_embeddings.unbatch().batch(moe_params[\"batch_size\"])\n",
    "        all_true_intents = []\n",
    "        all_pred_intents = []\n",
    "        all_true_domains = []\n",
    "        all_pred_domains = []\n",
    "        for batch_idx, batch in enumerate(test_dataset_small):\n",
    "            inputs, targets = batch\n",
    "            response_tokens = targets['response_output'].numpy()\n",
    "            non_padding_mask = response_tokens != 0\n",
    "            total_tokens += np.sum(non_padding_mask)\n",
    "            start_time = time.time()\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            batch_time = time.time() - start_time\n",
    "            total_time += batch_time\n",
    "            num_batches += tf.shape(inputs['user_utterance_tokens'])[0]\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            self.resource_stats['cpu_usage'].append(cpu_usage)\n",
    "            self.resource_stats['memory_used'].append(memory.used / (1024 ** 3))\n",
    "            self.resource_stats['gpu_load'].append(gpu_load)\n",
    "            self.resource_stats['gpu_memory'].append(gpu_memory)\n",
    "            self.resource_stats['batch_times'].append(batch_time)\n",
    "            nlu_gate_weights = outputs.get('nlu_gate_weights', tf.zeros([0, self.model.moe_nlu.routed_experts])).numpy()\n",
    "            nlu_top_indices = np.argsort(-nlu_gate_weights, axis=-1)[:, :self.nlu_top_k] + self.model.moe_nlu.k_s\n",
    "            for idx in range(self.nlu_top_k):\n",
    "                expert_ids = nlu_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlu_expert_usage_counts):\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "            nlg_gate_weights = outputs.get('nlg_gate_weights', tf.zeros([0, self.model.moe_nlg.routed_experts])).numpy()\n",
    "            if len(nlg_gate_weights.shape) == 3:\n",
    "                nlg_gate_weights = nlg_gate_weights.reshape(-1, nlg_gate_weights.shape[-1])\n",
    "            nlg_top_indices = np.argsort(-nlg_gate_weights, axis=-1)[:, :self.nlg_top_k] + self.model.moe_nlg.k_s\n",
    "            for idx in range(self.nlg_top_k):\n",
    "                expert_ids = nlg_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlg_expert_usage_counts):\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "            true_intents = targets.get('intent_output', np.zeros((moe_params[\"batch_size\"], self.model.num_intents))).numpy()\n",
    "            pred_intents = (outputs.get('intent_output', np.zeros_like(true_intents)).numpy() > 0.5).astype(np.int32)\n",
    "            all_true_intents.append(true_intents)\n",
    "            all_pred_intents.append(pred_intents)\n",
    "            true_domains = targets.get('domain_output', np.zeros((moe_params[\"batch_size\"], 1))).numpy().flatten()\n",
    "            pred_domains = np.argmax(outputs.get('domain_output', np.zeros((moe_params[\"batch_size\"], self.model.num_domains))).numpy(), axis=-1)\n",
    "            all_true_domains.append(true_domains)\n",
    "            all_pred_domains.append(pred_domains)\n",
    "            if batch_idx % 50 == 0:\n",
    "                gc.collect()\n",
    "                tf.keras.backend.clear_session()\n",
    "        avg_time_per_token = (total_time / total_tokens) * 1000 if total_tokens > 0 else 0\n",
    "        avg_cpu = np.mean(self.resource_stats['cpu_usage']) if self.resource_stats['cpu_usage'] else 0\n",
    "        avg_memory = np.mean(self.resource_stats['memory_used']) if self.resource_stats['memory_used'] else 0\n",
    "        avg_gpu_load = np.mean(self.resource_stats['gpu_load']) if self.resource_stats['gpu_load'] else 0\n",
    "        avg_gpu_memory = np.mean(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        peak_gpu_memory = np.max(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "        all_true_intents = np.vstack(all_true_intents)\n",
    "        all_pred_intents = np.vstack(all_pred_intents)\n",
    "        all_true_domains = np.concatenate(all_true_domains)\n",
    "        all_pred_domains = np.concatenate(all_pred_domains)\n",
    "        intent_f1_score = f1_score(all_true_intents, all_pred_intents, average='micro')\n",
    "        intent_f1_score_macro = f1_score(all_true_intents, all_pred_intents, average='macro')\n",
    "        intent_f1_score_weighted = f1_score(all_true_intents, all_pred_intents, average='weighted')\n",
    "        intent_accuracy_score = accuracy_score(all_true_intents, all_pred_intents)\n",
    "        domain_accuracy_score = accuracy_score(all_true_domains, all_pred_domains)\n",
    "        domain_f1_score_macro = f1_score(all_true_domains, all_pred_domains, average='macro')\n",
    "        hypotheses = []\n",
    "        references = []\n",
    "        meteor_scores = []\n",
    "        rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        domain_metrics = defaultdict(lambda: {\n",
    "            'bleu': [],\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'meteor': [],\n",
    "            'count': 0\n",
    "        })\n",
    "        if not self.raw_data.get('test'):\n",
    "            print(\"Warning: raw_data['test'] is empty. Skipping NLG metrics calculation.\")\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "            domain_metrics = {}\n",
    "        else:\n",
    "            for i, batch in enumerate(test_dataset_small):\n",
    "                if i >= len(self.raw_data.get('test', [])):\n",
    "                    print(f\"Warning: raw_data['test'] has fewer items ({len(self.raw_data.get('test', []))}) than test dataset. Stopping NLG metrics calculation.\")\n",
    "                    break\n",
    "                try:\n",
    "                    inputs, targets = batch\n",
    "                    with tf.device('/CPU:0'):\n",
    "                        outputs = self.model(inputs, training=False)\n",
    "                    domain_id = targets['domain_output'].numpy()[0][0]\n",
    "                    domain_name = f\"domain_{domain_id}\"\n",
    "                    response_probs = outputs['response_output'].numpy()\n",
    "                    generated_tokens = np.argmax(response_probs, axis=-1)[0]\n",
    "                    true_tokens = targets['response_output'].numpy()[0]\n",
    "                    raw_item = self.raw_data['test'][i]\n",
    "                    ref_text = raw_item.get('raw_next_system_response', '')\n",
    "                    if not ref_text:\n",
    "                        continue\n",
    "                    gen_text = self.tokens_to_text(generated_tokens)\n",
    "                    ref_tokens = self.tokens_to_text(true_tokens)\n",
    "                    if not gen_text.strip() or not ref_text.strip():\n",
    "                        continue\n",
    "                    hypotheses.append(gen_text)\n",
    "                    references.append([ref_text])\n",
    "                    ref_tokens = word_tokenize(ref_text.lower())\n",
    "                    gen_tokens = word_tokenize(gen_text.lower())\n",
    "                    try:\n",
    "                        meteor = meteor_score([ref_tokens], gen_tokens)\n",
    "                        meteor_scores.append(meteor)\n",
    "                        domain_metrics[domain_name]['meteor'].append(meteor)\n",
    "                    except Exception as e:\n",
    "                        print(f\"METEOR calculation error for example {i}: {str(e)}\")\n",
    "                        meteor_scores.append(0)\n",
    "                        domain_metrics[domain_name]['meteor'].append(0)\n",
    "                    try:\n",
    "                        rouge_scores = rouge_scorer_instance.score(ref_text, gen_text)\n",
    "                        domain_metrics[domain_name]['rouge1'].append(rouge_scores['rouge1'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rouge2'].append(rouge_scores['rouge2'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rougeL'].append(rouge_scores['rougeL'].fmeasure)\n",
    "                    except Exception as e:\n",
    "                        print(f\"ROUGE calculation error for example {i}: {str(e)}\")\n",
    "                    domain_metrics[domain_name]['count'] += 1\n",
    "                    if i % 50 == 0:\n",
    "                        gc.collect()\n",
    "                        tf.keras.backend.clear_session()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing example {i}: {str(e)}\")\n",
    "                    continue\n",
    "        if hypotheses and references:\n",
    "            hypotheses_tok = []\n",
    "            references_tok = []\n",
    "            for hyp, ref_list in zip(hypotheses, references):\n",
    "                if not hyp or not ref_list[0]:\n",
    "                    continue\n",
    "                hyp_tokens = word_tokenize(hyp.lower())\n",
    "                ref_tokens = [word_tokenize(ref.lower()) for ref in ref_list]\n",
    "                hypotheses_tok.append(hyp_tokens)\n",
    "                references_tok.append(ref_tokens)\n",
    "            chencherry = SmoothingFunction()\n",
    "            try:\n",
    "                bleu_score = corpus_bleu(references_tok, hypotheses_tok, smoothing_function=chencherry.method1)\n",
    "            except Exception as e:\n",
    "                print(f\"BLEU calculation error: {str(e)}\")\n",
    "                bleu_score = 0.0\n",
    "            rouge_scores = []\n",
    "            for hyp, ref in zip(hypotheses, references):\n",
    "                if hyp and ref[0]:\n",
    "                    try:\n",
    "                        rouge_scores.append(rouge_scorer_instance.score(ref[0], hyp))\n",
    "                    except Exception as e:\n",
    "                        print(f\"ROUGE scoring error: {str(e)}\")\n",
    "                        continue\n",
    "            avg_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_meteor = np.mean(meteor_scores) if meteor_scores else 0.0\n",
    "        else:\n",
    "            print(\"Warning: No valid hypotheses or references for NLG metrics. Setting to 0.0.\")\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "        return {\n",
    "            'intent_f1_micro': intent_f1_score,\n",
    "            'intent_f1_macro': intent_f1_score_macro,\n",
    "            'intent_f1_weighted': intent_f1_score_weighted,\n",
    "            'intent_accuracy': intent_accuracy_score,\n",
    "            'domain_accuracy': domain_accuracy_score,\n",
    "            'domain_f1_macro': domain_f1_score_macro,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge1': avg_rouge1,\n",
    "            'rouge2': avg_rouge2,\n",
    "            'rougeL': avg_rougeL,\n",
    "            'meteor': avg_meteor,\n",
    "            'perplexity': perplexity_value,\n",
    "            'pred_speed': avg_time_per_token,\n",
    "            'domain_metrics': domain_metrics,\n",
    "            'eval_time': eval_time\n",
    "        }\n",
    "    \n",
    "    def tokens_to_text(self, tokens):\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            if token == 0:\n",
    "                continue\n",
    "            word = self.id_to_word.get(token, '<unk>')\n",
    "            if word not in self.special_tokens:\n",
    "                words.append(word)\n",
    "        return \" \".join(words).strip()\n",
    "\n",
    "results_table = {\n",
    "    'Evaluation Time (seconds)': [],\n",
    "    'Prediction Speed (ms/token)': [],\n",
    "    'Average CPU Usage (percent)': [],\n",
    "    'Average GPU Usage (percent)': [],\n",
    "    'Average Memory (GB)': [],\n",
    "    'Average GPU Memory (GB)': [],\n",
    "    'Average FLOPs Estimate (GFLOPs)': [],\n",
    "    'NLU Expert 1 (percent)': [],\n",
    "    'NLU Expert 2 (percent)': [],\n",
    "    'NLU Expert 3 (percent)': [],\n",
    "    'NLU Expert 4 (percent)': [],\n",
    "    'NLU Expert 5 (percent)': [],\n",
    "    'NLU Expert 6 (percent)': [],\n",
    "    'NLU Expert 7 (percent)': [],\n",
    "    'NLU Expert 8 (percent)': [],\n",
    "    'NLG Expert 1 (percent)': [],\n",
    "    'NLG Expert 2 (percent)': [],\n",
    "    'NLG Expert 3 (percent)': [],\n",
    "    'NLG Expert 4 (percent)': [],\n",
    "    'NLG Expert 5 (percent)': [],\n",
    "    'NLG Expert 6 (percent)': [],\n",
    "    'NLG Expert 7 (percent)': [],\n",
    "    'NLG Expert 8 (percent)': [],\n",
    "    'Intent F1-score (Micro)': [],\n",
    "    'Intent F1-score (Macro)': [],\n",
    "    'Intent F1-score (Weighted)': [],\n",
    "    'Intent Accuracy': [],\n",
    "    'Domain Accuracy': [],\n",
    "    'Domain F1-score (Macro)': [],\n",
    "    'BLEU Score': [],\n",
    "    'ROUGE-1 F1': [],\n",
    "    'ROUGE-2 F1': [],\n",
    "    'ROUGE-L F1': [],\n",
    "    'METEOR Score': [],\n",
    "    'Perplexity': [],\n",
    "    'NLU Expert Load Stability (Variance)': [],\n",
    "    'NLG Expert Load Stability (Variance)': []\n",
    "}\n",
    "\n",
    "num_iterations = 5\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    print(f\"\\nStarting Iteration {iteration}\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "    model = compile_model(model, moe_params)\n",
    "    evaluator = EvaluationMetrics(model, test_tf_dataset, moe_params, raw_data, word_to_id, id_to_word)\n",
    "    evaluation_results = evaluator.evaluate()\n",
    "    results_table['Evaluation Time (seconds)'].append(evaluation_results['eval_time'])\n",
    "    results_table['Prediction Speed (ms/token)'].append(evaluation_results['pred_speed'])\n",
    "    results_table['Average CPU Usage (percent)'].append(np.mean(evaluator.resource_stats['cpu_usage']) if evaluator.resource_stats['cpu_usage'] else 0)\n",
    "    results_table['Average GPU Usage (percent)'].append(np.mean(evaluator.resource_stats['gpu_load']) if evaluator.resource_stats['gpu_load'] else 0)\n",
    "    results_table['Average Memory (GB)'].append(np.mean(evaluator.resource_stats['memory_used']) if evaluator.resource_stats['memory_used'] else 0)\n",
    "    results_table['Average GPU Memory (GB)'].append(np.mean(evaluator.resource_stats['gpu_memory']) if evaluator.resource_stats['gpu_memory'] else 0)\n",
    "    results_table['Average FLOPs Estimate (GFLOPs)'].append(calculate_flops(model))\n",
    "    nlu_counts = evaluator.nlu_expert_usage_counts\n",
    "    nlg_counts = evaluator.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) if np.sum(nlu_counts) > 0 else 1\n",
    "    total_nlg = np.sum(nlg_counts) if np.sum(nlg_counts) > 0 else 1\n",
    "    results_table['NLU Expert 1 (percent)'].append((nlu_counts[0] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 2 (percent)'].append((nlu_counts[1] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 3 (percent)'].append((nlu_counts[2] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 4 (percent)'].append((nlu_counts[3] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 5 (percent)'].append((nlu_counts[4] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 6 (percent)'].append((nlu_counts[5] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 7 (percent)'].append((nlu_counts[6] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 8 (percent)'].append((nlu_counts[7] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLG Expert 1 (percent)'].append((nlg_counts[0] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 2 (percent)'].append((nlg_counts[1] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 3 (percent)'].append((nlg_counts[2] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 4 (percent)'].append((nlg_counts[3] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 5 (percent)'].append((nlg_counts[4] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 6 (percent)'].append((nlg_counts[5] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 7 (percent)'].append((nlg_counts[6] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 8 (percent)'].append((nlg_counts[7] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['Intent F1-score (Micro)'].append(evaluation_results['intent_f1_micro'])\n",
    "    results_table['Intent F1-score (Macro)'].append(evaluation_results['intent_f1_macro'])\n",
    "    results_table['Intent F1-score (Weighted)'].append(evaluation_results['intent_f1_weighted'])\n",
    "    results_table['Intent Accuracy'].append(evaluation_results['intent_accuracy'])\n",
    "    results_table['Domain Accuracy'].append(evaluation_results['domain_accuracy'])\n",
    "    results_table['Domain F1-score (Macro)'].append(evaluation_results['domain_f1_macro'])\n",
    "    results_table['BLEU Score'].append(evaluation_results['bleu'])\n",
    "    results_table['ROUGE-1 F1'].append(evaluation_results['rouge1'])\n",
    "    results_table['ROUGE-2 F1'].append(evaluation_results['rouge2'])\n",
    "    results_table['ROUGE-L F1'].append(evaluation_results['rougeL'])\n",
    "    results_table['METEOR Score'].append(evaluation_results['meteor'])\n",
    "    results_table['Perplexity'].append(evaluation_results['perplexity'])\n",
    "    results_table['NLU Expert Load Stability (Variance)'].append(np.var(nlu_counts / total_nlu * 100) if total_nlu > 0 else 0)\n",
    "    results_table['NLG Expert Load Stability (Variance)'].append(np.var(nlg_counts / total_nlg * 100) if total_nlg > 0 else 0)\n",
    "\n",
    "for key in results_table:\n",
    "    if results_table[key]:\n",
    "        results_table[key].append(np.mean(results_table[key]))\n",
    "    else:\n",
    "        results_table[key].append(0)\n",
    "\n",
    "print(\"\\nTraining results saved\")\n",
    "final_df = pd.DataFrame(results_table)\n",
    "final_df = final_df.T\n",
    "final_df.columns = [f'Iteration {i+1}' for i in range(num_iterations)] + ['Average']\n",
    "final_df.to_excel('prediction_deepseekefficientdis_results.xlsx', index=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE with Adaptive Gating experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 07:45:18.064145: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 07:45:18.525698: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 07:45:18.525836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 07:45:18.607107: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 07:45:18.788772: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 07:45:20.345274: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training Iteration 1\n",
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 07:45:24.242955: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:45:24.533699: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:45:24.533929: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:45:24.535418: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:45:24.535572: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:45:24.535658: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:45:24.613847: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:45:24.614035: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:45:24.614141: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:45:24.614218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/4428 [00:00<?, ?it/s]2025-07-04 07:45:47.801211: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f9930001a50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 07:45:47.801252: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2025-07-04 07:45:47.820690: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 07:45:47.859124: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751615147.988813   12261 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 1: 100%|██████████| 4428/4428 [02:47<00:00, 26.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Iteration 2</th>\n",
       "      <th>Iteration 3</th>\n",
       "      <th>Iteration 4</th>\n",
       "      <th>Iteration 5</th>\n",
       "      <th>Iteration 6</th>\n",
       "      <th>Iteration 7</th>\n",
       "      <th>Iteration 8</th>\n",
       "      <th>Iteration 9</th>\n",
       "      <th>Iteration 10</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Speed (epochs per second)</th>\n",
       "      <td>0.006109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training Time (second/epoch)</th>\n",
       "      <td>163.685818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.368582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Training Time (second/iteration)</th>\n",
       "      <td>169.437418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.943742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computational Resource Usage</th>\n",
       "      <td>49.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>26.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>6.552757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.531799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.453180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPS Estimate (GFLOPS)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLU Load Distribution Stability</th>\n",
       "      <td>16.736093</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.673609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLG Load Distribution Stability</th>\n",
       "      <td>250.585326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.058533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 1 (percent)</th>\n",
       "      <td>18.064991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.806499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 2 (percent)</th>\n",
       "      <td>13.081680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.308168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 3 (percent)</th>\n",
       "      <td>24.168298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.416830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 4 (percent)</th>\n",
       "      <td>11.690591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.169059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 5 (percent)</th>\n",
       "      <td>14.994572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.499457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 6 (percent)</th>\n",
       "      <td>17.999863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.799986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 7 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 8 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 1 (percent)</th>\n",
       "      <td>12.826283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.282628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 2 (percent)</th>\n",
       "      <td>26.077804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.607780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 3 (percent)</th>\n",
       "      <td>4.663063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.466306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 4 (percent)</th>\n",
       "      <td>5.504985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.550499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 5 (percent)</th>\n",
       "      <td>3.364836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.336484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 6 (percent)</th>\n",
       "      <td>47.563035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.756303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 7 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 8 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Entity Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Intent Accuracy</th>\n",
       "      <td>0.845980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Perplexity</th>\n",
       "      <td>18.248472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.824847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Response Cosine Similarity</th>\n",
       "      <td>0.338724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Iteration 1  Iteration 2  \\\n",
       "Training Speed (epochs per second)                0.006109          0.0   \n",
       "Training Time (second/epoch)                    163.685818          0.0   \n",
       "Total Training Time (second/iteration)          169.437418          0.0   \n",
       "Computational Resource Usage                     49.200000          0.0   \n",
       "Average CPU Usage (percent)                      26.200000          0.0   \n",
       "Average GPU Usage (percent)                      23.000000          0.0   \n",
       "Average Memory (GB)                               6.552757          0.0   \n",
       "Average GPU Memory (GB)                          14.531799          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                   2.377871          0.0   \n",
       "Expert NLU Load Distribution Stability           16.736093          0.0   \n",
       "Expert NLG Load Distribution Stability          250.585326          0.0   \n",
       "Average NLU Expert 1 (percent)                   18.064991          0.0   \n",
       "Average NLU Expert 2 (percent)                   13.081680          0.0   \n",
       "Average NLU Expert 3 (percent)                   24.168298          0.0   \n",
       "Average NLU Expert 4 (percent)                   11.690591          0.0   \n",
       "Average NLU Expert 5 (percent)                   14.994572          0.0   \n",
       "Average NLU Expert 6 (percent)                   17.999863          0.0   \n",
       "Average NLU Expert 7 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 8 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 1 (percent)                   12.826283          0.0   \n",
       "Average NLG Expert 2 (percent)                   26.077804          0.0   \n",
       "Average NLG Expert 3 (percent)                    4.663063          0.0   \n",
       "Average NLG Expert 4 (percent)                    5.504985          0.0   \n",
       "Average NLG Expert 5 (percent)                    3.364836          0.0   \n",
       "Average NLG Expert 6 (percent)                   47.563035          0.0   \n",
       "Average NLG Expert 7 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 8 (percent)                    0.000000          0.0   \n",
       "Average Validation Entity Accuracy                1.000000          0.0   \n",
       "Average Intent Accuracy                           0.845980          0.0   \n",
       "Average Validation Perplexity                    18.248472          0.0   \n",
       "Average Validation Response Cosine Similarity     0.338724          0.0   \n",
       "\n",
       "                                               Iteration 3  Iteration 4  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 5  Iteration 6  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 7  Iteration 8  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 9  Iteration 10  \\\n",
       "Training Speed (epochs per second)                     0.0           0.0   \n",
       "Training Time (second/epoch)                           0.0           0.0   \n",
       "Total Training Time (second/iteration)                 0.0           0.0   \n",
       "Computational Resource Usage                           0.0           0.0   \n",
       "Average CPU Usage (percent)                            0.0           0.0   \n",
       "Average GPU Usage (percent)                            0.0           0.0   \n",
       "Average Memory (GB)                                    0.0           0.0   \n",
       "Average GPU Memory (GB)                                0.0           0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0           0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0           0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0           0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0           0.0   \n",
       "Average Validation Entity Accuracy                     0.0           0.0   \n",
       "Average Intent Accuracy                                0.0           0.0   \n",
       "Average Validation Perplexity                          0.0           0.0   \n",
       "Average Validation Response Cosine Similarity          0.0           0.0   \n",
       "\n",
       "                                                 Average  \n",
       "Training Speed (epochs per second)              0.000611  \n",
       "Training Time (second/epoch)                   16.368582  \n",
       "Total Training Time (second/iteration)         16.943742  \n",
       "Computational Resource Usage                    4.920000  \n",
       "Average CPU Usage (percent)                     2.620000  \n",
       "Average GPU Usage (percent)                     2.300000  \n",
       "Average Memory (GB)                             0.655276  \n",
       "Average GPU Memory (GB)                         1.453180  \n",
       "Average FLOPS Estimate (GFLOPS)                 0.237787  \n",
       "Expert NLU Load Distribution Stability          1.673609  \n",
       "Expert NLG Load Distribution Stability         25.058533  \n",
       "Average NLU Expert 1 (percent)                  1.806499  \n",
       "Average NLU Expert 2 (percent)                  1.308168  \n",
       "Average NLU Expert 3 (percent)                  2.416830  \n",
       "Average NLU Expert 4 (percent)                  1.169059  \n",
       "Average NLU Expert 5 (percent)                  1.499457  \n",
       "Average NLU Expert 6 (percent)                  1.799986  \n",
       "Average NLU Expert 7 (percent)                  0.000000  \n",
       "Average NLU Expert 8 (percent)                  0.000000  \n",
       "Average NLG Expert 1 (percent)                  1.282628  \n",
       "Average NLG Expert 2 (percent)                  2.607780  \n",
       "Average NLG Expert 3 (percent)                  0.466306  \n",
       "Average NLG Expert 4 (percent)                  0.550499  \n",
       "Average NLG Expert 5 (percent)                  0.336484  \n",
       "Average NLG Expert 6 (percent)                  4.756303  \n",
       "Average NLG Expert 7 (percent)                  0.000000  \n",
       "Average NLG Expert 8 (percent)                  0.000000  \n",
       "Average Validation Entity Accuracy              0.100000  \n",
       "Average Intent Accuracy                         0.084598  \n",
       "Average Validation Perplexity                   1.824847  \n",
       "Average Validation Response Cosine Similarity   0.033872  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reset_kernel():\n",
    "    tf.keras.backend.clear_session()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "results_table = {\n",
    "    'Training Speed (epochs per second)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Training Time (second/epoch)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Total Training Time (second/iteration)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Computational Resource Usage': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average CPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average GPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average GPU Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average FLOPS Estimate (GFLOPS)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Expert NLU Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Expert NLG Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average Validation Entity Accuracy': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average Intent Accuracy': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average Validation Perplexity': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average Validation Response Cosine Similarity': {f'Iteration {i+1}': 0 for i in range(10)}\n",
    "}\n",
    "for key in results_table:\n",
    "    results_table[key]['Average'] = 0\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_deepseekmoe_adaptive_gating_model\"\n",
    "\n",
    "for iteration in range(5):\n",
    "    print(f\"\\nStarting Training Iteration {iteration + 1}\")\n",
    "    reset_kernel()\n",
    "\n",
    "    def load_tf_datasets_from_disk(load_path):\n",
    "        print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "        try:\n",
    "            with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "                loaded_moe_params = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "        element_spec = (\n",
    "            {\n",
    "                'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            },\n",
    "            {\n",
    "                'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "                'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "                'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "            test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "        raw_data = {}\n",
    "        for dataset_name in ['train', 'test']:\n",
    "            path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "            if os.path.exists(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    raw_data[dataset_name] = pickle.load(f)\n",
    "                print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Raw data file {path} not found.\")\n",
    "                raw_data[dataset_name] = []\n",
    "\n",
    "        return {\n",
    "            \"train_dataset\": train_tf_dataset,\n",
    "            \"test_dataset\": test_tf_dataset,\n",
    "            \"moe_params\": loaded_moe_params,\n",
    "            \"raw_data\": raw_data\n",
    "        }\n",
    "\n",
    "    tf_dataset_save_path = \"tf_datasets\"\n",
    "    loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "    train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "    moe_params = loaded_data[\"moe_params\"]\n",
    "    raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "    class DeepSeekMoELayer(layers.Layer):\n",
    "        def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, tau=0.1, name=None):\n",
    "            super(DeepSeekMoELayer, self).__init__(name=name)\n",
    "            self.m = m\n",
    "            self.k_s = k_s\n",
    "            self.total_experts = num_experts * self.m\n",
    "            self.routed_experts = self.total_experts - self.k_s\n",
    "            self.alpha = alpha\n",
    "            self.tau = tau\n",
    "            self.input_dim = input_dim\n",
    "            self.seq_length = None\n",
    "            self.adjusted_expert_dim = expert_dim // self.m\n",
    "\n",
    "            self.shared_experts = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                    layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "                ], name=f'shared_expert_{i}') for i in range(self.k_s)\n",
    "            ]\n",
    "            self.routed_experts_list = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                    layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "                ], name=f'routed_expert_{i}') for i in range(self.routed_experts)\n",
    "            ]\n",
    "            self.experts = self.shared_experts + self.routed_experts_list\n",
    "            self.gate = layers.Dense(self.routed_experts, activation=None, name='gate_weights')\n",
    "            self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            input_rank = inputs.shape.rank\n",
    "            if input_rank == 3:\n",
    "                batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "                flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "                is_3d = True\n",
    "            else:\n",
    "                batch_size = tf.shape(inputs)[0]\n",
    "                seq_length = 1\n",
    "                flat_inputs = inputs\n",
    "                is_3d = False\n",
    "\n",
    "            flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "            num_tokens = tf.shape(flat_inputs)[0]\n",
    "\n",
    "            shared_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "            for i in range(self.k_s):\n",
    "                shared_output += self.shared_experts[i](flat_inputs)\n",
    "\n",
    "            gate_logits = self.gate(flat_inputs)\n",
    "            gate_weights = tf.nn.softmax(gate_logits, axis=-1)\n",
    "            sorted_weights = tf.sort(gate_weights, axis=-1, direction='DESCENDING')\n",
    "            p1 = sorted_weights[:, 0]\n",
    "            p2 = sorted_weights[:, 1]\n",
    "            use_top_1 = tf.cast(p1 - p2 >= self.tau, tf.float32)\n",
    "            k_per_token = tf.where(use_top_1 > 0, 1, 2)\n",
    "\n",
    "            max_k = 2\n",
    "            top_k_values, top_k_indices = tf.nn.top_k(gate_weights, k=max_k, sorted=True)\n",
    "            top_k_indices = top_k_indices + self.k_s\n",
    "            top_k_weights = top_k_values / (tf.reduce_sum(top_k_values, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "\n",
    "            k_mask = tf.range(max_k, dtype=tf.int32) < tf.expand_dims(k_per_token, -1)\n",
    "            k_mask = tf.cast(k_mask, tf.float32)\n",
    "            top_k_weights = top_k_weights * k_mask\n",
    "            top_k_weights = top_k_weights / (tf.reduce_sum(top_k_weights, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "\n",
    "            expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "            routed_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "            for k in range(max_k):\n",
    "                kth_weights = top_k_weights[:, k]\n",
    "                kth_indices = top_k_indices[:, k]\n",
    "                mask = tf.one_hot(kth_indices, depth=self.total_experts, dtype=tf.float32)\n",
    "                kth_expert_output = tf.reduce_sum(expert_outputs * tf.expand_dims(mask, -1), axis=1)\n",
    "                weighted_kth_output = kth_expert_output * tf.expand_dims(kth_weights, -1)\n",
    "                routed_output += weighted_kth_output\n",
    "\n",
    "            output_flat = shared_output + routed_output + flat_inputs\n",
    "\n",
    "            if is_3d:\n",
    "                output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "                if self.seq_length is not None:\n",
    "                    output.set_shape([None, self.seq_length, self.input_dim])\n",
    "                gate_weights_reshaped = tf.reshape(gate_weights, [batch_size, seq_length, self.routed_experts])\n",
    "                if self.seq_length is not None:\n",
    "                    gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "            else:\n",
    "                output = output_flat\n",
    "                gate_weights_reshaped = gate_weights\n",
    "\n",
    "            f_i = tf.reduce_mean(tf.reduce_sum(tf.one_hot(tf.argmax(gate_weights, axis=-1), depth=self.routed_experts), axis=0))\n",
    "            P_i = tf.reduce_mean(gate_weights, axis=0)\n",
    "            load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "            self.add_loss(load_balancing_loss)\n",
    "            self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "\n",
    "            return output, gate_weights_reshaped\n",
    "\n",
    "        def get_metrics(self):\n",
    "            return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'num_experts': self.total_experts // self.m,\n",
    "                'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "                'input_dim': self.input_dim,\n",
    "                'm': self.m,\n",
    "                'k_s': self.k_s,\n",
    "                'alpha': self.alpha,\n",
    "                'tau': self.tau,\n",
    "                'name': self.name\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class TransformerDecoderLayer(layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "            super(TransformerDecoderLayer, self).__init__()\n",
    "            self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.ffn = keras.Sequential([\n",
    "                layers.Dense(dff, activation='relu'),\n",
    "                layers.Dense(d_model)\n",
    "            ])\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "            self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "        def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "            if look_ahead_mask is not None:\n",
    "                look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "                look_ahead_mask = 1.0 - look_ahead_mask\n",
    "            attn1_output = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)\n",
    "            attn1 = self.dropout1(attn1_output, training=training)\n",
    "            out1 = self.layernorm1(attn1 + x)\n",
    "            attn2_output = self.mha2(query=out1, value=enc_output, key=enc_output, training=training)\n",
    "            attn2 = self.dropout2(attn2_output, training=training)\n",
    "            out2 = self.layernorm2(attn2 + out1)\n",
    "            ffn_output = self.ffn(out2)\n",
    "            ffn_output = self.dropout3(ffn_output, training=training)\n",
    "            out3 = self.layernorm3(ffn_output + out2)\n",
    "            return out3\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            mha1_config = self.mha1.get_config()\n",
    "            config.update({\n",
    "                'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "                'num_heads': mha1_config['num_heads'],\n",
    "                'dff': self.ffn.layers[0].units,\n",
    "                'rate': self.dropout1.rate\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class MoEModel(models.Model):\n",
    "        def __init__(self, moe_params):\n",
    "            super(MoEModel, self).__init__()\n",
    "            self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "            self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "            self.num_domains = moe_params[\"num_domains\"]\n",
    "            self.num_intents = moe_params[\"num_intents\"]\n",
    "            self.vocab_size = moe_params[\"vocab_size\"]\n",
    "            self.num_experts = moe_params[\"num_experts\"]\n",
    "            self.expert_dim = moe_params[\"expert_dim\"]\n",
    "            self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "            self.num_heads = 4\n",
    "            self.dff = self.embedding_dim * 4\n",
    "            self.dropout_rate = 0.1\n",
    "            self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                                 self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "            self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "            self.embedding = layers.Embedding(\n",
    "                self.vocab_size,\n",
    "                self.embedding_dim,\n",
    "                mask_zero=True,\n",
    "                embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "                name=\"embedding\"\n",
    "            )\n",
    "            self.embedding.build((None,))\n",
    "            with tf.init_scope():\n",
    "                embedding_weights = self.embedding.get_weights()\n",
    "                if embedding_weights and len(embedding_weights) > 0:\n",
    "                    embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                    self.embedding.set_weights(embedding_weights)\n",
    "            self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "            self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "            self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "            self.moe_nlu = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlu_input_dim, m=2, k_s=2, alpha=0.01, tau=0.1, name='moe_nlu')\n",
    "            self.moe_nlg = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlg_input_dim, m=2, k_s=2, alpha=0.01, tau=0.1, name='moe_nlg')\n",
    "            self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "            self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "            self.response_output = layers.TimeDistributed(layers.Dense(self.vocab_size, activation='softmax'), name='response_output')\n",
    "            self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "            self.nlg_projection = layers.TimeDistributed(layers.Dense(self.embedding_dim, activation='relu'), name='nlg_projection')\n",
    "\n",
    "        def create_look_ahead_mask(self, size):\n",
    "            mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "            return mask\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "            prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "            decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "            domain_onehot_input = inputs['domain_onehot_input']\n",
    "            turn_id_embedding = inputs['turn_id_embedding']\n",
    "            ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "            pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "            user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "            prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "            decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "            user_enc_out = user_embedded\n",
    "            prev_system_enc_out = prev_system_embedded\n",
    "            look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "            dec_output = decoder_embedded\n",
    "            for i, layer in enumerate(self.decoder_layers):\n",
    "                dec_output = layer(dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask)\n",
    "            decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "            user_attn_out = self.attn_layer(query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), value=user_enc_out, key=user_enc_out, training=training)\n",
    "            prev_system_attn_out = self.attn_layer(query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), value=prev_system_enc_out, key=prev_system_enc_out, training=training)\n",
    "            decoder_attn_out = self.attn_layer(query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), value=decoder_output, key=decoder_output, training=training)\n",
    "            combined_features = layers.Concatenate()([\n",
    "                tf.squeeze(user_attn_out, 1),\n",
    "                tf.squeeze(prev_system_attn_out, 1),\n",
    "                tf.squeeze(decoder_attn_out, 1),\n",
    "                domain_onehot_input,\n",
    "                turn_id_embedding,\n",
    "                ontology_multihot_input\n",
    "            ])\n",
    "            combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "            nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "            nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "            intent_out = self.intent_output(nlu_out)\n",
    "            domain_out = self.domain_output(nlu_out)\n",
    "            intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "            domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "            intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "            domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "            nlg_input = layers.Concatenate(axis=-1)([\n",
    "                decoder_output,\n",
    "                intent_out_tiled,\n",
    "                domain_out_tiled\n",
    "            ])\n",
    "            nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "            nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            response_embeddings = self.nlg_projection(nlg_out)\n",
    "            response_embeddings = tf.ensure_shape(response_embeddings, [None, self.max_seq_length, self.embedding_dim])\n",
    "            response_out = self.response_output(nlg_out)\n",
    "            return {\n",
    "                'intent_output': intent_out,\n",
    "                'domain_output': domain_out,\n",
    "                'response_output': response_out,\n",
    "                'response_embeddings': response_embeddings,\n",
    "                'nlu_gate_weights': nlu_gate_weights,\n",
    "                'nlg_gate_weights': nlg_gate_weights\n",
    "            }\n",
    "\n",
    "        def build_graph(self):\n",
    "            inputs = {\n",
    "                'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "                'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='prev_system_response_tokens'),\n",
    "                'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='decoder_input_tokens'),\n",
    "                'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32, name='domain_onehot_input'),\n",
    "                'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32, name='turn_id_embedding'),\n",
    "                'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32, name='ontology_multihot_input'),\n",
    "            }\n",
    "            return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'moe_params': {\n",
    "                    'embedding_dim': self.embedding_dim,\n",
    "                    'max_seq_length': self.max_seq_length,\n",
    "                    'num_domains': self.num_domains,\n",
    "                    'num_intents': self.num_intents,\n",
    "                    'vocab_size': self.vocab_size,\n",
    "                    'num_experts': self.num_experts,\n",
    "                    'expert_dim': self.expert_dim,\n",
    "                    'turn_id_dim': self.turn_id_dim\n",
    "                }\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class IntentAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros')\n",
    "            self.total_preds = self.add_weight(name='total_preds', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "            self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "            self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.correct_preds.assign(0.)\n",
    "            self.total_preds.assign(0.)\n",
    "\n",
    "    class IntentPrecision(metrics.Metric):\n",
    "        def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "            self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            predicted_positives = tf.reduce_sum(y_pred)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.predicted_positives.assign(0.)\n",
    "\n",
    "    class IntentRecall(metrics.Metric):\n",
    "        def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "            self.actual_positives = self.add_weight(name='ap', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            actual_positives = tf.reduce_sum(y_true)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.actual_positives.assign(0.)\n",
    "\n",
    "    class IntentF1(metrics.Metric):\n",
    "        def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.precision = IntentPrecision(threshold=threshold)\n",
    "            self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "            self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "        def result(self):\n",
    "            p = self.precision.result()\n",
    "            r = self.recall.result()\n",
    "            return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.precision.reset_state()\n",
    "            self.recall.reset_state()\n",
    "\n",
    "    class DomainAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='domain_accuracy', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.accuracy = self.add_weight(name='acc', initializer='zeros')\n",
    "            self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "            pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "            matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "            self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "            self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.accuracy.assign(0.)\n",
    "            self.count.assign(0.)\n",
    "\n",
    "    class Perplexity(metrics.Metric):\n",
    "        def __init__(self, name='perplexity', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "            self.total_loss = self.add_weight(name='total_loss', initializer='zeros')\n",
    "            self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "            loss = self.cross_entropy(y_true, y_pred)\n",
    "            masked_loss = loss * mask\n",
    "            sum_loss = tf.reduce_sum(masked_loss)\n",
    "            num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "            self.total_loss.assign_add(sum_loss)\n",
    "            self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "        def result(self):\n",
    "            avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "            return tf.exp(avg_loss)\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_loss.assign(0.)\n",
    "            self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "    class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "        def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "            self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "            y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "            y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "            y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "            cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "            non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "            non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "            masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "            num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "            avg_cosine_sim_per_sample = tf.where(\n",
    "                num_non_zero_tokens > 0,\n",
    "                tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "                tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "            )\n",
    "            self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "            self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_cosine_sim.assign(0.)\n",
    "            self.num_samples.assign(0.)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "        flops = 0\n",
    "        functional_model = model.build_graph()\n",
    "\n",
    "        def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "            layer_flops_count = 0\n",
    "\n",
    "            if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "                return 0\n",
    "\n",
    "            if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "                for sub_layer in layer_instance.layers:\n",
    "                    layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "                return layer_flops_count\n",
    "\n",
    "            if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "                return 0\n",
    "\n",
    "            if isinstance(layer_instance, layers.Dense):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                output_dim = layer_instance.output_shape[-1]\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.LSTM):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                hidden_dim = layer_instance.units\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "                layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.Embedding):\n",
    "                pass\n",
    "\n",
    "            elif isinstance(layer_instance, DeepSeekMoELayer):\n",
    "                moe_input_dim = layer_instance.input_dim\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "                layer_flops_count += gate_flops\n",
    "                for expert in layer_instance.experts:\n",
    "                    layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "                config = layer_instance.get_config()\n",
    "                num_heads = config['num_heads']\n",
    "                key_dim = config['key_dim']\n",
    "                d_model = num_heads * key_dim\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "                projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "                attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "                context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "                output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "                layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "            elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "                inner_layer = layer_instance.layer\n",
    "                td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "                layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "                ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "            return layer_flops_count\n",
    "\n",
    "        for layer in functional_model.layers:\n",
    "            flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "        return flops / 1e9\n",
    "\n",
    "    def get_gpu_stats():\n",
    "        if nvidia_smi is None:\n",
    "            return 0, 0\n",
    "        try:\n",
    "            nvidia_smi.nvmlInit()\n",
    "            handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "            gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "            mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "            gpu_memory = mem_info.used / (1024 ** 3)\n",
    "            nvidia_smi.nvmlShutdown()\n",
    "            return gpu_load, gpu_memory\n",
    "        except Exception:\n",
    "            return 0, 0\n",
    "\n",
    "    class ResourceMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, validation_data):\n",
    "            super().__init__()\n",
    "            self.resource_stats = []\n",
    "            self.expert_load_history = []\n",
    "            self.start_time = None\n",
    "            self.epoch_start_time = None\n",
    "            self.flops = None\n",
    "            self.validation_data = validation_data\n",
    "            self.nlu_expert_usage_counts = None\n",
    "            self.nlg_expert_usage_counts = None\n",
    "            self.nlu_gate_weights_history = []\n",
    "            self.nlg_gate_weights_history = []\n",
    "\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = time.time()\n",
    "            try:\n",
    "                self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "                self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "                self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "            except Exception:\n",
    "                self.flops = 0\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.epoch_start_time = time.time()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.resource_stats.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_time': epoch_time,\n",
    "                'cpu_usage': cpu_usage,\n",
    "                'memory_used': memory.used / (1024 ** 3),\n",
    "                'memory_total': memory.total / (1024 ** 3),\n",
    "                'gpu_load': gpu_load,\n",
    "                'gpu_memory': gpu_memory,\n",
    "                'loss': logs.get('loss', 0),\n",
    "                'val_loss': logs.get('val_loss', 0),\n",
    "                'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "                'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "                'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "                'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "                'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "                'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "                'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "                'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "                'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "                'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "                'perplexity': logs.get('response_output_perplexity', 0),\n",
    "                'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "                'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "                'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "            })\n",
    "            if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "                self.expert_load_history.append({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                    'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "                })\n",
    "\n",
    "            sample_size = 100\n",
    "            for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "                inputs, _ = batch\n",
    "                outputs = self.model(inputs, training=False)\n",
    "                nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "                reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "                self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "                nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "                reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "                self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            self.total_time = time.time() - self.start_time\n",
    "\n",
    "            if len(self.nlu_gate_weights_history) > 0:\n",
    "                all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "                nlu_usage = np.mean(all_nlu_weights, axis=0)\n",
    "                total_nlu_usage = np.sum(nlu_usage)\n",
    "                self.nlu_expert_usage_counts = np.array([usage * 100 / total_nlu_usage for usage in nlu_usage]) if total_nlu_usage > 0 else np.zeros(self.model.moe_nlu.routed_experts)\n",
    "\n",
    "            if len(self.nlg_gate_weights_history) > 0:\n",
    "                all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "                nlg_usage = np.mean(all_nlg_weights, axis=0)\n",
    "                total_nlg_usage = np.sum(nlg_usage)\n",
    "                self.nlg_expert_usage_counts = np.array([usage * 100 / total_nlg_usage for usage in nlg_usage]) if total_nlg_usage > 0 else np.zeros(self.model.moe_nlg.routed_experts)\n",
    "\n",
    "        def visualize_expert_load_distribution(self):\n",
    "            nlu_stability = np.var(self.nlu_expert_usage_counts)\n",
    "            nlg_stability = np.var(self.nlg_expert_usage_counts)\n",
    "            return {'nlu_percentages': self.nlu_expert_usage_counts[:8], 'nlg_percentages': self.nlg_expert_usage_counts[:8], 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "    class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super().__init__()\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "    model = MoEModel(moe_params)\n",
    "    embedding_layer = model.embedding\n",
    "\n",
    "    def create_validation_split(dataset, embedding_layer, validation_split=0.2, batch_size=moe_params[\"batch_size\"]):\n",
    "        element_spec = dataset.element_spec\n",
    "        data_list = [(features, targets) for features, targets in dataset.unbatch().as_numpy_iterator()]\n",
    "        if not data_list:\n",
    "            raise ValueError(\"Dataset is empty.\")\n",
    "        if len(data_list) < 2:\n",
    "            raise ValueError(\"Dataset too small to split.\")\n",
    "        train_data, val_data = train_test_split(data_list, test_size=validation_split, shuffle=True)\n",
    "\n",
    "        def create_dataset_from_list(data):\n",
    "            if not data:\n",
    "                raise ValueError(\"Empty data list provided.\")\n",
    "            features = {\n",
    "                'user_utterance_tokens': np.array([d[0]['user_utterance_tokens'] for d in data], dtype=np.int32),\n",
    "                'prev_system_response_tokens': np.array([d[0]['prev_system_response_tokens'] for d in data], dtype=np.int32),\n",
    "                'decoder_input_tokens': np.array([d[0]['decoder_input_tokens'] for d in data], dtype=np.int32),\n",
    "                'domain_onehot_input': np.array([d[0]['domain_onehot_input'] for d in data], dtype=np.float32),\n",
    "                'turn_id_embedding': np.array([d[0]['turn_id_embedding'] for d in data], dtype=np.float32),\n",
    "                'ontology_multihot_input': np.array([d[0]['ontology_multihot_input'] for d in data], dtype=np.float32),\n",
    "            }\n",
    "            response_output = np.array([d[1]['response_output'] for d in data], dtype=np.int32)\n",
    "            response_embeddings = embedding_layer(response_output).numpy()\n",
    "            targets = {\n",
    "                'domain_output': np.array([d[1]['domain_output'] for d in data], dtype=np.int32),\n",
    "                'intent_output': np.array([d[1]['intent_output'] for d in data], dtype=np.float32),\n",
    "                'response_output': response_output,\n",
    "                'response_embeddings': response_embeddings\n",
    "            }\n",
    "            return tf.data.Dataset.from_tensor_slices((features, targets)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        train_dataset = create_dataset_from_list(train_data)\n",
    "        val_dataset = create_dataset_from_list(val_data)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    train_tf_dataset, val_tf_dataset = create_validation_split(train_tf_dataset, embedding_layer)\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': losses.BinaryCrossentropy(),\n",
    "        'domain_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={'intent_output': 0.5, 'domain_output': 0.5, 'response_output': 1.0, 'response_embeddings': 1.0}\n",
    "    )\n",
    "\n",
    "    sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "    model(sample_input[0])\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=8, mode='min', restore_best_weights=True)\n",
    "    resource_monitor = ResourceMonitor(val_tf_dataset)\n",
    "\n",
    "    class TQDMProgressBar(callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.progress_bar = tqdm(total=len(train_tf_dataset), desc=f'Epoch {epoch + 1}')\n",
    "\n",
    "        def on_batch_end(self, batch, logs=None):\n",
    "            self.progress_bar.update(1)\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            self.progress_bar.close()\n",
    "\n",
    "    tqdm_callback = TQDMProgressBar()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_tf_dataset,\n",
    "        epochs=100,\n",
    "        validation_data=val_tf_dataset,\n",
    "        callbacks=[early_stopping, resource_monitor, tqdm_callback],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    expert_stats = resource_monitor.visualize_expert_load_distribution()\n",
    "    stats = resource_monitor.resource_stats\n",
    "    avg_epoch_duration = np.mean([s['epoch_time'] for s in stats]) if stats else 0\n",
    "    total_training_time = resource_monitor.total_time\n",
    "    avg_cpu_usage = np.mean([s['cpu_usage'] for s in stats]) if stats else 0\n",
    "    avg_memory = np.mean([s['memory_used'] for s in stats]) if stats else 0\n",
    "    avg_gpu_load = np.mean([s['gpu_load'] for s in stats]) if stats else 0\n",
    "    avg_gpu_memory = np.mean([s['gpu_memory'] for s in stats]) if stats else 0\n",
    "    avg_flops = resource_monitor.flops if resource_monitor.flops else 0\n",
    "    nlu_counts = resource_monitor.nlu_expert_usage_counts\n",
    "    nlg_counts = resource_monitor.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) or 1\n",
    "    total_nlg = np.sum(nlg_counts) or 1\n",
    "    expert_nlu_percentages = resource_monitor.visualize_expert_load_distribution()['nlu_percentages']\n",
    "    expert_nlg_percentages = resource_monitor.visualize_expert_load_distribution()['nlg_percentages']\n",
    "    val_intent_accuracy = np.mean([s['val_intent_accuracy'] for s in stats if s['val_intent_accuracy'] > 0]) if stats else 0\n",
    "    val_entity_accuracy = np.mean([s['val_domain_accuracy'] for s in stats if s['val_domain_accuracy'] > 0]) if stats else 0\n",
    "    val_perplexity = np.mean([s['val_perplexity'] for s in stats if s['val_perplexity'] > 0]) if stats else 0\n",
    "    val_cosine_similarity = np.mean([s['val_cosine_similarity'] for s in stats if s['val_cosine_similarity'] > 0]) if stats and any(s['val_cosine_similarity'] > 0 for s in stats) else 0\n",
    "    \n",
    "    results_table['Training Speed (epochs per second)'][f'Iteration {iteration + 1}'] = 1 / avg_epoch_duration if avg_epoch_duration > 0 else 0\n",
    "    results_table['Training Time (second/epoch)'][f'Iteration {iteration + 1}'] = avg_epoch_duration\n",
    "    results_table['Total Training Time (second/iteration)'][f'Iteration {iteration + 1}'] = total_training_time\n",
    "    results_table['Computational Resource Usage'][f'Iteration {iteration + 1}'] = avg_cpu_usage + avg_gpu_load\n",
    "    results_table['Average CPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_cpu_usage\n",
    "    results_table['Average GPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_gpu_load\n",
    "    results_table['Average Memory (GB)'][f'Iteration {iteration + 1}'] = avg_memory\n",
    "    results_table['Average GPU Memory (GB)'][f'Iteration {iteration + 1}'] = avg_gpu_memory\n",
    "    results_table['Average FLOPS Estimate (GFLOPS)'][f'Iteration {iteration + 1}'] = avg_flops\n",
    "    results_table['Expert NLU Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlu_stability']\n",
    "    results_table['Expert NLG Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlg_stability']\n",
    "    for i in range(8):\n",
    "        if i < len(expert_stats['nlu_percentages']):\n",
    "            results_table[f'Average NLU Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlu_percentages'][i]\n",
    "        if i < len(expert_stats['nlg_percentages']):\n",
    "            results_table[f'Average NLG Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlg_percentages'][i]\n",
    "    results_table['Average Validation Entity Accuracy'][f'Iteration {iteration + 1}'] = val_entity_accuracy\n",
    "    results_table['Average Intent Accuracy'][f'Iteration {iteration + 1}'] = val_intent_accuracy\n",
    "    results_table['Average Validation Perplexity'][f'Iteration {iteration + 1}'] = val_perplexity\n",
    "    results_table['Average Validation Response Cosine Similarity'][f'Iteration {iteration + 1}'] = val_cosine_similarity\n",
    "\n",
    "for key in results_table:\n",
    "    results_table[key]['Average'] = np.mean([results_table[key][f'Iteration {i+1}'] for i in range(10)])\n",
    "\n",
    "final_result = pd.DataFrame(results_table)\n",
    "final_result = final_result.T\n",
    "final_result.to_excel('training_deepseekmoe_adaptive_gating_model.xlsx', index=True)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE with Adaptive Gating evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 07:59:04.188956: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 07:59:04.663638: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 07:59:04.663738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 07:59:04.747540: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 07:59:04.914094: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 07:59:06.334702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 07:59:10.452379: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:59:10.784964: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:59:10.785152: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:59:10.786842: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:59:10.787022: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:59:10.787116: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:59:10.861064: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:59:10.861221: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:59:10.861330: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 07:59:10.861410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n",
      "Loaded vocabulary from preprocessor_models/preprocessor_params.json\n",
      "\n",
      "Loading model from: best_deepseekmoe_adaptive_gating_model\n",
      "\n",
      "Model loaded and compiled successfully!\n",
      "Model: \"mo_e_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  784896    \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  8576      \n",
      "                                                                 \n",
      " layer_normalization (Layer  multiple                  256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_layer   multiple                  264576    \n",
      " (TransformerDecoderLayer)                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " moe_nlu (DeepSeekMoELayer)  multiple                  244852    \n",
      "                                                                 \n",
      " moe_nlg (DeepSeekMoELayer)  multiple                  87578     \n",
      "                                                                 \n",
      " intent_output (Dense)       multiple                  14446     \n",
      "                                                                 \n",
      " domain_output (Dense)       multiple                  3262      \n",
      "                                                                 \n",
      " response_output (TimeDistr  multiple                  1024044   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " multi_head_attention_2 (Mu  multiple                  66048     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      " nlg_projection (TimeDistri  multiple                  21376     \n",
      " buted)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2519910 (9.61 MB)\n",
      "Trainable params: 2519910 (9.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting Iteration 1\n",
      "\n",
      "Evaluating model on test set...\n",
      "1397/1397 [==============================] - 20s 11ms/step - loss: 3.0062 - domain_output_loss: 0.0037 - intent_output_loss: 0.0281 - response_embeddings_loss: 0.9306 - response_output_loss: 1.8075 - domain_output_domain_accuracy: 1.0000 - intent_output_intent_accuracy: 0.7230 - intent_output_intent_precision: 0.9995 - intent_output_intent_recall: 0.6668 - intent_output_intent_f1: 0.7999 - response_embeddings_response_embedding_cosine_similarity: 0.0694 - response_output_perplexity: 30.8806 - moe_nlu_load_balancing_loss: 0.0033 - moe_nlg_load_balancing_loss: 0.2233\n",
      "\n",
      "Training results saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evaluation Time (seconds)</th>\n",
       "      <td>19.700496</td>\n",
       "      <td>19.700496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction Speed (ms/token)</th>\n",
       "      <td>2.034601</td>\n",
       "      <td>2.034601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>21.487688</td>\n",
       "      <td>21.487688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>1.944882</td>\n",
       "      <td>1.944882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>5.635143</td>\n",
       "      <td>5.635143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.504456</td>\n",
       "      <td>14.504456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPs Estimate (GFLOPs)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>2.377871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 3 (percent)</th>\n",
       "      <td>25.143164</td>\n",
       "      <td>25.143164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 4 (percent)</th>\n",
       "      <td>0.214746</td>\n",
       "      <td>0.214746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 5 (percent)</th>\n",
       "      <td>26.377953</td>\n",
       "      <td>26.377953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 6 (percent)</th>\n",
       "      <td>2.827487</td>\n",
       "      <td>2.827487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 7 (percent)</th>\n",
       "      <td>22.780959</td>\n",
       "      <td>22.780959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 8 (percent)</th>\n",
       "      <td>22.655691</td>\n",
       "      <td>22.655691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 3 (percent)</th>\n",
       "      <td>23.126315</td>\n",
       "      <td>23.126315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 4 (percent)</th>\n",
       "      <td>6.560166</td>\n",
       "      <td>6.560166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 5 (percent)</th>\n",
       "      <td>31.297610</td>\n",
       "      <td>31.297610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 6 (percent)</th>\n",
       "      <td>17.246979</td>\n",
       "      <td>17.246979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 7 (percent)</th>\n",
       "      <td>4.959455</td>\n",
       "      <td>4.959455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 8 (percent)</th>\n",
       "      <td>16.809474</td>\n",
       "      <td>16.809474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Micro)</th>\n",
       "      <td>0.799923</td>\n",
       "      <td>0.799923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Macro)</th>\n",
       "      <td>0.531604</td>\n",
       "      <td>0.531604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Weighted)</th>\n",
       "      <td>0.700796</td>\n",
       "      <td>0.700796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent Accuracy</th>\n",
       "      <td>0.722978</td>\n",
       "      <td>0.722978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain F1-score (Macro)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU Score</th>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.002264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1 F1</th>\n",
       "      <td>0.162557</td>\n",
       "      <td>0.162557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-2 F1</th>\n",
       "      <td>0.014930</td>\n",
       "      <td>0.014930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-L F1</th>\n",
       "      <td>0.130316</td>\n",
       "      <td>0.130316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METEOR Score</th>\n",
       "      <td>0.127970</td>\n",
       "      <td>0.127970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>30.880650</td>\n",
       "      <td>30.880650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert Load Stability (Variance)</th>\n",
       "      <td>139.783539</td>\n",
       "      <td>139.783539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert Load Stability (Variance)</th>\n",
       "      <td>114.001944</td>\n",
       "      <td>114.001944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Iteration 1     Average\n",
       "Evaluation Time (seconds)               19.700496   19.700496\n",
       "Prediction Speed (ms/token)              2.034601    2.034601\n",
       "Average CPU Usage (percent)             21.487688   21.487688\n",
       "Average GPU Usage (percent)              1.944882    1.944882\n",
       "Average Memory (GB)                      5.635143    5.635143\n",
       "Average GPU Memory (GB)                 14.504456   14.504456\n",
       "Average FLOPs Estimate (GFLOPs)          2.377871    2.377871\n",
       "NLU Expert 1 (percent)                   0.000000    0.000000\n",
       "NLU Expert 2 (percent)                   0.000000    0.000000\n",
       "NLU Expert 3 (percent)                  25.143164   25.143164\n",
       "NLU Expert 4 (percent)                   0.214746    0.214746\n",
       "NLU Expert 5 (percent)                  26.377953   26.377953\n",
       "NLU Expert 6 (percent)                   2.827487    2.827487\n",
       "NLU Expert 7 (percent)                  22.780959   22.780959\n",
       "NLU Expert 8 (percent)                  22.655691   22.655691\n",
       "NLG Expert 1 (percent)                   0.000000    0.000000\n",
       "NLG Expert 2 (percent)                   0.000000    0.000000\n",
       "NLG Expert 3 (percent)                  23.126315   23.126315\n",
       "NLG Expert 4 (percent)                   6.560166    6.560166\n",
       "NLG Expert 5 (percent)                  31.297610   31.297610\n",
       "NLG Expert 6 (percent)                  17.246979   17.246979\n",
       "NLG Expert 7 (percent)                   4.959455    4.959455\n",
       "NLG Expert 8 (percent)                  16.809474   16.809474\n",
       "Intent F1-score (Micro)                  0.799923    0.799923\n",
       "Intent F1-score (Macro)                  0.531604    0.531604\n",
       "Intent F1-score (Weighted)               0.700796    0.700796\n",
       "Intent Accuracy                          0.722978    0.722978\n",
       "Domain Accuracy                          1.000000    1.000000\n",
       "Domain F1-score (Macro)                  1.000000    1.000000\n",
       "BLEU Score                               0.002264    0.002264\n",
       "ROUGE-1 F1                               0.162557    0.162557\n",
       "ROUGE-2 F1                               0.014930    0.014930\n",
       "ROUGE-L F1                               0.130316    0.130316\n",
       "METEOR Score                             0.127970    0.127970\n",
       "Perplexity                              30.880650   30.880650\n",
       "NLU Expert Load Stability (Variance)   139.783539  139.783539\n",
       "NLG Expert Load Stability (Variance)   114.001944  114.001944"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tf_datasets_from_disk(load_path):\n",
    "    print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "    try:\n",
    "        with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "            loaded_moe_params = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "    element_spec = (\n",
    "        {\n",
    "            'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "        },\n",
    "        {\n",
    "            'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "            'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "        test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "    raw_data = {}\n",
    "    for dataset_name in ['train', 'test']:\n",
    "        path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                raw_data[dataset_name] = pickle.load(f)\n",
    "            print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Raw data file {path} not found.\")\n",
    "            raw_data[dataset_name] = []\n",
    "\n",
    "    return {\n",
    "        \"train_dataset\": train_tf_dataset,\n",
    "        \"test_dataset\": test_tf_dataset,\n",
    "        \"moe_params\": loaded_moe_params,\n",
    "        \"raw_data\": raw_data\n",
    "    }\n",
    "\n",
    "tf_dataset_save_path = \"tf_datasets\"\n",
    "loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "test_tf_dataset = loaded_data[\"test_dataset\"]\n",
    "moe_params = loaded_data[\"moe_params\"]\n",
    "raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "moe_params[\"num_experts\"] = 4\n",
    "\n",
    "class DeepSeekMoELayer(layers.Layer):\n",
    "    def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, tau=0.1, name=None):\n",
    "        super(DeepSeekMoELayer, self).__init__(name=name)\n",
    "        self.m = m\n",
    "        self.k_s = k_s\n",
    "        self.total_experts = num_experts * self.m\n",
    "        self.routed_experts = self.total_experts - self.k_s\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = None\n",
    "        self.adjusted_expert_dim = expert_dim // self.m\n",
    "\n",
    "        self.shared_experts = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "            ], name=f'shared_expert_{i}') for i in range(self.k_s)\n",
    "        ]\n",
    "        self.routed_experts_list = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "            ], name=f'routed_expert_{i}') for i in range(self.routed_experts)\n",
    "        ]\n",
    "        self.experts = self.shared_experts + self.routed_experts_list\n",
    "        self.gate = layers.Dense(self.routed_experts, activation=None, name='gate_weights')\n",
    "        self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_rank = inputs.shape.rank\n",
    "        if input_rank == 3:\n",
    "            batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "            flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "            is_3d = True\n",
    "        else:\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            seq_length = 1\n",
    "            flat_inputs = inputs\n",
    "            is_3d = False\n",
    "\n",
    "        flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "        num_tokens = tf.shape(flat_inputs)[0]\n",
    "\n",
    "        shared_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "        for i in range(self.k_s):\n",
    "            shared_output += self.shared_experts[i](flat_inputs)\n",
    "\n",
    "        gate_logits = self.gate(flat_inputs)\n",
    "        gate_weights = tf.nn.softmax(gate_logits, axis=-1)\n",
    "        sorted_weights = tf.sort(gate_weights, axis=-1, direction='DESCENDING')\n",
    "        p1 = sorted_weights[:, 0]\n",
    "        p2 = sorted_weights[:, 1]\n",
    "        use_top_1 = tf.cast(p1 - p2 >= self.tau, tf.float32)\n",
    "        k_per_token = tf.where(use_top_1 > 0, 1, 2)\n",
    "\n",
    "        max_k = 2\n",
    "        top_k_values, top_k_indices = tf.nn.top_k(gate_weights, k=max_k, sorted=True)\n",
    "        top_k_indices = top_k_indices + self.k_s\n",
    "        top_k_weights = top_k_values / (tf.reduce_sum(top_k_values, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "\n",
    "        k_mask = tf.range(max_k, dtype=tf.int32) < tf.expand_dims(k_per_token, -1)\n",
    "        k_mask = tf.cast(k_mask, tf.float32)\n",
    "        top_k_weights = top_k_weights * k_mask\n",
    "        top_k_weights = top_k_weights / (tf.reduce_sum(top_k_weights, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "\n",
    "        expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "        routed_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "        for k in range(max_k):\n",
    "            kth_weights = top_k_weights[:, k]\n",
    "            kth_indices = top_k_indices[:, k]\n",
    "            mask = tf.one_hot(kth_indices, depth=self.total_experts, dtype=tf.float32)\n",
    "            kth_expert_output = tf.reduce_sum(expert_outputs * tf.expand_dims(mask, -1), axis=1)\n",
    "            weighted_kth_output = kth_expert_output * tf.expand_dims(kth_weights, -1)\n",
    "            routed_output += weighted_kth_output\n",
    "\n",
    "        output_flat = shared_output + routed_output + flat_inputs\n",
    "\n",
    "        if is_3d:\n",
    "            output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "            if self.seq_length is not None:\n",
    "                output.set_shape([None, self.seq_length, self.input_dim])\n",
    "            gate_weights_reshaped = tf.reshape(gate_weights, [batch_size, seq_length, self.routed_experts])\n",
    "            if self.seq_length is not None:\n",
    "                gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "        else:\n",
    "            output = output_flat\n",
    "            gate_weights_reshaped = gate_weights\n",
    "\n",
    "        f_i = tf.reduce_mean(tf.reduce_sum(tf.one_hot(tf.argmax(gate_weights, axis=-1), depth=self.routed_experts), axis=0))\n",
    "        P_i = tf.reduce_mean(gate_weights, axis=0)\n",
    "        load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "        self.add_loss(load_balancing_loss)\n",
    "        self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "\n",
    "        return output, gate_weights_reshaped\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_experts': self.total_experts // self.m,\n",
    "            'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "            'input_dim': self.input_dim,\n",
    "            'm': self.m,\n",
    "            'k_s': self.k_s,\n",
    "            'alpha': self.alpha,\n",
    "            'tau': self.tau,\n",
    "            'name': self.name\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "        if look_ahead_mask is not None:\n",
    "            look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "            look_ahead_mask = 1.0 - look_ahead_mask\n",
    "        attn1_output = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)\n",
    "        attn1 = self.dropout1(attn1_output, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        attn2_output = self.mha2(query=out1, value=enc_output, key=enc_output, training=training)\n",
    "        attn2 = self.dropout2(attn2_output, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        return out3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        mha1_config = self.mha1.get_config()\n",
    "        config.update({\n",
    "            'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "            'num_heads': mha1_config['num_heads'],\n",
    "            'dff': self.ffn.layers[0].units,\n",
    "            'rate': self.dropout1.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MoEModel(models.Model):\n",
    "    def __init__(self, moe_params):\n",
    "        super(MoEModel, self).__init__()\n",
    "        self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "        self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "        self.num_domains = moe_params[\"num_domains\"]\n",
    "        self.num_intents = moe_params[\"num_intents\"]\n",
    "        self.vocab_size = moe_params[\"vocab_size\"]\n",
    "        self.num_experts = moe_params[\"num_experts\"]\n",
    "        self.expert_dim = moe_params[\"expert_dim\"]\n",
    "        self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "        self.num_heads = 4\n",
    "        self.dff = self.embedding_dim * 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                             self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "        self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "        self.embedding = layers.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "            name=\"embedding\"\n",
    "        )\n",
    "        self.embedding.build((None,))\n",
    "        with tf.init_scope():\n",
    "            embedding_weights = self.embedding.get_weights()\n",
    "            if embedding_weights and len(embedding_weights) > 0:\n",
    "                embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                self.embedding.set_weights(embedding_weights)\n",
    "        self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "        self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "        self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "        self.moe_nlu = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlu_input_dim, m=2, k_s=2, alpha=0.01, tau=0.1, name='moe_nlu')\n",
    "        self.moe_nlg = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlg_input_dim, m=2, k_s=2, alpha=0.01, tau=0.1, name='moe_nlg')\n",
    "        self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "        self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "        self.response_output = layers.TimeDistributed(layers.Dense(self.vocab_size, activation='softmax'), name='response_output')\n",
    "        self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "        self.nlg_projection = layers.TimeDistributed(layers.Dense(self.embedding_dim, activation='relu'), name='nlg_projection')\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "        prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "        decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "        domain_onehot_input = inputs['domain_onehot_input']\n",
    "        turn_id_embedding = inputs['turn_id_embedding']\n",
    "        ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "        pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "        user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "        prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "        decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "        user_enc_out = user_embedded\n",
    "        prev_system_enc_out = prev_system_embedded\n",
    "        look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "        dec_output = decoder_embedded\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            dec_output = layer(dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask)\n",
    "        decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "        user_attn_out = self.attn_layer(query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), value=user_enc_out, key=user_enc_out, training=training)\n",
    "        prev_system_attn_out = self.attn_layer(query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), value=prev_system_enc_out, key=prev_system_enc_out, training=training)\n",
    "        decoder_attn_out = self.attn_layer(query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), value=decoder_output, key=decoder_output, training=training)\n",
    "        combined_features = layers.Concatenate()([\n",
    "            tf.squeeze(user_attn_out, 1),\n",
    "            tf.squeeze(prev_system_attn_out, 1),\n",
    "            tf.squeeze(decoder_attn_out, 1),\n",
    "            domain_onehot_input,\n",
    "            turn_id_embedding,\n",
    "            ontology_multihot_input\n",
    "        ])\n",
    "        combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "        nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "        nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "        intent_out = self.intent_output(nlu_out)\n",
    "        domain_out = self.domain_output(nlu_out)\n",
    "        intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "        domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "        intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "        domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "        nlg_input = layers.Concatenate(axis=-1)([\n",
    "            decoder_output,\n",
    "            intent_out_tiled,\n",
    "            domain_out_tiled\n",
    "        ])\n",
    "        nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "        nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        response_embeddings = self.nlg_projection(nlg_out)\n",
    "        response_embeddings = tf.ensure_shape(response_embeddings, [None, self.max_seq_length, self.embedding_dim])\n",
    "        response_out = self.response_output(nlg_out)\n",
    "        return {\n",
    "            'intent_output': intent_out,\n",
    "            'domain_output': domain_out,\n",
    "            'response_output': response_out,\n",
    "            'response_embeddings': response_embeddings,\n",
    "            'nlu_gate_weights': nlu_gate_weights,\n",
    "            'nlg_gate_weights': nlg_gate_weights\n",
    "        }\n",
    "\n",
    "    def build_graph(self):\n",
    "        inputs = {\n",
    "            'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "            'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='prev_system_response_tokens'),\n",
    "            'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='decoder_input_tokens'),\n",
    "            'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32, name='domain_onehot_input'),\n",
    "            'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32, name='turn_id_embedding'),\n",
    "            'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32, name='ontology_multihot_input'),\n",
    "        }\n",
    "        return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'moe_params': {\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'max_seq_length': self.max_seq_length,\n",
    "                'num_domains': self.num_domains,\n",
    "                'num_intents': self.num_intents,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'num_experts': self.num_experts,\n",
    "                'expert_dim': self.expert_dim,\n",
    "                'turn_id_dim': self.turn_id_dim\n",
    "            }\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class IntentAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros')\n",
    "        self.total_preds = self.add_weight(name='total_preds', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "        self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "        self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.correct_preds.assign(0.)\n",
    "        self.total_preds.assign(0.)\n",
    "\n",
    "class IntentPrecision(metrics.Metric):\n",
    "    def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        predicted_positives = tf.reduce_sum(y_pred)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.predicted_positives.assign(0.)\n",
    "\n",
    "class IntentRecall(metrics.Metric):\n",
    "    def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.actual_positives = self.add_weight(name='ap', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        actual_positives = tf.reduce_sum(y_true)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.actual_positives.assign(0.)\n",
    "\n",
    "class IntentF1(metrics.Metric):\n",
    "    def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = IntentPrecision(threshold=threshold)\n",
    "        self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()\n",
    "\n",
    "class DomainAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='domain_accuracy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.accuracy = self.add_weight(name='acc', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "        pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "        matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "        self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.accuracy.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "class Perplexity(metrics.Metric):\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "        self.total_loss = self.add_weight(name='total_loss', initializer='zeros')\n",
    "        self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "        loss = self.cross_entropy(y_true, y_pred)\n",
    "        masked_loss = loss * mask\n",
    "        sum_loss = tf.reduce_sum(masked_loss)\n",
    "        num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "        self.total_loss.assign_add(sum_loss)\n",
    "        self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "    def result(self):\n",
    "        avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "        return tf.exp(avg_loss)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_loss.assign(0.)\n",
    "        self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "    def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "        y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "        cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "        non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "        non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "        masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "        num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "        avg_cosine_sim_per_sample = tf.where(\n",
    "            num_non_zero_tokens > 0,\n",
    "            tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "            tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "        )\n",
    "        self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_cosine_sim.assign(0.)\n",
    "        self.num_samples.assign(0.)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "    y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "    y_true = y_true / (y_true_norm + epsilon)\n",
    "    y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "    cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    positive_loss = 1.0 - cosine_sim\n",
    "    mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "    masked_loss = positive_loss * mask\n",
    "    total_loss = tf.reduce_sum(masked_loss)\n",
    "    num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "    return total_loss / num_tokens\n",
    "\n",
    "def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "    flops = 0\n",
    "    functional_model = model.build_graph()\n",
    "\n",
    "    def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "        layer_flops_count = 0\n",
    "\n",
    "        if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "            return 0\n",
    "\n",
    "        if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "            for sub_layer in layer_instance.layers:\n",
    "                layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "            return layer_flops_count\n",
    "\n",
    "        if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "            return 0\n",
    "\n",
    "        if isinstance(layer_instance, layers.Dense):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            output_dim = layer_instance.output_shape[-1]\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "        elif isinstance(layer_instance, layers.LSTM):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            hidden_dim = layer_instance.units\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "            layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "        elif isinstance(layer_instance, layers.Embedding):\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer_instance, DeepSeekMoELayer):\n",
    "            moe_input_dim = layer_instance.input_dim\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "            layer_flops_count += gate_flops\n",
    "            for expert in layer_instance.experts:\n",
    "                layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "        elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "            config = layer_instance.get_config()\n",
    "            num_heads = config['num_heads']\n",
    "            key_dim = config['key_dim']\n",
    "            d_model = num_heads * key_dim\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "            projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "            attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "            context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "            output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "            layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "        elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "            inner_layer = layer_instance.layer\n",
    "            td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "            layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "        elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "            ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "        return layer_flops_count\n",
    "\n",
    "    for layer in functional_model.layers:\n",
    "        flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "    return flops / 1e9\n",
    "\n",
    "def get_gpu_stats():\n",
    "    if nvidia_smi is None:\n",
    "        return 0, 0\n",
    "    try:\n",
    "        nvidia_smi.nvmlInit()\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "        mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_memory = mem_info.used / (1024 ** 3)\n",
    "        nvidia_smi.nvmlShutdown()\n",
    "        return gpu_load, gpu_memory\n",
    "    except Exception:\n",
    "        return 0, 0\n",
    "\n",
    "class ResourceMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.resource_stats = []\n",
    "        self.expert_load_history = []\n",
    "        self.start_time = None\n",
    "        self.epoch_start_time = None\n",
    "        self.flops = None\n",
    "        self.validation_data = validation_data\n",
    "        self.nlu_expert_usage_counts = None\n",
    "        self.nlg_expert_usage_counts = None\n",
    "        self.nlu_gate_weights_history = []\n",
    "        self.nlg_gate_weights_history = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        try:\n",
    "            self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "            self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "            self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "        except Exception:\n",
    "            self.flops = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory()\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.resource_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'epoch_time': epoch_time,\n",
    "            'cpu_usage': cpu_usage,\n",
    "            'memory_used': memory.used / (1024 ** 3),\n",
    "            'memory_total': memory.total / (1024 ** 3),\n",
    "            'gpu_load': gpu_load,\n",
    "            'gpu_memory': gpu_memory,\n",
    "            'loss': logs.get('loss', 0),\n",
    "            'val_loss': logs.get('val_loss', 0),\n",
    "            'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "            'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "            'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "            'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "            'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "            'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "            'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "            'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "            'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "            'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "            'perplexity': logs.get('response_output_perplexity', 0),\n",
    "            'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "            'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "            'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "        })\n",
    "        if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "            self.expert_load_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "            })\n",
    "\n",
    "        sample_size = 100\n",
    "        for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "            inputs, _ = batch\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "            reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "            self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "            nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "            reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "            self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.total_time = time.time() - self.start_time\n",
    "\n",
    "        if len(self.nlu_gate_weights_history) > 0:\n",
    "            all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "            nlu_usage = np.mean(all_nlu_weights, axis=0)\n",
    "            total_nlu_usage = np.sum(nlu_usage)\n",
    "            self.nlu_expert_usage_counts = np.array([usage * 100 / total_nlu_usage for usage in nlu_usage]) if total_nlu_usage > 0 else np.zeros(self.model.moe_nlu.routed_experts)\n",
    "\n",
    "        if len(self.nlg_gate_weights_history) > 0:\n",
    "            all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "            nlg_usage = np.mean(all_nlg_weights, axis=0)\n",
    "            total_nlg_usage = np.sum(nlg_usage)\n",
    "            self.nlg_expert_usage_counts = np.array([usage * 100 / total_nlg_usage for usage in nlg_usage]) if total_nlg_usage > 0 else np.zeros(self.model.moe_nlg.routed_experts)\n",
    "\n",
    "    def visualize_expert_load_distribution(self):\n",
    "        nlu_stability = np.var(self.nlu_expert_usage_counts)\n",
    "        nlg_stability = np.var(self.nlg_expert_usage_counts)\n",
    "        return {'nlu_percentages': self.nlu_expert_usage_counts[:8], 'nlg_percentages': self.nlg_expert_usage_counts[:8], 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "try:\n",
    "    possible_paths = [\n",
    "        os.path.join(\"preprocessor_models\", \"preprocessor_params.json\"),\n",
    "        os.path.join(\"tf_datasets\", \"preprocessor_params.json\"),\n",
    "        \"preprocessor_params.json\"\n",
    "    ]\n",
    "    \n",
    "    vocab_loaded = False\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                preprocessor_params = json.load(f)\n",
    "            word_to_id = {k: int(v) for k, v in preprocessor_params['word_to_id'].items()}\n",
    "            id_to_word = {int(k): v for k, v in preprocessor_params['id_to_word'].items()}\n",
    "            print(f\"Loaded vocabulary from {path}\")\n",
    "            vocab_loaded = True\n",
    "            break\n",
    "    \n",
    "    if not vocab_loaded:\n",
    "        raise FileNotFoundError(\"Vocabulary file not found in any location\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load vocabulary from preprocessor: {str(e)}\")\n",
    "    word_to_id = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    id_to_word = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
    "\n",
    "model_save_path = \"best_deepseekmoe_adaptive_gating_model\"\n",
    "print(f\"\\nLoading model from: {model_save_path}\")\n",
    "custom_objects = {\n",
    "    \"MoEModel\":MoEModel,\n",
    "    \"CustomLearningRateSchedule\": CustomLearningRateSchedule,\n",
    "    \"DeepSeekMoELayer\": DeepSeekMoELayer,\n",
    "    \"TransformerDecoderLayer\": TransformerDecoderLayer,\n",
    "    \"IntentAccuracy\": IntentAccuracy,\n",
    "    \"IntentPrecision\": IntentPrecision,\n",
    "    \"IntentRecall\": IntentRecall,\n",
    "    \"IntentF1\": IntentF1,\n",
    "    \"DomainAccuracy\": DomainAccuracy,\n",
    "    \"Perplexity\": Perplexity,\n",
    "    \"ResponseEmbeddingCosineSimilarity\": ResponseEmbeddingCosineSimilarity\n",
    "}\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model from {model_save_path}: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "def compile_model(model, moe_params):\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'domain_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss,\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={\n",
    "            'intent_output': 0.5,\n",
    "            'domain_output': 0.5,\n",
    "            'response_output': 1.0,\n",
    "            'response_embeddings': 1.0\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = compile_model(model, moe_params)\n",
    "print(\"\\nModel loaded and compiled successfully!\")\n",
    "sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "model(sample_input[0])\n",
    "model.summary()\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self, model, test_dataset, moe_params, raw_data, word_to_id, id_to_word):\n",
    "        self.model = model\n",
    "        self.test_dataset = test_dataset\n",
    "        self.moe_params = moe_params\n",
    "        self.raw_data = raw_data\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.nlu_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.nlg_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.resource_stats = {\n",
    "            'cpu_usage': [],\n",
    "            'memory_used': [],\n",
    "            'gpu_load': [],\n",
    "            'gpu_memory': [],\n",
    "            'batch_times': []\n",
    "        }\n",
    "        self.special_tokens = {'<pad>', '<sos>', '<eos>', '<unk>'}\n",
    "        self.nlu_top_k = model.get_layer('moe_nlu').get_config().get('top_k', 2)\n",
    "        self.nlg_top_k = model.get_layer('moe_nlg').get_config().get('top_k', 2)\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(\"\\nEvaluating model on test set...\")\n",
    "        \n",
    "        def add_response_embeddings(features, targets):\n",
    "            response_tokens = targets['response_output']\n",
    "            response_embeddings = self.model.embedding(response_tokens)\n",
    "            targets['response_embeddings'] = response_embeddings\n",
    "            return features, targets\n",
    "        \n",
    "        test_dataset_with_embeddings = self.test_dataset.map(add_response_embeddings, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        start_time = time.time()\n",
    "        test_results = self.model.evaluate(test_dataset_with_embeddings, verbose=1)\n",
    "        eval_time = time.time() - start_time\n",
    "        \n",
    "        metric_names = self.model.metrics_names\n",
    "        \n",
    "        perplexity_value = None\n",
    "        for name, value in zip(metric_names, test_results):\n",
    "            if name == 'response_output_perplexity':\n",
    "                perplexity_value = value\n",
    "                break\n",
    "        if perplexity_value is None:\n",
    "            print(\"Warning: Could not find perplexity in standard evaluation results.\")\n",
    "            perplexity_value = 0.0\n",
    "        \n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory().used / (1024 ** 3)\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "\n",
    "        total_tokens = 0\n",
    "        total_time = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        test_dataset_small = test_dataset_with_embeddings.unbatch().batch(moe_params[\"batch_size\"])\n",
    "        \n",
    "        all_true_intents = []\n",
    "        all_pred_intents = []\n",
    "        all_true_domains = []\n",
    "        all_pred_domains = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(test_dataset_small):\n",
    "            inputs, targets = batch\n",
    "            response_tokens = targets['response_output'].numpy()\n",
    "            non_padding_mask = response_tokens != 0\n",
    "            total_tokens += np.sum(non_padding_mask)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            total_time += batch_time\n",
    "            num_batches += tf.shape(inputs['user_utterance_tokens'])[0]\n",
    "            \n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            \n",
    "            self.resource_stats['cpu_usage'].append(cpu_usage)\n",
    "            self.resource_stats['memory_used'].append(memory.used / (1024 ** 3))\n",
    "            self.resource_stats['gpu_load'].append(gpu_load)\n",
    "            self.resource_stats['gpu_memory'].append(gpu_memory)\n",
    "            self.resource_stats['batch_times'].append(batch_time)\n",
    "            \n",
    "            nlu_gate_weights = outputs.get('nlu_gate_weights', tf.zeros([0, self.model.moe_nlu.routed_experts])).numpy()\n",
    "            nlu_top_indices = np.argsort(-nlu_gate_weights, axis=-1)[:, :self.nlu_top_k] + self.model.moe_nlu.k_s\n",
    "            for idx in range(self.nlu_top_k):\n",
    "                expert_ids = nlu_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlu_expert_usage_counts):\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "            \n",
    "            nlg_gate_weights = outputs.get('nlg_gate_weights', tf.zeros([0, self.model.moe_nlg.routed_experts])).numpy()\n",
    "            if len(nlg_gate_weights.shape) == 3:\n",
    "                nlg_gate_weights = nlg_gate_weights.reshape(-1, nlg_gate_weights.shape[-1])\n",
    "            nlg_top_indices = np.argsort(-nlg_gate_weights, axis=-1)[:, :self.nlg_top_k] + self.model.moe_nlg.k_s\n",
    "            for idx in range(self.nlg_top_k):\n",
    "                expert_ids = nlg_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlg_expert_usage_counts):\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "            \n",
    "            true_intents = targets.get('intent_output', np.zeros((moe_params[\"batch_size\"], self.model.num_intents))).numpy()\n",
    "            pred_intents = (outputs.get('intent_output', np.zeros_like(true_intents)).numpy() > 0.5).astype(np.int32)\n",
    "            all_true_intents.append(true_intents)\n",
    "            all_pred_intents.append(pred_intents)\n",
    "            \n",
    "            true_domains = targets.get('domain_output', np.zeros((moe_params[\"batch_size\"], 1))).numpy().flatten()\n",
    "            pred_domains = np.argmax(outputs.get('domain_output', np.zeros((moe_params[\"batch_size\"], self.model.num_domains))).numpy(), axis=-1)\n",
    "            all_true_domains.append(true_domains)\n",
    "            all_pred_domains.append(pred_domains)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                gc.collect()\n",
    "                tf.keras.backend.clear_session()\n",
    "        \n",
    "        avg_time_per_token = (total_time / total_tokens) * 1000 if total_tokens > 0 else 0\n",
    "        \n",
    "        avg_cpu = np.mean(self.resource_stats['cpu_usage']) if self.resource_stats['cpu_usage'] else 0\n",
    "        avg_memory = np.mean(self.resource_stats['memory_used']) if self.resource_stats['memory_used'] else 0\n",
    "        avg_gpu_load = np.mean(self.resource_stats['gpu_load']) if self.resource_stats['gpu_load'] else 0\n",
    "        avg_gpu_memory = np.mean(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        peak_gpu_memory = np.max(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        \n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "        \n",
    "        all_true_intents = np.vstack(all_true_intents)\n",
    "        all_pred_intents = np.vstack(all_pred_intents)\n",
    "        all_true_domains = np.concatenate(all_true_domains)\n",
    "        all_pred_domains = np.concatenate(all_pred_domains)\n",
    "        \n",
    "        intent_f1_score = f1_score(all_true_intents, all_pred_intents, average='micro')\n",
    "        intent_f1_score_macro = f1_score(all_true_intents, all_pred_intents, average='macro')\n",
    "        intent_f1_score_weighted = f1_score(all_true_intents, all_pred_intents, average='weighted')\n",
    "        intent_accuracy_score = accuracy_score(all_true_intents, all_pred_intents)\n",
    "        \n",
    "        domain_accuracy_score = accuracy_score(all_true_domains, all_pred_domains)\n",
    "        domain_f1_score_macro = f1_score(all_true_domains, all_pred_domains, average='macro')\n",
    "\n",
    "        hypotheses = []\n",
    "        references = []\n",
    "        meteor_scores = []\n",
    "        rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "        domain_metrics = defaultdict(lambda: {\n",
    "            'bleu': [],\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'meteor': [],\n",
    "            'count': 0\n",
    "        })\n",
    "        \n",
    "        if not self.raw_data.get('test'):\n",
    "            print(\"Warning: raw_data['test'] is empty. Skipping NLG metrics calculation.\")\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "            domain_metrics = {}\n",
    "        else:\n",
    "            for i, batch in enumerate(test_dataset_small):\n",
    "                if i >= len(self.raw_data.get('test', [])):\n",
    "                    print(f\"Warning: raw_data['test'] has fewer items ({len(self.raw_data.get('test', []))}) than test dataset. Stopping NLG metrics calculation.\")\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    inputs, targets = batch\n",
    "                    with tf.device('/CPU:0'):\n",
    "                        outputs = self.model(inputs, training=False)\n",
    "                    \n",
    "                    domain_id = targets['domain_output'].numpy()[0][0]\n",
    "                    domain_name = f\"domain_{domain_id}\"\n",
    "                    \n",
    "                    response_probs = outputs['response_output'].numpy()\n",
    "                    generated_tokens = np.argmax(response_probs, axis=-1)[0]\n",
    "                    true_tokens = targets['response_output'].numpy()[0]\n",
    "                    \n",
    "                    raw_item = self.raw_data['test'][i]\n",
    "                    ref_text = raw_item.get('raw_next_system_response', '')\n",
    "                    if not ref_text:\n",
    "                        continue\n",
    "                    \n",
    "                    gen_text = self.tokens_to_text(generated_tokens)\n",
    "                    ref_tokens = self.tokens_to_text(true_tokens)\n",
    "                    \n",
    "                    if not gen_text.strip() or not ref_text.strip():\n",
    "                        continue\n",
    "                    \n",
    "                    hypotheses.append(gen_text)\n",
    "                    references.append([ref_text])\n",
    "                    \n",
    "                    ref_tokens = word_tokenize(ref_text.lower())\n",
    "                    gen_tokens = word_tokenize(gen_text.lower())\n",
    "                    \n",
    "                    try:\n",
    "                        meteor = meteor_score([ref_tokens], gen_tokens)\n",
    "                        meteor_scores.append(meteor)\n",
    "                        domain_metrics[domain_name]['meteor'].append(meteor)\n",
    "                    except Exception as e:\n",
    "                        print(f\"METEOR calculation error for example {i}: {str(e)}\")\n",
    "                        meteor_scores.append(0)\n",
    "                        domain_metrics[domain_name]['meteor'].append(0)\n",
    "                    \n",
    "                    try:\n",
    "                        rouge_scores = rouge_scorer_instance.score(ref_text, gen_text)\n",
    "                        domain_metrics[domain_name]['rouge1'].append(rouge_scores['rouge1'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rouge2'].append(rouge_scores['rouge2'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rougeL'].append(rouge_scores['rougeL'].fmeasure)\n",
    "                    except Exception as e:\n",
    "                        print(f\"ROUGE calculation error for example {i}: {str(e)}\")\n",
    "                    \n",
    "                    domain_metrics[domain_name]['count'] += 1\n",
    "                    \n",
    "                    if i % 50 == 0:\n",
    "                        gc.collect()\n",
    "                        tf.keras.backend.clear_session()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing example {i}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if hypotheses and references:\n",
    "            hypotheses_tok = []\n",
    "            references_tok = []\n",
    "            \n",
    "            for hyp, ref_list in zip(hypotheses, references):\n",
    "                if not hyp or not ref_list[0]:\n",
    "                    continue\n",
    "                hyp_tokens = word_tokenize(hyp.lower())\n",
    "                ref_tokens = [word_tokenize(ref.lower()) for ref in ref_list]\n",
    "                hypotheses_tok.append(hyp_tokens)\n",
    "                references_tok.append(ref_tokens)\n",
    "            \n",
    "            chencherry = SmoothingFunction()\n",
    "            try:\n",
    "                bleu_score = corpus_bleu(references_tok, hypotheses_tok, smoothing_function=chencherry.method1)\n",
    "            except Exception as e:\n",
    "                print(f\"BLEU calculation error: {str(e)}\")\n",
    "                bleu_score = 0.0\n",
    "            \n",
    "            rouge_scores = []\n",
    "            for hyp, ref in zip(hypotheses, references):\n",
    "                if hyp and ref[0]:\n",
    "                    try:\n",
    "                        rouge_scores.append(rouge_scorer_instance.score(ref[0], hyp))\n",
    "                    except Exception as e:\n",
    "                        print(f\"ROUGE scoring error: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            avg_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_meteor = np.mean(meteor_scores) if meteor_scores else 0.0\n",
    "        else:\n",
    "            print(\"Warning: No valid hypotheses or references for NLG metrics. Setting to 0.0.\")\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "\n",
    "        return {\n",
    "            'intent_f1_micro': intent_f1_score,\n",
    "            'intent_f1_macro': intent_f1_score_macro,\n",
    "            'intent_f1_weighted': intent_f1_score_weighted,\n",
    "            'intent_accuracy': intent_accuracy_score,\n",
    "            'domain_accuracy': domain_accuracy_score,\n",
    "            'domain_f1_macro': domain_f1_score_macro,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge1': avg_rouge1,\n",
    "            'rouge2': avg_rouge2,\n",
    "            'rougeL': avg_rougeL,\n",
    "            'meteor': avg_meteor,\n",
    "            'perplexity': perplexity_value,\n",
    "            'pred_speed': avg_time_per_token,\n",
    "            'domain_metrics': domain_metrics,\n",
    "            'eval_time': eval_time\n",
    "        }\n",
    "    \n",
    "    def tokens_to_text(self, tokens):\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            if token == 0:\n",
    "                continue\n",
    "            word = self.id_to_word.get(token, '<unk>')\n",
    "            if word not in self.special_tokens:\n",
    "                words.append(word)\n",
    "        return \" \".join(words).strip()\n",
    "\n",
    "results_table = {\n",
    "    'Evaluation Time (seconds)': [],\n",
    "    'Prediction Speed (ms/token)': [],\n",
    "    'Average CPU Usage (percent)': [],\n",
    "    'Average GPU Usage (percent)': [],\n",
    "    'Average Memory (GB)': [],\n",
    "    'Average GPU Memory (GB)': [],\n",
    "    'Average FLOPs Estimate (GFLOPs)': [],\n",
    "    'NLU Expert 1 (percent)': [],\n",
    "    'NLU Expert 2 (percent)': [],\n",
    "    'NLU Expert 3 (percent)': [],\n",
    "    'NLU Expert 4 (percent)': [],\n",
    "    'NLU Expert 5 (percent)': [],\n",
    "    'NLU Expert 6 (percent)': [],\n",
    "    'NLU Expert 7 (percent)': [],\n",
    "    'NLU Expert 8 (percent)': [],\n",
    "    'NLG Expert 1 (percent)': [],\n",
    "    'NLG Expert 2 (percent)': [],\n",
    "    'NLG Expert 3 (percent)': [],\n",
    "    'NLG Expert 4 (percent)': [],\n",
    "    'NLG Expert 5 (percent)': [],\n",
    "    'NLG Expert 6 (percent)': [],\n",
    "    'NLG Expert 7 (percent)': [],\n",
    "    'NLG Expert 8 (percent)': [],\n",
    "    'Intent F1-score (Micro)': [],\n",
    "    'Intent F1-score (Macro)': [],\n",
    "    'Intent F1-score (Weighted)': [],\n",
    "    'Intent Accuracy': [],\n",
    "    'Domain Accuracy': [],\n",
    "    'Domain F1-score (Macro)': [],\n",
    "    'BLEU Score': [],\n",
    "    'ROUGE-1 F1': [],\n",
    "    'ROUGE-2 F1': [],\n",
    "    'ROUGE-L F1': [],\n",
    "    'METEOR Score': [],\n",
    "    'Perplexity': [],\n",
    "    'NLU Expert Load Stability (Variance)': [],\n",
    "    'NLG Expert Load Stability (Variance)': []\n",
    "}\n",
    "\n",
    "num_iterations = 5\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    print(f\"\\nStarting Iteration {iteration}\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "    model = compile_model(model, moe_params)\n",
    "    evaluator = EvaluationMetrics(model, test_tf_dataset, moe_params, raw_data, word_to_id, id_to_word)\n",
    "    evaluation_results = evaluator.evaluate()\n",
    "    results_table['Evaluation Time (seconds)'].append(evaluation_results['eval_time'])\n",
    "    results_table['Prediction Speed (ms/token)'].append(evaluation_results['pred_speed'])\n",
    "    results_table['Average CPU Usage (percent)'].append(np.mean(evaluator.resource_stats['cpu_usage']) if evaluator.resource_stats['cpu_usage'] else 0)\n",
    "    results_table['Average GPU Usage (percent)'].append(np.mean(evaluator.resource_stats['gpu_load']) if evaluator.resource_stats['gpu_load'] else 0)\n",
    "    results_table['Average Memory (GB)'].append(np.mean(evaluator.resource_stats['memory_used']) if evaluator.resource_stats['memory_used'] else 0)\n",
    "    results_table['Average GPU Memory (GB)'].append(np.mean(evaluator.resource_stats['gpu_memory']) if evaluator.resource_stats['gpu_memory'] else 0)\n",
    "    results_table['Average FLOPs Estimate (GFLOPs)'].append(calculate_flops(model))\n",
    "    nlu_counts = evaluator.nlu_expert_usage_counts\n",
    "    nlg_counts = evaluator.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) if np.sum(nlu_counts) > 0 else 1\n",
    "    total_nlg = np.sum(nlg_counts) if np.sum(nlg_counts) > 0 else 1\n",
    "    results_table['NLU Expert 1 (percent)'].append((nlu_counts[0] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 2 (percent)'].append((nlu_counts[1] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 3 (percent)'].append((nlu_counts[2] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 4 (percent)'].append((nlu_counts[3] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 5 (percent)'].append((nlu_counts[4] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 6 (percent)'].append((nlu_counts[5] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 7 (percent)'].append((nlu_counts[6] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 8 (percent)'].append((nlu_counts[7] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLG Expert 1 (percent)'].append((nlg_counts[0] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 2 (percent)'].append((nlg_counts[1] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 3 (percent)'].append((nlg_counts[2] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 4 (percent)'].append((nlg_counts[3] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 5 (percent)'].append((nlg_counts[4] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 6 (percent)'].append((nlg_counts[5] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 7 (percent)'].append((nlg_counts[6] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 8 (percent)'].append((nlg_counts[7] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['Intent F1-score (Micro)'].append(evaluation_results['intent_f1_micro'])\n",
    "    results_table['Intent F1-score (Macro)'].append(evaluation_results['intent_f1_macro'])\n",
    "    results_table['Intent F1-score (Weighted)'].append(evaluation_results['intent_f1_weighted'])\n",
    "    results_table['Intent Accuracy'].append(evaluation_results['intent_accuracy'])\n",
    "    results_table['Domain Accuracy'].append(evaluation_results['domain_accuracy'])\n",
    "    results_table['Domain F1-score (Macro)'].append(evaluation_results['domain_f1_macro'])\n",
    "    results_table['BLEU Score'].append(evaluation_results['bleu'])\n",
    "    results_table['ROUGE-1 F1'].append(evaluation_results['rouge1'])\n",
    "    results_table['ROUGE-2 F1'].append(evaluation_results['rouge2'])\n",
    "    results_table['ROUGE-L F1'].append(evaluation_results['rougeL'])\n",
    "    results_table['METEOR Score'].append(evaluation_results['meteor'])\n",
    "    results_table['Perplexity'].append(evaluation_results['perplexity'])\n",
    "    results_table['NLU Expert Load Stability (Variance)'].append(np.var(nlu_counts / total_nlu * 100) if total_nlu > 0 else 0)\n",
    "    results_table['NLG Expert Load Stability (Variance)'].append(np.var(nlg_counts / total_nlg * 100) if total_nlg > 0 else 0)\n",
    "\n",
    "for key in results_table:\n",
    "    if results_table[key]:\n",
    "        results_table[key].append(np.mean(results_table[key]))\n",
    "    else:\n",
    "        results_table[key].append(0)\n",
    "\n",
    "print(\"\\nTraining results saved\")\n",
    "final_df = pd.DataFrame(results_table)\n",
    "final_df = final_df.T\n",
    "final_df.columns = [f'Iteration {i+1}' for i in range(num_iterations)] + ['Average']\n",
    "final_df.to_excel('prediction_deepseekefficientdis_results.xlsx', index=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE with Expert Choice Routing experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:05:57.798234: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 08:05:58.283565: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 08:05:58.283791: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 08:05:58.371756: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 08:05:58.540853: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 08:05:59.875404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training Iteration 1\n",
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:06:03.401302: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:06:03.721094: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:06:03.721281: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:06:03.722551: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:06:03.722706: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:06:03.722800: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:06:03.796639: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:06:03.796795: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:06:03.796902: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:06:03.796979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/4428 [00:00<?, ?it/s]2025-07-04 08:06:28.617508: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f719405e6e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 08:06:28.617548: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2025-07-04 08:06:28.642013: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 08:06:28.691690: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751616388.788481   15049 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 1: 100%|██████████| 4428/4428 [02:54<00:00, 25.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deepseekmoe_expert_choice_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deepseekmoe_expert_choice_model/assets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Iteration 2</th>\n",
       "      <th>Iteration 3</th>\n",
       "      <th>Iteration 4</th>\n",
       "      <th>Iteration 5</th>\n",
       "      <th>Iteration 6</th>\n",
       "      <th>Iteration 7</th>\n",
       "      <th>Iteration 8</th>\n",
       "      <th>Iteration 9</th>\n",
       "      <th>Iteration 10</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Speed (epochs per second)</th>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training Time (second/epoch)</th>\n",
       "      <td>169.477412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.947741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Training Time (second/iteration)</th>\n",
       "      <td>175.517203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.551720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computational Resource Usage</th>\n",
       "      <td>54.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>24.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>6.612045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.661205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.531799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.453180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPS Estimate (GFLOPS)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLU Load Distribution Stability</th>\n",
       "      <td>176.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLG Load Distribution Stability</th>\n",
       "      <td>223.956171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.395617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 3 (percent)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 4 (percent)</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 5 (percent)</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 6 (percent)</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 7 (percent)</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 8 (percent)</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 3 (percent)</th>\n",
       "      <td>34.880597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.488060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 4 (percent)</th>\n",
       "      <td>38.850746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.885075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 5 (percent)</th>\n",
       "      <td>16.149254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.614925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 6 (percent)</th>\n",
       "      <td>6.850746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.685075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 7 (percent)</th>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 8 (percent)</th>\n",
       "      <td>2.761194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Entity Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Intent Accuracy</th>\n",
       "      <td>0.754291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.075429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Perplexity</th>\n",
       "      <td>19.156031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.915603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Response Cosine Similarity</th>\n",
       "      <td>0.326236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Iteration 1  Iteration 2  \\\n",
       "Training Speed (epochs per second)                0.005900          0.0   \n",
       "Training Time (second/epoch)                    169.477412          0.0   \n",
       "Total Training Time (second/iteration)          175.517203          0.0   \n",
       "Computational Resource Usage                     54.400000          0.0   \n",
       "Average CPU Usage (percent)                      24.400000          0.0   \n",
       "Average GPU Usage (percent)                      30.000000          0.0   \n",
       "Average Memory (GB)                               6.612045          0.0   \n",
       "Average GPU Memory (GB)                          14.531799          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                   2.377871          0.0   \n",
       "Expert NLU Load Distribution Stability          176.000000          0.0   \n",
       "Expert NLG Load Distribution Stability          223.956171          0.0   \n",
       "Average NLU Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 3 (percent)                    1.000000          0.0   \n",
       "Average NLU Expert 4 (percent)                   19.000000          0.0   \n",
       "Average NLU Expert 5 (percent)                    8.000000          0.0   \n",
       "Average NLU Expert 6 (percent)                   42.000000          0.0   \n",
       "Average NLU Expert 7 (percent)                   18.000000          0.0   \n",
       "Average NLU Expert 8 (percent)                   12.000000          0.0   \n",
       "Average NLG Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 3 (percent)                   34.880597          0.0   \n",
       "Average NLG Expert 4 (percent)                   38.850746          0.0   \n",
       "Average NLG Expert 5 (percent)                   16.149254          0.0   \n",
       "Average NLG Expert 6 (percent)                    6.850746          0.0   \n",
       "Average NLG Expert 7 (percent)                    0.507463          0.0   \n",
       "Average NLG Expert 8 (percent)                    2.761194          0.0   \n",
       "Average Validation Entity Accuracy                1.000000          0.0   \n",
       "Average Intent Accuracy                           0.754291          0.0   \n",
       "Average Validation Perplexity                    19.156031          0.0   \n",
       "Average Validation Response Cosine Similarity     0.326236          0.0   \n",
       "\n",
       "                                               Iteration 3  Iteration 4  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 5  Iteration 6  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 7  Iteration 8  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 9  Iteration 10  \\\n",
       "Training Speed (epochs per second)                     0.0           0.0   \n",
       "Training Time (second/epoch)                           0.0           0.0   \n",
       "Total Training Time (second/iteration)                 0.0           0.0   \n",
       "Computational Resource Usage                           0.0           0.0   \n",
       "Average CPU Usage (percent)                            0.0           0.0   \n",
       "Average GPU Usage (percent)                            0.0           0.0   \n",
       "Average Memory (GB)                                    0.0           0.0   \n",
       "Average GPU Memory (GB)                                0.0           0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0           0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0           0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0           0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0           0.0   \n",
       "Average Validation Entity Accuracy                     0.0           0.0   \n",
       "Average Intent Accuracy                                0.0           0.0   \n",
       "Average Validation Perplexity                          0.0           0.0   \n",
       "Average Validation Response Cosine Similarity          0.0           0.0   \n",
       "\n",
       "                                                 Average  \n",
       "Training Speed (epochs per second)              0.000590  \n",
       "Training Time (second/epoch)                   16.947741  \n",
       "Total Training Time (second/iteration)         17.551720  \n",
       "Computational Resource Usage                    5.440000  \n",
       "Average CPU Usage (percent)                     2.440000  \n",
       "Average GPU Usage (percent)                     3.000000  \n",
       "Average Memory (GB)                             0.661205  \n",
       "Average GPU Memory (GB)                         1.453180  \n",
       "Average FLOPS Estimate (GFLOPS)                 0.237787  \n",
       "Expert NLU Load Distribution Stability         17.600000  \n",
       "Expert NLG Load Distribution Stability         22.395617  \n",
       "Average NLU Expert 1 (percent)                  0.000000  \n",
       "Average NLU Expert 2 (percent)                  0.000000  \n",
       "Average NLU Expert 3 (percent)                  0.100000  \n",
       "Average NLU Expert 4 (percent)                  1.900000  \n",
       "Average NLU Expert 5 (percent)                  0.800000  \n",
       "Average NLU Expert 6 (percent)                  4.200000  \n",
       "Average NLU Expert 7 (percent)                  1.800000  \n",
       "Average NLU Expert 8 (percent)                  1.200000  \n",
       "Average NLG Expert 1 (percent)                  0.000000  \n",
       "Average NLG Expert 2 (percent)                  0.000000  \n",
       "Average NLG Expert 3 (percent)                  3.488060  \n",
       "Average NLG Expert 4 (percent)                  3.885075  \n",
       "Average NLG Expert 5 (percent)                  1.614925  \n",
       "Average NLG Expert 6 (percent)                  0.685075  \n",
       "Average NLG Expert 7 (percent)                  0.050746  \n",
       "Average NLG Expert 8 (percent)                  0.276119  \n",
       "Average Validation Entity Accuracy              0.100000  \n",
       "Average Intent Accuracy                         0.075429  \n",
       "Average Validation Perplexity                   1.915603  \n",
       "Average Validation Response Cosine Similarity   0.032624  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reset_kernel():\n",
    "    tf.keras.backend.clear_session()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "results_table = {\n",
    "    'Training Speed (epochs per second)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Training Time (second/epoch)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Total Training Time (second/iteration)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Computational Resource Usage': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average CPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average FLOPS Estimate (GFLOPS)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLU Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLG Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Entity Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Intent Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Perplexity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Response Cosine Similarity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0}\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_deepseekmoe_expert_choice_model\"\n",
    "\n",
    "for iteration in range(5):\n",
    "    print(f\"\\nStarting Training Iteration {iteration + 1}\")\n",
    "    reset_kernel()\n",
    "\n",
    "    def load_tf_datasets_from_disk(load_path):\n",
    "        print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "        try:\n",
    "            with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "                loaded_moe_params = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "        element_spec = (\n",
    "            {\n",
    "                'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            },\n",
    "            {\n",
    "                'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "                'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "                'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "            test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "        raw_data = {}\n",
    "        for dataset_name in ['train', 'test']:\n",
    "            path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "            if os.path.exists(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    raw_data[dataset_name] = pickle.load(f)\n",
    "                print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Raw data file {path} not found.\")\n",
    "                raw_data[dataset_name] = []\n",
    "\n",
    "        return {\n",
    "            \"train_dataset\": train_tf_dataset,\n",
    "            \"test_dataset\": test_tf_dataset,\n",
    "            \"moe_params\": loaded_moe_params,\n",
    "            \"raw_data\": raw_data\n",
    "        }\n",
    "\n",
    "    tf_dataset_save_path = \"tf_datasets\"\n",
    "    loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "    train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "    moe_params = loaded_data[\"moe_params\"]\n",
    "    raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "    class DeepSeekMoELayer(layers.Layer):\n",
    "        def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, capacity_factor=1.0, name=None):\n",
    "            super(DeepSeekMoELayer, self).__init__(name=name)\n",
    "            self.m = m\n",
    "            self.k_s = k_s\n",
    "            self.total_experts = num_experts * self.m\n",
    "            self.routed_experts = self.total_experts - self.k_s\n",
    "            self.alpha = alpha\n",
    "            self.capacity_factor = capacity_factor\n",
    "            self.input_dim = input_dim\n",
    "            self.seq_length = None\n",
    "            self.adjusted_expert_dim = expert_dim // self.m\n",
    "            self.max_k = 2\n",
    "\n",
    "            self.shared_experts = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                    layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "                ], name=f'shared_expert_{i}') for i in range(self.k_s)\n",
    "            ]\n",
    "            self.routed_experts_list = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                    layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "                ], name=f'routed_expert_{i}') for i in range(self.routed_experts)\n",
    "            ]\n",
    "            self.experts = self.shared_experts + self.routed_experts_list\n",
    "            self.gate = layers.Dense(self.routed_experts, activation=None, name='gate_weights')\n",
    "            self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            input_rank = inputs.shape.rank\n",
    "            if input_rank == 3:\n",
    "                batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "                flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "                is_3d = True\n",
    "            else:\n",
    "                batch_size = tf.shape(inputs)[0]\n",
    "                seq_length = 1\n",
    "                flat_inputs = inputs\n",
    "                is_3d = False\n",
    "\n",
    "            num_tokens = tf.shape(flat_inputs)[0]\n",
    "            flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "\n",
    "            shared_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "            for i in range(self.k_s):\n",
    "                shared_output += self.shared_experts[i](flat_inputs)\n",
    "\n",
    "            gate_logits = self.gate(flat_inputs)\n",
    "            gate_weights = tf.nn.softmax(gate_logits, axis=-1)\n",
    "\n",
    "            k = self.max_k\n",
    "            gate_weights_transposed = tf.transpose(gate_weights)\n",
    "            top_k_values, top_k_indices = tf.nn.top_k(gate_weights_transposed, k=k, sorted=True)\n",
    "            gating_matrix = top_k_values / (tf.reduce_sum(top_k_values, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "            index_matrix = top_k_indices + self.k_s\n",
    "\n",
    "            one_hot_matrix = tf.one_hot(index_matrix, depth=tf.cast(num_tokens, tf.int32), dtype=tf.float32)\n",
    "            inputs_per_expert = tf.matmul(one_hot_matrix, flat_inputs)\n",
    "\n",
    "            expert_outputs = []\n",
    "            for i, expert in enumerate(self.routed_experts_list):\n",
    "                expert_output = expert(inputs_per_expert[i])\n",
    "                expert_outputs.append(expert_output)\n",
    "            expert_outputs = tf.stack(expert_outputs, axis=0)\n",
    "\n",
    "            routed_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "            for i in range(self.routed_experts):\n",
    "                for j in range(k):\n",
    "                    token_idx = index_matrix[i, j]\n",
    "                    weight = gating_matrix[i, j]\n",
    "                    expert_output = expert_outputs[i, j]\n",
    "                    update = tf.expand_dims(expert_output * weight, 0)\n",
    "                    scatter_indices = tf.expand_dims([token_idx], 1)\n",
    "                    routed_output += tf.scatter_nd(scatter_indices, update, [num_tokens, self.input_dim])\n",
    "\n",
    "            output_flat = shared_output + routed_output + flat_inputs\n",
    "\n",
    "            if is_3d:\n",
    "                output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "                if self.seq_length is not None:\n",
    "                    output.set_shape([None, self.seq_length, self.input_dim])\n",
    "                gate_weights_reshaped = tf.reshape(gate_weights, [batch_size, seq_length, self.routed_experts])\n",
    "                if self.seq_length is not None:\n",
    "                    gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "            else:\n",
    "                output = output_flat\n",
    "                gate_weights_reshaped = gate_weights\n",
    "\n",
    "            f_i = tf.reduce_mean(tf.reduce_sum(tf.one_hot(tf.argmax(gate_weights, axis=-1), depth=self.routed_experts), axis=0))\n",
    "            P_i = tf.reduce_mean(gate_weights, axis=0)\n",
    "            load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "            self.add_loss(load_balancing_loss)\n",
    "            self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "\n",
    "            return output, gate_weights_reshaped\n",
    "\n",
    "        def get_metrics(self):\n",
    "            return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'num_experts': self.total_experts // self.m,\n",
    "                'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "                'input_dim': self.input_dim,\n",
    "                'm': self.m,\n",
    "                'k_s': self.k_s,\n",
    "                'alpha': self.alpha,\n",
    "                'capacity_factor': self.capacity_factor,\n",
    "                'name': self.name\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class TransformerDecoderLayer(layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "            super(TransformerDecoderLayer, self).__init__()\n",
    "            self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.ffn = keras.Sequential([\n",
    "                layers.Dense(dff, activation='relu'),\n",
    "                layers.Dense(d_model)\n",
    "            ])\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "            self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "        def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "            if look_ahead_mask is not None:\n",
    "                look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "                look_ahead_mask = 1.0 - look_ahead_mask\n",
    "\n",
    "            attn1_output = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)\n",
    "            attn1 = self.dropout1(attn1_output, training=training)\n",
    "            out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "            attn2_output = self.mha2(query=out1, value=enc_output, key=enc_output, training=training)\n",
    "            attn2 = self.dropout2(attn2_output, training=training)\n",
    "            out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "            ffn_output = self.ffn(out2)\n",
    "            ffn_output = self.dropout3(ffn_output, training=training)\n",
    "            out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "            return out3\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            mha1_config = self.mha1.get_config()\n",
    "            config.update({\n",
    "                'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "                'num_heads': mha1_config['num_heads'],\n",
    "                'dff': self.ffn.layers[0].units,\n",
    "                'rate': self.dropout1.rate\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class MoEModel(models.Model):\n",
    "        def __init__(self, moe_params):\n",
    "            super(MoEModel, self).__init__()\n",
    "            self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "            self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "            self.num_domains = moe_params[\"num_domains\"]\n",
    "            self.num_intents = moe_params[\"num_intents\"]\n",
    "            self.vocab_size = moe_params[\"vocab_size\"]\n",
    "            self.num_experts = moe_params[\"num_experts\"]\n",
    "            self.expert_dim = moe_params[\"expert_dim\"]\n",
    "            self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "            self.num_heads = 4\n",
    "            self.dff = self.embedding_dim * 4\n",
    "            self.dropout_rate = 0.1\n",
    "            self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                                 self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "            self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "\n",
    "            self.embedding = layers.Embedding(\n",
    "                self.vocab_size,\n",
    "                self.embedding_dim,\n",
    "                mask_zero=True,\n",
    "                embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "                name=\"embedding\"\n",
    "            )\n",
    "            self.embedding.build((None,))\n",
    "            with tf.init_scope():\n",
    "                embedding_weights = self.embedding.get_weights()\n",
    "                if embedding_weights and len(embedding_weights) > 0:\n",
    "                    embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                    self.embedding.set_weights(embedding_weights)\n",
    "\n",
    "            self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "            self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "            self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "            self.moe_nlu = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlu_input_dim, m=2, k_s=2, alpha=0.01, capacity_factor=1.0, name='moe_nlu')\n",
    "            self.moe_nlg = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlg_input_dim, m=2, k_s=2, alpha=0.01, capacity_factor=1.0, name='moe_nlg')\n",
    "            self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "            self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "            self.response_output = layers.TimeDistributed(layers.Dense(self.vocab_size, activation='softmax'), name='response_output')\n",
    "            self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "            self.nlg_projection = layers.TimeDistributed(layers.Dense(self.embedding_dim, activation='relu'), name='nlg_projection')\n",
    "\n",
    "        def create_look_ahead_mask(self, size):\n",
    "            mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "            return mask\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "            prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "            decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "            domain_onehot_input = inputs['domain_onehot_input']\n",
    "            turn_id_embedding = inputs['turn_id_embedding']\n",
    "            ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "            pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "            user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "            prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "            decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "            user_enc_out = user_embedded\n",
    "            prev_system_enc_out = prev_system_embedded\n",
    "            look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "            dec_output = decoder_embedded\n",
    "            for i, layer in enumerate(self.decoder_layers):\n",
    "                dec_output = layer(dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask)\n",
    "            decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "            user_attn_out = self.attn_layer(query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), value=user_enc_out, key=user_enc_out, training=training)\n",
    "            prev_system_attn_out = self.attn_layer(query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), value=prev_system_enc_out, key=prev_system_enc_out, training=training)\n",
    "            decoder_attn_out = self.attn_layer(query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), value=decoder_output, key=decoder_output, training=training)\n",
    "            combined_features = layers.Concatenate()([\n",
    "                tf.squeeze(user_attn_out, 1),\n",
    "                tf.squeeze(prev_system_attn_out, 1),\n",
    "                tf.squeeze(decoder_attn_out, 1),\n",
    "                domain_onehot_input,\n",
    "                turn_id_embedding,\n",
    "                ontology_multihot_input\n",
    "            ])\n",
    "            combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "            nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "            nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "            intent_out = self.intent_output(nlu_out)\n",
    "            domain_out = self.domain_output(nlu_out)\n",
    "            intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "            domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "            intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "            domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "            nlg_input = layers.Concatenate(axis=-1)([\n",
    "                decoder_output,\n",
    "                intent_out_tiled,\n",
    "                domain_out_tiled\n",
    "            ])\n",
    "            nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "            nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            response_embeddings = self.nlg_projection(nlg_out)\n",
    "            response_out = self.response_output(nlg_out)\n",
    "            return {\n",
    "                'intent_output': intent_out,\n",
    "                'domain_output': domain_out,\n",
    "                'response_output': response_out,\n",
    "                'response_embeddings': response_embeddings,\n",
    "                'nlu_gate_weights': nlu_gate_weights,\n",
    "                'nlg_gate_weights': nlg_gate_weights\n",
    "            }\n",
    "\n",
    "        def build_graph(self):\n",
    "            inputs = {\n",
    "                'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "                'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='prev_system_response_tokens'),\n",
    "                'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='decoder_input_tokens'),\n",
    "                'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32, name='domain_onehot_input'),\n",
    "                'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32, name='turn_id_embedding'),\n",
    "                'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32, name='ontology_multihot_input'),\n",
    "            }\n",
    "            return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'moe_params': {\n",
    "                    'embedding_dim': self.embedding_dim,\n",
    "                    'max_seq_length': self.max_seq_length,\n",
    "                    'num_domains': self.num_domains,\n",
    "                    'num_intents': self.num_intents,\n",
    "                    'vocab_size': self.vocab_size,\n",
    "                    'num_experts': self.num_experts,\n",
    "                    'expert_dim': self.expert_dim,\n",
    "                    'turn_id_dim': self.turn_id_dim\n",
    "                }\n",
    "            })\n",
    "            return config\n",
    "    \n",
    "    class IntentAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros')\n",
    "            self.total_preds = self.add_weight(name='total_preds', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "            self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "            self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.correct_preds.assign(0.)\n",
    "            self.total_preds.assign(0.)\n",
    "\n",
    "    class IntentPrecision(metrics.Metric):\n",
    "        def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "            self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            predicted_positives = tf.reduce_sum(y_pred)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.predicted_positives.assign(0.)\n",
    "\n",
    "    class IntentRecall(metrics.Metric):\n",
    "        def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "            self.actual_positives = self.add_weight(name='ap', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            actual_positives = tf.reduce_sum(y_true)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.actual_positives.assign(0.)\n",
    "\n",
    "    class IntentF1(metrics.Metric):\n",
    "        def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.precision = IntentPrecision(threshold=threshold)\n",
    "            self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "            self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "        def result(self):\n",
    "            p = self.precision.result()\n",
    "            r = self.recall.result()\n",
    "            return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.precision.reset_state()\n",
    "            self.recall.reset_state()\n",
    "\n",
    "    class DomainAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='domain_accuracy', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.accuracy = self.add_weight(name='acc', initializer='zeros')\n",
    "            self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "            pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "            matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "            self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "            self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.accuracy.assign(0.)\n",
    "            self.count.assign(0.)\n",
    "\n",
    "    class Perplexity(metrics.Metric):\n",
    "        def __init__(self, name='perplexity', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "            self.total_loss = self.add_weight(name='total_loss', initializer='zeros')\n",
    "            self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "            loss = self.cross_entropy(y_true, y_pred)\n",
    "            masked_loss = loss * mask\n",
    "            sum_loss = tf.reduce_sum(masked_loss)\n",
    "            num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "            self.total_loss.assign_add(sum_loss)\n",
    "            self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "        def result(self):\n",
    "            avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "            return tf.exp(avg_loss)\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_loss.assign(0.)\n",
    "            self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "    class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "        def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "            self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "            y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "            y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "            y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "            cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "            non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "            non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "            masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "            num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "            avg_cosine_sim_per_sample = tf.where(\n",
    "                num_non_zero_tokens > 0,\n",
    "                tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "                tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "            )\n",
    "            self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "            self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_cosine_sim.assign(0.)\n",
    "            self.num_samples.assign(0.)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "        flops = 0\n",
    "        functional_model = model.build_graph()\n",
    "\n",
    "        def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "            layer_flops_count = 0\n",
    "\n",
    "            if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "                return 0\n",
    "\n",
    "            if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "                for sub_layer in layer_instance.layers:\n",
    "                    layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "                return layer_flops_count\n",
    "\n",
    "            if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "                return 0\n",
    "\n",
    "            if isinstance(layer_instance, layers.Dense):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                output_dim = layer_instance.output_shape[-1]\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.LSTM):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                hidden_dim = layer_instance.units\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "                layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.Embedding):\n",
    "                pass\n",
    "\n",
    "            elif isinstance(layer_instance, DeepSeekMoELayer):\n",
    "                moe_input_dim = layer_instance.input_dim\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "                layer_flops_count += gate_flops\n",
    "                for expert in layer_instance.experts:\n",
    "                    layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "                config = layer_instance.get_config()\n",
    "                num_heads = config['num_heads']\n",
    "                key_dim = config['key_dim']\n",
    "                d_model = num_heads * key_dim\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "                projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "                attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "                context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "                output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "                layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "            elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "                inner_layer = layer_instance.layer\n",
    "                td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "                layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "                ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "            return layer_flops_count\n",
    "\n",
    "        for layer in functional_model.layers:\n",
    "            flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "        return flops / 1e9\n",
    "\n",
    "    def get_gpu_stats():\n",
    "        if nvidia_smi is None:\n",
    "            return 0, 0\n",
    "        try:\n",
    "            nvidia_smi.nvmlInit()\n",
    "            handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "            gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "            mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "            gpu_memory = mem_info.used / (1024 ** 3)\n",
    "            nvidia_smi.nvmlShutdown()\n",
    "            return gpu_load, gpu_memory\n",
    "        except Exception:\n",
    "            return 0, 0\n",
    "\n",
    "    class ResourceMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, validation_data):\n",
    "            super().__init__()\n",
    "            self.resource_stats = []\n",
    "            self.expert_load_history = []\n",
    "            self.start_time = None\n",
    "            self.epoch_start_time = None\n",
    "            self.flops = None\n",
    "            self.validation_data = validation_data\n",
    "            self.nlu_expert_usage_counts = None\n",
    "            self.nlg_expert_usage_counts = None\n",
    "            self.nlu_gate_weights_history = []\n",
    "            self.nlg_gate_weights_history = []\n",
    "\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = time.time()\n",
    "            try:\n",
    "                self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "                self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "                self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "            except Exception:\n",
    "                self.flops = 0\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.epoch_start_time = time.time()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.resource_stats.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_time': epoch_time,\n",
    "                'cpu_usage': cpu_usage,\n",
    "                'memory_used': memory.used / (1024 ** 3),\n",
    "                'memory_total': memory.total / (1024 ** 3),\n",
    "                'gpu_load': gpu_load,\n",
    "                'gpu_memory': gpu_memory,\n",
    "                'loss': logs.get('loss', 0),\n",
    "                'val_loss': logs.get('val_loss', 0),\n",
    "                'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "                'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "                'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "                'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "                'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "                'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "                'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "                'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "                'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "                'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "                'perplexity': logs.get('response_output_perplexity', 0),\n",
    "                'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "                'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "                'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "            })\n",
    "            if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "                self.expert_load_history.append({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                    'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "                })\n",
    "\n",
    "            sample_size = 100\n",
    "            for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "                inputs, _ = batch\n",
    "                outputs = self.model(inputs, training=False)\n",
    "                nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "                reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "                self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "                nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "                reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "                self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            self.total_time = time.time() - self.start_time\n",
    "\n",
    "            if len(self.nlu_gate_weights_history) > 0:\n",
    "                all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "                nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "                for expert_id in nlu_top_indices:\n",
    "                    if expert_id < self.model.moe_nlu.total_experts:\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "            if len(self.nlg_gate_weights_history) > 0:\n",
    "                all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "                nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "                for expert_id in nlg_top_indices:\n",
    "                    if expert_id < self.model.moe_nlg.total_experts:\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        def visualize_expert_load_distribution(self):\n",
    "            total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "            total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "            nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "            nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "            nlu_stability = np.var(nlu_percentages)\n",
    "            nlg_stability = np.var(nlg_percentages)\n",
    "            return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "    class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super().__init__()\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "    model = MoEModel(moe_params)\n",
    "    embedding_layer = model.embedding\n",
    "\n",
    "    def create_validation_split(dataset, embedding_layer, validation_split=0.2, batch_size=moe_params[\"batch_size\"]):\n",
    "        element_spec = dataset.element_spec\n",
    "        data_list = [(features, targets) for features, targets in dataset.unbatch().as_numpy_iterator()]\n",
    "        if not data_list:\n",
    "            raise ValueError(\"Dataset is empty.\")\n",
    "        if len(data_list) < 2:\n",
    "            raise ValueError(\"Dataset too small to split.\")\n",
    "        train_data, val_data = train_test_split(data_list, test_size=validation_split, shuffle=True)\n",
    "\n",
    "        def create_dataset_from_list(data):\n",
    "            if not data:\n",
    "                raise ValueError(\"Empty data list provided.\")\n",
    "            features = {\n",
    "                'user_utterance_tokens': np.array([d[0]['user_utterance_tokens'] for d in data], dtype=np.int32),\n",
    "                'prev_system_response_tokens': np.array([d[0]['prev_system_response_tokens'] for d in data], dtype=np.int32),\n",
    "                'decoder_input_tokens': np.array([d[0]['decoder_input_tokens'] for d in data], dtype=np.int32),\n",
    "                'domain_onehot_input': np.array([d[0]['domain_onehot_input'] for d in data], dtype=np.float32),\n",
    "                'turn_id_embedding': np.array([d[0]['turn_id_embedding'] for d in data], dtype=np.float32),\n",
    "                'ontology_multihot_input': np.array([d[0]['ontology_multihot_input'] for d in data], dtype=np.float32),\n",
    "            }\n",
    "            response_output = np.array([d[1]['response_output'] for d in data], dtype=np.int32)\n",
    "            response_embeddings = embedding_layer(response_output).numpy()\n",
    "            targets = {\n",
    "                'domain_output': np.array([d[1]['domain_output'] for d in data], dtype=np.int32),\n",
    "                'intent_output': np.array([d[1]['intent_output'] for d in data], dtype=np.float32),\n",
    "                'response_output': response_output,\n",
    "                'response_embeddings': response_embeddings\n",
    "            }\n",
    "            return tf.data.Dataset.from_tensor_slices((features, targets)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        train_dataset = create_dataset_from_list(train_data)\n",
    "        val_dataset = create_dataset_from_list(val_data)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    train_tf_dataset, val_tf_dataset = create_validation_split(train_tf_dataset, embedding_layer)\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': losses.BinaryCrossentropy(),\n",
    "        'domain_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={'intent_output': 0.5, 'domain_output': 0.5, 'response_output': 1.0, 'response_embeddings': 1.0}\n",
    "    )\n",
    "\n",
    "    sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "    model(sample_input[0])\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=8, mode='min', restore_best_weights=True)\n",
    "    resource_monitor = ResourceMonitor(val_tf_dataset)\n",
    "    class TQDMProgressBar(callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.progress_bar = tqdm(total=len(train_tf_dataset), desc=f'Epoch {epoch + 1}')\n",
    "\n",
    "        def on_batch_end(self, batch, logs=None):\n",
    "            self.progress_bar.update(1)\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            self.progress_bar.close()\n",
    "\n",
    "    tqdm_callback = TQDMProgressBar()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_tf_dataset,\n",
    "        epochs=100,\n",
    "        validation_data=val_tf_dataset,\n",
    "        callbacks=[early_stopping, resource_monitor, tqdm_callback],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    val_batch = next(iter(val_tf_dataset.take(1)))\n",
    "    model(val_batch[0], training=False)\n",
    "\n",
    "    expert_stats = resource_monitor.visualize_expert_load_distribution()\n",
    "    stats = resource_monitor.resource_stats\n",
    "    avg_epoch_duration = np.mean([s['epoch_time'] for s in stats]) if stats else 0\n",
    "    total_training_time = resource_monitor.total_time\n",
    "    avg_cpu_usage = np.mean([s['cpu_usage'] for s in stats]) if stats else 0\n",
    "    avg_memory = np.mean([s['memory_used'] for s in stats]) if stats else 0\n",
    "    avg_gpu_load = np.mean([s['gpu_load'] for s in stats]) if stats else 0\n",
    "    avg_gpu_memory = np.mean([s['gpu_memory'] for s in stats]) if stats else 0\n",
    "    avg_flops = resource_monitor.flops if resource_monitor.flops else 0\n",
    "    nlu_counts = resource_monitor.nlu_expert_usage_counts\n",
    "    nlg_counts = resource_monitor.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) or 1\n",
    "    total_nlg = np.sum(nlg_counts) or 1\n",
    "    expert_nlu_percentages = [(nlu_counts[i] / total_nlu) * 100 for i in range(8)]\n",
    "    expert_nlg_percentages = [(nlg_counts[i] / total_nlg) * 100 for i in range(8)]\n",
    "    val_intent_accuracy = np.mean([s['val_intent_accuracy'] for s in stats if s['val_intent_accuracy'] > 0]) if stats else 0\n",
    "    val_entity_accuracy = np.mean([s['val_domain_accuracy'] for s in stats if s['val_domain_accuracy'] > 0]) if stats else 0\n",
    "    val_perplexity = np.mean([s['val_perplexity'] for s in stats if s['val_perplexity'] > 0]) if stats else 0\n",
    "    val_cosine_similarity = np.mean([s['val_cosine_similarity'] for s in stats if s['val_cosine_similarity'] > 0]) if stats and any(s['val_cosine_similarity'] > 0 for s in stats) else 0\n",
    "    \n",
    "    results_table['Average Validation Response Cosine Similarity'][f'Iteration {iteration + 1}'] = val_cosine_similarity\n",
    "    results_table['Training Speed (epochs per second)'][f'Iteration {iteration + 1}'] = 1 / avg_epoch_duration if avg_epoch_duration > 0 else 0\n",
    "    results_table['Training Time (second/epoch)'][f'Iteration {iteration + 1}'] = avg_epoch_duration\n",
    "    results_table['Total Training Time (second/iteration)'][f'Iteration {iteration + 1}'] = total_training_time\n",
    "    results_table['Computational Resource Usage'][f'Iteration {iteration + 1}'] = avg_cpu_usage + avg_gpu_load\n",
    "    results_table['Average CPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_cpu_usage\n",
    "    results_table['Average GPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_gpu_load\n",
    "    results_table['Average Memory (GB)'][f'Iteration {iteration + 1}'] = avg_memory\n",
    "    results_table['Average GPU Memory (GB)'][f'Iteration {iteration + 1}'] = avg_gpu_memory\n",
    "    results_table['Average FLOPS Estimate (GFLOPS)'][f'Iteration {iteration + 1}'] = avg_flops\n",
    "    results_table['Expert NLU Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlu_stability']\n",
    "    results_table['Expert NLG Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlg_stability']\n",
    "    for i in range(8):\n",
    "        if i < len(expert_stats['nlu_percentages']):\n",
    "            results_table[f'Average NLU Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlu_percentages'][i]\n",
    "        if i < len(expert_stats['nlg_percentages']):\n",
    "            results_table[f'Average NLG Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlg_percentages'][i]\n",
    "    results_table['Average Validation Entity Accuracy'][f'Iteration {iteration + 1}'] = val_entity_accuracy\n",
    "    results_table['Average Intent Accuracy'][f'Iteration {iteration + 1}'] = val_intent_accuracy\n",
    "    results_table['Average Validation Perplexity'][f'Iteration {iteration + 1}'] = val_perplexity\n",
    "    results_table['Average Validation Response Cosine Similarity'][f'Iteration {iteration + 1}'] = val_cosine_similarity\n",
    "    val_loss = min([s['val_loss'] for s in stats]) if stats else float('inf')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        try:\n",
    "            model.save(best_model_path, save_format='tf', include_optimizer=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to save best model to {best_model_path}: {str(e)}\")\n",
    "\n",
    "for key in results_table:\n",
    "    results_table[key]['Average'] = np.mean([results_table[key][f'Iteration {i+1}'] for i in range(10)])\n",
    "\n",
    "final_result = pd.DataFrame(results_table)\n",
    "final_result = final_result.T\n",
    "final_result.to_excel('training_deepseekmoe_expert_choice_result.xlsx', index=True)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE with Expert Choice Routing evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:09:54.202250: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 08:09:54.501931: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 08:09:54.502031: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 08:09:54.526501: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 08:09:54.624110: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 08:09:55.924642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:09:59.133447: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:09:59.276388: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:09:59.276604: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:09:59.278196: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:09:59.278343: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:09:59.278429: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:09:59.359855: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:09:59.360037: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:09:59.360139: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:09:59.360570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n",
      "Loaded vocabulary from preprocessor_models/preprocessor_params.json\n",
      "\n",
      "Loading model from: best_deepseekmoe_expert_choice_model\n",
      "\n",
      "Model loaded and compiled successfully!\n",
      "Model: \"mo_e_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  784896    \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  8576      \n",
      "                                                                 \n",
      " layer_normalization (Layer  multiple                  256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_layer   multiple                  264576    \n",
      " (TransformerDecoderLayer)                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " moe_nlu (DeepSeekMoELayer)  multiple                  244852    \n",
      "                                                                 \n",
      " moe_nlg (DeepSeekMoELayer)  multiple                  87578     \n",
      "                                                                 \n",
      " intent_output (Dense)       multiple                  14446     \n",
      "                                                                 \n",
      " domain_output (Dense)       multiple                  3262      \n",
      "                                                                 \n",
      " response_output (TimeDistr  multiple                  1024044   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " multi_head_attention_2 (Mu  multiple                  66048     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      " nlg_projection (TimeDistri  multiple                  21376     \n",
      " buted)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2519910 (9.61 MB)\n",
      "Trainable params: 2519910 (9.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting Iteration 1\n",
      "\n",
      "Evaluating model on test set...\n",
      "1397/1397 [==============================] - 24s 13ms/step - loss: 2.2965 - domain_output_loss: 0.0016 - intent_output_loss: 0.0265 - response_embeddings_loss: 0.9650 - response_output_loss: 1.0659 - domain_output_domain_accuracy: 1.0000 - intent_output_intent_accuracy: 0.7623 - intent_output_intent_precision: 0.9833 - intent_output_intent_recall: 0.7358 - intent_output_intent_f1: 0.8417 - response_embeddings_response_embedding_cosine_similarity: 0.0350 - response_output_perplexity: 33.4686 - moe_nlu_load_balancing_loss: 0.0033 - moe_nlg_load_balancing_loss: 0.2233\n",
      "\n",
      "Training results saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evaluation Time (seconds)</th>\n",
       "      <td>23.802339</td>\n",
       "      <td>23.802339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction Speed (ms/token)</th>\n",
       "      <td>2.434583</td>\n",
       "      <td>2.434583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>23.217394</td>\n",
       "      <td>23.217394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>1.866142</td>\n",
       "      <td>1.866142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>5.696944</td>\n",
       "      <td>5.696944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.504456</td>\n",
       "      <td>14.504456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPs Estimate (GFLOPs)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>2.377871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 3 (percent)</th>\n",
       "      <td>18.486042</td>\n",
       "      <td>18.486042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 4 (percent)</th>\n",
       "      <td>21.617752</td>\n",
       "      <td>21.617752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 5 (percent)</th>\n",
       "      <td>12.670007</td>\n",
       "      <td>12.670007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 6 (percent)</th>\n",
       "      <td>6.388690</td>\n",
       "      <td>6.388690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 7 (percent)</th>\n",
       "      <td>29.652827</td>\n",
       "      <td>29.652827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 8 (percent)</th>\n",
       "      <td>11.184681</td>\n",
       "      <td>11.184681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 3 (percent)</th>\n",
       "      <td>28.567880</td>\n",
       "      <td>28.567880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 4 (percent)</th>\n",
       "      <td>39.978258</td>\n",
       "      <td>39.978258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 5 (percent)</th>\n",
       "      <td>15.618757</td>\n",
       "      <td>15.618757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 6 (percent)</th>\n",
       "      <td>9.933066</td>\n",
       "      <td>9.933066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 7 (percent)</th>\n",
       "      <td>0.563841</td>\n",
       "      <td>0.563841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 8 (percent)</th>\n",
       "      <td>5.338198</td>\n",
       "      <td>5.338198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Micro)</th>\n",
       "      <td>0.841740</td>\n",
       "      <td>0.841740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Macro)</th>\n",
       "      <td>0.613364</td>\n",
       "      <td>0.613364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Weighted)</th>\n",
       "      <td>0.767825</td>\n",
       "      <td>0.767825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent Accuracy</th>\n",
       "      <td>0.762348</td>\n",
       "      <td>0.762348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain F1-score (Macro)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU Score</th>\n",
       "      <td>0.004864</td>\n",
       "      <td>0.004864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1 F1</th>\n",
       "      <td>0.158363</td>\n",
       "      <td>0.158363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-2 F1</th>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.016401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-L F1</th>\n",
       "      <td>0.130161</td>\n",
       "      <td>0.130161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METEOR Score</th>\n",
       "      <td>0.129938</td>\n",
       "      <td>0.129938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>33.468578</td>\n",
       "      <td>33.468578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert Load Stability (Variance)</th>\n",
       "      <td>95.599083</td>\n",
       "      <td>95.599083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert Load Stability (Variance)</th>\n",
       "      <td>191.976316</td>\n",
       "      <td>191.976316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Iteration 1     Average\n",
       "Evaluation Time (seconds)               23.802339   23.802339\n",
       "Prediction Speed (ms/token)              2.434583    2.434583\n",
       "Average CPU Usage (percent)             23.217394   23.217394\n",
       "Average GPU Usage (percent)              1.866142    1.866142\n",
       "Average Memory (GB)                      5.696944    5.696944\n",
       "Average GPU Memory (GB)                 14.504456   14.504456\n",
       "Average FLOPs Estimate (GFLOPs)          2.377871    2.377871\n",
       "NLU Expert 1 (percent)                   0.000000    0.000000\n",
       "NLU Expert 2 (percent)                   0.000000    0.000000\n",
       "NLU Expert 3 (percent)                  18.486042   18.486042\n",
       "NLU Expert 4 (percent)                  21.617752   21.617752\n",
       "NLU Expert 5 (percent)                  12.670007   12.670007\n",
       "NLU Expert 6 (percent)                   6.388690    6.388690\n",
       "NLU Expert 7 (percent)                  29.652827   29.652827\n",
       "NLU Expert 8 (percent)                  11.184681   11.184681\n",
       "NLG Expert 1 (percent)                   0.000000    0.000000\n",
       "NLG Expert 2 (percent)                   0.000000    0.000000\n",
       "NLG Expert 3 (percent)                  28.567880   28.567880\n",
       "NLG Expert 4 (percent)                  39.978258   39.978258\n",
       "NLG Expert 5 (percent)                  15.618757   15.618757\n",
       "NLG Expert 6 (percent)                   9.933066    9.933066\n",
       "NLG Expert 7 (percent)                   0.563841    0.563841\n",
       "NLG Expert 8 (percent)                   5.338198    5.338198\n",
       "Intent F1-score (Micro)                  0.841740    0.841740\n",
       "Intent F1-score (Macro)                  0.613364    0.613364\n",
       "Intent F1-score (Weighted)               0.767825    0.767825\n",
       "Intent Accuracy                          0.762348    0.762348\n",
       "Domain Accuracy                          1.000000    1.000000\n",
       "Domain F1-score (Macro)                  1.000000    1.000000\n",
       "BLEU Score                               0.004864    0.004864\n",
       "ROUGE-1 F1                               0.158363    0.158363\n",
       "ROUGE-2 F1                               0.016401    0.016401\n",
       "ROUGE-L F1                               0.130161    0.130161\n",
       "METEOR Score                             0.129938    0.129938\n",
       "Perplexity                              33.468578   33.468578\n",
       "NLU Expert Load Stability (Variance)    95.599083   95.599083\n",
       "NLG Expert Load Stability (Variance)   191.976316  191.976316"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tf_datasets_from_disk(load_path):\n",
    "    print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "    try:\n",
    "        with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "            loaded_moe_params = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "    element_spec = (\n",
    "        {\n",
    "            'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "        },\n",
    "        {\n",
    "            'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "            'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "        test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "    raw_data = {}\n",
    "    for dataset_name in ['train', 'test']:\n",
    "        path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                raw_data[dataset_name] = pickle.load(f)\n",
    "            print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Raw data file {path} not found.\")\n",
    "            raw_data[dataset_name] = []\n",
    "\n",
    "    return {\n",
    "        \"train_dataset\": train_tf_dataset,\n",
    "        \"test_dataset\": test_tf_dataset,\n",
    "        \"moe_params\": loaded_moe_params,\n",
    "        \"raw_data\": raw_data\n",
    "    }\n",
    "\n",
    "tf_dataset_save_path = \"tf_datasets\"\n",
    "loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "test_tf_dataset = loaded_data[\"test_dataset\"]\n",
    "moe_params = loaded_data[\"moe_params\"]\n",
    "raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "moe_params[\"num_experts\"] = 4\n",
    "\n",
    "class DeepSeekMoELayer(layers.Layer):\n",
    "    def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, capacity_factor=1.0, name=None):\n",
    "        super(DeepSeekMoELayer, self).__init__(name=name)\n",
    "        self.m = m\n",
    "        self.k_s = k_s\n",
    "        self.total_experts = num_experts * self.m\n",
    "        self.routed_experts = self.total_experts - self.k_s\n",
    "        self.alpha = alpha\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.input_dim = input_dim\n",
    "        self.adjusted_expert_dim = expert_dim // self.m\n",
    "        self.max_k = min(2, self.routed_experts)\n",
    "\n",
    "        self.shared_experts = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "            ], name=f'shared_expert_{i}') for i in range(self.k_s)\n",
    "        ]\n",
    "        self.routed_experts_list = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "            ], name=f'routed_expert_{i}') for i in range(self.routed_experts)\n",
    "        ]\n",
    "        self.experts = self.shared_experts + self.routed_experts_list\n",
    "        self.gate = layers.Dense(self.routed_experts, activation=None, name='gate_weights')\n",
    "        self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_rank = inputs.shape.rank\n",
    "        if input_rank == 3:\n",
    "            batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "            flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "            is_3d = True\n",
    "        else:\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            seq_length = 1\n",
    "            flat_inputs = inputs\n",
    "            is_3d = False\n",
    "\n",
    "        num_tokens = tf.shape(flat_inputs)[0]\n",
    "        flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "\n",
    "        shared_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "        for i in range(self.k_s):\n",
    "            shared_output += self.shared_experts[i](flat_inputs)\n",
    "\n",
    "        gate_logits = self.gate(flat_inputs)\n",
    "        gate_weights = tf.nn.softmax(gate_logits, axis=-1)\n",
    "\n",
    "        k = min(self.max_k, self.routed_experts)\n",
    "        top_k_values, top_k_indices = tf.nn.top_k(gate_weights, k=k, sorted=True)\n",
    "        top_k_weights = top_k_values / (tf.reduce_sum(top_k_values, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "        top_k_indices = top_k_indices + self.k_s\n",
    "\n",
    "        routed_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "        for i in range(k):\n",
    "            expert_indices = top_k_indices[:, i]\n",
    "            weights = top_k_weights[:, i]\n",
    "            valid_mask = tf.logical_and(expert_indices >= self.k_s, expert_indices < self.total_experts)\n",
    "            valid_indices = tf.where(valid_mask, expert_indices - self.k_s, 0)\n",
    "            valid_weights = tf.where(valid_mask, weights, 0.0)\n",
    "\n",
    "            expert_outputs = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "            for j in range(self.routed_experts):\n",
    "                expert_mask = tf.equal(valid_indices, j)\n",
    "                if tf.reduce_any(expert_mask):\n",
    "                    expert_input = tf.where(tf.expand_dims(expert_mask, -1), flat_inputs, tf.zeros_like(flat_inputs))\n",
    "                    expert_output = self.routed_experts_list[j](expert_input)\n",
    "                    expert_outputs += tf.where(tf.expand_dims(expert_mask, -1), expert_output, tf.zeros_like(expert_output))\n",
    "\n",
    "            weighted_output = expert_outputs * tf.expand_dims(valid_weights, -1)\n",
    "            routed_output += weighted_output\n",
    "\n",
    "        output_flat = shared_output + routed_output + flat_inputs\n",
    "\n",
    "        if is_3d:\n",
    "            output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "        else:\n",
    "            output = output_flat\n",
    "\n",
    "        f_i = tf.reduce_mean(tf.reduce_sum(tf.one_hot(tf.argmax(gate_weights, axis=-1), depth=self.routed_experts), axis=0))\n",
    "        P_i = tf.reduce_mean(gate_weights, axis=0)\n",
    "        load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "        self.add_loss(load_balancing_loss)\n",
    "        self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "\n",
    "        gate_weights_reshaped = tf.reshape(gate_weights, [batch_size, seq_length, self.routed_experts]) if is_3d else gate_weights\n",
    "        return output, gate_weights_reshaped\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_experts': self.total_experts // self.m,\n",
    "            'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "            'input_dim': self.input_dim,\n",
    "            'm': self.m,\n",
    "            'k_s': self.k_s,\n",
    "            'alpha': self.alpha,\n",
    "            'capacity_factor': self.capacity_factor,\n",
    "            'name': self.name\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "        if look_ahead_mask is not None:\n",
    "            look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "            look_ahead_mask = 1.0 - look_ahead_mask\n",
    "\n",
    "        attn1_output = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)\n",
    "        attn1 = self.dropout1(attn1_output, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2_output = self.mha2(query=out1, value=enc_output, key=enc_output, training=training)\n",
    "        attn2 = self.dropout2(attn2_output, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        mha1_config = self.mha1.get_config()\n",
    "        config.update({\n",
    "            'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "            'num_heads': mha1_config['num_heads'],\n",
    "            'dff': self.ffn.layers[0].units,\n",
    "            'rate': self.dropout1.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MoEModel(models.Model):\n",
    "    def __init__(self, moe_params):\n",
    "        super(MoEModel, self).__init__()\n",
    "        self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "        self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "        self.num_domains = moe_params[\"num_domains\"]\n",
    "        self.num_intents = moe_params[\"num_intents\"]\n",
    "        self.vocab_size = moe_params[\"vocab_size\"]\n",
    "        self.num_experts = moe_params[\"num_experts\"]\n",
    "        self.expert_dim = moe_params[\"expert_dim\"]\n",
    "        self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "        self.num_heads = 4\n",
    "        self.dff = self.embedding_dim * 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                             self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "        self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "\n",
    "        self.embedding = layers.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "            name=\"embedding\"\n",
    "        )\n",
    "        self.embedding.build((None,))\n",
    "        with tf.init_scope():\n",
    "            embedding_weights = self.embedding.get_weights()\n",
    "            if embedding_weights and len(embedding_weights) > 0:\n",
    "                embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                self.embedding.set_weights(embedding_weights)\n",
    "\n",
    "        self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "        self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "        self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "        self.moe_nlu = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlu_input_dim, m=2, k_s=2, alpha=0.01, capacity_factor=1.0, name='moe_nlu')\n",
    "        self.moe_nlg = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlg_input_dim, m=2, k_s=2, alpha=0.01, capacity_factor=1.0, name='moe_nlg')\n",
    "        self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "        self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "        self.response_output = layers.TimeDistributed(layers.Dense(self.vocab_size, activation='softmax'), name='response_output')\n",
    "        self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "        self.nlg_projection = layers.TimeDistributed(layers.Dense(self.embedding_dim, activation='relu'), name='nlg_projection')\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "        prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "        decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "        domain_onehot_input = inputs['domain_onehot_input']\n",
    "        turn_id_embedding = inputs['turn_id_embedding']\n",
    "        ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "        pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "        user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "        prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "        decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "        user_enc_out = user_embedded\n",
    "        prev_system_enc_out = prev_system_embedded\n",
    "        look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "        dec_output = decoder_embedded\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            dec_output = layer(dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask)\n",
    "        decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "        user_attn_out = self.attn_layer(query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), value=user_enc_out, key=user_enc_out, training=training)\n",
    "        prev_system_attn_out = self.attn_layer(query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), value=prev_system_enc_out, key=prev_system_enc_out, training=training)\n",
    "        decoder_attn_out = self.attn_layer(query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), value=decoder_output, key=decoder_output, training=training)\n",
    "        combined_features = layers.Concatenate()([\n",
    "            tf.squeeze(user_attn_out, 1),\n",
    "            tf.squeeze(prev_system_attn_out, 1),\n",
    "            tf.squeeze(decoder_attn_out, 1),\n",
    "            domain_onehot_input,\n",
    "            turn_id_embedding,\n",
    "            ontology_multihot_input\n",
    "        ])\n",
    "        combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "        nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "        nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "        intent_out = self.intent_output(nlu_out)\n",
    "        domain_out = self.domain_output(nlu_out)\n",
    "        intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "        domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "        intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "        domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "        nlg_input = layers.Concatenate(axis=-1)([\n",
    "            decoder_output,\n",
    "            intent_out_tiled,\n",
    "            domain_out_tiled\n",
    "        ])\n",
    "        nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "        nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        response_embeddings = self.nlg_projection(nlg_out)\n",
    "        response_out = self.response_output(nlg_out)\n",
    "        return {\n",
    "            'intent_output': intent_out,\n",
    "            'domain_output': domain_out,\n",
    "            'response_output': response_out,\n",
    "            'response_embeddings': response_embeddings,\n",
    "            'nlu_gate_weights': nlu_gate_weights,\n",
    "            'nlg_gate_weights': nlg_gate_weights\n",
    "        }\n",
    "\n",
    "    def build_graph(self):\n",
    "        inputs = {\n",
    "            'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "            'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='prev_system_response_tokens'),\n",
    "            'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='decoder_input_tokens'),\n",
    "            'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32, name='domain_onehot_input'),\n",
    "            'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32, name='turn_id_embedding'),\n",
    "            'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32, name='ontology_multihot_input'),\n",
    "        }\n",
    "        return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'moe_params': {\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'max_seq_length': self.max_seq_length,\n",
    "                'num_domains': self.num_domains,\n",
    "                'num_intents': self.num_intents,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'num_experts': self.num_experts,\n",
    "                'expert_dim': self.expert_dim,\n",
    "                'turn_id_dim': self.turn_id_dim\n",
    "            }\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class IntentAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros')\n",
    "        self.total_preds = self.add_weight(name='total_preds', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "        self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "        self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.correct_preds.assign(0.)\n",
    "        self.total_preds.assign(0.)\n",
    "\n",
    "class IntentPrecision(metrics.Metric):\n",
    "    def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        predicted_positives = tf.reduce_sum(y_pred)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.predicted_positives.assign(0.)\n",
    "\n",
    "class IntentRecall(metrics.Metric):\n",
    "    def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.actual_positives = self.add_weight(name='ap', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        actual_positives = tf.reduce_sum(y_true)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.actual_positives.assign(0.)\n",
    "\n",
    "class IntentF1(metrics.Metric):\n",
    "    def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = IntentPrecision(threshold=threshold)\n",
    "        self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()\n",
    "\n",
    "class DomainAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='domain_accuracy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.accuracy = self.add_weight(name='acc', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "        pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "        matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "        self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.accuracy.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "class Perplexity(metrics.Metric):\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "        self.total_loss = self.add_weight(name='total_loss', initializer='zeros')\n",
    "        self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "        loss = self.cross_entropy(y_true, y_pred)\n",
    "        masked_loss = loss * mask\n",
    "        sum_loss = tf.reduce_sum(masked_loss)\n",
    "        num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "        self.total_loss.assign_add(sum_loss)\n",
    "        self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "    def result(self):\n",
    "        avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "        return tf.exp(avg_loss)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_loss.assign(0.)\n",
    "        self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "    def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "        y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "        cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "        non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "        non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "        masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "        num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "        avg_cosine_sim_per_sample = tf.where(\n",
    "            num_non_zero_tokens > 0,\n",
    "            tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "            tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "        )\n",
    "        self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_cosine_sim.assign(0.)\n",
    "        self.num_samples.assign(0.)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "    y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "    y_true = y_true / (y_true_norm + epsilon)\n",
    "    y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "    cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    positive_loss = 1.0 - cosine_sim\n",
    "    mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "    masked_loss = positive_loss * mask\n",
    "    total_loss = tf.reduce_sum(masked_loss)\n",
    "    num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "    return total_loss / num_tokens\n",
    "\n",
    "def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "    flops = 0\n",
    "    functional_model = model.build_graph()\n",
    "\n",
    "    def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "        layer_flops_count = 0\n",
    "\n",
    "        if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "            return 0\n",
    "\n",
    "        if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "            for sub_layer in layer_instance.layers:\n",
    "                layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "            return layer_flops_count\n",
    "\n",
    "        if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "            return 0\n",
    "\n",
    "        if isinstance(layer_instance, layers.Dense):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            output_dim = layer_instance.output_shape[-1]\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "        elif isinstance(layer_instance, layers.LSTM):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            hidden_dim = layer_instance.units\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "            layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "        elif isinstance(layer_instance, layers.Embedding):\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer_instance, DeepSeekMoELayer):\n",
    "            moe_input_dim = layer_instance.input_dim\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "            layer_flops_count += gate_flops\n",
    "            for expert in layer_instance.experts:\n",
    "                layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "        elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "            config = layer_instance.get_config()\n",
    "            num_heads = config['num_heads']\n",
    "            key_dim = config['key_dim']\n",
    "            d_model = num_heads * key_dim\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "            projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "            attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "            context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "            output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "            layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "        elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "            inner_layer = layer_instance.layer\n",
    "            td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "            layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "        elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "            ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "        return layer_flops_count\n",
    "\n",
    "    for layer in functional_model.layers:\n",
    "        flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "    return flops / 1e9\n",
    "\n",
    "def get_gpu_stats():\n",
    "    if nvidia_smi is None:\n",
    "        return 0, 0\n",
    "    try:\n",
    "        nvidia_smi.nvmlInit()\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "        mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_memory = mem_info.used / (1024 ** 3)\n",
    "        nvidia_smi.nvmlShutdown()\n",
    "        return gpu_load, gpu_memory\n",
    "    except Exception:\n",
    "        return 0, 0\n",
    "\n",
    "class ResourceMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.resource_stats = []\n",
    "        self.expert_load_history = []\n",
    "        self.start_time = None\n",
    "        self.epoch_start_time = None\n",
    "        self.flops = None\n",
    "        self.validation_data = validation_data\n",
    "        self.nlu_expert_usage_counts = None\n",
    "        self.nlg_expert_usage_counts = None\n",
    "        self.nlu_gate_weights_history = []\n",
    "        self.nlg_gate_weights_history = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        try:\n",
    "            self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "            self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "            self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "        except Exception:\n",
    "            self.flops = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory()\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.resource_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'epoch_time': epoch_time,\n",
    "            'cpu_usage': cpu_usage,\n",
    "            'memory_used': memory.used / (1024 ** 3),\n",
    "            'memory_total': memory.total / (1024 ** 3),\n",
    "            'gpu_load': gpu_load,\n",
    "            'gpu_memory': gpu_memory,\n",
    "            'loss': logs.get('loss', 0),\n",
    "            'val_loss': logs.get('val_loss', 0),\n",
    "            'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "            'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "            'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "            'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "            'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "            'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "            'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "            'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "            'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "            'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "            'perplexity': logs.get('response_output_perplexity', 0),\n",
    "            'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "            'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "            'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "        })\n",
    "        if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "            self.expert_load_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "            })\n",
    "\n",
    "        sample_size = 100\n",
    "        for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "            inputs, _ = batch\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "            reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "            self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "            nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "            reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "            self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.total_time = time.time() - self.start_time\n",
    "\n",
    "        if len(self.nlu_gate_weights_history) > 0:\n",
    "            all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "            nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "            for expert_id in nlu_top_indices:\n",
    "                if expert_id < self.model.moe_nlu.total_experts:\n",
    "                    self.nlu_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        if len(self.nlg_gate_weights_history) > 0:\n",
    "            all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "            nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "            for expert_id in nlg_top_indices:\n",
    "                if expert_id < self.model.moe_nlg.total_experts:\n",
    "                    self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "    def visualize_expert_load_distribution(self):\n",
    "        total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "        total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "        nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "        nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "        nlu_stability = np.var(nlu_percentages)\n",
    "        nlg_stability = np.var(nlg_percentages)\n",
    "        return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "try:\n",
    "    possible_paths = [\n",
    "        os.path.join(\"preprocessor_models\", \"preprocessor_params.json\"),\n",
    "        os.path.join(\"tf_datasets\", \"preprocessor_params.json\"),\n",
    "        \"preprocessor_params.json\"\n",
    "    ]\n",
    "    \n",
    "    vocab_loaded = False\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                preprocessor_params = json.load(f)\n",
    "            word_to_id = {k: int(v) for k, v in preprocessor_params['word_to_id'].items()}\n",
    "            id_to_word = {int(k): v for k, v in preprocessor_params['id_to_word'].items()}\n",
    "            print(f\"Loaded vocabulary from {path}\")\n",
    "            vocab_loaded = True\n",
    "            break\n",
    "    \n",
    "    if not vocab_loaded:\n",
    "        raise FileNotFoundError(\"Vocabulary file not found in any location\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load vocabulary from preprocessor: {str(e)}\")\n",
    "    word_to_id = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    id_to_word = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
    "\n",
    "model_save_path = \"best_deepseekmoe_expert_choice_model\"\n",
    "print(f\"\\nLoading model from: {model_save_path}\")\n",
    "custom_objects = {\n",
    "    \"MoEModel\":MoEModel,\n",
    "    \"CustomLearningRateSchedule\": CustomLearningRateSchedule,\n",
    "    \"DeepSeekMoELayer\": DeepSeekMoELayer,\n",
    "    \"TransformerDecoderLayer\": TransformerDecoderLayer,\n",
    "    \"IntentAccuracy\": IntentAccuracy,\n",
    "    \"IntentPrecision\": IntentPrecision,\n",
    "    \"IntentRecall\": IntentRecall,\n",
    "    \"IntentF1\": IntentF1,\n",
    "    \"DomainAccuracy\": DomainAccuracy,\n",
    "    \"Perplexity\": Perplexity,\n",
    "    \"ResponseEmbeddingCosineSimilarity\": ResponseEmbeddingCosineSimilarity\n",
    "}\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model from {model_save_path}: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "def compile_model(model, moe_params):\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'domain_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss,\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={\n",
    "            'intent_output': 0.5,\n",
    "            'domain_output': 0.5,\n",
    "            'response_output': 1.0,\n",
    "            'response_embeddings': 1.0\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = compile_model(model, moe_params)\n",
    "print(\"\\nModel loaded and compiled successfully!\")\n",
    "sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "model(sample_input[0])\n",
    "model.summary()\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self, model, test_dataset, moe_params, raw_data, word_to_id, id_to_word):\n",
    "        self.model = model\n",
    "        self.test_dataset = test_dataset\n",
    "        self.moe_params = moe_params\n",
    "        self.raw_data = raw_data\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.nlu_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.nlg_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.resource_stats = {\n",
    "            'cpu_usage': [],\n",
    "            'memory_used': [],\n",
    "            'gpu_load': [],\n",
    "            'gpu_memory': [],\n",
    "            'batch_times': []\n",
    "        }\n",
    "        self.special_tokens = {'<pad>', '<sos>', '<eos>', '<unk>'}\n",
    "        self.nlu_top_k = model.get_layer('moe_nlu').get_config().get('top_k', 2)\n",
    "        self.nlg_top_k = model.get_layer('moe_nlg').get_config().get('top_k', 2)\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(\"\\nEvaluating model on test set...\")\n",
    "        \n",
    "        def add_response_embeddings(features, targets):\n",
    "            response_tokens = targets['response_output']\n",
    "            response_embeddings = self.model.embedding(response_tokens)\n",
    "            targets['response_embeddings'] = response_embeddings\n",
    "            return features, targets\n",
    "        \n",
    "        test_dataset_with_embeddings = self.test_dataset.map(add_response_embeddings, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        start_time = time.time()\n",
    "        test_results = self.model.evaluate(test_dataset_with_embeddings, verbose=1)\n",
    "        eval_time = time.time() - start_time\n",
    "        \n",
    "        metric_names = self.model.metrics_names\n",
    "        \n",
    "        perplexity_value = None\n",
    "        for name, value in zip(metric_names, test_results):\n",
    "            if name == 'response_output_perplexity':\n",
    "                perplexity_value = value\n",
    "                break\n",
    "        if perplexity_value is None:\n",
    "            print(\"Warning: Could not find perplexity in standard evaluation results.\")\n",
    "            perplexity_value = 0.0\n",
    "        \n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory().used / (1024 ** 3)\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "\n",
    "        total_tokens = 0\n",
    "        total_time = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        test_dataset_small = test_dataset_with_embeddings.unbatch().batch(moe_params[\"batch_size\"])\n",
    "        \n",
    "        all_true_intents = []\n",
    "        all_pred_intents = []\n",
    "        all_true_domains = []\n",
    "        all_pred_domains = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(test_dataset_small):\n",
    "            inputs, targets = batch\n",
    "            response_tokens = targets['response_output'].numpy()\n",
    "            non_padding_mask = response_tokens != 0\n",
    "            total_tokens += np.sum(non_padding_mask)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            total_time += batch_time\n",
    "            num_batches += tf.shape(inputs['user_utterance_tokens'])[0]\n",
    "            \n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            \n",
    "            self.resource_stats['cpu_usage'].append(cpu_usage)\n",
    "            self.resource_stats['memory_used'].append(memory.used / (1024 ** 3))\n",
    "            self.resource_stats['gpu_load'].append(gpu_load)\n",
    "            self.resource_stats['gpu_memory'].append(gpu_memory)\n",
    "            self.resource_stats['batch_times'].append(batch_time)\n",
    "            \n",
    "            nlu_gate_weights = outputs.get('nlu_gate_weights', tf.zeros([0, self.model.moe_nlu.routed_experts])).numpy()\n",
    "            nlu_top_indices = np.argsort(-nlu_gate_weights, axis=-1)[:, :self.nlu_top_k] + self.model.moe_nlu.k_s\n",
    "            for idx in range(self.nlu_top_k):\n",
    "                expert_ids = nlu_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlu_expert_usage_counts):\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "            \n",
    "            nlg_gate_weights = outputs.get('nlg_gate_weights', tf.zeros([0, self.model.moe_nlg.routed_experts])).numpy()\n",
    "            if len(nlg_gate_weights.shape) == 3:\n",
    "                nlg_gate_weights = nlg_gate_weights.reshape(-1, nlg_gate_weights.shape[-1])\n",
    "            nlg_top_indices = np.argsort(-nlg_gate_weights, axis=-1)[:, :self.nlg_top_k] + self.model.moe_nlg.k_s\n",
    "            for idx in range(self.nlg_top_k):\n",
    "                expert_ids = nlg_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlg_expert_usage_counts):\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "            \n",
    "            true_intents = targets.get('intent_output', np.zeros((moe_params[\"batch_size\"], self.model.num_intents))).numpy()\n",
    "            pred_intents = (outputs.get('intent_output', np.zeros_like(true_intents)).numpy() > 0.5).astype(np.int32)\n",
    "            all_true_intents.append(true_intents)\n",
    "            all_pred_intents.append(pred_intents)\n",
    "            \n",
    "            true_domains = targets.get('domain_output', np.zeros((moe_params[\"batch_size\"], 1))).numpy().flatten()\n",
    "            pred_domains = np.argmax(outputs.get('domain_output', np.zeros((moe_params[\"batch_size\"], self.model.num_domains))).numpy(), axis=-1)\n",
    "            all_true_domains.append(true_domains)\n",
    "            all_pred_domains.append(pred_domains)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                gc.collect()\n",
    "                tf.keras.backend.clear_session()\n",
    "        \n",
    "        avg_time_per_token = (total_time / total_tokens) * 1000 if total_tokens > 0 else 0\n",
    "        \n",
    "        avg_cpu = np.mean(self.resource_stats['cpu_usage']) if self.resource_stats['cpu_usage'] else 0\n",
    "        avg_memory = np.mean(self.resource_stats['memory_used']) if self.resource_stats['memory_used'] else 0\n",
    "        avg_gpu_load = np.mean(self.resource_stats['gpu_load']) if self.resource_stats['gpu_load'] else 0\n",
    "        avg_gpu_memory = np.mean(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        peak_gpu_memory = np.max(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        \n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "        \n",
    "        all_true_intents = np.vstack(all_true_intents)\n",
    "        all_pred_intents = np.vstack(all_pred_intents)\n",
    "        all_true_domains = np.concatenate(all_true_domains)\n",
    "        all_pred_domains = np.concatenate(all_pred_domains)\n",
    "        \n",
    "        intent_f1_score = f1_score(all_true_intents, all_pred_intents, average='micro')\n",
    "        intent_f1_score_macro = f1_score(all_true_intents, all_pred_intents, average='macro')\n",
    "        intent_f1_score_weighted = f1_score(all_true_intents, all_pred_intents, average='weighted')\n",
    "        intent_accuracy_score = accuracy_score(all_true_intents, all_pred_intents)\n",
    "        \n",
    "        domain_accuracy_score = accuracy_score(all_true_domains, all_pred_domains)\n",
    "        domain_f1_score_macro = f1_score(all_true_domains, all_pred_domains, average='macro')\n",
    "\n",
    "        hypotheses = []\n",
    "        references = []\n",
    "        meteor_scores = []\n",
    "        rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "        domain_metrics = defaultdict(lambda: {\n",
    "            'bleu': [],\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'meteor': [],\n",
    "            'count': 0\n",
    "        })\n",
    "        \n",
    "        if not self.raw_data.get('test'):\n",
    "            print(\"Warning: raw_data['test'] is empty. Skipping NLG metrics calculation.\")\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "            domain_metrics = {}\n",
    "        else:\n",
    "            for i, batch in enumerate(test_dataset_small):\n",
    "                if i >= len(self.raw_data.get('test', [])):\n",
    "                    print(f\"Warning: raw_data['test'] has fewer items ({len(self.raw_data.get('test', []))}) than test dataset. Stopping NLG metrics calculation.\")\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    inputs, targets = batch\n",
    "                    with tf.device('/CPU:0'):\n",
    "                        outputs = self.model(inputs, training=False)\n",
    "                    \n",
    "                    domain_id = targets['domain_output'].numpy()[0][0]\n",
    "                    domain_name = f\"domain_{domain_id}\"\n",
    "                    \n",
    "                    response_probs = outputs['response_output'].numpy()\n",
    "                    generated_tokens = np.argmax(response_probs, axis=-1)[0]\n",
    "                    true_tokens = targets['response_output'].numpy()[0]\n",
    "                    \n",
    "                    raw_item = self.raw_data['test'][i]\n",
    "                    ref_text = raw_item.get('raw_next_system_response', '')\n",
    "                    if not ref_text:\n",
    "                        continue\n",
    "                    \n",
    "                    gen_text = self.tokens_to_text(generated_tokens)\n",
    "                    ref_tokens = self.tokens_to_text(true_tokens)\n",
    "                    \n",
    "                    if not gen_text.strip() or not ref_text.strip():\n",
    "                        continue\n",
    "                    \n",
    "                    hypotheses.append(gen_text)\n",
    "                    references.append([ref_text])\n",
    "                    \n",
    "                    ref_tokens = word_tokenize(ref_text.lower())\n",
    "                    gen_tokens = word_tokenize(gen_text.lower())\n",
    "                    \n",
    "                    try:\n",
    "                        meteor = meteor_score([ref_tokens], gen_tokens)\n",
    "                        meteor_scores.append(meteor)\n",
    "                        domain_metrics[domain_name]['meteor'].append(meteor)\n",
    "                    except Exception as e:\n",
    "                        print(f\"METEOR calculation error for example {i}: {str(e)}\")\n",
    "                        meteor_scores.append(0)\n",
    "                        domain_metrics[domain_name]['meteor'].append(0)\n",
    "                    \n",
    "                    try:\n",
    "                        rouge_scores = rouge_scorer_instance.score(ref_text, gen_text)\n",
    "                        domain_metrics[domain_name]['rouge1'].append(rouge_scores['rouge1'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rouge2'].append(rouge_scores['rouge2'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rougeL'].append(rouge_scores['rougeL'].fmeasure)\n",
    "                    except Exception as e:\n",
    "                        print(f\"ROUGE calculation error for example {i}: {str(e)}\")\n",
    "                    \n",
    "                    domain_metrics[domain_name]['count'] += 1\n",
    "                    \n",
    "                    if i % 50 == 0:\n",
    "                        gc.collect()\n",
    "                        tf.keras.backend.clear_session()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing example {i}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if hypotheses and references:\n",
    "            hypotheses_tok = []\n",
    "            references_tok = []\n",
    "            \n",
    "            for hyp, ref_list in zip(hypotheses, references):\n",
    "                if not hyp or not ref_list[0]:\n",
    "                    continue\n",
    "                hyp_tokens = word_tokenize(hyp.lower())\n",
    "                ref_tokens = [word_tokenize(ref.lower()) for ref in ref_list]\n",
    "                hypotheses_tok.append(hyp_tokens)\n",
    "                references_tok.append(ref_tokens)\n",
    "            \n",
    "            chencherry = SmoothingFunction()\n",
    "            try:\n",
    "                bleu_score = corpus_bleu(references_tok, hypotheses_tok, smoothing_function=chencherry.method1)\n",
    "            except Exception as e:\n",
    "                print(f\"BLEU calculation error: {str(e)}\")\n",
    "                bleu_score = 0.0\n",
    "            \n",
    "            rouge_scores = []\n",
    "            for hyp, ref in zip(hypotheses, references):\n",
    "                if hyp and ref[0]:\n",
    "                    try:\n",
    "                        rouge_scores.append(rouge_scorer_instance.score(ref[0], hyp))\n",
    "                    except Exception as e:\n",
    "                        print(f\"ROUGE scoring error: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            avg_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_meteor = np.mean(meteor_scores) if meteor_scores else 0.0\n",
    "        else:\n",
    "            print(\"Warning: No valid hypotheses or references for NLG metrics. Setting to 0.0.\")\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "\n",
    "        return {\n",
    "            'intent_f1_micro': intent_f1_score,\n",
    "            'intent_f1_macro': intent_f1_score_macro,\n",
    "            'intent_f1_weighted': intent_f1_score_weighted,\n",
    "            'intent_accuracy': intent_accuracy_score,\n",
    "            'domain_accuracy': domain_accuracy_score,\n",
    "            'domain_f1_macro': domain_f1_score_macro,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge1': avg_rouge1,\n",
    "            'rouge2': avg_rouge2,\n",
    "            'rougeL': avg_rougeL,\n",
    "            'meteor': avg_meteor,\n",
    "            'perplexity': perplexity_value,\n",
    "            'pred_speed': avg_time_per_token,\n",
    "            'domain_metrics': domain_metrics,\n",
    "            'eval_time': eval_time\n",
    "        }\n",
    "    \n",
    "    def tokens_to_text(self, tokens):\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            if token == 0:\n",
    "                continue\n",
    "            word = self.id_to_word.get(token, '<unk>')\n",
    "            if word not in self.special_tokens:\n",
    "                words.append(word)\n",
    "        return \" \".join(words).strip()\n",
    "\n",
    "results_table = {\n",
    "    'Evaluation Time (seconds)': [],\n",
    "    'Prediction Speed (ms/token)': [],\n",
    "    'Average CPU Usage (percent)': [],\n",
    "    'Average GPU Usage (percent)': [],\n",
    "    'Average Memory (GB)': [],\n",
    "    'Average GPU Memory (GB)': [],\n",
    "    'Average FLOPs Estimate (GFLOPs)': [],\n",
    "    'NLU Expert 1 (percent)': [],\n",
    "    'NLU Expert 2 (percent)': [],\n",
    "    'NLU Expert 3 (percent)': [],\n",
    "    'NLU Expert 4 (percent)': [],\n",
    "    'NLU Expert 5 (percent)': [],\n",
    "    'NLU Expert 6 (percent)': [],\n",
    "    'NLU Expert 7 (percent)': [],\n",
    "    'NLU Expert 8 (percent)': [],\n",
    "    'NLG Expert 1 (percent)': [],\n",
    "    'NLG Expert 2 (percent)': [],\n",
    "    'NLG Expert 3 (percent)': [],\n",
    "    'NLG Expert 4 (percent)': [],\n",
    "    'NLG Expert 5 (percent)': [],\n",
    "    'NLG Expert 6 (percent)': [],\n",
    "    'NLG Expert 7 (percent)': [],\n",
    "    'NLG Expert 8 (percent)': [],\n",
    "    'Intent F1-score (Micro)': [],\n",
    "    'Intent F1-score (Macro)': [],\n",
    "    'Intent F1-score (Weighted)': [],\n",
    "    'Intent Accuracy': [],\n",
    "    'Domain Accuracy': [],\n",
    "    'Domain F1-score (Macro)': [],\n",
    "    'BLEU Score': [],\n",
    "    'ROUGE-1 F1': [],\n",
    "    'ROUGE-2 F1': [],\n",
    "    'ROUGE-L F1': [],\n",
    "    'METEOR Score': [],\n",
    "    'Perplexity': [],\n",
    "    'NLU Expert Load Stability (Variance)': [],\n",
    "    'NLG Expert Load Stability (Variance)': []\n",
    "}\n",
    "\n",
    "num_iterations = 5\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    print(f\"\\nStarting Iteration {iteration}\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "    model = compile_model(model, moe_params)\n",
    "    \n",
    "    evaluator = EvaluationMetrics(model, test_tf_dataset, moe_params, raw_data, word_to_id, id_to_word)\n",
    "\n",
    "    evaluation_results = evaluator.evaluate()\n",
    "    results_table['Evaluation Time (seconds)'].append(evaluation_results['eval_time'])\n",
    "    results_table['Prediction Speed (ms/token)'].append(evaluation_results['pred_speed'])\n",
    "    results_table['Average CPU Usage (percent)'].append(np.mean(evaluator.resource_stats['cpu_usage']) if evaluator.resource_stats['cpu_usage'] else 0)\n",
    "    results_table['Average GPU Usage (percent)'].append(np.mean(evaluator.resource_stats['gpu_load']) if evaluator.resource_stats['gpu_load'] else 0)\n",
    "    results_table['Average Memory (GB)'].append(np.mean(evaluator.resource_stats['memory_used']) if evaluator.resource_stats['memory_used'] else 0)\n",
    "    results_table['Average GPU Memory (GB)'].append(np.mean(evaluator.resource_stats['gpu_memory']) if evaluator.resource_stats['gpu_memory'] else 0)\n",
    "    results_table['Average FLOPs Estimate (GFLOPs)'].append(calculate_flops(model))\n",
    "    nlu_counts = evaluator.nlu_expert_usage_counts\n",
    "    nlg_counts = evaluator.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) if np.sum(nlu_counts) > 0 else 1\n",
    "    total_nlg = np.sum(nlg_counts) if np.sum(nlg_counts) > 0 else 1\n",
    "    results_table['NLU Expert 1 (percent)'].append((nlu_counts[0] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 2 (percent)'].append((nlu_counts[1] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 3 (percent)'].append((nlu_counts[2] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 4 (percent)'].append((nlu_counts[3] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 5 (percent)'].append((nlu_counts[4] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 6 (percent)'].append((nlu_counts[5] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 7 (percent)'].append((nlu_counts[6] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 8 (percent)'].append((nlu_counts[7] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLG Expert 1 (percent)'].append((nlg_counts[0] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 2 (percent)'].append((nlg_counts[1] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 3 (percent)'].append((nlg_counts[2] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 4 (percent)'].append((nlg_counts[3] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 5 (percent)'].append((nlg_counts[4] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 6 (percent)'].append((nlg_counts[5] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 7 (percent)'].append((nlg_counts[6] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 8 (percent)'].append((nlg_counts[7] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['Intent F1-score (Micro)'].append(evaluation_results['intent_f1_micro'])\n",
    "    results_table['Intent F1-score (Macro)'].append(evaluation_results['intent_f1_macro'])\n",
    "    results_table['Intent F1-score (Weighted)'].append(evaluation_results['intent_f1_weighted'])\n",
    "    results_table['Intent Accuracy'].append(evaluation_results['intent_accuracy'])\n",
    "    results_table['Domain Accuracy'].append(evaluation_results['domain_accuracy'])\n",
    "    results_table['Domain F1-score (Macro)'].append(evaluation_results['domain_f1_macro'])\n",
    "    results_table['BLEU Score'].append(evaluation_results['bleu'])\n",
    "    results_table['ROUGE-1 F1'].append(evaluation_results['rouge1'])\n",
    "    results_table['ROUGE-2 F1'].append(evaluation_results['rouge2'])\n",
    "    results_table['ROUGE-L F1'].append(evaluation_results['rougeL'])\n",
    "    results_table['METEOR Score'].append(evaluation_results['meteor'])\n",
    "    results_table['Perplexity'].append(evaluation_results['perplexity'])\n",
    "    results_table['NLU Expert Load Stability (Variance)'].append(np.var(nlu_counts / total_nlu * 100) if total_nlu > 0 else 0)\n",
    "    results_table['NLG Expert Load Stability (Variance)'].append(np.var(nlg_counts / total_nlg * 100) if total_nlg > 0 else 0)\n",
    "\n",
    "for key in results_table:\n",
    "    if results_table[key]:\n",
    "        results_table[key].append(np.mean(results_table[key]))\n",
    "    else:\n",
    "        results_table[key].append(0)\n",
    "\n",
    "print(\"\\nTraining results saved\")\n",
    "final_df = pd.DataFrame(results_table)\n",
    "final_df = final_df.T\n",
    "\n",
    "final_df.columns = [f'Iteration {i+1}' for i in range(num_iterations)] + ['Average']\n",
    "\n",
    "final_df.to_excel('prediction_deepseekmoe_expert_choice_results.xlsx', index=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE with Top-P Routing with Adaptive Gating experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:19:55.073300: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 08:19:55.181673: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 08:19:55.181905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 08:19:55.182999: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 08:19:55.190726: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 08:19:56.116729: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training Iteration 1\n",
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:19:58.495729: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:19:58.520323: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:19:58.520499: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:19:58.521936: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:19:58.522090: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:19:58.522182: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:19:58.590321: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:19:58.590532: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:19:58.590681: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:19:58.590791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/4428 [00:00<?, ?it/s]2025-07-04 08:20:19.857910: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f4f2c1b4740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 08:20:19.857960: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2025-07-04 08:20:19.867321: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 08:20:19.890095: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751617219.988278   18423 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 1: 100%|██████████| 4428/4428 [02:50<00:00, 26.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deepseekmoe_topp_adaptive_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deepseekmoe_topp_adaptive_model/assets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Iteration 2</th>\n",
       "      <th>Iteration 3</th>\n",
       "      <th>Iteration 4</th>\n",
       "      <th>Iteration 5</th>\n",
       "      <th>Iteration 6</th>\n",
       "      <th>Iteration 7</th>\n",
       "      <th>Iteration 8</th>\n",
       "      <th>Iteration 9</th>\n",
       "      <th>Iteration 10</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Speed (epochs per second)</th>\n",
       "      <td>0.006023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training Time (second/epoch)</th>\n",
       "      <td>166.036098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.603610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Training Time (second/iteration)</th>\n",
       "      <td>171.864486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.186449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computational Resource Usage</th>\n",
       "      <td>34.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>25.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>6.590851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.659085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.531799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.453180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPS Estimate (GFLOPS)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLU Load Distribution Stability</th>\n",
       "      <td>239.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLG Load Distribution Stability</th>\n",
       "      <td>68.993763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.899376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 3 (percent)</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 4 (percent)</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 5 (percent)</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 6 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 7 (percent)</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 8 (percent)</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 3 (percent)</th>\n",
       "      <td>11.850746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.185075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 4 (percent)</th>\n",
       "      <td>23.432836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.343284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 5 (percent)</th>\n",
       "      <td>21.925373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.192537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 6 (percent)</th>\n",
       "      <td>13.850746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.385075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 7 (percent)</th>\n",
       "      <td>11.223881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.122388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 8 (percent)</th>\n",
       "      <td>17.716418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.771642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Entity Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Intent Accuracy</th>\n",
       "      <td>0.747064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Perplexity</th>\n",
       "      <td>19.074549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.907455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Response Cosine Similarity</th>\n",
       "      <td>0.331149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Iteration 1  Iteration 2  \\\n",
       "Training Speed (epochs per second)                0.006023          0.0   \n",
       "Training Time (second/epoch)                    166.036098          0.0   \n",
       "Total Training Time (second/iteration)          171.864486          0.0   \n",
       "Computational Resource Usage                     34.600000          0.0   \n",
       "Average CPU Usage (percent)                      25.600000          0.0   \n",
       "Average GPU Usage (percent)                       9.000000          0.0   \n",
       "Average Memory (GB)                               6.590851          0.0   \n",
       "Average GPU Memory (GB)                          14.531799          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                   2.377871          0.0   \n",
       "Expert NLU Load Distribution Stability          239.000000          0.0   \n",
       "Expert NLG Load Distribution Stability           68.993763          0.0   \n",
       "Average NLU Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 3 (percent)                   10.000000          0.0   \n",
       "Average NLU Expert 4 (percent)                   39.000000          0.0   \n",
       "Average NLU Expert 5 (percent)                    4.000000          0.0   \n",
       "Average NLU Expert 6 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 7 (percent)                   38.000000          0.0   \n",
       "Average NLU Expert 8 (percent)                    9.000000          0.0   \n",
       "Average NLG Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 3 (percent)                   11.850746          0.0   \n",
       "Average NLG Expert 4 (percent)                   23.432836          0.0   \n",
       "Average NLG Expert 5 (percent)                   21.925373          0.0   \n",
       "Average NLG Expert 6 (percent)                   13.850746          0.0   \n",
       "Average NLG Expert 7 (percent)                   11.223881          0.0   \n",
       "Average NLG Expert 8 (percent)                   17.716418          0.0   \n",
       "Average Validation Entity Accuracy                1.000000          0.0   \n",
       "Average Intent Accuracy                           0.747064          0.0   \n",
       "Average Validation Perplexity                    19.074549          0.0   \n",
       "Average Validation Response Cosine Similarity     0.331149          0.0   \n",
       "\n",
       "                                               Iteration 3  Iteration 4  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 5  Iteration 6  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 7  Iteration 8  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 9  Iteration 10  \\\n",
       "Training Speed (epochs per second)                     0.0           0.0   \n",
       "Training Time (second/epoch)                           0.0           0.0   \n",
       "Total Training Time (second/iteration)                 0.0           0.0   \n",
       "Computational Resource Usage                           0.0           0.0   \n",
       "Average CPU Usage (percent)                            0.0           0.0   \n",
       "Average GPU Usage (percent)                            0.0           0.0   \n",
       "Average Memory (GB)                                    0.0           0.0   \n",
       "Average GPU Memory (GB)                                0.0           0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0           0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0           0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0           0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0           0.0   \n",
       "Average Validation Entity Accuracy                     0.0           0.0   \n",
       "Average Intent Accuracy                                0.0           0.0   \n",
       "Average Validation Perplexity                          0.0           0.0   \n",
       "Average Validation Response Cosine Similarity          0.0           0.0   \n",
       "\n",
       "                                                 Average  \n",
       "Training Speed (epochs per second)              0.000602  \n",
       "Training Time (second/epoch)                   16.603610  \n",
       "Total Training Time (second/iteration)         17.186449  \n",
       "Computational Resource Usage                    3.460000  \n",
       "Average CPU Usage (percent)                     2.560000  \n",
       "Average GPU Usage (percent)                     0.900000  \n",
       "Average Memory (GB)                             0.659085  \n",
       "Average GPU Memory (GB)                         1.453180  \n",
       "Average FLOPS Estimate (GFLOPS)                 0.237787  \n",
       "Expert NLU Load Distribution Stability         23.900000  \n",
       "Expert NLG Load Distribution Stability          6.899376  \n",
       "Average NLU Expert 1 (percent)                  0.000000  \n",
       "Average NLU Expert 2 (percent)                  0.000000  \n",
       "Average NLU Expert 3 (percent)                  1.000000  \n",
       "Average NLU Expert 4 (percent)                  3.900000  \n",
       "Average NLU Expert 5 (percent)                  0.400000  \n",
       "Average NLU Expert 6 (percent)                  0.000000  \n",
       "Average NLU Expert 7 (percent)                  3.800000  \n",
       "Average NLU Expert 8 (percent)                  0.900000  \n",
       "Average NLG Expert 1 (percent)                  0.000000  \n",
       "Average NLG Expert 2 (percent)                  0.000000  \n",
       "Average NLG Expert 3 (percent)                  1.185075  \n",
       "Average NLG Expert 4 (percent)                  2.343284  \n",
       "Average NLG Expert 5 (percent)                  2.192537  \n",
       "Average NLG Expert 6 (percent)                  1.385075  \n",
       "Average NLG Expert 7 (percent)                  1.122388  \n",
       "Average NLG Expert 8 (percent)                  1.771642  \n",
       "Average Validation Entity Accuracy              0.100000  \n",
       "Average Intent Accuracy                         0.074706  \n",
       "Average Validation Perplexity                   1.907455  \n",
       "Average Validation Response Cosine Similarity   0.033115  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reset_kernel():\n",
    "    tf.keras.backend.clear_session()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "results_table = {\n",
    "    'Training Speed (epochs per second)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Training Time (second/epoch)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Total Training Time (second/iteration)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Computational Resource Usage': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average CPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average FLOPS Estimate (GFLOPS)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLU Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLG Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Entity Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Intent Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Perplexity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Response Cosine Similarity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0}\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_deepseekmoe_topp_adaptive_model\"\n",
    "\n",
    "for iteration in range(5):\n",
    "    print(f\"\\nStarting Training Iteration {iteration + 1}\")\n",
    "    reset_kernel()\n",
    "\n",
    "    def load_tf_datasets_from_disk(load_path):\n",
    "        print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "        try:\n",
    "            with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "                loaded_moe_params = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "        element_spec = (\n",
    "            {\n",
    "                'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            },\n",
    "            {\n",
    "                'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "                'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "                'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "            test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "        raw_data = {}\n",
    "        for dataset_name in ['train', 'test']:\n",
    "            path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "            if os.path.exists(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    raw_data[dataset_name] = pickle.load(f)\n",
    "                print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Raw data file {path} not found.\")\n",
    "                raw_data[dataset_name] = []\n",
    "\n",
    "        return {\n",
    "            \"train_dataset\": train_tf_dataset,\n",
    "            \"test_dataset\": test_tf_dataset,\n",
    "            \"moe_params\": loaded_moe_params,\n",
    "            \"raw_data\": raw_data\n",
    "        }\n",
    "\n",
    "    tf_dataset_save_path = \"tf_datasets\"\n",
    "    loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "    train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "    moe_params = loaded_data[\"moe_params\"]\n",
    "    raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "    class TopPAdaptiveRouter:\n",
    "        def __init__(self, p=0.9, tau=0.1):\n",
    "            self.p = p\n",
    "            self.tau = tau\n",
    "\n",
    "        def __call__(self, gate_logits):\n",
    "            probs = tf.nn.softmax(gate_logits, axis=-1)\n",
    "            sorted_probs, sorted_indices = tf.math.top_k(probs, k=tf.shape(probs)[-1])\n",
    "            top1_prob = sorted_probs[:, 0]\n",
    "            top2_prob = sorted_probs[:, 1]\n",
    "            prob_diff = top1_prob - top2_prob\n",
    "            use_top1 = prob_diff >= self.tau\n",
    "            max_k = tf.where(use_top1, 1, 2)\n",
    "            cum_probs = tf.cumsum(sorted_probs, axis=-1)\n",
    "            p_mask = cum_probs >= self.p\n",
    "            first_over_p = tf.argmax(tf.cast(p_mask, tf.int32), axis=-1, output_type=tf.int32)\n",
    "            first_over_p = tf.maximum(first_over_p, 0)\n",
    "            num_selected_top_p = first_over_p + 1\n",
    "            num_selected = tf.minimum(num_selected_top_p, max_k)\n",
    "            max_selected = tf.reduce_max(num_selected)\n",
    "            batch_size = tf.shape(gate_logits)[0]\n",
    "            num_experts = tf.shape(gate_logits)[1]\n",
    "            selection_mask = tf.sequence_mask(num_selected, maxlen=num_experts, dtype=tf.bool)\n",
    "            selected_indices = tf.where(selection_mask, sorted_indices, num_experts)\n",
    "            selected_probs = tf.where(selection_mask, sorted_probs, 0.0)\n",
    "            selected_indices = selected_indices[:, :max_selected]\n",
    "            selected_probs = selected_probs[:, :max_selected]\n",
    "            weights_sum = tf.reduce_sum(selected_probs, axis=-1, keepdims=True) + tf.keras.backend.epsilon()\n",
    "            weights = selected_probs / weights_sum\n",
    "            valid_mask = tf.sequence_mask(num_selected, maxlen=max_selected, dtype=tf.int32)\n",
    "            selected_indices = tf.where(valid_mask == 1, selected_indices, -1)\n",
    "            return selected_indices, weights, num_selected\n",
    "\n",
    "    class DeepSeekMoELayer(layers.Layer):\n",
    "        def __init__(self, num_experts, expert_dim, input_dim, m=4, k_s=2, alpha=0.01, top_p=0.9, tau=0.1, name=None):\n",
    "            super(DeepSeekMoELayer, self).__init__(name=name)\n",
    "            self.m = m\n",
    "            self.k_s = k_s\n",
    "            self.total_experts = num_experts * self.m\n",
    "            self.routed_experts = self.total_experts - self.k_s\n",
    "            self.alpha = alpha\n",
    "            self.top_p = top_p\n",
    "            self.tau = tau\n",
    "            self.input_dim = input_dim\n",
    "            self.seq_length = None\n",
    "            self.adjusted_expert_dim = expert_dim // self.m\n",
    "            self.shared_experts = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4), name=f'shared_expert_{i}_dense1'),\n",
    "                    layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4), name=f'shared_expert_{i}_dense2')\n",
    "                ], name=f'shared_expert_{i}') for i in range(self.k_s)\n",
    "            ]\n",
    "            self.routed_experts_list = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4), name=f'routed_expert_{i}_dense1'),\n",
    "                    layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4), name=f'routed_expert_{i}_dense2')\n",
    "                ], name=f'routed_expert_{i}') for i in range(self.k_s, self.total_experts)\n",
    "            ]\n",
    "            self.experts = self.shared_experts + self.routed_experts_list\n",
    "            self.gate = layers.Dense(self.routed_experts, activation=None, name='gate_weights')\n",
    "            self.router = TopPAdaptiveRouter(p=self.top_p, tau=self.tau)\n",
    "            self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            input_rank = inputs.shape.rank\n",
    "            if input_rank == 3:\n",
    "                batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "                flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "                is_3d = True\n",
    "            else:\n",
    "                batch_size = tf.shape(inputs)[0]\n",
    "                seq_length = 1\n",
    "                flat_inputs = inputs\n",
    "                is_3d = False\n",
    "            num_tokens = tf.shape(flat_inputs)[0]\n",
    "            flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "            shared_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "            for i in range(self.k_s):\n",
    "                shared_output += self.shared_experts[i](flat_inputs)\n",
    "            gate_logits = self.gate(flat_inputs)\n",
    "            expert_indices, expert_weights, num_selected = self.router(gate_logits)\n",
    "            expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "            expert_indices_adjusted = expert_indices + self.k_s\n",
    "            batch_indices = tf.tile(tf.expand_dims(tf.range(num_tokens), 1), [1, tf.shape(expert_indices)[1]])\n",
    "            gather_indices = tf.stack([batch_indices, expert_indices_adjusted], axis=-1)\n",
    "            valid_mask = tf.cast(expert_indices_adjusted >= 0, tf.float32)\n",
    "            valid_mask = tf.expand_dims(valid_mask, -1)\n",
    "            selected_outputs = tf.gather_nd(expert_outputs, gather_indices)\n",
    "            weighted_outputs = selected_outputs * tf.expand_dims(expert_weights, -1) * valid_mask\n",
    "            routed_output = tf.reduce_sum(weighted_outputs, axis=1)\n",
    "            output_flat = shared_output + routed_output + flat_inputs\n",
    "            if is_3d:\n",
    "                output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "                if self.seq_length is not None:\n",
    "                    output.set_shape([None, self.seq_length, self.input_dim])\n",
    "                gate_weights_reshaped = tf.reshape(tf.nn.softmax(gate_logits, axis=-1), [batch_size, seq_length, self.routed_experts])\n",
    "                if self.seq_length is not None:\n",
    "                    gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "            else:\n",
    "                output = output_flat\n",
    "                gate_weights_reshaped = tf.nn.softmax(gate_logits, axis=-1)\n",
    "            gate_probs = tf.nn.softmax(gate_logits, axis=-1)\n",
    "            expert_selection = tf.reduce_sum(tf.one_hot(expert_indices_adjusted, depth=self.total_experts), axis=1)\n",
    "            expert_selection = expert_selection[:, self.k_s:]\n",
    "            f_i = tf.reduce_mean(expert_selection, axis=0)\n",
    "            P_i = tf.reduce_mean(gate_probs, axis=0)\n",
    "            load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "            self.add_loss(load_balancing_loss)\n",
    "            self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "            return output, gate_weights_reshaped\n",
    "\n",
    "        def get_metrics(self):\n",
    "            return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'num_experts': self.total_experts // self.m,\n",
    "                'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "                'input_dim': self.input_dim,\n",
    "                'm': self.m,\n",
    "                'k_s': self.k_s,\n",
    "                'alpha': self.alpha,\n",
    "                'top_p': self.top_p,\n",
    "                'tau': self.tau,\n",
    "                'name': self.name\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class TransformerDecoderLayer(layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "            super(TransformerDecoderLayer, self).__init__()\n",
    "            self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.ffn = keras.Sequential([\n",
    "                layers.Dense(dff, activation='relu'),\n",
    "                layers.Dense(d_model)\n",
    "            ])\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "            self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "        def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "            if look_ahead_mask is not None:\n",
    "                look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "                look_ahead_mask = 1.0 - look_ahead_mask\n",
    "            attn1_output = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)\n",
    "            attn1 = self.dropout1(attn1_output, training=training)\n",
    "            out1 = self.layernorm1(attn1 + x)\n",
    "            attn2_output = self.mha2(query=out1, value=enc_output, key=enc_output, training=training)\n",
    "            attn2 = self.dropout2(attn2_output, training=training)\n",
    "            out2 = self.layernorm2(attn2 + out1)\n",
    "            ffn_output = self.ffn(out2)\n",
    "            ffn_output = self.dropout3(ffn_output, training=training)\n",
    "            out3 = self.layernorm3(ffn_output + out2)\n",
    "            return out3\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            mha1_config = self.mha1.get_config()\n",
    "            config.update({\n",
    "                'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "                'num_heads': mha1_config['num_heads'],\n",
    "                'dff': self.ffn.layers[0].units,\n",
    "                'rate': self.dropout1.rate\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class MoEModel(models.Model):\n",
    "        def __init__(self, moe_params):\n",
    "            super(MoEModel, self).__init__()\n",
    "            self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "            self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "            self.num_domains = moe_params[\"num_domains\"]\n",
    "            self.num_intents = moe_params[\"num_intents\"]\n",
    "            self.vocab_size = moe_params[\"vocab_size\"]\n",
    "            self.num_experts = moe_params[\"num_experts\"]\n",
    "            self.expert_dim = moe_params[\"expert_dim\"]\n",
    "            self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "            self.num_heads = 4\n",
    "            self.dff = self.embedding_dim * 4\n",
    "            self.dropout_rate = 0.1\n",
    "            self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                                 self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "            self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "\n",
    "            self.embedding = layers.Embedding(\n",
    "                self.vocab_size,\n",
    "                self.embedding_dim,\n",
    "                mask_zero=True,\n",
    "                embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "                name=\"embedding\"\n",
    "            )\n",
    "            self.embedding.build((None,))\n",
    "            with tf.init_scope():\n",
    "                embedding_weights = self.embedding.get_weights()\n",
    "                if embedding_weights and len(embedding_weights) > 0:\n",
    "                    embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                    self.embedding.set_weights(embedding_weights)\n",
    "\n",
    "            self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "            self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "            self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "            self.moe_nlu = DeepSeekMoELayer(\n",
    "                num_experts=self.num_experts,\n",
    "                expert_dim=self.expert_dim,\n",
    "                input_dim=self.nlu_input_dim,\n",
    "                m=2,\n",
    "                k_s=2,\n",
    "                alpha=0.01,\n",
    "                top_p=0.9,\n",
    "                tau=0.1,\n",
    "                name='moe_nlu'\n",
    "            )\n",
    "            self.moe_nlg = DeepSeekMoELayer(\n",
    "                num_experts=self.num_experts,\n",
    "                expert_dim=self.expert_dim,\n",
    "                input_dim=self.nlg_input_dim,\n",
    "                m=2,\n",
    "                k_s=2,\n",
    "                alpha=0.01,\n",
    "                top_p=0.9,\n",
    "                tau=0.1,\n",
    "                name='moe_nlg'\n",
    "            )\n",
    "            self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "            self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "            self.response_output = layers.TimeDistributed(\n",
    "                layers.Dense(self.vocab_size, activation='softmax'), name='response_output'\n",
    "            )\n",
    "            self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "            self.nlg_projection = layers.TimeDistributed(\n",
    "                layers.Dense(self.embedding_dim, activation='relu', name='nlg_projection')\n",
    "            )\n",
    "\n",
    "        def create_look_ahead_mask(self, size):\n",
    "            mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "            return mask\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "            prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "            decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "            domain_onehot_input = inputs['domain_onehot_input']\n",
    "            turn_id_embedding = inputs['turn_id_embedding']\n",
    "            ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "\n",
    "            pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "            user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "            prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "            decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "\n",
    "            user_enc_out = user_embedded\n",
    "            prev_system_enc_out = prev_system_embedded\n",
    "\n",
    "            look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "            dec_output = decoder_embedded\n",
    "            for i, layer in enumerate(self.decoder_layers):\n",
    "                dec_output = layer(dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask)\n",
    "            decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "\n",
    "            user_attn_out = self.attn_layer(\n",
    "                query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), \n",
    "                value=user_enc_out, key=user_enc_out, training=training\n",
    "            )\n",
    "            prev_system_attn_out = self.attn_layer(\n",
    "                query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), \n",
    "                value=prev_system_enc_out, key=prev_system_enc_out, training=training\n",
    "            )\n",
    "            decoder_attn_out = self.attn_layer(\n",
    "                query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), \n",
    "                value=decoder_output, key=decoder_output, training=training\n",
    "            )\n",
    "\n",
    "            combined_features = layers.Concatenate()([\n",
    "                tf.squeeze(user_attn_out, axis=1),\n",
    "                tf.squeeze(prev_system_attn_out, axis=1),\n",
    "                tf.squeeze(decoder_attn_out, axis=1),\n",
    "                domain_onehot_input,\n",
    "                turn_id_embedding,\n",
    "                ontology_multihot_input\n",
    "            ])\n",
    "            combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "            nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "            nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "            intent_out = self.intent_output(nlu_out)\n",
    "            domain_out = self.domain_output(nlu_out)\n",
    "\n",
    "            intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "            domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "            intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "            domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "\n",
    "            nlg_input = layers.Concatenate(axis=-1)([\n",
    "                decoder_output,\n",
    "                intent_out_tiled,\n",
    "                domain_out_tiled\n",
    "            ])\n",
    "            nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "\n",
    "            nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "            nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            response_embeddings = self.nlg_projection(nlg_out)\n",
    "            response_embeddings = tf.ensure_shape(response_embeddings, [None, self.max_seq_length, self.embedding_dim])\n",
    "            response_out = self.response_output(nlg_out)\n",
    "\n",
    "            return {\n",
    "                'intent_output': intent_out,\n",
    "                'domain_output': domain_out,\n",
    "                'response_output': response_out,\n",
    "                'response_embeddings': response_embeddings,\n",
    "                'nlu_gate_weights': nlu_gate_weights,\n",
    "                'nlg_gate_weights': nlg_gate_weights\n",
    "            }\n",
    "\n",
    "        def build_graph(self):\n",
    "            inputs = {\n",
    "                'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "                'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='prev_system_response_tokens'),\n",
    "                'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='decoder_input_tokens'),\n",
    "                'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32, name='domain_onehot_input'),\n",
    "                'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32, name='turn_id_embedding'),\n",
    "                'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32, name='ontology_multihot_input'),\n",
    "            }\n",
    "            return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'moe_params': {\n",
    "                    'embedding_dim': self.embedding_dim,\n",
    "                    'max_seq_length': self.max_seq_length,\n",
    "                    'num_domains': self.num_domains,\n",
    "                    'num_intents': self.num_intents,\n",
    "                    'vocab_size': self.vocab_size,\n",
    "                    'num_experts': self.num_experts,\n",
    "                    'expert_dim': self.expert_dim,\n",
    "                    'turn_id_dim': self.turn_id_dim\n",
    "                }\n",
    "            })\n",
    "            return config\n",
    "    \n",
    "    class IntentAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros', dtype=tf.float32)\n",
    "            self.total_preds = self.add_weight(name='total_preds', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "            self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "            self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.correct_preds.assign(0.)\n",
    "            self.total_preds.assign(0.)\n",
    "\n",
    "    class IntentPrecision(metrics.Metric):\n",
    "        def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "            self.predicted_positives = self.add_weight(name='pp', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            predicted_positives = tf.reduce_sum(y_pred)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.predicted_positives.assign(0.)\n",
    "\n",
    "    class IntentRecall(metrics.Metric):\n",
    "        def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "            self.actual_positives = self.add_weight(name='ap', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            actual_positives = tf.reduce_sum(y_true)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.actual_positives.assign(0.)\n",
    "\n",
    "    class IntentF1(metrics.Metric):\n",
    "        def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.precision = IntentPrecision(threshold=threshold)\n",
    "            self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "            self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "        def result(self):\n",
    "            p = self.precision.result()\n",
    "            r = self.recall.result()\n",
    "            return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.precision.reset_state()\n",
    "            self.recall.reset_state()\n",
    "\n",
    "    class DomainAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='domain_accuracy', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.accuracy = self.add_weight(name='acc', initializer='zeros', dtype=tf.float32)\n",
    "            self.count = self.add_weight(name='count', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "            pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "            matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "            self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "            self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.accuracy.assign(0.)\n",
    "            self.count.assign(0.)\n",
    "\n",
    "    class Perplexity(metrics.Metric):\n",
    "        def __init__(self, name='perplexity', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "            self.total_loss = self.add_weight(name='total_loss', initializer='zeros', dtype=tf.float32)\n",
    "            self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "            loss = self.cross_entropy(y_true, y_pred)\n",
    "            masked_loss = loss * mask\n",
    "            sum_loss = tf.reduce_sum(masked_loss)\n",
    "            num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "            self.total_loss.assign_add(sum_loss)\n",
    "            self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "        def result(self):\n",
    "            avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "            return tf.exp(avg_loss)\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_loss.assign(0.)\n",
    "            self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "    class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "        def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "            super().__init__(name=name)\n",
    "            self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "            self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "            y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "            y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "            y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "            cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "            non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "            non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "            masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "            num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "            avg_cosine_sim_per_sample = tf.where(\n",
    "                num_non_zero_tokens > 0,\n",
    "                tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "                tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "            )\n",
    "            self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "            self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_cosine_sim.assign(0.)\n",
    "            self.num_samples.assign(0.)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "        flops = 0\n",
    "        functional_model = model.build_graph()\n",
    "\n",
    "        def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "            layer_flops_count = 0\n",
    "\n",
    "            if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "                return 0\n",
    "\n",
    "            if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "                for sub_layer in layer_instance.layers:\n",
    "                    layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "                return layer_flops_count\n",
    "\n",
    "            if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "                return 0\n",
    "\n",
    "            if isinstance(layer_instance, layers.Dense):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                output_dim = layer_instance.output_shape[-1]\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.LSTM):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                hidden_dim = layer_instance.units\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "                layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.Embedding):\n",
    "                pass\n",
    "\n",
    "            elif isinstance(layer_instance, DeepSeekMoELayer):\n",
    "                moe_input_dim = layer_instance.input_dim\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "                layer_flops_count += gate_flops\n",
    "                for expert in layer_instance.experts:\n",
    "                    layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "                config = layer_instance.get_config()\n",
    "                num_heads = config['num_heads']\n",
    "                key_dim = config['key_dim']\n",
    "                d_model = num_heads * key_dim\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "                projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "                attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "                context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "                output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "                layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "            elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "                inner_layer = layer_instance.layer\n",
    "                td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "                layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "                ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "            return layer_flops_count\n",
    "\n",
    "        for layer in functional_model.layers:\n",
    "            flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "        return flops / 1e9\n",
    "\n",
    "    def get_gpu_stats():\n",
    "        if nvidia_smi is None:\n",
    "            return 0, 0\n",
    "        try:\n",
    "            nvidia_smi.nvmlInit()\n",
    "            handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "            gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "            mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "            gpu_memory = mem_info.used / (1024 ** 3)\n",
    "            nvidia_smi.nvmlShutdown()\n",
    "            return gpu_load, gpu_memory\n",
    "        except Exception:\n",
    "            return 0, 0\n",
    "\n",
    "    class ResourceMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, validation_data):\n",
    "            super().__init__()\n",
    "            self.resource_stats = []\n",
    "            self.expert_load_history = []\n",
    "            self.start_time = None\n",
    "            self.epoch_start_time = None\n",
    "            self.flops = None\n",
    "            self.validation_data = validation_data\n",
    "            self.nlu_expert_usage_counts = None\n",
    "            self.nlg_expert_usage_counts = None\n",
    "            self.nlu_gate_weights_history = []\n",
    "            self.nlg_gate_weights_history = []\n",
    "\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = time.time()\n",
    "            try:\n",
    "                self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "                self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "                self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "            except Exception:\n",
    "                self.flops = 0\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.epoch_start_time = time.time()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.resource_stats.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_time': epoch_time,\n",
    "                'cpu_usage': cpu_usage,\n",
    "                'memory_used': memory.used / (1024 ** 3),\n",
    "                'memory_total': memory.total / (1024 ** 3),\n",
    "                'gpu_load': gpu_load,\n",
    "                'gpu_memory': gpu_memory,\n",
    "                'loss': logs.get('loss', 0),\n",
    "                'val_loss': logs.get('val_loss', 0),\n",
    "                'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "                'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "                'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "                'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "                'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "                'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "                'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "                'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "                'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "                'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "                'perplexity': logs.get('response_output_perplexity', 0),\n",
    "                'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "                'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "                'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "            })\n",
    "            if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "                self.expert_load_history.append({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                    'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "                })\n",
    "\n",
    "            sample_size = 100\n",
    "            for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "                inputs, _ = batch\n",
    "                outputs = self.model(inputs, training=False)\n",
    "                nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "                reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "                self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "                nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "                reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "                self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            self.total_time = time.time() - self.start_time\n",
    "\n",
    "            if len(self.nlu_gate_weights_history) > 0:\n",
    "                all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "                nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "                for expert_id in nlu_top_indices:\n",
    "                    if expert_id < self.model.moe_nlu.total_experts:\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "            if len(self.nlg_gate_weights_history) > 0:\n",
    "                all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "                nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "                for expert_id in nlg_top_indices:\n",
    "                    if expert_id < self.model.moe_nlg.total_experts:\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        def visualize_expert_load_distribution(self):\n",
    "            total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "            total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "            nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "            nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "            nlu_stability = np.var(nlu_percentages)\n",
    "            nlg_stability = np.var(nlg_percentages)\n",
    "            return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "    class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super().__init__()\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "    model = MoEModel(moe_params)\n",
    "    embedding_layer = model.embedding\n",
    "\n",
    "    def create_validation_split(dataset, embedding_layer, validation_split=0.2, batch_size=moe_params[\"batch_size\"]):\n",
    "        element_spec = dataset.element_spec\n",
    "        data_list = [(features, targets) for features, targets in dataset.unbatch().as_numpy_iterator()]\n",
    "        if not data_list:\n",
    "            raise ValueError(\"Dataset is empty.\")\n",
    "        if len(data_list) < 2:\n",
    "            raise ValueError(\"Dataset too small to split.\")\n",
    "        train_data, val_data = train_test_split(data_list, test_size=validation_split, shuffle=True)\n",
    "\n",
    "        def create_dataset_from_list(data):\n",
    "            if not data:\n",
    "                raise ValueError(\"Empty data list provided.\")\n",
    "            features = {\n",
    "                'user_utterance_tokens': np.array([d[0]['user_utterance_tokens'] for d in data], dtype=np.int32),\n",
    "                'prev_system_response_tokens': np.array([d[0]['prev_system_response_tokens'] for d in data], dtype=np.int32),\n",
    "                'decoder_input_tokens': np.array([d[0]['decoder_input_tokens'] for d in data], dtype=np.int32),\n",
    "                'domain_onehot_input': np.array([d[0]['domain_onehot_input'] for d in data], dtype=np.float32),\n",
    "                'turn_id_embedding': np.array([d[0]['turn_id_embedding'] for d in data], dtype=np.float32),\n",
    "                'ontology_multihot_input': np.array([d[0]['ontology_multihot_input'] for d in data], dtype=np.float32),\n",
    "            }\n",
    "            response_output = np.array([d[1]['response_output'] for d in data], dtype=np.int32)\n",
    "            response_embeddings = embedding_layer(response_output).numpy()\n",
    "            targets = {\n",
    "                'domain_output': np.array([d[1]['domain_output'] for d in data], dtype=np.int32),\n",
    "                'intent_output': np.array([d[1]['intent_output'] for d in data], dtype=np.float32),\n",
    "                'response_output': response_output,\n",
    "                'response_embeddings': response_embeddings\n",
    "            }\n",
    "            return tf.data.Dataset.from_tensor_slices((features, targets)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        train_dataset = create_dataset_from_list(train_data)\n",
    "        val_dataset = create_dataset_from_list(val_data)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    train_tf_dataset, val_tf_dataset = create_validation_split(train_tf_dataset, embedding_layer)\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': losses.BinaryCrossentropy(),\n",
    "        'domain_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={'intent_output': 0.5, 'domain_output': 0.5, 'response_output': 1.0, 'response_embeddings': 1.0}\n",
    "    )\n",
    "\n",
    "    sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "    model(sample_input[0])\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=8, mode='min', restore_best_weights=True)\n",
    "    resource_monitor = ResourceMonitor(val_tf_dataset)\n",
    "    class TQDMProgressBar(callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.progress_bar = tqdm(total=len(train_tf_dataset), desc=f'Epoch {epoch + 1}')\n",
    "\n",
    "        def on_batch_end(self, batch, logs=None):\n",
    "            self.progress_bar.update(1)\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            self.progress_bar.close()\n",
    "\n",
    "    tqdm_callback = TQDMProgressBar()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_tf_dataset,\n",
    "        epochs=100,\n",
    "        validation_data=val_tf_dataset,\n",
    "        callbacks=[early_stopping, resource_monitor, tqdm_callback],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    expert_stats = resource_monitor.visualize_expert_load_distribution()\n",
    "    stats = resource_monitor.resource_stats\n",
    "    avg_epoch_duration = np.mean([s['epoch_time'] for s in stats]) if stats else 0\n",
    "    total_training_time = resource_monitor.total_time\n",
    "    avg_cpu_usage = np.mean([s['cpu_usage'] for s in stats]) if stats else 0\n",
    "    avg_memory = np.mean([s['memory_used'] for s in stats]) if stats else 0\n",
    "    avg_gpu_load = np.mean([s['gpu_load'] for s in stats]) if stats else 0\n",
    "    avg_gpu_memory = np.mean([s['gpu_memory'] for s in stats]) if stats else 0\n",
    "    avg_flops = resource_monitor.flops if resource_monitor.flops else 0\n",
    "    nlu_counts = resource_monitor.nlu_expert_usage_counts\n",
    "    nlg_counts = resource_monitor.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) or 1\n",
    "    total_nlg = np.sum(nlg_counts) or 1\n",
    "    expert_nlu_percentages = [(nlu_counts[i] / total_nlu) * 100 for i in range(8)]\n",
    "    expert_nlg_percentages = [(nlg_counts[i] / total_nlg) * 100 for i in range(8)]\n",
    "    val_intent_accuracy = np.mean([s['val_intent_accuracy'] for s in stats if s['val_intent_accuracy'] > 0]) if stats else 0\n",
    "    val_entity_accuracy = np.mean([s['val_domain_accuracy'] for s in stats if s['val_domain_accuracy'] > 0]) if stats else 0\n",
    "    val_perplexity = np.mean([s['val_perplexity'] for s in stats if s['val_perplexity'] > 0]) if stats else 0\n",
    "    val_cosine_similarity = np.mean([s['val_cosine_similarity'] for s in stats if s['val_cosine_similarity'] > 0]) if stats and any(s['val_cosine_similarity'] > 0 for s in stats) else 0\n",
    "    \n",
    "    results_table['Training Speed (epochs per second)'][f'Iteration {iteration + 1}'] = 1 / avg_epoch_duration if avg_epoch_duration > 0 else 0\n",
    "    results_table['Training Time (second/epoch)'][f'Iteration {iteration + 1}'] = avg_epoch_duration\n",
    "    results_table['Total Training Time (second/iteration)'][f'Iteration {iteration + 1}'] = total_training_time\n",
    "    results_table['Computational Resource Usage'][f'Iteration {iteration + 1}'] = avg_cpu_usage + avg_gpu_load\n",
    "    results_table['Average CPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_cpu_usage\n",
    "    results_table['Average GPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_gpu_load\n",
    "    results_table['Average Memory (GB)'][f'Iteration {iteration + 1}'] = avg_memory\n",
    "    results_table['Average GPU Memory (GB)'][f'Iteration {iteration + 1}'] = avg_gpu_memory\n",
    "    results_table['Average FLOPS Estimate (GFLOPS)'][f'Iteration {iteration + 1}'] = avg_flops\n",
    "    results_table['Expert NLU Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlu_stability']\n",
    "    results_table['Expert NLG Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlg_stability']\n",
    "    for i in range(8):\n",
    "        if i < len(expert_stats['nlu_percentages']):\n",
    "            results_table[f'Average NLU Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlu_percentages'][i]\n",
    "        if i < len(expert_stats['nlg_percentages']):\n",
    "            results_table[f'Average NLG Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlg_percentages'][i]\n",
    "    results_table['Average Validation Entity Accuracy'][f'Iteration {iteration + 1}'] = val_entity_accuracy\n",
    "    results_table['Average Intent Accuracy'][f'Iteration {iteration + 1}'] = val_intent_accuracy\n",
    "    results_table['Average Validation Perplexity'][f'Iteration {iteration + 1}'] = val_perplexity\n",
    "    results_table['Average Validation Response Cosine Similarity'][f'Iteration {iteration + 1}'] = val_cosine_similarity\n",
    "\n",
    "    val_loss = min([s['val_loss'] for s in stats]) if stats else float('inf')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        try:\n",
    "            model.save(best_model_path, save_format='tf', include_optimizer=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to save best model to {best_model_path}: {str(e)}\")\n",
    "\n",
    "for key in results_table:\n",
    "    results_table[key]['Average'] = np.mean([results_table[key][f'Iteration {i+1}'] for i in range(10)])\n",
    "\n",
    "final_result = pd.DataFrame(results_table)\n",
    "final_result = final_result.T\n",
    "final_result.to_excel('training_deepseekmoe_topp_adaptive_result.xlsx', index=True)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE with Top-P Routing with Adaptive Gating evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:25:30.365036: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 08:25:30.786695: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 08:25:30.786846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 08:25:30.866476: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 08:25:31.029536: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 08:25:32.584691: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:25:38.558679: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:25:38.830601: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:25:38.830824: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:25:38.832616: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:25:38.832793: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:25:38.832906: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:25:38.923662: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:25:38.923841: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:25:38.923947: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:25:38.924026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n",
      "Loaded vocabulary from preprocessor_models/preprocessor_params.json\n",
      "\n",
      "Loading model from: best_deepseekmoe_topp_adaptive_model\n",
      "\n",
      "Model loaded and compiled successfully!\n",
      "Model: \"mo_e_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  784896    \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  8576      \n",
      "                                                                 \n",
      " layer_normalization (Layer  multiple                  256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_layer   multiple                  264576    \n",
      " (TransformerDecoderLayer)                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " moe_nlu (DeepSeekMoELayer)  multiple                  244852    \n",
      "                                                                 \n",
      " moe_nlg (DeepSeekMoELayer)  multiple                  87578     \n",
      "                                                                 \n",
      " intent_output (Dense)       multiple                  14446     \n",
      "                                                                 \n",
      " domain_output (Dense)       multiple                  3262      \n",
      "                                                                 \n",
      " response_output (TimeDistr  multiple                  1024044   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " multi_head_attention_2 (Mu  multiple                  66048     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      " time_distributed (TimeDist  multiple                  21376     \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2519910 (9.61 MB)\n",
      "Trainable params: 2519910 (9.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Starting Iteration 1\n",
      "\n",
      "Evaluating model on test set...\n",
      "1397/1397 [==============================] - 21s 12ms/step - loss: 3.4863 - domain_output_loss: 0.0023 - intent_output_loss: 0.0299 - response_embeddings_loss: 0.8960 - response_output_loss: 2.5439 - domain_output_domain_accuracy: 1.0000 - intent_output_intent_accuracy: 0.7072 - intent_output_intent_precision: 0.9926 - intent_output_intent_recall: 0.6425 - intent_output_intent_f1: 0.7801 - response_embeddings_response_embedding_cosine_similarity: 0.1040 - response_output_perplexity: 30.0106 - moe_nlu_load_balancing_loss: 0.0038 - moe_nlg_load_balancing_loss: 0.0021\n",
      "\n",
      "Starting Iteration 2\n",
      "\n",
      "Evaluating model on test set...\n",
      "1397/1397 [==============================] - 18s 11ms/step - loss: 3.3438 - domain_output_loss: 0.0031 - intent_output_loss: 0.0301 - response_embeddings_loss: 0.8963 - response_output_loss: 2.4007 - domain_output_domain_accuracy: 1.0000 - intent_output_intent_accuracy: 0.7047 - intent_output_intent_precision: 0.9928 - intent_output_intent_recall: 0.6208 - intent_output_intent_f1: 0.7639 - response_embeddings_response_embedding_cosine_similarity: 0.1037 - response_output_perplexity: 30.0363 - moe_nlu_load_balancing_loss: 0.0036 - moe_nlg_load_balancing_loss: 0.0021\n",
      "\n",
      "Training results saved\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Iteration 2</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evaluation Time (seconds)</th>\n",
       "      <td>21.247510</td>\n",
       "      <td>18.251247</td>\n",
       "      <td>19.749379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction Speed (ms/token)</th>\n",
       "      <td>2.205226</td>\n",
       "      <td>2.275832</td>\n",
       "      <td>2.240529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>22.516535</td>\n",
       "      <td>24.888189</td>\n",
       "      <td>23.702362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>1.853973</td>\n",
       "      <td>1.863278</td>\n",
       "      <td>1.858626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>5.649051</td>\n",
       "      <td>10.894524</td>\n",
       "      <td>8.271787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.504456</td>\n",
       "      <td>14.504456</td>\n",
       "      <td>14.504456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPs Estimate (GFLOPs)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>2.377871</td>\n",
       "      <td>2.377871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 3 (percent)</th>\n",
       "      <td>23.443092</td>\n",
       "      <td>6.335004</td>\n",
       "      <td>14.889048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 4 (percent)</th>\n",
       "      <td>2.934860</td>\n",
       "      <td>6.102362</td>\n",
       "      <td>4.518611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 5 (percent)</th>\n",
       "      <td>19.076593</td>\n",
       "      <td>16.195419</td>\n",
       "      <td>17.636006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 6 (percent)</th>\n",
       "      <td>46.993558</td>\n",
       "      <td>45.848246</td>\n",
       "      <td>46.420902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 7 (percent)</th>\n",
       "      <td>4.366500</td>\n",
       "      <td>5.100215</td>\n",
       "      <td>4.733357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 8 (percent)</th>\n",
       "      <td>3.185397</td>\n",
       "      <td>20.418754</td>\n",
       "      <td>11.802076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 3 (percent)</th>\n",
       "      <td>15.000962</td>\n",
       "      <td>15.569878</td>\n",
       "      <td>15.285420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 4 (percent)</th>\n",
       "      <td>19.483648</td>\n",
       "      <td>19.735521</td>\n",
       "      <td>19.609585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 5 (percent)</th>\n",
       "      <td>17.798801</td>\n",
       "      <td>18.035449</td>\n",
       "      <td>17.917125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 6 (percent)</th>\n",
       "      <td>10.463253</td>\n",
       "      <td>10.574098</td>\n",
       "      <td>10.518675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 7 (percent)</th>\n",
       "      <td>20.129756</td>\n",
       "      <td>20.499685</td>\n",
       "      <td>20.314720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 8 (percent)</th>\n",
       "      <td>17.123580</td>\n",
       "      <td>15.585370</td>\n",
       "      <td>16.354475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Micro)</th>\n",
       "      <td>0.780062</td>\n",
       "      <td>0.763908</td>\n",
       "      <td>0.771985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Macro)</th>\n",
       "      <td>0.525925</td>\n",
       "      <td>0.503717</td>\n",
       "      <td>0.514821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Weighted)</th>\n",
       "      <td>0.698073</td>\n",
       "      <td>0.674708</td>\n",
       "      <td>0.686390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent Accuracy</th>\n",
       "      <td>0.707230</td>\n",
       "      <td>0.704724</td>\n",
       "      <td>0.705977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain F1-score (Macro)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU Score</th>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.001616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1 F1</th>\n",
       "      <td>0.155565</td>\n",
       "      <td>0.155927</td>\n",
       "      <td>0.155746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-2 F1</th>\n",
       "      <td>0.013361</td>\n",
       "      <td>0.013287</td>\n",
       "      <td>0.013324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-L F1</th>\n",
       "      <td>0.128817</td>\n",
       "      <td>0.128984</td>\n",
       "      <td>0.128901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METEOR Score</th>\n",
       "      <td>0.117034</td>\n",
       "      <td>0.118081</td>\n",
       "      <td>0.117558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>30.010618</td>\n",
       "      <td>30.036303</td>\n",
       "      <td>30.023460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert Load Stability (Variance)</th>\n",
       "      <td>238.714488</td>\n",
       "      <td>204.332761</td>\n",
       "      <td>221.523625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert Load Stability (Variance)</th>\n",
       "      <td>59.917807</td>\n",
       "      <td>60.267708</td>\n",
       "      <td>60.092758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Iteration 1  Iteration 2     Average\n",
       "Evaluation Time (seconds)               21.247510    18.251247   19.749379\n",
       "Prediction Speed (ms/token)              2.205226     2.275832    2.240529\n",
       "Average CPU Usage (percent)             22.516535    24.888189   23.702362\n",
       "Average GPU Usage (percent)              1.853973     1.863278    1.858626\n",
       "Average Memory (GB)                      5.649051    10.894524    8.271787\n",
       "Average GPU Memory (GB)                 14.504456    14.504456   14.504456\n",
       "Average FLOPs Estimate (GFLOPs)          2.377871     2.377871    2.377871\n",
       "NLU Expert 1 (percent)                   0.000000     0.000000    0.000000\n",
       "NLU Expert 2 (percent)                   0.000000     0.000000    0.000000\n",
       "NLU Expert 3 (percent)                  23.443092     6.335004   14.889048\n",
       "NLU Expert 4 (percent)                   2.934860     6.102362    4.518611\n",
       "NLU Expert 5 (percent)                  19.076593    16.195419   17.636006\n",
       "NLU Expert 6 (percent)                  46.993558    45.848246   46.420902\n",
       "NLU Expert 7 (percent)                   4.366500     5.100215    4.733357\n",
       "NLU Expert 8 (percent)                   3.185397    20.418754   11.802076\n",
       "NLG Expert 1 (percent)                   0.000000     0.000000    0.000000\n",
       "NLG Expert 2 (percent)                   0.000000     0.000000    0.000000\n",
       "NLG Expert 3 (percent)                  15.000962    15.569878   15.285420\n",
       "NLG Expert 4 (percent)                  19.483648    19.735521   19.609585\n",
       "NLG Expert 5 (percent)                  17.798801    18.035449   17.917125\n",
       "NLG Expert 6 (percent)                  10.463253    10.574098   10.518675\n",
       "NLG Expert 7 (percent)                  20.129756    20.499685   20.314720\n",
       "NLG Expert 8 (percent)                  17.123580    15.585370   16.354475\n",
       "Intent F1-score (Micro)                  0.780062     0.763908    0.771985\n",
       "Intent F1-score (Macro)                  0.525925     0.503717    0.514821\n",
       "Intent F1-score (Weighted)               0.698073     0.674708    0.686390\n",
       "Intent Accuracy                          0.707230     0.704724    0.705977\n",
       "Domain Accuracy                          1.000000     1.000000    1.000000\n",
       "Domain F1-score (Macro)                  1.000000     1.000000    1.000000\n",
       "BLEU Score                               0.001581     0.001651    0.001616\n",
       "ROUGE-1 F1                               0.155565     0.155927    0.155746\n",
       "ROUGE-2 F1                               0.013361     0.013287    0.013324\n",
       "ROUGE-L F1                               0.128817     0.128984    0.128901\n",
       "METEOR Score                             0.117034     0.118081    0.117558\n",
       "Perplexity                              30.010618    30.036303   30.023460\n",
       "NLU Expert Load Stability (Variance)   238.714488   204.332761  221.523625\n",
       "NLG Expert Load Stability (Variance)    59.917807    60.267708   60.092758"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tf_datasets_from_disk(load_path):\n",
    "    print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "    try:\n",
    "        with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "            loaded_moe_params = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "    element_spec = (\n",
    "        {\n",
    "            'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "        },\n",
    "        {\n",
    "            'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "            'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "        test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "    raw_data = {}\n",
    "    for dataset_name in ['train', 'test']:\n",
    "        path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                raw_data[dataset_name] = pickle.load(f)\n",
    "            print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Raw data file {path} not found.\")\n",
    "            raw_data[dataset_name] = []\n",
    "\n",
    "    return {\n",
    "        \"train_dataset\": train_tf_dataset,\n",
    "        \"test_dataset\": test_tf_dataset,\n",
    "        \"moe_params\": loaded_moe_params,\n",
    "        \"raw_data\": raw_data\n",
    "    }\n",
    "\n",
    "tf_dataset_save_path = \"tf_datasets\"\n",
    "loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "test_tf_dataset = loaded_data[\"test_dataset\"]\n",
    "moe_params = loaded_data[\"moe_params\"]\n",
    "raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "moe_params[\"num_experts\"] = 4\n",
    "\n",
    "class TopPAdaptiveRouter:\n",
    "    def __init__(self, p=0.9, tau=0.1):\n",
    "        self.p = p\n",
    "        self.tau = tau\n",
    "\n",
    "    def __call__(self, gate_logits):\n",
    "        probs = tf.nn.softmax(gate_logits, axis=-1)\n",
    "        sorted_probs, sorted_indices = tf.math.top_k(probs, k=tf.shape(probs)[-1])\n",
    "        top1_prob = sorted_probs[:, 0]\n",
    "        top2_prob = sorted_probs[:, 1]\n",
    "        prob_diff = top1_prob - top2_prob\n",
    "        use_top1 = prob_diff >= self.tau\n",
    "        max_k = tf.where(use_top1, 1, 2)\n",
    "        cum_probs = tf.cumsum(sorted_probs, axis=-1)\n",
    "        p_mask = cum_probs >= self.p\n",
    "        first_over_p = tf.argmax(tf.cast(p_mask, tf.int32), axis=-1, output_type=tf.int32)\n",
    "        first_over_p = tf.maximum(first_over_p, 0)\n",
    "        num_selected_top_p = first_over_p + 1\n",
    "        num_selected = tf.minimum(num_selected_top_p, max_k)\n",
    "        max_selected = tf.reduce_max(num_selected)\n",
    "        batch_size = tf.shape(gate_logits)[0]\n",
    "        num_experts = tf.shape(gate_logits)[1]\n",
    "        selection_mask = tf.sequence_mask(num_selected, maxlen=num_experts, dtype=tf.bool)\n",
    "        selected_indices = tf.where(selection_mask, sorted_indices, num_experts)\n",
    "        selected_probs = tf.where(selection_mask, sorted_probs, 0.0)\n",
    "        selected_indices = selected_indices[:, :max_selected]\n",
    "        selected_probs = selected_probs[:, :max_selected]\n",
    "        weights_sum = tf.reduce_sum(selected_probs, axis=-1, keepdims=True) + tf.keras.backend.epsilon()\n",
    "        weights = selected_probs / weights_sum\n",
    "        valid_mask = tf.sequence_mask(num_selected, maxlen=max_selected, dtype=tf.int32)\n",
    "        selected_indices = tf.where(valid_mask == 1, selected_indices, -1)\n",
    "        return selected_indices, weights, num_selected\n",
    "\n",
    "class DeepSeekMoELayer(layers.Layer):\n",
    "    def __init__(self, num_experts, expert_dim, input_dim, m=4, k_s=2, alpha=0.01, top_p=0.9, tau=0.1, name=None):\n",
    "        super(DeepSeekMoELayer, self).__init__(name=name)\n",
    "        self.m = m\n",
    "        self.k_s = k_s\n",
    "        self.total_experts = num_experts * self.m\n",
    "        self.routed_experts = self.total_experts - self.k_s\n",
    "        self.alpha = alpha\n",
    "        self.top_p = top_p\n",
    "        self.tau = tau\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = None\n",
    "        self.adjusted_expert_dim = expert_dim // self.m\n",
    "        self.shared_experts = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4), name=f'shared_expert_{i}_dense1'),\n",
    "                layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4), name=f'shared_expert_{i}_dense2')\n",
    "            ], name=f'shared_expert_{i}') for i in range(self.k_s)\n",
    "        ]\n",
    "        self.routed_experts_list = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4), name=f'routed_expert_{i}_dense1'),\n",
    "                layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4), name=f'routed_expert_{i}_dense2')\n",
    "            ], name=f'routed_expert_{i}') for i in range(self.k_s, self.total_experts)\n",
    "        ]\n",
    "        self.experts = self.shared_experts + self.routed_experts_list\n",
    "        self.gate = layers.Dense(self.routed_experts, activation=None, name='gate_weights')\n",
    "        self.router = TopPAdaptiveRouter(p=self.top_p, tau=self.tau)\n",
    "        self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_rank = inputs.shape.rank\n",
    "        if input_rank == 3:\n",
    "            batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "            flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "            is_3d = True\n",
    "        else:\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            seq_length = 1\n",
    "            flat_inputs = inputs\n",
    "            is_3d = False\n",
    "        num_tokens = tf.shape(flat_inputs)[0]\n",
    "        flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "        shared_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "        for i in range(self.k_s):\n",
    "            shared_output += self.shared_experts[i](flat_inputs)\n",
    "        gate_logits = self.gate(flat_inputs)\n",
    "        expert_indices, expert_weights, num_selected = self.router(gate_logits)\n",
    "        expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "        expert_indices_adjusted = expert_indices + self.k_s\n",
    "        batch_indices = tf.tile(tf.expand_dims(tf.range(num_tokens), 1), [1, tf.shape(expert_indices)[1]])\n",
    "        gather_indices = tf.stack([batch_indices, expert_indices_adjusted], axis=-1)\n",
    "        valid_mask = tf.cast(expert_indices_adjusted >= 0, tf.float32)\n",
    "        valid_mask = tf.expand_dims(valid_mask, -1)\n",
    "        selected_outputs = tf.gather_nd(expert_outputs, gather_indices)\n",
    "        weighted_outputs = selected_outputs * tf.expand_dims(expert_weights, -1) * valid_mask\n",
    "        routed_output = tf.reduce_sum(weighted_outputs, axis=1)\n",
    "        output_flat = shared_output + routed_output + flat_inputs\n",
    "        if is_3d:\n",
    "            output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "            if self.seq_length is not None:\n",
    "                output.set_shape([None, self.seq_length, self.input_dim])\n",
    "            gate_weights_reshaped = tf.reshape(tf.nn.softmax(gate_logits, axis=-1), [batch_size, seq_length, self.routed_experts])\n",
    "            if self.seq_length is not None:\n",
    "                gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "        else:\n",
    "            output = output_flat\n",
    "            gate_weights_reshaped = tf.nn.softmax(gate_logits, axis=-1)\n",
    "        gate_probs = tf.nn.softmax(gate_logits, axis=-1)\n",
    "        expert_selection = tf.reduce_sum(tf.one_hot(expert_indices_adjusted, depth=self.total_experts), axis=1)\n",
    "        expert_selection = expert_selection[:, self.k_s:]\n",
    "        f_i = tf.reduce_mean(expert_selection, axis=0)\n",
    "        P_i = tf.reduce_mean(gate_probs, axis=0)\n",
    "        load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "        self.add_loss(load_balancing_loss)\n",
    "        self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "        return output, gate_weights_reshaped\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_experts': self.total_experts // self.m,\n",
    "            'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "            'input_dim': self.input_dim,\n",
    "            'm': self.m,\n",
    "            'k_s': self.k_s,\n",
    "            'alpha': self.alpha,\n",
    "            'top_p': self.top_p,\n",
    "            'tau': self.tau,\n",
    "            'name': self.name\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "        if look_ahead_mask is not None:\n",
    "            look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "            look_ahead_mask = 1.0 - look_ahead_mask\n",
    "        attn1_output = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)\n",
    "        attn1 = self.dropout1(attn1_output, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        attn2_output = self.mha2(query=out1, value=enc_output, key=enc_output, training=training)\n",
    "        attn2 = self.dropout2(attn2_output, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        return out3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        mha1_config = self.mha1.get_config()\n",
    "        config.update({\n",
    "            'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "            'num_heads': mha1_config['num_heads'],\n",
    "            'dff': self.ffn.layers[0].units,\n",
    "            'rate': self.dropout1.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MoEModel(models.Model):\n",
    "    def __init__(self, moe_params):\n",
    "        super(MoEModel, self).__init__()\n",
    "        self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "        self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "        self.num_domains = moe_params[\"num_domains\"]\n",
    "        self.num_intents = moe_params[\"num_intents\"]\n",
    "        self.vocab_size = moe_params[\"vocab_size\"]\n",
    "        self.num_experts = moe_params[\"num_experts\"]\n",
    "        self.expert_dim = moe_params[\"expert_dim\"]\n",
    "        self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "        self.num_heads = 4\n",
    "        self.dff = self.embedding_dim * 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                             self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "        self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "\n",
    "        self.embedding = layers.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "            name=\"embedding\"\n",
    "        )\n",
    "        self.embedding.build((None,))\n",
    "        with tf.init_scope():\n",
    "            embedding_weights = self.embedding.get_weights()\n",
    "            if embedding_weights and len(embedding_weights) > 0:\n",
    "                embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                self.embedding.set_weights(embedding_weights)\n",
    "\n",
    "        self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "        self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "        self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "        self.moe_nlu = DeepSeekMoELayer(\n",
    "            num_experts=self.num_experts,\n",
    "            expert_dim=self.expert_dim,\n",
    "            input_dim=self.nlu_input_dim,\n",
    "            m=2,\n",
    "            k_s=2,\n",
    "            alpha=0.01,\n",
    "            top_p=0.9,\n",
    "            tau=0.1,\n",
    "            name='moe_nlu'\n",
    "        )\n",
    "        self.moe_nlg = DeepSeekMoELayer(\n",
    "            num_experts=self.num_experts,\n",
    "            expert_dim=self.expert_dim,\n",
    "            input_dim=self.nlg_input_dim,\n",
    "            m=2,\n",
    "            k_s=2,\n",
    "            alpha=0.01,\n",
    "            top_p=0.9,\n",
    "            tau=0.1,\n",
    "            name='moe_nlg'\n",
    "        )\n",
    "        self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "        self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "        self.response_output = layers.TimeDistributed(\n",
    "            layers.Dense(self.vocab_size, activation='softmax'), name='response_output'\n",
    "        )\n",
    "        self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "        self.nlg_projection = layers.TimeDistributed(\n",
    "            layers.Dense(self.embedding_dim, activation='relu', name='nlg_projection')\n",
    "        )\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "        prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "        decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "        domain_onehot_input = inputs['domain_onehot_input']\n",
    "        turn_id_embedding = inputs['turn_id_embedding']\n",
    "        ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "\n",
    "        pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "        user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "        prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "        decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "\n",
    "        user_enc_out = user_embedded\n",
    "        prev_system_enc_out = prev_system_embedded\n",
    "\n",
    "        look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "        dec_output = decoder_embedded\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            dec_output = layer(dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask)\n",
    "        decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "\n",
    "        user_attn_out = self.attn_layer(\n",
    "            query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), \n",
    "            value=user_enc_out, key=user_enc_out, training=training\n",
    "        )\n",
    "        prev_system_attn_out = self.attn_layer(\n",
    "            query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), \n",
    "            value=prev_system_enc_out, key=prev_system_enc_out, training=training\n",
    "        )\n",
    "        decoder_attn_out = self.attn_layer(\n",
    "            query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), \n",
    "            value=decoder_output, key=decoder_output, training=training\n",
    "        )\n",
    "\n",
    "        combined_features = layers.Concatenate()([\n",
    "            tf.squeeze(user_attn_out, axis=1),\n",
    "            tf.squeeze(prev_system_attn_out, axis=1),\n",
    "            tf.squeeze(decoder_attn_out, axis=1),\n",
    "            domain_onehot_input,\n",
    "            turn_id_embedding,\n",
    "            ontology_multihot_input\n",
    "        ])\n",
    "        combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "        nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "        nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "        intent_out = self.intent_output(nlu_out)\n",
    "        domain_out = self.domain_output(nlu_out)\n",
    "\n",
    "        intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "        domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "        intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "        domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "\n",
    "        nlg_input = layers.Concatenate(axis=-1)([\n",
    "            decoder_output,\n",
    "            intent_out_tiled,\n",
    "            domain_out_tiled\n",
    "        ])\n",
    "        nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "\n",
    "        nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "        nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        response_embeddings = self.nlg_projection(nlg_out)\n",
    "        response_embeddings = tf.ensure_shape(response_embeddings, [None, self.max_seq_length, self.embedding_dim])\n",
    "        response_out = self.response_output(nlg_out)\n",
    "\n",
    "        return {\n",
    "            'intent_output': intent_out,\n",
    "            'domain_output': domain_out,\n",
    "            'response_output': response_out,\n",
    "            'response_embeddings': response_embeddings,\n",
    "            'nlu_gate_weights': nlu_gate_weights,\n",
    "            'nlg_gate_weights': nlg_gate_weights\n",
    "        }\n",
    "\n",
    "    def build_graph(self):\n",
    "        inputs = {\n",
    "            'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "            'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='prev_system_response_tokens'),\n",
    "            'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='decoder_input_tokens'),\n",
    "            'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32, name='domain_onehot_input'),\n",
    "            'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32, name='turn_id_embedding'),\n",
    "            'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32, name='ontology_multihot_input'),\n",
    "        }\n",
    "        return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'moe_params': {\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'max_seq_length': self.max_seq_length,\n",
    "                'num_domains': self.num_domains,\n",
    "                'num_intents': self.num_intents,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'num_experts': self.num_experts,\n",
    "                'expert_dim': self.expert_dim,\n",
    "                'turn_id_dim': self.turn_id_dim\n",
    "            }\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class IntentAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros', dtype=tf.float32)\n",
    "        self.total_preds = self.add_weight(name='total_preds', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "        self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "        self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.correct_preds.assign(0.)\n",
    "        self.total_preds.assign(0.)\n",
    "\n",
    "class IntentPrecision(metrics.Metric):\n",
    "    def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "        self.predicted_positives = self.add_weight(name='pp', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        predicted_positives = tf.reduce_sum(y_pred)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.predicted_positives.assign(0.)\n",
    "\n",
    "class IntentRecall(metrics.Metric):\n",
    "    def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "        self.actual_positives = self.add_weight(name='ap', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        actual_positives = tf.reduce_sum(y_true)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.actual_positives.assign(0.)\n",
    "\n",
    "class IntentF1(metrics.Metric):\n",
    "    def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = IntentPrecision(threshold=threshold)\n",
    "        self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()\n",
    "\n",
    "class DomainAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='domain_accuracy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.accuracy = self.add_weight(name='acc', initializer='zeros', dtype=tf.float32)\n",
    "        self.count = self.add_weight(name='count', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "        pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "        matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "        self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.accuracy.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "class Perplexity(metrics.Metric):\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "        self.total_loss = self.add_weight(name='total_loss', initializer='zeros', dtype=tf.float32)\n",
    "        self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "        loss = self.cross_entropy(y_true, y_pred)\n",
    "        masked_loss = loss * mask\n",
    "        sum_loss = tf.reduce_sum(masked_loss)\n",
    "        num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "        self.total_loss.assign_add(sum_loss)\n",
    "        self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "    def result(self):\n",
    "        avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "        return tf.exp(avg_loss)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_loss.assign(0.)\n",
    "        self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "    def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "        y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "        cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "        non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "        non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "        masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "        num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "        avg_cosine_sim_per_sample = tf.where(\n",
    "            num_non_zero_tokens > 0,\n",
    "            tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "            tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "        )\n",
    "        self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_cosine_sim.assign(0.)\n",
    "        self.num_samples.assign(0.)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "    y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "    y_true = y_true / (y_true_norm + epsilon)\n",
    "    y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "    cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    positive_loss = 1.0 - cosine_sim\n",
    "    mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "    masked_loss = positive_loss * mask\n",
    "    total_loss = tf.reduce_sum(masked_loss)\n",
    "    num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "    return total_loss / num_tokens\n",
    "\n",
    "def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "    flops = 0\n",
    "    functional_model = model.build_graph()\n",
    "\n",
    "    def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "        layer_flops_count = 0\n",
    "\n",
    "        if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "            return 0\n",
    "\n",
    "        if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "            for sub_layer in layer_instance.layers:\n",
    "                layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "            return layer_flops_count\n",
    "\n",
    "        if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "            return 0\n",
    "\n",
    "        if isinstance(layer_instance, layers.Dense):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            output_dim = layer_instance.output_shape[-1]\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "        elif isinstance(layer_instance, layers.LSTM):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            hidden_dim = layer_instance.units\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "            layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "        elif isinstance(layer_instance, layers.Embedding):\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer_instance, DeepSeekMoELayer):\n",
    "            moe_input_dim = layer_instance.input_dim\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "            layer_flops_count += gate_flops\n",
    "            for expert in layer_instance.experts:\n",
    "                layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "        elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "            config = layer_instance.get_config()\n",
    "            num_heads = config['num_heads']\n",
    "            key_dim = config['key_dim']\n",
    "            d_model = num_heads * key_dim\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "            projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "            attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "            context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "            output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "            layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "        elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "            inner_layer = layer_instance.layer\n",
    "            td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "            layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "        elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "            ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "        return layer_flops_count\n",
    "\n",
    "    for layer in functional_model.layers:\n",
    "        flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "    return flops / 1e9\n",
    "\n",
    "def get_gpu_stats():\n",
    "    if nvidia_smi is None:\n",
    "        return 0, 0\n",
    "    try:\n",
    "        nvidia_smi.nvmlInit()\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "        mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_memory = mem_info.used / (1024 ** 3)\n",
    "        nvidia_smi.nvmlShutdown()\n",
    "        return gpu_load, gpu_memory\n",
    "    except Exception:\n",
    "        return 0, 0\n",
    "\n",
    "class ResourceMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.resource_stats = []\n",
    "        self.expert_load_history = []\n",
    "        self.start_time = None\n",
    "        self.epoch_start_time = None\n",
    "        self.flops = None\n",
    "        self.validation_data = validation_data\n",
    "        self.nlu_expert_usage_counts = None\n",
    "        self.nlg_expert_usage_counts = None\n",
    "        self.nlu_gate_weights_history = []\n",
    "        self.nlg_gate_weights_history = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        try:\n",
    "            self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "            self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "            self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "        except Exception:\n",
    "            self.flops = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory()\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.resource_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'epoch_time': epoch_time,\n",
    "            'cpu_usage': cpu_usage,\n",
    "            'memory_used': memory.used / (1024 ** 3),\n",
    "            'memory_total': memory.total / (1024 ** 3),\n",
    "            'gpu_load': gpu_load,\n",
    "            'gpu_memory': gpu_memory,\n",
    "            'loss': logs.get('loss', 0),\n",
    "            'val_loss': logs.get('val_loss', 0),\n",
    "            'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "            'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "            'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "            'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "            'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "            'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "            'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "            'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "            'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "            'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "            'perplexity': logs.get('response_output_perplexity', 0),\n",
    "            'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "            'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "            'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "        })\n",
    "        if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "            self.expert_load_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "            })\n",
    "\n",
    "        sample_size = 100\n",
    "        for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "            inputs, _ = batch\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "            reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "            self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "            nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "            reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "            self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.total_time = time.time() - self.start_time\n",
    "\n",
    "        if len(self.nlu_gate_weights_history) > 0:\n",
    "            all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "            nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "            for expert_id in nlu_top_indices:\n",
    "                if expert_id < self.model.moe_nlu.total_experts:\n",
    "                    self.nlu_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        if len(self.nlg_gate_weights_history) > 0:\n",
    "            all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "            nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "            for expert_id in nlg_top_indices:\n",
    "                if expert_id < self.model.moe_nlg.total_experts:\n",
    "                    self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "    def visualize_expert_load_distribution(self):\n",
    "        total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "        total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "        nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "        nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "        nlu_stability = np.var(nlu_percentages)\n",
    "        nlg_stability = np.var(nlg_percentages)\n",
    "        return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "try:\n",
    "    possible_paths = [\n",
    "        os.path.join(\"preprocessor_models\", \"preprocessor_params.json\"),\n",
    "        os.path.join(\"tf_datasets\", \"preprocessor_params.json\"),\n",
    "        \"preprocessor_params.json\"\n",
    "    ]\n",
    "    \n",
    "    vocab_loaded = False\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                preprocessor_params = json.load(f)\n",
    "            word_to_id = {k: int(v) for k, v in preprocessor_params['word_to_id'].items()}\n",
    "            id_to_word = {int(k): v for k, v in preprocessor_params['id_to_word'].items()}\n",
    "            print(f\"Loaded vocabulary from {path}\")\n",
    "            vocab_loaded = True\n",
    "            break\n",
    "    \n",
    "    if not vocab_loaded:\n",
    "        raise FileNotFoundError(\"Vocabulary file not found in any location\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load vocabulary from preprocessor: {str(e)}\")\n",
    "    word_to_id = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    id_to_word = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
    "\n",
    "model_save_path = \"best_deepseekmoe_topp_adaptive_model\"\n",
    "print(f\"\\nLoading model from: {model_save_path}\")\n",
    "custom_objects = {\n",
    "    \"TopPAdaptiveRouter\":TopPAdaptiveRouter,\n",
    "    \"DeepSeekMoELayer\": DeepSeekMoELayer,\n",
    "    \"CustomLearningRateSchedule\": CustomLearningRateSchedule,\n",
    "    \"MoEModel\": MoEModel,\n",
    "    \"TransformerDecoderLayer\": TransformerDecoderLayer,\n",
    "    \"IntentAccuracy\": IntentAccuracy,\n",
    "    \"IntentPrecision\": IntentPrecision,\n",
    "    \"IntentRecall\": IntentRecall,\n",
    "    \"IntentF1\": IntentF1,\n",
    "    \"DomainAccuracy\": DomainAccuracy,\n",
    "    \"Perplexity\": Perplexity,\n",
    "    \"ResponseEmbeddingCosineSimilarity\": ResponseEmbeddingCosineSimilarity\n",
    "}\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model from {model_save_path}: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "def compile_model(model, moe_params):\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'domain_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss,\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={\n",
    "            'intent_output': 0.5,\n",
    "            'domain_output': 0.5,\n",
    "            'response_output': 1.0,\n",
    "            'response_embeddings': 1.0\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = compile_model(model, moe_params)\n",
    "print(\"\\nModel loaded and compiled successfully!\")\n",
    "sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "model(sample_input[0])\n",
    "model.summary()\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self, model, test_dataset, moe_params, raw_data, word_to_id, id_to_word):\n",
    "        self.model = model\n",
    "        self.test_dataset = test_dataset\n",
    "        self.moe_params = moe_params\n",
    "        self.raw_data = raw_data\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.nlu_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.nlg_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.resource_stats = {\n",
    "            'cpu_usage': [],\n",
    "            'memory_used': [],\n",
    "            'gpu_load': [],\n",
    "            'gpu_memory': [],\n",
    "            'batch_times': []\n",
    "        }\n",
    "        self.special_tokens = {'<pad>', '<sos>', '<eos>', '<unk>'}\n",
    "        self.nlu_top_k = model.get_layer('moe_nlu').get_config().get('top_k', 2)\n",
    "        self.nlg_top_k = model.get_layer('moe_nlg').get_config().get('top_k', 2)\n",
    "\n",
    "    def evaluate(self):\n",
    "        print(\"\\nEvaluating model on test set...\")\n",
    "        \n",
    "        def add_response_embeddings(features, targets):\n",
    "            response_tokens = targets['response_output']\n",
    "            response_embeddings = self.model.embedding(response_tokens)\n",
    "            targets['response_embeddings'] = response_embeddings\n",
    "            return features, targets\n",
    "        \n",
    "        test_dataset_with_embeddings = self.test_dataset.map(add_response_embeddings, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        start_time = time.time()\n",
    "        test_results = self.model.evaluate(test_dataset_with_embeddings, verbose=1)\n",
    "        eval_time = time.time() - start_time\n",
    "        \n",
    "        metric_names = self.model.metrics_names\n",
    "        \n",
    "        perplexity_value = None\n",
    "        for name, value in zip(metric_names, test_results):\n",
    "            if name == 'response_output_perplexity':\n",
    "                perplexity_value = value\n",
    "                break\n",
    "        if perplexity_value is None:\n",
    "            print(\"Warning: Could not find perplexity in standard evaluation results.\")\n",
    "            perplexity_value = 0.0\n",
    "        \n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory().used / (1024 ** 3)\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "\n",
    "        total_tokens = 0\n",
    "        total_time = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        test_dataset_small = test_dataset_with_embeddings.unbatch().batch(moe_params[\"batch_size\"])\n",
    "        \n",
    "        all_true_intents = []\n",
    "        all_pred_intents = []\n",
    "        all_true_domains = []\n",
    "        all_pred_domains = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(test_dataset_small):\n",
    "            inputs, targets = batch\n",
    "            response_tokens = targets['response_output'].numpy()\n",
    "            non_padding_mask = response_tokens != 0\n",
    "            total_tokens += np.sum(non_padding_mask)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            total_time += batch_time\n",
    "            num_batches += tf.shape(inputs['user_utterance_tokens'])[0]\n",
    "            \n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            \n",
    "            self.resource_stats['cpu_usage'].append(cpu_usage)\n",
    "            self.resource_stats['memory_used'].append(memory.used / (1024 ** 3))\n",
    "            self.resource_stats['gpu_load'].append(gpu_load)\n",
    "            self.resource_stats['gpu_memory'].append(gpu_memory)\n",
    "            self.resource_stats['batch_times'].append(batch_time)\n",
    "            \n",
    "            nlu_gate_weights = outputs.get('nlu_gate_weights', tf.zeros([0, self.model.moe_nlu.routed_experts])).numpy()\n",
    "            nlu_top_indices = np.argsort(-nlu_gate_weights, axis=-1)[:, :self.nlu_top_k] + self.model.moe_nlu.k_s\n",
    "            for idx in range(self.nlu_top_k):\n",
    "                expert_ids = nlu_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlu_expert_usage_counts):\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "            \n",
    "            nlg_gate_weights = outputs.get('nlg_gate_weights', tf.zeros([0, self.model.moe_nlg.routed_experts])).numpy()\n",
    "            if len(nlg_gate_weights.shape) == 3:\n",
    "                nlg_gate_weights = nlg_gate_weights.reshape(-1, nlg_gate_weights.shape[-1])\n",
    "            nlg_top_indices = np.argsort(-nlg_gate_weights, axis=-1)[:, :self.nlg_top_k] + self.model.moe_nlg.k_s\n",
    "            for idx in range(self.nlg_top_k):\n",
    "                expert_ids = nlg_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlg_expert_usage_counts):\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "            \n",
    "            true_intents = targets.get('intent_output', np.zeros((moe_params[\"batch_size\"], self.model.num_intents))).numpy()\n",
    "            pred_intents = (outputs.get('intent_output', np.zeros_like(true_intents)).numpy() > 0.5).astype(np.int32)\n",
    "            all_true_intents.append(true_intents)\n",
    "            all_pred_intents.append(pred_intents)\n",
    "            \n",
    "            true_domains = targets.get('domain_output', np.zeros((moe_params[\"batch_size\"], 1))).numpy().flatten()\n",
    "            pred_domains = np.argmax(outputs.get('domain_output', np.zeros((moe_params[\"batch_size\"], self.model.num_domains))).numpy(), axis=-1)\n",
    "            all_true_domains.append(true_domains)\n",
    "            all_pred_domains.append(pred_domains)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                gc.collect()\n",
    "                tf.keras.backend.clear_session()\n",
    "        \n",
    "        avg_time_per_token = (total_time / total_tokens) * 1000 if total_tokens > 0 else 0\n",
    "        \n",
    "        avg_cpu = np.mean(self.resource_stats['cpu_usage']) if self.resource_stats['cpu_usage'] else 0\n",
    "        avg_memory = np.mean(self.resource_stats['memory_used']) if self.resource_stats['memory_used'] else 0\n",
    "        avg_gpu_load = np.mean(self.resource_stats['gpu_load']) if self.resource_stats['gpu_load'] else 0\n",
    "        avg_gpu_memory = np.mean(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        peak_gpu_memory = np.max(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        \n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "        \n",
    "        all_true_intents = np.vstack(all_true_intents)\n",
    "        all_pred_intents = np.vstack(all_pred_intents)\n",
    "        all_true_domains = np.concatenate(all_true_domains)\n",
    "        all_pred_domains = np.concatenate(all_pred_domains)\n",
    "        \n",
    "        intent_f1_score = f1_score(all_true_intents, all_pred_intents, average='micro')\n",
    "        intent_f1_score_macro = f1_score(all_true_intents, all_pred_intents, average='macro')\n",
    "        intent_f1_score_weighted = f1_score(all_true_intents, all_pred_intents, average='weighted')\n",
    "        intent_accuracy_score = accuracy_score(all_true_intents, all_pred_intents)\n",
    "        \n",
    "        domain_accuracy_score = accuracy_score(all_true_domains, all_pred_domains)\n",
    "        domain_f1_score_macro = f1_score(all_true_domains, all_pred_domains, average='macro')\n",
    "\n",
    "        hypotheses = []\n",
    "        references = []\n",
    "        meteor_scores = []\n",
    "        rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "        domain_metrics = defaultdict(lambda: {\n",
    "            'bleu': [],\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'meteor': [],\n",
    "            'count': 0\n",
    "        })\n",
    "        \n",
    "        if not self.raw_data.get('test'):\n",
    "            print(\"Warning: raw_data['test'] is empty. Skipping NLG metrics calculation.\")\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "            domain_metrics = {}\n",
    "        else:\n",
    "            for i, batch in enumerate(test_dataset_small):\n",
    "                if i >= len(self.raw_data.get('test', [])):\n",
    "                    print(f\"Warning: raw_data['test'] has fewer items ({len(self.raw_data.get('test', []))}) than test dataset. Stopping NLG metrics calculation.\")\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    inputs, targets = batch\n",
    "                    with tf.device('/CPU:0'):\n",
    "                        outputs = self.model(inputs, training=False)\n",
    "                    \n",
    "                    domain_id = targets['domain_output'].numpy()[0][0]\n",
    "                    domain_name = f\"domain_{domain_id}\"\n",
    "                    \n",
    "                    response_probs = outputs['response_output'].numpy()\n",
    "                    generated_tokens = np.argmax(response_probs, axis=-1)[0]\n",
    "                    true_tokens = targets['response_output'].numpy()[0]\n",
    "                    \n",
    "                    raw_item = self.raw_data['test'][i]\n",
    "                    ref_text = raw_item.get('raw_next_system_response', '')\n",
    "                    if not ref_text:\n",
    "                        continue\n",
    "                    \n",
    "                    gen_text = self.tokens_to_text(generated_tokens)\n",
    "                    ref_tokens = self.tokens_to_text(true_tokens)\n",
    "                    \n",
    "                    if not gen_text.strip() or not ref_text.strip():\n",
    "                        continue\n",
    "                    \n",
    "                    hypotheses.append(gen_text)\n",
    "                    references.append([ref_text])\n",
    "                    \n",
    "                    ref_tokens = word_tokenize(ref_text.lower())\n",
    "                    gen_tokens = word_tokenize(gen_text.lower())\n",
    "                    \n",
    "                    try:\n",
    "                        meteor = meteor_score([ref_tokens], gen_tokens)\n",
    "                        meteor_scores.append(meteor)\n",
    "                        domain_metrics[domain_name]['meteor'].append(meteor)\n",
    "                    except Exception as e:\n",
    "                        print(f\"METEOR calculation error for example {i}: {str(e)}\")\n",
    "                        meteor_scores.append(0)\n",
    "                        domain_metrics[domain_name]['meteor'].append(0)\n",
    "                    \n",
    "                    try:\n",
    "                        rouge_scores = rouge_scorer_instance.score(ref_text, gen_text)\n",
    "                        domain_metrics[domain_name]['rouge1'].append(rouge_scores['rouge1'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rouge2'].append(rouge_scores['rouge2'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rougeL'].append(rouge_scores['rougeL'].fmeasure)\n",
    "                    except Exception as e:\n",
    "                        print(f\"ROUGE calculation error for example {i}: {str(e)}\")\n",
    "                    \n",
    "                    domain_metrics[domain_name]['count'] += 1\n",
    "                    \n",
    "                    if i % 50 == 0:\n",
    "                        gc.collect()\n",
    "                        tf.keras.backend.clear_session()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing example {i}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if hypotheses and references:\n",
    "            hypotheses_tok = []\n",
    "            references_tok = []\n",
    "            \n",
    "            for hyp, ref_list in zip(hypotheses, references):\n",
    "                if not hyp or not ref_list[0]:\n",
    "                    continue\n",
    "                hyp_tokens = word_tokenize(hyp.lower())\n",
    "                ref_tokens = [word_tokenize(ref.lower()) for ref in ref_list]\n",
    "                hypotheses_tok.append(hyp_tokens)\n",
    "                references_tok.append(ref_tokens)\n",
    "            \n",
    "            chencherry = SmoothingFunction()\n",
    "            try:\n",
    "                bleu_score = corpus_bleu(references_tok, hypotheses_tok, smoothing_function=chencherry.method1)\n",
    "            except Exception as e:\n",
    "                print(f\"BLEU calculation error: {str(e)}\")\n",
    "                bleu_score = 0.0\n",
    "            \n",
    "            rouge_scores = []\n",
    "            for hyp, ref in zip(hypotheses, references):\n",
    "                if hyp and ref[0]:\n",
    "                    try:\n",
    "                        rouge_scores.append(rouge_scorer_instance.score(ref[0], hyp))\n",
    "                    except Exception as e:\n",
    "                        print(f\"ROUGE scoring error: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            avg_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_meteor = np.mean(meteor_scores) if meteor_scores else 0.0\n",
    "        else:\n",
    "            print(\"Warning: No valid hypotheses or references for NLG metrics. Setting to 0.0.\")\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "\n",
    "        return {\n",
    "            'intent_f1_micro': intent_f1_score,\n",
    "            'intent_f1_macro': intent_f1_score_macro,\n",
    "            'intent_f1_weighted': intent_f1_score_weighted,\n",
    "            'intent_accuracy': intent_accuracy_score,\n",
    "            'domain_accuracy': domain_accuracy_score,\n",
    "            'domain_f1_macro': domain_f1_score_macro,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge1': avg_rouge1,\n",
    "            'rouge2': avg_rouge2,\n",
    "            'rougeL': avg_rougeL,\n",
    "            'meteor': avg_meteor,\n",
    "            'perplexity': perplexity_value,\n",
    "            'pred_speed': avg_time_per_token,\n",
    "            'domain_metrics': domain_metrics,\n",
    "            'eval_time': eval_time\n",
    "        }\n",
    "    \n",
    "    def tokens_to_text(self, tokens):\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            if token == 0:\n",
    "                continue\n",
    "            word = self.id_to_word.get(token, '<unk>')\n",
    "            if word not in self.special_tokens:\n",
    "                words.append(word)\n",
    "        return \" \".join(words).strip()\n",
    "\n",
    "results_table = {\n",
    "    'Evaluation Time (seconds)': [],\n",
    "    'Prediction Speed (ms/token)': [],\n",
    "    'Average CPU Usage (percent)': [],\n",
    "    'Average GPU Usage (percent)': [],\n",
    "    'Average Memory (GB)': [],\n",
    "    'Average GPU Memory (GB)': [],\n",
    "    'Average FLOPs Estimate (GFLOPs)': [],\n",
    "    'NLU Expert 1 (percent)': [],\n",
    "    'NLU Expert 2 (percent)': [],\n",
    "    'NLU Expert 3 (percent)': [],\n",
    "    'NLU Expert 4 (percent)': [],\n",
    "    'NLU Expert 5 (percent)': [],\n",
    "    'NLU Expert 6 (percent)': [],\n",
    "    'NLU Expert 7 (percent)': [],\n",
    "    'NLU Expert 8 (percent)': [],\n",
    "    'NLG Expert 1 (percent)': [],\n",
    "    'NLG Expert 2 (percent)': [],\n",
    "    'NLG Expert 3 (percent)': [],\n",
    "    'NLG Expert 4 (percent)': [],\n",
    "    'NLG Expert 5 (percent)': [],\n",
    "    'NLG Expert 6 (percent)': [],\n",
    "    'NLG Expert 7 (percent)': [],\n",
    "    'NLG Expert 8 (percent)': [],\n",
    "    'Intent F1-score (Micro)': [],\n",
    "    'Intent F1-score (Macro)': [],\n",
    "    'Intent F1-score (Weighted)': [],\n",
    "    'Intent Accuracy': [],\n",
    "    'Domain Accuracy': [],\n",
    "    'Domain F1-score (Macro)': [],\n",
    "    'BLEU Score': [],\n",
    "    'ROUGE-1 F1': [],\n",
    "    'ROUGE-2 F1': [],\n",
    "    'ROUGE-L F1': [],\n",
    "    'METEOR Score': [],\n",
    "    'Perplexity': [],\n",
    "    'NLU Expert Load Stability (Variance)': [],\n",
    "    'NLG Expert Load Stability (Variance)': []\n",
    "}\n",
    "\n",
    "num_iterations = 5\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    print(f\"\\nStarting Iteration {iteration}\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "    model = compile_model(model, moe_params)\n",
    "    \n",
    "    evaluator = EvaluationMetrics(model, test_tf_dataset, moe_params, raw_data, word_to_id, id_to_word)\n",
    "\n",
    "    evaluation_results = evaluator.evaluate()\n",
    "    results_table['Evaluation Time (seconds)'].append(evaluation_results['eval_time'])\n",
    "    results_table['Prediction Speed (ms/token)'].append(evaluation_results['pred_speed'])\n",
    "    results_table['Average CPU Usage (percent)'].append(np.mean(evaluator.resource_stats['cpu_usage']) if evaluator.resource_stats['cpu_usage'] else 0)\n",
    "    results_table['Average GPU Usage (percent)'].append(np.mean(evaluator.resource_stats['gpu_load']) if evaluator.resource_stats['gpu_load'] else 0)\n",
    "    results_table['Average Memory (GB)'].append(np.mean(evaluator.resource_stats['memory_used']) if evaluator.resource_stats['memory_used'] else 0)\n",
    "    results_table['Average GPU Memory (GB)'].append(np.mean(evaluator.resource_stats['gpu_memory']) if evaluator.resource_stats['gpu_memory'] else 0)\n",
    "    results_table['Average FLOPs Estimate (GFLOPs)'].append(calculate_flops(model))\n",
    "    nlu_counts = evaluator.nlu_expert_usage_counts\n",
    "    nlg_counts = evaluator.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) if np.sum(nlu_counts) > 0 else 1\n",
    "    total_nlg = np.sum(nlg_counts) if np.sum(nlg_counts) > 0 else 1\n",
    "    results_table['NLU Expert 1 (percent)'].append((nlu_counts[0] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 2 (percent)'].append((nlu_counts[1] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 3 (percent)'].append((nlu_counts[2] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 4 (percent)'].append((nlu_counts[3] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 5 (percent)'].append((nlu_counts[4] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 6 (percent)'].append((nlu_counts[5] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 7 (percent)'].append((nlu_counts[6] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 8 (percent)'].append((nlu_counts[7] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLG Expert 1 (percent)'].append((nlg_counts[0] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 2 (percent)'].append((nlg_counts[1] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 3 (percent)'].append((nlg_counts[2] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 4 (percent)'].append((nlg_counts[3] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 5 (percent)'].append((nlg_counts[4] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 6 (percent)'].append((nlg_counts[5] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 7 (percent)'].append((nlg_counts[6] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 8 (percent)'].append((nlg_counts[7] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['Intent F1-score (Micro)'].append(evaluation_results['intent_f1_micro'])\n",
    "    results_table['Intent F1-score (Macro)'].append(evaluation_results['intent_f1_macro'])\n",
    "    results_table['Intent F1-score (Weighted)'].append(evaluation_results['intent_f1_weighted'])\n",
    "    results_table['Intent Accuracy'].append(evaluation_results['intent_accuracy'])\n",
    "    results_table['Domain Accuracy'].append(evaluation_results['domain_accuracy'])\n",
    "    results_table['Domain F1-score (Macro)'].append(evaluation_results['domain_f1_macro'])\n",
    "    results_table['BLEU Score'].append(evaluation_results['bleu'])\n",
    "    results_table['ROUGE-1 F1'].append(evaluation_results['rouge1'])\n",
    "    results_table['ROUGE-2 F1'].append(evaluation_results['rouge2'])\n",
    "    results_table['ROUGE-L F1'].append(evaluation_results['rougeL'])\n",
    "    results_table['METEOR Score'].append(evaluation_results['meteor'])\n",
    "    results_table['Perplexity'].append(evaluation_results['perplexity'])\n",
    "    results_table['NLU Expert Load Stability (Variance)'].append(np.var(nlu_counts / total_nlu * 100) if total_nlu > 0 else 0)\n",
    "    results_table['NLG Expert Load Stability (Variance)'].append(np.var(nlg_counts / total_nlg * 100) if total_nlg > 0 else 0)\n",
    "\n",
    "for key in results_table:\n",
    "    if results_table[key]:\n",
    "        results_table[key].append(np.mean(results_table[key]))\n",
    "    else:\n",
    "        results_table[key].append(0)\n",
    "\n",
    "print(\"\\nTraining results saved\")\n",
    "final_df = pd.DataFrame(results_table)\n",
    "final_df = final_df.T\n",
    "final_df.columns = [f'Iteration {i+1}' for i in range(num_iterations)] + ['Average']\n",
    "final_df.to_excel('prediction_deepseekmoe_topp_adaptive_results.xlsx', index=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE with Top-P with Expert Choice Balance experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:38:21.890266: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 08:38:22.327596: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 08:38:22.327691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 08:38:22.405516: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 08:38:22.559413: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 08:38:24.117326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training Iteration 1\n",
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:38:28.177368: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:38:28.459275: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:38:28.459561: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:38:28.460797: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:38:28.460988: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:38:28.461077: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:38:28.548117: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:38:28.548293: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:38:28.548400: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:38:28.548479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/4428 [00:00<?, ?it/s]2025-07-04 08:38:51.811388: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f23c029c4b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 08:38:51.811479: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2025-07-04 08:38:51.833026: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 08:38:51.880942: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751618331.975172   20995 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 1: 100%|██████████| 4428/4428 [03:01<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_DeepseekTopPExpertChoice_moe_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_DeepseekTopPExpertChoice_moe_model/assets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Iteration 2</th>\n",
       "      <th>Iteration 3</th>\n",
       "      <th>Iteration 4</th>\n",
       "      <th>Iteration 5</th>\n",
       "      <th>Iteration 6</th>\n",
       "      <th>Iteration 7</th>\n",
       "      <th>Iteration 8</th>\n",
       "      <th>Iteration 9</th>\n",
       "      <th>Iteration 10</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Speed (epochs per second)</th>\n",
       "      <td>0.005670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training Time (second/epoch)</th>\n",
       "      <td>176.382080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.638208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Training Time (second/iteration)</th>\n",
       "      <td>183.105052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.310505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computational Resource Usage</th>\n",
       "      <td>49.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>25.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>6.604259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.531799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.453180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPS Estimate (GFLOPS)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLU Load Distribution Stability</th>\n",
       "      <td>97.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLG Load Distribution Stability</th>\n",
       "      <td>244.771720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.477172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 3 (percent)</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 4 (percent)</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 5 (percent)</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 6 (percent)</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 7 (percent)</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 8 (percent)</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 3 (percent)</th>\n",
       "      <td>47.373134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.737313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 4 (percent)</th>\n",
       "      <td>24.970149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.497015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 5 (percent)</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 6 (percent)</th>\n",
       "      <td>0.761194</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 7 (percent)</th>\n",
       "      <td>3.970149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 8 (percent)</th>\n",
       "      <td>5.925373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.592537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Entity Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Intent Accuracy</th>\n",
       "      <td>0.901987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Perplexity</th>\n",
       "      <td>19.029236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.902924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Response Cosine Similarity</th>\n",
       "      <td>0.328805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Iteration 1  Iteration 2  \\\n",
       "Training Speed (epochs per second)                0.005670          0.0   \n",
       "Training Time (second/epoch)                    176.382080          0.0   \n",
       "Total Training Time (second/iteration)          183.105052          0.0   \n",
       "Computational Resource Usage                     49.500000          0.0   \n",
       "Average CPU Usage (percent)                      25.500000          0.0   \n",
       "Average GPU Usage (percent)                      24.000000          0.0   \n",
       "Average Memory (GB)                               6.604259          0.0   \n",
       "Average GPU Memory (GB)                          14.531799          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                   2.377871          0.0   \n",
       "Expert NLU Load Distribution Stability           97.500000          0.0   \n",
       "Expert NLG Load Distribution Stability          244.771720          0.0   \n",
       "Average NLU Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 3 (percent)                   12.000000          0.0   \n",
       "Average NLU Expert 4 (percent)                   31.000000          0.0   \n",
       "Average NLU Expert 5 (percent)                    7.000000          0.0   \n",
       "Average NLU Expert 6 (percent)                   22.000000          0.0   \n",
       "Average NLU Expert 7 (percent)                   14.000000          0.0   \n",
       "Average NLU Expert 8 (percent)                   14.000000          0.0   \n",
       "Average NLG Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 3 (percent)                   47.373134          0.0   \n",
       "Average NLG Expert 4 (percent)                   24.970149          0.0   \n",
       "Average NLG Expert 5 (percent)                   17.000000          0.0   \n",
       "Average NLG Expert 6 (percent)                    0.761194          0.0   \n",
       "Average NLG Expert 7 (percent)                    3.970149          0.0   \n",
       "Average NLG Expert 8 (percent)                    5.925373          0.0   \n",
       "Average Validation Entity Accuracy                1.000000          0.0   \n",
       "Average Intent Accuracy                           0.901987          0.0   \n",
       "Average Validation Perplexity                    19.029236          0.0   \n",
       "Average Validation Response Cosine Similarity     0.328805          0.0   \n",
       "\n",
       "                                               Iteration 3  Iteration 4  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 5  Iteration 6  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 7  Iteration 8  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 9  Iteration 10  \\\n",
       "Training Speed (epochs per second)                     0.0           0.0   \n",
       "Training Time (second/epoch)                           0.0           0.0   \n",
       "Total Training Time (second/iteration)                 0.0           0.0   \n",
       "Computational Resource Usage                           0.0           0.0   \n",
       "Average CPU Usage (percent)                            0.0           0.0   \n",
       "Average GPU Usage (percent)                            0.0           0.0   \n",
       "Average Memory (GB)                                    0.0           0.0   \n",
       "Average GPU Memory (GB)                                0.0           0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0           0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0           0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0           0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0           0.0   \n",
       "Average Validation Entity Accuracy                     0.0           0.0   \n",
       "Average Intent Accuracy                                0.0           0.0   \n",
       "Average Validation Perplexity                          0.0           0.0   \n",
       "Average Validation Response Cosine Similarity          0.0           0.0   \n",
       "\n",
       "                                                 Average  \n",
       "Training Speed (epochs per second)              0.000567  \n",
       "Training Time (second/epoch)                   17.638208  \n",
       "Total Training Time (second/iteration)         18.310505  \n",
       "Computational Resource Usage                    4.950000  \n",
       "Average CPU Usage (percent)                     2.550000  \n",
       "Average GPU Usage (percent)                     2.400000  \n",
       "Average Memory (GB)                             0.660426  \n",
       "Average GPU Memory (GB)                         1.453180  \n",
       "Average FLOPS Estimate (GFLOPS)                 0.237787  \n",
       "Expert NLU Load Distribution Stability          9.750000  \n",
       "Expert NLG Load Distribution Stability         24.477172  \n",
       "Average NLU Expert 1 (percent)                  0.000000  \n",
       "Average NLU Expert 2 (percent)                  0.000000  \n",
       "Average NLU Expert 3 (percent)                  1.200000  \n",
       "Average NLU Expert 4 (percent)                  3.100000  \n",
       "Average NLU Expert 5 (percent)                  0.700000  \n",
       "Average NLU Expert 6 (percent)                  2.200000  \n",
       "Average NLU Expert 7 (percent)                  1.400000  \n",
       "Average NLU Expert 8 (percent)                  1.400000  \n",
       "Average NLG Expert 1 (percent)                  0.000000  \n",
       "Average NLG Expert 2 (percent)                  0.000000  \n",
       "Average NLG Expert 3 (percent)                  4.737313  \n",
       "Average NLG Expert 4 (percent)                  2.497015  \n",
       "Average NLG Expert 5 (percent)                  1.700000  \n",
       "Average NLG Expert 6 (percent)                  0.076119  \n",
       "Average NLG Expert 7 (percent)                  0.397015  \n",
       "Average NLG Expert 8 (percent)                  0.592537  \n",
       "Average Validation Entity Accuracy              0.100000  \n",
       "Average Intent Accuracy                         0.090199  \n",
       "Average Validation Perplexity                   1.902924  \n",
       "Average Validation Response Cosine Similarity   0.032880  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reset_kernel():\n",
    "    tf.keras.backend.clear_session()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "results_table = {\n",
    "    'Training Speed (epochs per second)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Training Time (second/epoch)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Total Training Time (second/iteration)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Computational Resource Usage': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average CPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average GPU Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average FLOPS Estimate (GFLOPS)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLU Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Expert NLG Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLU Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average NLG Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Entity Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Intent Accuracy': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Perplexity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0},\n",
    "    'Average Validation Response Cosine Similarity': {f'Iteration {i+1}': 0 for i in range(10)} | {'Average': 0}\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_DeepseekTopPExpertChoice_moe_model\"\n",
    "\n",
    "for iteration in range(5):\n",
    "    print(f\"\\nStarting Training Iteration {iteration + 1}\")\n",
    "    reset_kernel()\n",
    "\n",
    "    def load_tf_datasets_from_disk(load_path):\n",
    "        print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "        try:\n",
    "            with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "                loaded_moe_params = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "        element_spec = (\n",
    "            {\n",
    "                'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            },\n",
    "            {\n",
    "                'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "                'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "                'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "            test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "        raw_data = {}\n",
    "        for dataset_name in ['train', 'test']:\n",
    "            path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "            if os.path.exists(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    raw_data[dataset_name] = pickle.load(f)\n",
    "                print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Raw data file {path} not found.\")\n",
    "                raw_data[dataset_name] = []\n",
    "\n",
    "        return {\n",
    "            \"train_dataset\": train_tf_dataset,\n",
    "            \"test_dataset\": test_tf_dataset,\n",
    "            \"moe_params\": loaded_moe_params,\n",
    "            \"raw_data\": raw_data\n",
    "        }\n",
    "\n",
    "    tf_dataset_save_path = \"tf_datasets\"\n",
    "    loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "    train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "    moe_params = loaded_data[\"moe_params\"]\n",
    "    raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "    moe_params[\"num_experts\"] = 4\n",
    "    moe_params[\"capacity_factor\"] = 1.0\n",
    "\n",
    "    class HybridRouting:\n",
    "        def __init__(self, p=0.9, capacity_factor=1.0):\n",
    "            self.p = p\n",
    "            self.capacity_factor = capacity_factor\n",
    "\n",
    "        def __call__(self, gate_logits, num_tokens, num_experts, k_s):\n",
    "            batch_size = tf.shape(gate_logits)[0]\n",
    "            probs = tf.nn.softmax(gate_logits, axis=-1)\n",
    "            sorted_probs, sorted_indices = tf.math.top_k(probs, k=tf.shape(probs)[-1])\n",
    "            cum_probs = tf.cumsum(sorted_probs, axis=-1)\n",
    "            p_mask = cum_probs >= self.p\n",
    "            first_over_p = tf.argmax(tf.cast(p_mask, tf.int32), axis=-1, output_type=tf.int32)\n",
    "            first_over_p = tf.maximum(first_over_p, 0)\n",
    "            num_selected = first_over_p + 1\n",
    "            max_selected = tf.reduce_max(num_selected)\n",
    "            selection_mask = tf.less_equal(tf.range(tf.shape(probs)[-1]), tf.expand_dims(first_over_p, -1))\n",
    "            top_p_indices = tf.boolean_mask(sorted_indices, selection_mask)\n",
    "            top_p_probs = tf.boolean_mask(sorted_probs, selection_mask)\n",
    "            top_p_indices = tf.RaggedTensor.from_row_lengths(top_p_indices, num_selected).to_tensor(shape=[batch_size, max_selected])\n",
    "            top_p_probs = tf.RaggedTensor.from_row_lengths(top_p_probs, num_selected).to_tensor(shape=[batch_size, max_selected])\n",
    "            top_p_weights = top_p_probs / tf.reduce_sum(top_p_probs, axis=-1, keepdims=True)\n",
    "            k = tf.cast(tf.cast(num_tokens, tf.float32) * self.capacity_factor / num_experts, tf.int32)\n",
    "            k = tf.maximum(k, 1)\n",
    "            transposed_probs = tf.transpose(probs)\n",
    "            gating_weights, token_indices = tf.math.top_k(transposed_probs, k=k)\n",
    "            gating_weights = tf.nn.softmax(gating_weights, axis=-1)\n",
    "            return top_p_indices, top_p_weights, token_indices, gating_weights\n",
    "\n",
    "    class MoELayer(layers.Layer):\n",
    "        def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, top_p=0.9, capacity_factor=1.0, name=None):\n",
    "            super(MoELayer, self).__init__(name=name)\n",
    "            self.m = m\n",
    "            self.k_s = k_s\n",
    "            self.total_experts = num_experts * m\n",
    "            self.routed_experts = self.total_experts - k_s\n",
    "            self.alpha = alpha\n",
    "            self.input_dim = input_dim\n",
    "            self.top_p = top_p\n",
    "            self.capacity_factor = capacity_factor\n",
    "            self.adjusted_expert_dim = expert_dim // m\n",
    "            self.seq_length = None\n",
    "            self.shared_experts = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', name=f'shared_expert_{i}_dense1'),\n",
    "                    layers.Dense(input_dim, activation='relu', name=f'shared_expert_{i}_dense2')\n",
    "                ], name=f'shared_expert_{i}') for i in range(k_s)\n",
    "            ]\n",
    "            self.routed_experts_list = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', name=f'routed_expert_{i}_dense1'),\n",
    "                    layers.Dense(input_dim, activation='relu', name=f'routed_expert_{i}_dense2')\n",
    "                ], name=f'routed_expert_{i}') for i in range(k_s, self.total_experts)\n",
    "            ]\n",
    "            self.experts = self.shared_experts + self.routed_experts_list\n",
    "            self.gate = layers.Dense(self.routed_experts, activation='softmax', name='gate_weights')\n",
    "            self.hybrid_router = HybridRouting(p=self.top_p, capacity_factor=self.capacity_factor)\n",
    "            self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            input_rank = inputs.shape.rank\n",
    "            if input_rank == 3:\n",
    "                batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "                flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "                is_3d = True\n",
    "            else:\n",
    "                batch_size = tf.shape(inputs)[0]\n",
    "                seq_length = 1\n",
    "                flat_inputs = inputs\n",
    "                is_3d = False\n",
    "            flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "            num_tokens = tf.shape(flat_inputs)[0]\n",
    "            shared_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "            for i in range(self.k_s):\n",
    "                shared_output += self.shared_experts[i](flat_inputs)\n",
    "            gate_probs = self.gate(flat_inputs)\n",
    "            top_p_indices, top_p_weights, token_indices, gating_weights = self.hybrid_router(\n",
    "                gate_probs, num_tokens, self.routed_experts, self.k_s\n",
    "            )\n",
    "            expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "            top_p_indices_adjusted = top_p_indices + self.k_s\n",
    "            batch_indices = tf.tile(\n",
    "                tf.expand_dims(tf.range(num_tokens), 1),\n",
    "                [1, tf.shape(top_p_indices)[1]]\n",
    "            )\n",
    "            gather_indices = tf.stack([batch_indices, top_p_indices_adjusted], axis=-1)\n",
    "            selected_outputs = tf.gather_nd(expert_outputs, gather_indices)\n",
    "            top_p_output = tf.reduce_sum(selected_outputs * tf.expand_dims(top_p_weights, -1), axis=1)\n",
    "            k = tf.shape(token_indices)[1]\n",
    "            token_indices_expanded = tf.expand_dims(token_indices, axis=2)\n",
    "            expert_indices = tf.range(self.routed_experts)[:, None, None]\n",
    "            expert_indices = tf.tile(expert_indices, [1, k, 1])\n",
    "            indices = tf.concat([token_indices_expanded, expert_indices], axis=2)\n",
    "            selected_expert_outputs = tf.gather_nd(expert_outputs[:, self.k_s:, :], indices)\n",
    "            one_hot_tokens = tf.one_hot(token_indices, depth=num_tokens, dtype=tf.float32)\n",
    "            expert_choice_output = tf.einsum('ek,ekd,ekn->nd', gating_weights, selected_expert_outputs, one_hot_tokens)\n",
    "            output_flat = shared_output + (0.5 * top_p_output + 0.5 * expert_choice_output) + flat_inputs\n",
    "            if is_3d:\n",
    "                output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "                if self.seq_length is not None:\n",
    "                    output.set_shape([None, self.seq_length, self.input_dim])\n",
    "                gate_weights_reshaped = tf.reshape(gate_probs, [batch_size, seq_length, self.routed_experts])\n",
    "                if self.seq_length is not None:\n",
    "                    gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "            else:\n",
    "                output = output_flat\n",
    "                gate_weights_reshaped = gate_probs\n",
    "            expert_selection = tf.reduce_mean(tf.reduce_max(gate_probs, axis=0), axis=0)\n",
    "            f_i = tf.reduce_mean(tf.reduce_sum(tf.one_hot(tf.argmax(gate_probs, axis=-1), depth=self.routed_experts), axis=0))\n",
    "            P_i = tf.reduce_mean(gate_probs, axis=0)\n",
    "            load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "            self.add_loss(load_balancing_loss)\n",
    "            self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "            return output, gate_weights_reshaped\n",
    "\n",
    "    class TransformerDecoderLayer(layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "            super(TransformerDecoderLayer, self).__init__()\n",
    "            self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.ffn = keras.Sequential([\n",
    "                layers.Dense(dff, activation='relu'),\n",
    "                layers.Dense(d_model)\n",
    "            ])\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "            self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "        def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "            if look_ahead_mask is not None:\n",
    "                look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "                look_ahead_mask = 1.0 - look_ahead_mask\n",
    "            attn1_output = self.mha1(\n",
    "                query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training, return_attention_scores=True\n",
    "            )\n",
    "            attn1, attn_weights1 = attn1_output if isinstance(attn1_output, tuple) else (attn1_output, None)\n",
    "            attn1 = self.dropout1(attn1, training=training)\n",
    "            out1 = self.layernorm1(attn1 + x)\n",
    "            attn2_output = self.mha2(\n",
    "                query=out1, value=enc_output, key=enc_output, attention_mask=None, training=training, return_attention_scores=True\n",
    "            )\n",
    "            attn2, attn_weights2 = attn2_output if isinstance(attn2_output, tuple) else (attn2_output, None)\n",
    "            attn2 = self.dropout2(attn2, training=training)\n",
    "            out2 = self.layernorm2(attn2 + out1)\n",
    "            ffn_output = self.ffn(out2)\n",
    "            ffn_output = self.dropout3(ffn_output, training=training)\n",
    "            out3 = self.layernorm3(ffn_output + out2)\n",
    "            return out3, attn_weights1, attn_weights2\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            mha_config = self.mha1.get_config()\n",
    "            config.update({\n",
    "                'd_model': mha_config['key_dim'] * mha_config['num_heads'],\n",
    "                'num_heads': mha_config['num_heads'],\n",
    "                'dff': self.ffn.layers[0].units,\n",
    "                'rate': self.dropout1.rate\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class MoEModel(models.Model):\n",
    "        def __init__(self, moe_params):\n",
    "            super(MoEModel, self).__init__()\n",
    "            self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "            self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "            self.num_domains = moe_params[\"num_domains\"]\n",
    "            self.num_intents = moe_params[\"num_intents\"]\n",
    "            self.vocab_size = moe_params[\"vocab_size\"]\n",
    "            self.num_experts = moe_params[\"num_experts\"]\n",
    "            self.expert_dim = moe_params[\"expert_dim\"]\n",
    "            self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "            self.num_heads = 4\n",
    "            self.dff = self.embedding_dim * 4\n",
    "            self.dropout_rate = 0.1\n",
    "            self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                                 self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "            self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "\n",
    "            self.embedding = layers.Embedding(\n",
    "                self.vocab_size,\n",
    "                self.embedding_dim,\n",
    "                mask_zero=True,\n",
    "                embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "                name=\"embedding\"\n",
    "            )\n",
    "            self.embedding.build((None,))\n",
    "            with tf.init_scope():\n",
    "                embedding_weights = self.embedding.get_weights()\n",
    "                if embedding_weights and len(embedding_weights) > 0:\n",
    "                    embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                    self.embedding.set_weights(embedding_weights)\n",
    "\n",
    "            self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "            self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "            self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "            self.moe_nlu = MoELayer(\n",
    "                num_experts=self.num_experts,\n",
    "                expert_dim=self.expert_dim,\n",
    "                input_dim=self.nlu_input_dim,\n",
    "                m=2,\n",
    "                k_s=2,\n",
    "                alpha=0.1,\n",
    "                top_p=0.9,\n",
    "                capacity_factor=moe_params[\"capacity_factor\"],\n",
    "                name='moe_nlu'\n",
    "            )\n",
    "            self.moe_nlg = MoELayer(\n",
    "                num_experts=self.num_experts,\n",
    "                expert_dim=self.expert_dim,\n",
    "                input_dim=self.nlg_input_dim,\n",
    "                m=2,\n",
    "                k_s=2,\n",
    "                alpha=0.1,\n",
    "                top_p=0.0,\n",
    "                capacity_factor=moe_params[\"capacity_factor\"],\n",
    "                name='moe_nlg'\n",
    "            )\n",
    "            self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "            self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "            self.response_output = layers.TimeDistributed(\n",
    "                layers.Dense(self.vocab_size, activation='softmax'), name='response_output'\n",
    "            )\n",
    "            self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "            self.nlg_projection = layers.TimeDistributed(\n",
    "                layers.Dense(self.embedding_dim, activation='relu', name='nlg_projection')\n",
    "            )\n",
    "\n",
    "        def create_look_ahead_mask(self, size):\n",
    "            mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "            return mask\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            user_utterances_tokens = inputs['user_utterance_tokens']\n",
    "            prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "            decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "            domain_onehot_input = inputs['domain_onehot_input']\n",
    "            turn_id_embedding = inputs['turn_id_embedding']\n",
    "            ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "\n",
    "            pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "            user_embedded = self.embedding_norm(self.embedding(user_utterances_tokens) + pos_enc)\n",
    "            prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "            decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "\n",
    "            user_enc_out = user_embedded\n",
    "            prev_system_enc_out = prev_system_embedded\n",
    "\n",
    "            look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "            dec_output = decoder_embedded\n",
    "            attn_weights = {}\n",
    "            for i, layer in enumerate(self.decoder_layers):\n",
    "                dec_output, attn_w1, attn_w2 = layer(\n",
    "                    dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask\n",
    "                )\n",
    "                attn_weights[f'decoder_layer{i+1}_attn1'] = attn_w1\n",
    "                attn_weights[f'decoder_layer{i+1}_attn2'] = attn_w2\n",
    "\n",
    "            decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "\n",
    "            user_attn_out = self.attn_layer(\n",
    "                query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), \n",
    "                value=user_enc_out, key=user_enc_out, training=training\n",
    "            )\n",
    "            prev_system_attn_out = self.attn_layer(\n",
    "                query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), \n",
    "                value=prev_system_enc_out, key=prev_system_enc_out, training=training\n",
    "            )\n",
    "            decoder_attn_out = self.attn_layer(\n",
    "                query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), \n",
    "                value=decoder_output, key=decoder_output, training=training\n",
    "            )\n",
    "\n",
    "            combined_features = layers.Concatenate()([\n",
    "                tf.squeeze(user_attn_out, axis=1),\n",
    "                tf.squeeze(prev_system_attn_out, axis=1),\n",
    "                tf.squeeze(decoder_attn_out, axis=1),\n",
    "                domain_onehot_input,\n",
    "                turn_id_embedding,\n",
    "                ontology_multihot_input,\n",
    "            ])\n",
    "            combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "            nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "            nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "            intent_out = self.intent_output(nlu_out)\n",
    "            domain_out = self.domain_output(nlu_out)\n",
    "\n",
    "            intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "            domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "            intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "            domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "\n",
    "            nlg_input = layers.Concatenate(axis=-1)([\n",
    "                decoder_output,\n",
    "                intent_out_tiled,\n",
    "                domain_out_tiled\n",
    "            ])\n",
    "            nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "\n",
    "            nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "            nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            response_embeddings = self.nlg_projection(nlg_out)\n",
    "            response_embeddings = tf.ensure_shape(response_embeddings, [None, self.max_seq_length, self.embedding_dim])\n",
    "            response_out = self.response_output(nlg_out)\n",
    "\n",
    "            return {\n",
    "                'intent_output': intent_out,\n",
    "                'domain_output': domain_out,\n",
    "                'response_output': response_out,\n",
    "                'response_embeddings': response_embeddings,\n",
    "                'nlu_gate_weights': nlu_gate_weights,\n",
    "                'nlg_gate_weights': nlg_gate_weights\n",
    "            }\n",
    "\n",
    "        def build_graph(self, input_shape=None):\n",
    "            inputs = {\n",
    "                'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "                'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32),\n",
    "            }\n",
    "            return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'moe_params': {\n",
    "                    'embedding_dim': self.embedding_dim,\n",
    "                    'max_seq_length': self.max_seq_length,\n",
    "                    'num_domains': self.num_domains,\n",
    "                    'num_intents': self.num_intents,\n",
    "                    'vocab_size': self.vocab_size,\n",
    "                    'num_experts': self.num_experts,\n",
    "                    'expert_dim': self.expert_dim,\n",
    "                    'turn_id_dim': self.turn_id_dim,\n",
    "                    'capacity_factor': self.moe_nlg.capacity_factor\n",
    "                }\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class IntentAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros', dtype=tf.float32)\n",
    "            self.total_preds = self.add_weight(name='total_preds', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "            self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "            self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.correct_preds.assign(0.)\n",
    "            self.total_preds.assign(0.)\n",
    "\n",
    "    class IntentPrecision(metrics.Metric):\n",
    "        def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "            self.predicted_positives = self.add_weight(name='pp', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            predicted_positives = tf.reduce_sum(y_pred)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.predicted_positives.assign(0.)\n",
    "\n",
    "    class IntentRecall(metrics.Metric):\n",
    "        def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "            self.actual_positives = self.add_weight(name='ap', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            actual_positives = tf.reduce_sum(y_true)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.actual_positives.assign(0.)\n",
    "\n",
    "    class IntentF1(metrics.Metric):\n",
    "        def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.precision = IntentPrecision(threshold=threshold)\n",
    "            self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "            self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "        def result(self):\n",
    "            p = self.precision.result()\n",
    "            r = self.recall.result()\n",
    "            return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.precision.reset_state()\n",
    "            self.recall.reset_state()\n",
    "\n",
    "    class DomainAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='domain_accuracy', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.accuracy = self.add_weight(name='acc', initializer='zeros', dtype=tf.float32)\n",
    "            self.count = self.add_weight(name='count', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "            pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "            matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "            self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "            self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.accuracy.assign(0.)\n",
    "            self.count.assign(0.)\n",
    "\n",
    "    class Perplexity(metrics.Metric):\n",
    "        def __init__(self, name='perplexity', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "            self.total_loss = self.add_weight(name='total_loss', initializer='zeros', dtype=tf.float32)\n",
    "            self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "            loss = self.cross_entropy(y_true, y_pred)\n",
    "            masked_loss = loss * mask\n",
    "            sum_loss = tf.reduce_sum(masked_loss)\n",
    "            num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "            self.total_loss.assign_add(sum_loss)\n",
    "            self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "        def result(self):\n",
    "            avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "            return tf.exp(avg_loss)\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_loss.assign(0.)\n",
    "            self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "    class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "        def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "            super().__init__(name=name)\n",
    "            self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "            self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "            y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "            y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "            y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "            cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "            non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "            non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "            masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "            num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "            avg_cosine_sim_per_sample = tf.where(\n",
    "                num_non_zero_tokens > 0,\n",
    "                tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "                tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "            )\n",
    "            self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "            self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_cosine_sim.assign(0.)\n",
    "            self.num_samples.assign(0.)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "        flops = 0\n",
    "        functional_model = model.build_graph()\n",
    "\n",
    "        def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "            layer_flops_count = 0\n",
    "\n",
    "            if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "                return 0\n",
    "\n",
    "            if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "                for sub_layer in layer_instance.layers:\n",
    "                    layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "                return layer_flops_count\n",
    "\n",
    "            if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "                return 0\n",
    "\n",
    "            if isinstance(layer_instance, layers.Dense):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                output_dim = layer_instance.output_shape[-1]\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.LSTM):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                hidden_dim = layer_instance.units\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "                layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.Embedding):\n",
    "                pass\n",
    "\n",
    "            elif isinstance(layer_instance, MoELayer):\n",
    "                moe_input_dim = layer_instance.input_dim\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "                layer_flops_count += gate_flops\n",
    "                for expert in layer_instance.experts:\n",
    "                    layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "                config = layer_instance.get_config()\n",
    "                num_heads = config['num_heads']\n",
    "                key_dim = config['key_dim']\n",
    "                d_model = num_heads * key_dim\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "                projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "                attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "                context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "                output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "                layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "            elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "                inner_layer = layer_instance.layer\n",
    "                td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "                layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "                ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "            return layer_flops_count\n",
    "\n",
    "        for layer in functional_model.layers:\n",
    "            flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "        return flops / 1e9\n",
    "\n",
    "    def get_gpu_stats():\n",
    "        if nvidia_smi is None:\n",
    "            return 0, 0\n",
    "        try:\n",
    "            nvidia_smi.nvmlInit()\n",
    "            handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "            gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "            mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "            gpu_memory = mem_info.used / (1024 ** 2)\n",
    "            nvidia_smi.nvmlShutdown()\n",
    "            return gpu_load, gpu_memory / 1024\n",
    "        except Exception:\n",
    "            return 0, 0\n",
    "\n",
    "    class ResourceMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, validation_data):\n",
    "            super().__init__()\n",
    "            self.resource_stats = []\n",
    "            self.expert_load_history = []\n",
    "            self.start_time = None\n",
    "            self.epoch_start_time = None\n",
    "            self.flops = None\n",
    "            self.validation_data = validation_data\n",
    "            self.nlu_expert_usage_counts = None\n",
    "            self.nlg_expert_usage_counts = None\n",
    "            self.nlu_gate_weights_history = []\n",
    "            self.nlg_gate_weights_history = []\n",
    "\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = time.time()\n",
    "            try:\n",
    "                self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "            except Exception:\n",
    "                self.flops = 0\n",
    "            try:\n",
    "                self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "                self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "            except Exception:\n",
    "                self.nlu_expert_usage_counts = np.zeros(0)\n",
    "                self.nlg_expert_usage_counts = np.zeros(0)\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.epoch_start_time = time.time()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.resource_stats.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_time': epoch_time,\n",
    "                'cpu_usage': cpu_usage,\n",
    "                'memory_used': memory.used / (1024 ** 3),\n",
    "                'memory_total': memory.total / (1024 ** 3),\n",
    "                'gpu_load': gpu_load,\n",
    "                'gpu_memory': gpu_memory,\n",
    "                'loss': logs.get('loss', 0),\n",
    "                'val_loss': logs.get('val_loss', 0),\n",
    "                'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "                'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "                'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "                'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "                'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "                'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "                'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "                'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "                'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "                'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "                'perplexity': logs.get('response_output_perplexity', 0),\n",
    "                'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "                'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "                'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "            })\n",
    "            if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "                self.expert_load_history.append({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                    'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "                })\n",
    "\n",
    "            sample_size = 100\n",
    "            for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "                inputs, _ = batch\n",
    "                outputs = self.model(inputs, training=False)\n",
    "                nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "                reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "                self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "                nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "                reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "                self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            self.total_time = time.time() - self.start_time\n",
    "\n",
    "            if len(self.nlu_gate_weights_history) > 0:\n",
    "                all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "                nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "                for expert_id in nlu_top_indices:\n",
    "                    if expert_id < self.model.moe_nlu.total_experts:\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "            if len(self.nlg_gate_weights_history) > 0:\n",
    "                all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "                nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "                for expert_id in nlg_top_indices:\n",
    "                    if expert_id < self.model.moe_nlg.total_experts:\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        def visualize_expert_load_distribution(self):\n",
    "            total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "            total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "            nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "            nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "            nlu_stability = np.var(nlu_percentages)\n",
    "            nlg_stability = np.var(nlg_percentages)\n",
    "            return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "    class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super().__init__()\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\n",
    "                \"d_model\": self.d_model.numpy(),\n",
    "                \"warmup_steps\": self.warmup_steps\n",
    "            }\n",
    "\n",
    "    model = MoEModel(moe_params)\n",
    "    embedding_layer = model.embedding\n",
    "\n",
    "    def create_validation_split(dataset, embedding_layer, validation_split=0.2, batch_size=moe_params[\"batch_size\"]):\n",
    "        element_spec = dataset.element_spec\n",
    "        data_list = [(features, targets) for features, targets in dataset.unbatch().as_numpy_iterator()]\n",
    "        if not data_list:\n",
    "            raise ValueError(\"Dataset is empty.\")\n",
    "        if len(data_list) < 2:\n",
    "            raise ValueError(\"Dataset too small to split.\")\n",
    "        train_data, val_data = train_test_split(data_list, test_size=validation_split, shuffle=True)\n",
    "\n",
    "        def create_dataset_from_list(data):\n",
    "            if not data:\n",
    "                raise ValueError(\"Empty data list provided.\")\n",
    "            features = {\n",
    "                'user_utterance_tokens': np.array([d[0]['user_utterance_tokens'] for d in data], dtype=np.int32),\n",
    "                'prev_system_response_tokens': np.array([d[0]['prev_system_response_tokens'] for d in data], dtype=np.int32),\n",
    "                'decoder_input_tokens': np.array([d[0]['decoder_input_tokens'] for d in data], dtype=np.int32),\n",
    "                'domain_onehot_input': np.array([d[0]['domain_onehot_input'] for d in data], dtype=np.float32),\n",
    "                'turn_id_embedding': np.array([d[0]['turn_id_embedding'] for d in data], dtype=np.float32),\n",
    "                'ontology_multihot_input': np.array([d[0]['ontology_multihot_input'] for d in data], dtype=np.float32),\n",
    "            }\n",
    "            response_output = np.array([d[1]['response_output'] for d in data], dtype=np.int32)\n",
    "            response_embeddings = embedding_layer(response_output).numpy()\n",
    "            targets = {\n",
    "                'domain_output': np.array([d[1]['domain_output'] for d in data], dtype=np.int32),\n",
    "                'intent_output': np.array([d[1]['intent_output'] for d in data], dtype=np.float32),\n",
    "                'response_output': response_output,\n",
    "                'response_embeddings': response_embeddings\n",
    "            }\n",
    "            return tf.data.Dataset.from_tensor_slices((features, targets))\n",
    "\n",
    "        train_dataset = create_dataset_from_list(train_data).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        val_dataset = create_dataset_from_list(val_data).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    train_tf_dataset, val_tf_dataset = create_validation_split(train_tf_dataset, embedding_layer)\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': losses.BinaryCrossentropy(),\n",
    "        'domain_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={'intent_output': 0.5, 'domain_output': 0.5, 'response_output': 1.0, 'response_embeddings': 1.0}\n",
    "    )\n",
    "\n",
    "    sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "    model(sample_input[0])\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=8, mode='min', restore_best_weights=True)\n",
    "    resource_monitor = ResourceMonitor(val_tf_dataset)\n",
    "    class TQDMProgressBar(callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.progress_bar = tqdm(total=len(train_tf_dataset), desc=f'Epoch {epoch + 1}')\n",
    "\n",
    "        def on_batch_end(self, batch, logs=None):\n",
    "            self.progress_bar.update(1)\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            self.progress_bar.close()\n",
    "\n",
    "    tqdm_callback = TQDMProgressBar()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_tf_dataset,\n",
    "        epochs=100,\n",
    "        validation_data=val_tf_dataset,\n",
    "        callbacks=[early_stopping, resource_monitor, tqdm_callback],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    expert_stats = resource_monitor.visualize_expert_load_distribution()\n",
    "    stats = resource_monitor.resource_stats\n",
    "    avg_epoch_duration = np.mean([s['epoch_time'] for s in stats]) if stats else 0\n",
    "    total_training_time = resource_monitor.total_time\n",
    "    avg_cpu_usage = np.mean([s['cpu_usage'] for s in stats]) if stats else 0\n",
    "    avg_memory = np.mean([s['memory_used'] for s in stats]) if stats else 0\n",
    "    avg_gpu_load = np.mean([s['gpu_load'] for s in stats]) if stats else 0\n",
    "    avg_gpu_memory = np.mean([s['gpu_memory'] for s in stats]) if stats else 0\n",
    "    avg_flops = resource_monitor.flops if resource_monitor.flops else 0\n",
    "    nlu_counts = resource_monitor.nlu_expert_usage_counts\n",
    "    nlg_counts = resource_monitor.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) or 1\n",
    "    total_nlg = np.sum(nlg_counts) or 1\n",
    "    expert_nlu_percentages = [(nlu_counts[i] / total_nlu) * 100 for i in range(8)]\n",
    "    expert_nlg_percentages = [(nlg_counts[i] / total_nlg) * 100 for i in range(8)]\n",
    "    val_intent_accuracy = np.mean([s['val_intent_accuracy'] for s in stats if s['val_intent_accuracy'] > 0]) if stats else 0\n",
    "    val_entity_accuracy = np.mean([s['val_domain_accuracy'] for s in stats if s['val_domain_accuracy'] > 0]) if stats else 0\n",
    "    val_perplexity = np.mean([s['val_perplexity'] for s in stats if s['val_perplexity'] > 0]) if stats else 0\n",
    "    val_cosine_similarity = np.mean([s['val_cosine_similarity'] for s in stats if s['val_cosine_similarity'] > 0]) if stats else 0\n",
    "\n",
    "    results_table['Training Speed (epochs per second)'][f'Iteration {iteration + 1}'] = 1 / avg_epoch_duration if avg_epoch_duration > 0 else 0\n",
    "    results_table['Training Time (second/epoch)'][f'Iteration {iteration + 1}'] = avg_epoch_duration\n",
    "    results_table['Total Training Time (second/iteration)'][f'Iteration {iteration + 1}'] = total_training_time\n",
    "    results_table['Computational Resource Usage'][f'Iteration {iteration + 1}'] = avg_cpu_usage + avg_gpu_load\n",
    "    results_table['Average CPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_cpu_usage\n",
    "    results_table['Average GPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_gpu_load\n",
    "    results_table['Average Memory (GB)'][f'Iteration {iteration + 1}'] = avg_memory\n",
    "    results_table['Average GPU Memory (GB)'][f'Iteration {iteration + 1}'] = avg_gpu_memory\n",
    "    results_table['Average FLOPS Estimate (GFLOPS)'][f'Iteration {iteration + 1}'] = avg_flops\n",
    "    results_table['Expert NLU Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlu_stability']\n",
    "    results_table['Expert NLG Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlg_stability']\n",
    "    for i in range(8):\n",
    "        if i < len(expert_stats['nlu_percentages']):\n",
    "            results_table[f'Average NLU Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlu_percentages'][i]\n",
    "        if i < len(expert_stats['nlg_percentages']):\n",
    "            results_table[f'Average NLG Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlg_percentages'][i]\n",
    "    results_table['Average Validation Entity Accuracy'][f'Iteration {iteration + 1}'] = val_entity_accuracy\n",
    "    results_table['Average Intent Accuracy'][f'Iteration {iteration + 1}'] = val_intent_accuracy\n",
    "    results_table['Average Validation Perplexity'][f'Iteration {iteration + 1}'] = val_perplexity\n",
    "    results_table['Average Validation Response Cosine Similarity'][f'Iteration {iteration + 1}'] = val_cosine_similarity\n",
    "\n",
    "    val_loss = min([s['val_loss'] for s in stats]) if stats else float('inf')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        try:\n",
    "            model.save(best_model_path, save_format='tf', include_optimizer=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to save best model to {best_model_path}: {str(e)}\")\n",
    "\n",
    "for key in results_table:\n",
    "    results_table[key]['Average'] = np.mean([results_table[key][f'Iteration {i+1}'] for i in range(10)])\n",
    "\n",
    "final_result = pd.DataFrame(results_table)\n",
    "final_result = final_result.T\n",
    "final_result.to_excel('training_DeepseekTopPExpertChoice_result.xlsx', index=True)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE with Top-P with Expert Choice Balance evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:44:07.990557: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 08:44:08.305465: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 08:44:08.305620: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 08:44:08.352349: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 08:44:08.471319: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 08:44:09.905376: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-04 08:44:13.550073: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:44:13.721140: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:44:13.721439: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:44:13.723097: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:44:13.723243: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:44:13.723328: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:44:13.805257: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:44:13.805521: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:44:13.805693: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:44:13.805819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mo_e_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  784896    \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  8576      \n",
      "                                                                 \n",
      " layer_normalization (Layer  multiple                  256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_layer   multiple                  264576    \n",
      " (TransformerDecoderLayer)                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " moe_nlu (MoELayer)          multiple                  244852    \n",
      "                                                                 \n",
      " moe_nlg (MoELayer)          multiple                  87578     \n",
      "                                                                 \n",
      " intent_output (Dense)       multiple                  14446     \n",
      "                                                                 \n",
      " domain_output (Dense)       multiple                  3262      \n",
      "                                                                 \n",
      " response_output (TimeDistr  multiple                  1024044   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " multi_head_attention_2 (Mu  multiple                  66048     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      " time_distributed (TimeDist  multiple                  21376     \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2519910 (9.61 MB)\n",
      "Trainable params: 2519910 (9.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1397/1397 [==============================] - 23s 13ms/step - loss: 5.5714 - domain_output_loss: 7.1650e-05 - intent_output_loss: 0.0085 - response_embeddings_loss: 0.9573 - response_output_loss: 2.3431 - domain_output_domain_accuracy: 1.0000 - intent_output_intent_accuracy: 0.9198 - intent_output_intent_precision: 0.9924 - intent_output_intent_recall: 0.9214 - intent_output_intent_f1: 0.9556 - response_embeddings_response_embedding_cosine_similarity: 0.0427 - response_output_perplexity: 31.9998 - moe_nlu_load_balancing_loss: 0.0333 - moe_nlg_load_balancing_loss: 2.2333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evaluation Time (seconds)</th>\n",
       "      <td>23.373688</td>\n",
       "      <td>23.373688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction Speed (ms/token)</th>\n",
       "      <td>2.799684</td>\n",
       "      <td>2.799684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>24.294703</td>\n",
       "      <td>24.294703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>1.838225</td>\n",
       "      <td>1.838225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>5.701781</td>\n",
       "      <td>5.701781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.504456</td>\n",
       "      <td>14.504456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPs Estimate (GFLOPs)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>2.377871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 3 (percent)</th>\n",
       "      <td>23.443092</td>\n",
       "      <td>23.443092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 4 (percent)</th>\n",
       "      <td>24.087330</td>\n",
       "      <td>24.087330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 5 (percent)</th>\n",
       "      <td>5.654975</td>\n",
       "      <td>5.654975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 6 (percent)</th>\n",
       "      <td>13.385827</td>\n",
       "      <td>13.385827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 7 (percent)</th>\n",
       "      <td>5.798139</td>\n",
       "      <td>5.798139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 8 (percent)</th>\n",
       "      <td>27.630637</td>\n",
       "      <td>27.630637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 3 (percent)</th>\n",
       "      <td>19.682368</td>\n",
       "      <td>19.682368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 4 (percent)</th>\n",
       "      <td>15.539696</td>\n",
       "      <td>15.539696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 5 (percent)</th>\n",
       "      <td>37.847359</td>\n",
       "      <td>37.847359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 6 (percent)</th>\n",
       "      <td>7.844902</td>\n",
       "      <td>7.844902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 7 (percent)</th>\n",
       "      <td>2.641588</td>\n",
       "      <td>2.641588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 8 (percent)</th>\n",
       "      <td>16.444086</td>\n",
       "      <td>16.444086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Micro)</th>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.955600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Macro)</th>\n",
       "      <td>0.847914</td>\n",
       "      <td>0.847914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Weighted)</th>\n",
       "      <td>0.936496</td>\n",
       "      <td>0.936496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent Accuracy</th>\n",
       "      <td>0.919828</td>\n",
       "      <td>0.919828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain F1-score (Macro)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU Score</th>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1 F1</th>\n",
       "      <td>0.181707</td>\n",
       "      <td>0.181707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-2 F1</th>\n",
       "      <td>0.012799</td>\n",
       "      <td>0.012799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-L F1</th>\n",
       "      <td>0.139828</td>\n",
       "      <td>0.139828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METEOR Score</th>\n",
       "      <td>0.135965</td>\n",
       "      <td>0.135965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>31.999825</td>\n",
       "      <td>31.999825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert Load Stability (Variance)</th>\n",
       "      <td>111.000958</td>\n",
       "      <td>111.000958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert Load Stability (Variance)</th>\n",
       "      <td>143.778604</td>\n",
       "      <td>143.778604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Iteration 1     Average\n",
       "Evaluation Time (seconds)               23.373688   23.373688\n",
       "Prediction Speed (ms/token)              2.799684    2.799684\n",
       "Average CPU Usage (percent)             24.294703   24.294703\n",
       "Average GPU Usage (percent)              1.838225    1.838225\n",
       "Average Memory (GB)                      5.701781    5.701781\n",
       "Average GPU Memory (GB)                 14.504456   14.504456\n",
       "Average FLOPs Estimate (GFLOPs)          2.377871    2.377871\n",
       "NLU Expert 1 (percent)                   0.000000    0.000000\n",
       "NLU Expert 2 (percent)                   0.000000    0.000000\n",
       "NLU Expert 3 (percent)                  23.443092   23.443092\n",
       "NLU Expert 4 (percent)                  24.087330   24.087330\n",
       "NLU Expert 5 (percent)                   5.654975    5.654975\n",
       "NLU Expert 6 (percent)                  13.385827   13.385827\n",
       "NLU Expert 7 (percent)                   5.798139    5.798139\n",
       "NLU Expert 8 (percent)                  27.630637   27.630637\n",
       "NLG Expert 1 (percent)                   0.000000    0.000000\n",
       "NLG Expert 2 (percent)                   0.000000    0.000000\n",
       "NLG Expert 3 (percent)                  19.682368   19.682368\n",
       "NLG Expert 4 (percent)                  15.539696   15.539696\n",
       "NLG Expert 5 (percent)                  37.847359   37.847359\n",
       "NLG Expert 6 (percent)                   7.844902    7.844902\n",
       "NLG Expert 7 (percent)                   2.641588    2.641588\n",
       "NLG Expert 8 (percent)                  16.444086   16.444086\n",
       "Intent F1-score (Micro)                  0.955600    0.955600\n",
       "Intent F1-score (Macro)                  0.847914    0.847914\n",
       "Intent F1-score (Weighted)               0.936496    0.936496\n",
       "Intent Accuracy                          0.919828    0.919828\n",
       "Domain Accuracy                          1.000000    1.000000\n",
       "Domain F1-score (Macro)                  1.000000    1.000000\n",
       "BLEU Score                               0.000990    0.000990\n",
       "ROUGE-1 F1                               0.181707    0.181707\n",
       "ROUGE-2 F1                               0.012799    0.012799\n",
       "ROUGE-L F1                               0.139828    0.139828\n",
       "METEOR Score                             0.135965    0.135965\n",
       "Perplexity                              31.999825   31.999825\n",
       "NLU Expert Load Stability (Variance)   111.000958  111.000958\n",
       "NLG Expert Load Stability (Variance)   143.778604  143.778604"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tf_datasets_from_disk(load_path):\n",
    "    try:\n",
    "        with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "            loaded_moe_params = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "    element_spec = (\n",
    "        {\n",
    "            'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "        },\n",
    "        {\n",
    "            'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "            'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "        test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "    raw_data = {}\n",
    "    for dataset_name in ['train', 'test']:\n",
    "        path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                raw_data[dataset_name] = pickle.load(f)\n",
    "        else:\n",
    "            raw_data[dataset_name] = []\n",
    "\n",
    "    return {\n",
    "        \"train_dataset\": train_tf_dataset,\n",
    "        \"test_dataset\": test_tf_dataset,\n",
    "        \"moe_params\": loaded_moe_params,\n",
    "        \"raw_data\": raw_data\n",
    "    }\n",
    "\n",
    "tf_dataset_save_path = \"tf_datasets\"\n",
    "loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "test_tf_dataset = loaded_data[\"test_dataset\"]\n",
    "moe_params = loaded_data[\"moe_params\"]\n",
    "raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "moe_params[\"num_experts\"] = 4\n",
    "\n",
    "class HybridRouting:\n",
    "    def __init__(self, p=0.9, capacity_factor=1.0):\n",
    "        self.p = p\n",
    "        self.capacity_factor = capacity_factor\n",
    "\n",
    "    def __call__(self, gate_logits, num_tokens, num_experts, k_s):\n",
    "        batch_size = tf.shape(gate_logits)[0]\n",
    "        probs = tf.nn.softmax(gate_logits, axis=-1)\n",
    "        sorted_probs, sorted_indices = tf.math.top_k(probs, k=tf.shape(probs)[-1])\n",
    "        cum_probs = tf.cumsum(sorted_probs, axis=-1)\n",
    "        p_mask = cum_probs >= self.p\n",
    "        first_over_p = tf.argmax(tf.cast(p_mask, tf.int32), axis=-1, output_type=tf.int32)\n",
    "        first_over_p = tf.maximum(first_over_p, 0)\n",
    "        num_selected = first_over_p + 1\n",
    "        max_selected = tf.reduce_max(num_selected)\n",
    "        selection_mask = tf.less_equal(tf.range(tf.shape(probs)[-1]), tf.expand_dims(first_over_p, -1))\n",
    "        top_p_indices = tf.boolean_mask(sorted_indices, selection_mask)\n",
    "        top_p_probs = tf.boolean_mask(sorted_probs, selection_mask)\n",
    "        top_p_indices = tf.RaggedTensor.from_row_lengths(top_p_indices, num_selected).to_tensor(shape=[batch_size, max_selected])\n",
    "        top_p_probs = tf.RaggedTensor.from_row_lengths(top_p_probs, num_selected).to_tensor(shape=[batch_size, max_selected])\n",
    "        top_p_weights = top_p_probs / tf.reduce_sum(top_p_probs, axis=-1, keepdims=True)\n",
    "        k = tf.cast(tf.cast(num_tokens, tf.float32) * self.capacity_factor / num_experts, tf.int32)\n",
    "        k = tf.maximum(k, 1)\n",
    "        transposed_probs = tf.transpose(probs)\n",
    "        gating_weights, token_indices = tf.math.top_k(transposed_probs, k=k)\n",
    "        gating_weights = tf.nn.softmax(gating_weights, axis=-1)\n",
    "        return top_p_indices, top_p_weights, token_indices, gating_weights\n",
    "\n",
    "class MoELayer(layers.Layer):\n",
    "    def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, top_p=0.9, capacity_factor=1.0, name=None):\n",
    "        super(MoELayer, self).__init__(name=name)\n",
    "        self.m = m\n",
    "        self.k_s = k_s\n",
    "        self.total_experts = num_experts * m\n",
    "        self.routed_experts = self.total_experts - k_s\n",
    "        self.alpha = alpha\n",
    "        self.input_dim = input_dim\n",
    "        self.top_p = top_p\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.adjusted_expert_dim = expert_dim // m\n",
    "        self.seq_length = None\n",
    "        self.shared_experts = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', name=f'shared_expert_{i}_dense1'),\n",
    "                layers.Dense(input_dim, activation='relu', name=f'shared_expert_{i}_dense2')\n",
    "            ], name=f'shared_expert_{i}') for i in range(k_s)\n",
    "        ]\n",
    "        self.routed_experts_list = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', name=f'routed_expert_{i}_dense1'),\n",
    "                layers.Dense(input_dim, activation='relu', name=f'routed_expert_{i}_dense2')\n",
    "            ], name=f'routed_expert_{i}') for i in range(k_s, self.total_experts)\n",
    "        ]\n",
    "        self.experts = self.shared_experts + self.routed_experts_list\n",
    "        self.gate = layers.Dense(self.routed_experts, activation='softmax', name='gate_weights')\n",
    "        self.hybrid_router = HybridRouting(p=self.top_p, capacity_factor=self.capacity_factor)\n",
    "        self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_rank = inputs.shape.rank\n",
    "        if input_rank == 3:\n",
    "            batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "            flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "            is_3d = True\n",
    "        else:\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            seq_length = 1\n",
    "            flat_inputs = inputs\n",
    "            is_3d = False\n",
    "        flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "        num_tokens = tf.shape(flat_inputs)[0]\n",
    "        shared_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "        for i in range(self.k_s):\n",
    "            shared_output += self.shared_experts[i](flat_inputs)\n",
    "        gate_probs = self.gate(flat_inputs)\n",
    "        top_p_indices, top_p_weights, token_indices, gating_weights = self.hybrid_router(\n",
    "            gate_probs, num_tokens, self.routed_experts, self.k_s\n",
    "        )\n",
    "        expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "        top_p_indices_adjusted = top_p_indices + self.k_s\n",
    "        batch_indices = tf.tile(\n",
    "            tf.expand_dims(tf.range(num_tokens), 1),\n",
    "            [1, tf.shape(top_p_indices)[1]]\n",
    "        )\n",
    "        gather_indices = tf.stack([batch_indices, top_p_indices_adjusted], axis=-1)\n",
    "        selected_outputs = tf.gather_nd(expert_outputs, gather_indices)\n",
    "        top_p_output = tf.reduce_sum(selected_outputs * tf.expand_dims(top_p_weights, -1), axis=1)\n",
    "        k = tf.shape(token_indices)[1]\n",
    "        token_indices_expanded = tf.expand_dims(token_indices, axis=2)\n",
    "        expert_indices = tf.range(self.routed_experts)[:, None, None]\n",
    "        expert_indices = tf.tile(expert_indices, [1, k, 1])\n",
    "        indices = tf.concat([token_indices_expanded, expert_indices], axis=2)\n",
    "        selected_expert_outputs = tf.gather_nd(expert_outputs[:, self.k_s:, :], indices)\n",
    "        one_hot_tokens = tf.one_hot(token_indices, depth=num_tokens, dtype=tf.float32)\n",
    "        expert_choice_output = tf.einsum('ek,ekd,ekn->nd', gating_weights, selected_expert_outputs, one_hot_tokens)\n",
    "        output_flat = shared_output + (0.5 * top_p_output + 0.5 * expert_choice_output) + flat_inputs\n",
    "        if is_3d:\n",
    "            output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "            if self.seq_length is not None:\n",
    "                output.set_shape([None, self.seq_length, self.input_dim])\n",
    "            gate_weights_reshaped = tf.reshape(gate_probs, [batch_size, seq_length, self.routed_experts])\n",
    "            if self.seq_length is not None:\n",
    "                gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "        else:\n",
    "            output = output_flat\n",
    "            gate_weights_reshaped = gate_probs\n",
    "        expert_selection = tf.reduce_mean(tf.reduce_max(gate_probs, axis=0), axis=0)\n",
    "        f_i = tf.reduce_mean(tf.reduce_sum(tf.one_hot(tf.argmax(gate_probs, axis=-1), depth=self.routed_experts), axis=0))\n",
    "        P_i = tf.reduce_mean(gate_probs, axis=0)\n",
    "        load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "        self.add_loss(load_balancing_loss)\n",
    "        self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "        return output, gate_weights_reshaped\n",
    "\n",
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "        if look_ahead_mask is not None:\n",
    "            look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "            look_ahead_mask = 1.0 - look_ahead_mask\n",
    "        attn1_output = self.mha1(\n",
    "            query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training, return_attention_scores=True\n",
    "        )\n",
    "        attn1, attn_weights1 = attn1_output if isinstance(attn1_output, tuple) else (attn1_output, None)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        attn2_output = self.mha2(\n",
    "            query=out1, value=enc_output, key=enc_output, attention_mask=None, training=training, return_attention_scores=True\n",
    "        )\n",
    "        attn2, attn_weights2 = attn2_output if isinstance(attn2_output, tuple) else (attn2_output, None)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        return out3, attn_weights1, attn_weights2\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        mha_config = self.mha1.get_config()\n",
    "        config.update({\n",
    "            'd_model': mha_config['key_dim'] * mha_config['num_heads'],\n",
    "            'num_heads': mha_config['num_heads'],\n",
    "            'dff': self.ffn.layers[0].units,\n",
    "            'rate': self.dropout1.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MoEModel(models.Model):\n",
    "    def __init__(self, moe_params):\n",
    "        super(MoEModel, self).__init__()\n",
    "        self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "        self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "        self.num_domains = moe_params[\"num_domains\"]\n",
    "        self.num_intents = moe_params[\"num_intents\"]\n",
    "        self.vocab_size = moe_params[\"vocab_size\"]\n",
    "        self.num_experts = moe_params[\"num_experts\"]\n",
    "        self.expert_dim = moe_params[\"expert_dim\"]\n",
    "        self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "        self.num_heads = 4\n",
    "        self.dff = self.embedding_dim * 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                             self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "        self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "\n",
    "        self.embedding = layers.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "            name=\"embedding\"\n",
    "        )\n",
    "        self.embedding.build((None,))\n",
    "        with tf.init_scope():\n",
    "            embedding_weights = self.embedding.get_weights()\n",
    "            if embedding_weights and len(embedding_weights) > 0:\n",
    "                embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                self.embedding.set_weights(embedding_weights)\n",
    "\n",
    "        self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "        self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "        self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "        self.moe_nlu = MoELayer(\n",
    "            num_experts=self.num_experts,\n",
    "            expert_dim=self.expert_dim,\n",
    "            input_dim=self.nlu_input_dim,\n",
    "            m=2,\n",
    "            k_s=2,\n",
    "            alpha=0.1,\n",
    "            top_p=0.9,\n",
    "            capacity_factor=moe_params[\"capacity_factor\"],\n",
    "            name='moe_nlu'\n",
    "        )\n",
    "        self.moe_nlg = MoELayer(\n",
    "            num_experts=self.num_experts,\n",
    "            expert_dim=self.expert_dim,\n",
    "            input_dim=self.nlg_input_dim,\n",
    "            m=2,\n",
    "            k_s=2,\n",
    "            alpha=0.1,\n",
    "            top_p=0.0,\n",
    "            capacity_factor=moe_params[\"capacity_factor\"],\n",
    "            name='moe_nlg'\n",
    "        )\n",
    "        self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "        self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "        self.response_output = layers.TimeDistributed(\n",
    "            layers.Dense(self.vocab_size, activation='softmax'), name='response_output'\n",
    "        )\n",
    "        self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "        self.nlg_projection = layers.TimeDistributed(\n",
    "            layers.Dense(self.embedding_dim, activation='relu', name='nlg_projection')\n",
    "        )\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        user_utterances_tokens = inputs['user_utterance_tokens']\n",
    "        prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "        decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "        domain_onehot_input = inputs['domain_onehot_input']\n",
    "        turn_id_embedding = inputs['turn_id_embedding']\n",
    "        ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "\n",
    "        pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "        user_embedded = self.embedding_norm(self.embedding(user_utterances_tokens) + pos_enc)\n",
    "        prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "        decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "\n",
    "        user_enc_out = user_embedded\n",
    "        prev_system_enc_out = prev_system_embedded\n",
    "\n",
    "        look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "        dec_output = decoder_embedded\n",
    "        attn_weights = {}\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            dec_output, attn_w1, attn_w2 = layer(\n",
    "                dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask\n",
    "            )\n",
    "            attn_weights[f'decoder_layer{i+1}_attn1'] = attn_w1\n",
    "            attn_weights[f'decoder_layer{i+1}_attn2'] = attn_w2\n",
    "\n",
    "        decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "\n",
    "        user_attn_out = self.attn_layer(\n",
    "            query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), \n",
    "            value=user_enc_out, key=user_enc_out, training=training\n",
    "        )\n",
    "        prev_system_attn_out = self.attn_layer(\n",
    "            query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), \n",
    "            value=prev_system_enc_out, key=prev_system_enc_out, training=training\n",
    "        )\n",
    "        decoder_attn_out = self.attn_layer(\n",
    "            query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), \n",
    "            value=decoder_output, key=decoder_output, training=training\n",
    "        )\n",
    "\n",
    "        combined_features = layers.Concatenate()([\n",
    "            tf.squeeze(user_attn_out, axis=1),\n",
    "            tf.squeeze(prev_system_attn_out, axis=1),\n",
    "            tf.squeeze(decoder_attn_out, axis=1),\n",
    "            domain_onehot_input,\n",
    "            turn_id_embedding,\n",
    "            ontology_multihot_input,\n",
    "        ])\n",
    "        combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "        nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "        nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "        intent_out = self.intent_output(nlu_out)\n",
    "        domain_out = self.domain_output(nlu_out)\n",
    "\n",
    "        intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "        domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "        intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "        domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "\n",
    "        nlg_input = layers.Concatenate(axis=-1)([\n",
    "            decoder_output,\n",
    "            intent_out_tiled,\n",
    "            domain_out_tiled\n",
    "        ])\n",
    "        nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "\n",
    "        nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "        nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        response_embeddings = self.nlg_projection(nlg_out)\n",
    "        response_embeddings = tf.ensure_shape(response_embeddings, [None, self.max_seq_length, self.embedding_dim])\n",
    "        response_out = self.response_output(nlg_out)\n",
    "\n",
    "        return {\n",
    "            'intent_output': intent_out,\n",
    "            'domain_output': domain_out,\n",
    "            'response_output': response_out,\n",
    "            'response_embeddings': response_embeddings,\n",
    "            'nlu_gate_weights': nlu_gate_weights,\n",
    "            'nlg_gate_weights': nlg_gate_weights\n",
    "        }\n",
    "\n",
    "    def build_graph(self, input_shape=None):\n",
    "        inputs = {\n",
    "            'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "            'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32),\n",
    "        }\n",
    "        return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'moe_params': {\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'max_seq_length': self.max_seq_length,\n",
    "                'num_domains': self.num_domains,\n",
    "                'num_intents': self.num_intents,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'num_experts': self.num_experts,\n",
    "                'expert_dim': self.expert_dim,\n",
    "                'turn_id_dim': self.turn_id_dim,\n",
    "                'capacity_factor': self.moe_nlg.capacity_factor\n",
    "            }\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class IntentAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros', dtype=tf.float32)\n",
    "        self.total_preds = self.add_weight(name='total_preds', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "        self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "        self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.correct_preds.assign(0.)\n",
    "        self.total_preds.assign(0.)\n",
    "\n",
    "class IntentPrecision(metrics.Metric):\n",
    "    def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "        self.predicted_positives = self.add_weight(name='pp', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        predicted_positives = tf.reduce_sum(y_pred)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.predicted_positives.assign(0.)\n",
    "\n",
    "class IntentRecall(metrics.Metric):\n",
    "    def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros', dtype=tf.float32)\n",
    "        self.actual_positives = self.add_weight(name='ap', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        actual_positives = tf.reduce_sum(y_true)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.actual_positives.assign(0.)\n",
    "\n",
    "class IntentF1(metrics.Metric):\n",
    "    def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = IntentPrecision(threshold=threshold)\n",
    "        self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()\n",
    "\n",
    "class DomainAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='domain_accuracy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.accuracy = self.add_weight(name='acc', initializer='zeros', dtype=tf.float32)\n",
    "        self.count = self.add_weight(name='count', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "        pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "        matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "        self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.accuracy.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "class Perplexity(metrics.Metric):\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "        self.total_loss = self.add_weight(name='total_loss', initializer='zeros', dtype=tf.float32)\n",
    "        self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "        loss = self.cross_entropy(y_true, y_pred)\n",
    "        masked_loss = loss * mask\n",
    "        sum_loss = tf.reduce_sum(masked_loss)\n",
    "        num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "        self.total_loss.assign_add(sum_loss)\n",
    "        self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "    def result(self):\n",
    "        avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "        return tf.exp(avg_loss)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_loss.assign(0.)\n",
    "        self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "    def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "        super().__init__(name=name)\n",
    "        self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "        y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "        cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "        non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "        non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "        masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "        num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "        avg_cosine_sim_per_sample = tf.where(\n",
    "            num_non_zero_tokens > 0,\n",
    "            tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "            tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "        )\n",
    "        self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_cosine_sim.assign(0.)\n",
    "        self.num_samples.assign(0.)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "    y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "    y_true = y_true / (y_true_norm + epsilon)\n",
    "    y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "    cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    positive_loss = 1.0 - cosine_sim\n",
    "    mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "    masked_loss = positive_loss * mask\n",
    "    total_loss = tf.reduce_sum(masked_loss)\n",
    "    num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "    return total_loss / num_tokens\n",
    "\n",
    "def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "    flops = 0\n",
    "    functional_model = model.build_graph()\n",
    "\n",
    "    def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "        layer_flops_count = 0\n",
    "\n",
    "        if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "            return 0\n",
    "\n",
    "        if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "            for sub_layer in layer_instance.layers:\n",
    "                layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "            return layer_flops_count\n",
    "\n",
    "        if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "            return 0\n",
    "\n",
    "        if isinstance(layer_instance, layers.Dense):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            output_dim = layer_instance.output_shape[-1]\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "        elif isinstance(layer_instance, layers.LSTM):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            hidden_dim = layer_instance.units\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "            layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "        elif isinstance(layer_instance, layers.Embedding):\n",
    "            pass\n",
    "\n",
    "        elif isinstance(layer_instance, MoELayer):\n",
    "            moe_input_dim = layer_instance.input_dim\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "            layer_flops_count += gate_flops\n",
    "            for expert in layer_instance.experts:\n",
    "                layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "        elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "            config = layer_instance.get_config()\n",
    "            num_heads = config['num_heads']\n",
    "            key_dim = config['key_dim']\n",
    "            d_model = num_heads * key_dim\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "            projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "            attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "            context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "            output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "            layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "        elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "            inner_layer = layer_instance.layer\n",
    "            td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "            layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "        elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "            ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "        return layer_flops_count\n",
    "\n",
    "    for layer in functional_model.layers:\n",
    "        flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "    return flops / 1e9\n",
    "\n",
    "def get_gpu_stats():\n",
    "    if nvidia_smi is None:\n",
    "        return 0, 0\n",
    "    try:\n",
    "        nvidia_smi.nvmlInit()\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "        mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_memory = mem_info.used / (1024 ** 2)\n",
    "        nvidia_smi.nvmlShutdown()\n",
    "        return gpu_load, gpu_memory / 1024\n",
    "    except Exception:\n",
    "        return 0, 0\n",
    "\n",
    "class ResourceMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.resource_stats = []\n",
    "        self.expert_load_history = []\n",
    "        self.start_time = None\n",
    "        self.epoch_start_time = None\n",
    "        self.flops = None\n",
    "        self.validation_data = validation_data\n",
    "        self.nlu_expert_usage_counts = None\n",
    "        self.nlg_expert_usage_counts = None\n",
    "        self.nlu_gate_weights_history = []\n",
    "        self.nlg_gate_weights_history = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        try:\n",
    "            self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "        except Exception:\n",
    "            self.flops = 0\n",
    "        try:\n",
    "            self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "            self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "        except Exception:\n",
    "            self.nlu_expert_usage_counts = np.zeros(0)\n",
    "            self.nlg_expert_usage_counts = np.zeros(0)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory()\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.resource_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'epoch_time': epoch_time,\n",
    "            'cpu_usage': cpu_usage,\n",
    "            'memory_used': memory.used / (1024 ** 3),\n",
    "            'memory_total': memory.total / (1024 ** 3),\n",
    "            'gpu_load': gpu_load,\n",
    "            'gpu_memory': gpu_memory,\n",
    "            'loss': logs.get('loss', 0),\n",
    "            'val_loss': logs.get('val_loss', 0),\n",
    "            'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "            'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "            'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "            'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "            'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "            'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "            'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "            'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "            'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "            'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "            'perplexity': logs.get('response_output_perplexity', 0),\n",
    "            'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "            'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "            'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "        })\n",
    "        if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "            self.expert_load_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "            })\n",
    "\n",
    "        sample_size = 100\n",
    "        for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "            inputs, _ = batch\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "            reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "            self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "            nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "            reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "            self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.total_time = time.time() - self.start_time\n",
    "\n",
    "        if len(self.nlu_gate_weights_history) > 0:\n",
    "            all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "            nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "            for expert_id in nlu_top_indices:\n",
    "                if expert_id < self.model.moe_nlu.total_experts:\n",
    "                    self.nlu_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        if len(self.nlg_gate_weights_history) > 0:\n",
    "            all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "            nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "            for expert_id in nlg_top_indices:\n",
    "                if expert_id < self.model.moe_nlg.total_experts:\n",
    "                    self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "    def visualize_expert_load_distribution(self):\n",
    "        total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "        total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "        nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "        nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "        nlu_stability = np.var(nlu_percentages)\n",
    "        nlg_stability = np.var(nlg_percentages)\n",
    "        return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"d_model\": self.d_model.numpy(),\n",
    "            \"warmup_steps\": self.warmup_steps\n",
    "        }\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "try:\n",
    "    possible_paths = [\n",
    "        os.path.join(\"preprocessor_models\", \"preprocessor_params.json\"),\n",
    "        os.path.join(\"tf_datasets\", \"preprocessor_params.json\"),\n",
    "        \"preprocessor_params.json\"\n",
    "    ]\n",
    "    \n",
    "    vocab_loaded = False\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                preprocessor_params = json.load(f)\n",
    "            word_to_id = {k: int(v) for k, v in preprocessor_params['word_to_id'].items()}\n",
    "            id_to_word = {int(k): v for k, v in preprocessor_params['id_to_word'].items()}\n",
    "            vocab_loaded = True\n",
    "            break\n",
    "    \n",
    "    if not vocab_loaded:\n",
    "        raise FileNotFoundError(\"Vocabulary file not found in any location\")\n",
    "except Exception as e:\n",
    "    word_to_id = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    id_to_word = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
    "\n",
    "model_save_path = \"best_DeepseekTopPExpertChoice_moe_model\"\n",
    "custom_objects = {\n",
    "    \"HybridRouting\":HybridRouting,\n",
    "    \"MoELayer\": MoELayer,\n",
    "    \"CustomLearningRateSchedule\": CustomLearningRateSchedule,\n",
    "    \"MoEModel\": MoEModel,\n",
    "    \"TransformerDecoderLayer\": TransformerDecoderLayer,\n",
    "    \"IntentAccuracy\": IntentAccuracy,\n",
    "    \"IntentPrecision\": IntentPrecision,\n",
    "    \"IntentRecall\": IntentRecall,\n",
    "    \"IntentF1\": IntentF1,\n",
    "    \"DomainAccuracy\": DomainAccuracy,\n",
    "    \"Perplexity\": Perplexity,\n",
    "    \"ResponseEmbeddingCosineSimilarity\": ResponseEmbeddingCosineSimilarity\n",
    "}\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "except Exception as e:\n",
    "    raise\n",
    "\n",
    "def compile_model(model, moe_params):\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': losses.BinaryCrossentropy(),\n",
    "        'domain_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={\n",
    "            'intent_output': 0.5,\n",
    "            'domain_output': 0.5,\n",
    "            'response_output': 1.0,\n",
    "            'response_embeddings': 1.0\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = compile_model(model, moe_params)\n",
    "sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "model(sample_input[0])\n",
    "model.summary()\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self, model, test_dataset, moe_params, raw_data, word_to_id, id_to_word):\n",
    "        self.model = model\n",
    "        self.test_dataset = test_dataset\n",
    "        self.moe_params = moe_params\n",
    "        self.raw_data = raw_data\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.nlu_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.nlg_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.resource_stats = {\n",
    "            'cpu_usage': [],\n",
    "            'memory_used': [],\n",
    "            'gpu_load': [],\n",
    "            'gpu_memory': [],\n",
    "            'batch_times': []\n",
    "        }\n",
    "        self.special_tokens = {'<pad>', '<sos>', '<eos>', '<unk>'}\n",
    "        self.nlu_top_k = min(model.get_layer('moe_nlu').routed_experts, 2)\n",
    "        self.nlg_top_k = min(model.get_layer('moe_nlg').routed_experts, 2)\n",
    "\n",
    "    def evaluate(self):\n",
    "        def add_response_embeddings(features, targets):\n",
    "            response_tokens = targets['response_output']\n",
    "            response_embeddings = self.model.embedding(response_tokens)\n",
    "            targets['response_embeddings'] = response_embeddings\n",
    "            return features, targets\n",
    "        \n",
    "        test_dataset_with_embeddings = self.test_dataset.map(add_response_embeddings, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "        start_time = time.time()\n",
    "        test_results = self.model.evaluate(test_dataset_with_embeddings, verbose=1)\n",
    "        eval_time = time.time() - start_time\n",
    "        \n",
    "        metric_names = self.model.metrics_names\n",
    "        \n",
    "        perplexity_value = None\n",
    "        for name, value in zip(metric_names, test_results):\n",
    "            if name == 'response_output_perplexity':\n",
    "                perplexity_value = value\n",
    "                break\n",
    "        if perplexity_value is None:\n",
    "            perplexity_value = 0.0\n",
    "        \n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory().used / (1024 ** 3)\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "\n",
    "        total_tokens = 0\n",
    "        total_time = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        test_dataset_small = test_dataset_with_embeddings.unbatch().batch(moe_params[\"batch_size\"])\n",
    "        \n",
    "        all_true_intents = []\n",
    "        all_pred_intents = []\n",
    "        all_true_domains = []\n",
    "        all_pred_domains = []\n",
    "        \n",
    "        for batch_idx, batch in enumerate(test_dataset_small):\n",
    "            inputs, targets = batch\n",
    "            response_tokens = targets['response_output'].numpy()\n",
    "            non_padding_mask = response_tokens != 0\n",
    "            total_tokens += np.sum(non_padding_mask)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            batch_time = time.time() - start_time\n",
    "            \n",
    "            total_time += batch_time\n",
    "            num_batches += tf.shape(inputs['user_utterance_tokens'])[0]\n",
    "            \n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            \n",
    "            self.resource_stats['cpu_usage'].append(cpu_usage)\n",
    "            self.resource_stats['memory_used'].append(memory.used / (1024 ** 3))\n",
    "            self.resource_stats['gpu_load'].append(gpu_load)\n",
    "            self.resource_stats['gpu_memory'].append(gpu_memory)\n",
    "            self.resource_stats['batch_times'].append(batch_time)\n",
    "            \n",
    "            nlu_gate_weights = outputs.get('nlu_gate_weights', tf.zeros([0, self.model.moe_nlu.routed_experts])).numpy()\n",
    "            nlu_top_indices = np.argsort(-nlu_gate_weights, axis=-1)[:, :self.nlu_top_k] + self.model.moe_nlu.k_s\n",
    "            for idx in range(self.nlu_top_k):\n",
    "                expert_ids = nlu_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlu_expert_usage_counts):\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "            \n",
    "            nlg_gate_weights = outputs.get('nlg_gate_weights', tf.zeros([0, self.model.moe_nlg.routed_experts])).numpy()\n",
    "            if len(nlg_gate_weights.shape) == 3:\n",
    "                nlg_gate_weights = nlg_gate_weights.reshape(-1, nlg_gate_weights.shape[-1])\n",
    "            nlg_top_indices = np.argsort(-nlg_gate_weights, axis=-1)[:, :self.nlg_top_k] + self.model.moe_nlg.k_s\n",
    "            for idx in range(self.nlg_top_k):\n",
    "                expert_ids = nlg_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlg_expert_usage_counts):\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "            \n",
    "            true_intents = targets.get('intent_output', np.zeros((moe_params[\"batch_size\"], self.model.num_intents))).numpy()\n",
    "            pred_intents = (outputs.get('intent_output', np.zeros_like(true_intents)).numpy() > 0.5).astype(np.int32)\n",
    "            all_true_intents.append(true_intents)\n",
    "            all_pred_intents.append(pred_intents)\n",
    "            \n",
    "            true_domains = targets.get('domain_output', np.zeros((moe_params[\"batch_size\"], 1))).numpy().flatten()\n",
    "            pred_domains = np.argmax(outputs.get('domain_output', np.zeros((moe_params[\"batch_size\"], self.model.num_domains))).numpy(), axis=-1)\n",
    "            all_true_domains.append(true_domains)\n",
    "            all_pred_domains.append(pred_domains)\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                gc.collect()\n",
    "                tf.keras.backend.clear_session()\n",
    "        \n",
    "        avg_time_per_token = (total_time / total_tokens) * 1000 if total_tokens > 0 else 0\n",
    "        \n",
    "        avg_cpu = np.mean(self.resource_stats['cpu_usage']) if self.resource_stats['cpu_usage'] else 0\n",
    "        avg_memory = np.mean(self.resource_stats['memory_used']) if self.resource_stats['memory_used'] else 0\n",
    "        avg_gpu_load = np.mean(self.resource_stats['gpu_load']) if self.resource_stats['gpu_load'] else 0\n",
    "        avg_gpu_memory = np.mean(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        peak_gpu_memory = np.max(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        \n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "        \n",
    "        all_true_intents = np.vstack(all_true_intents)\n",
    "        all_pred_intents = np.vstack(all_pred_intents)\n",
    "        all_true_domains = np.concatenate(all_true_domains)\n",
    "        all_pred_domains = np.concatenate(all_pred_domains)\n",
    "        \n",
    "        intent_f1_score = f1_score(all_true_intents, all_pred_intents, average='micro')\n",
    "        intent_f1_score_macro = f1_score(all_true_intents, all_pred_intents, average='macro')\n",
    "        intent_f1_score_weighted = f1_score(all_true_intents, all_pred_intents, average='weighted')\n",
    "        intent_accuracy_score = accuracy_score(all_true_intents, all_pred_intents)\n",
    "        \n",
    "        domain_accuracy_score = accuracy_score(all_true_domains, all_pred_domains)\n",
    "        domain_f1_score_macro = f1_score(all_true_domains, all_pred_domains, average='macro')\n",
    "\n",
    "        hypotheses = []\n",
    "        references = []\n",
    "        meteor_scores = []\n",
    "        rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "        domain_metrics = defaultdict(lambda: {\n",
    "            'bleu': [],\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'meteor': [],\n",
    "            'count': 0\n",
    "        })\n",
    "        \n",
    "        if not self.raw_data.get('test'):\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "            domain_metrics = {}\n",
    "        else:\n",
    "            for i, batch in enumerate(test_dataset_small):\n",
    "                if i >= len(self.raw_data.get('test', [])):\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    inputs, targets = batch\n",
    "                    with tf.device('/CPU:0'):\n",
    "                        outputs = self.model(inputs, training=False)\n",
    "                    \n",
    "                    domain_id = targets['domain_output'].numpy()[0][0]\n",
    "                    domain_name = f\"domain_{domain_id}\"\n",
    "                    \n",
    "                    response_probs = outputs['response_output'].numpy()\n",
    "                    generated_tokens = np.argmax(response_probs, axis=-1)[0]\n",
    "                    true_tokens = targets['response_output'].numpy()[0]\n",
    "                    \n",
    "                    raw_item = self.raw_data['test'][i]\n",
    "                    ref_text = raw_item.get('raw_next_system_response', '')\n",
    "                    if not ref_text:\n",
    "                        continue\n",
    "                    \n",
    "                    gen_text = self.tokens_to_text(generated_tokens)\n",
    "                    ref_tokens = self.tokens_to_text(true_tokens)\n",
    "                    \n",
    "                    if not gen_text.strip() or not ref_text.strip():\n",
    "                        continue\n",
    "                    \n",
    "                    hypotheses.append(gen_text)\n",
    "                    references.append([ref_text])\n",
    "                    \n",
    "                    ref_tokens = word_tokenize(ref_text.lower())\n",
    "                    gen_tokens = word_tokenize(gen_text.lower())\n",
    "                    \n",
    "                    try:\n",
    "                        meteor = meteor_score([ref_tokens], gen_tokens)\n",
    "                        meteor_scores.append(meteor)\n",
    "                        domain_metrics[domain_name]['meteor'].append(meteor)\n",
    "                    except Exception as e:\n",
    "                        meteor_scores.append(0)\n",
    "                        domain_metrics[domain_name]['meteor'].append(0)\n",
    "                    \n",
    "                    try:\n",
    "                        rouge_scores = rouge_scorer_instance.score(ref_text, gen_text)\n",
    "                        domain_metrics[domain_name]['rouge1'].append(rouge_scores['rouge1'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rouge2'].append(rouge_scores['rouge2'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rougeL'].append(rouge_scores['rougeL'].fmeasure)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                    \n",
    "                    domain_metrics[domain_name]['count'] += 1\n",
    "                    \n",
    "                    if i % 50 == 0:\n",
    "                        gc.collect()\n",
    "                        tf.keras.backend.clear_session()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        if hypotheses and references:\n",
    "            hypotheses_tok = []\n",
    "            references_tok = []\n",
    "            \n",
    "            for hyp, ref_list in zip(hypotheses, references):\n",
    "                if not hyp or not ref_list[0]:\n",
    "                    continue\n",
    "                hyp_tokens = word_tokenize(hyp.lower())\n",
    "                ref_tokens = [word_tokenize(ref.lower()) for ref in ref_list]\n",
    "                hypotheses_tok.append(hyp_tokens)\n",
    "                references_tok.append(ref_tokens)\n",
    "            \n",
    "            chencherry = SmoothingFunction()\n",
    "            try:\n",
    "                bleu_score = corpus_bleu(references_tok, hypotheses_tok, smoothing_function=chencherry.method1)\n",
    "            except Exception as e:\n",
    "                bleu_score = 0.0\n",
    "            \n",
    "            rouge_scores = []\n",
    "            for hyp, ref in zip(hypotheses, references):\n",
    "                if hyp and ref[0]:\n",
    "                    try:\n",
    "                        rouge_scores.append(rouge_scorer_instance.score(ref[0], hyp))\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "            \n",
    "            avg_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_meteor = np.mean(meteor_scores) if meteor_scores else 0.0\n",
    "        else:\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "\n",
    "        return {\n",
    "            'intent_f1_micro': intent_f1_score,\n",
    "            'intent_f1_macro': intent_f1_score_macro,\n",
    "            'intent_f1_weighted': intent_f1_score_weighted,\n",
    "            'intent_accuracy': intent_accuracy_score,\n",
    "            'domain_accuracy': domain_accuracy_score,\n",
    "            'domain_f1_macro': domain_f1_score_macro,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge1': avg_rouge1,\n",
    "            'rouge2': avg_rouge2,\n",
    "            'rougeL': avg_rougeL,\n",
    "            'meteor': avg_meteor,\n",
    "            'perplexity': perplexity_value,\n",
    "            'pred_speed': avg_time_per_token,\n",
    "            'domain_metrics': domain_metrics,\n",
    "            'eval_time': eval_time\n",
    "        }\n",
    "    \n",
    "    def tokens_to_text(self, tokens):\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            if token == 0:\n",
    "                continue\n",
    "            word = self.id_to_word.get(token, '<unk>')\n",
    "            if word not in self.special_tokens:\n",
    "                words.append(word)\n",
    "        return \" \".join(words).strip()\n",
    "\n",
    "results_table = {\n",
    "    'Evaluation Time (seconds)': [],\n",
    "    'Prediction Speed (ms/token)': [],\n",
    "    'Average CPU Usage (percent)': [],\n",
    "    'Average GPU Usage (percent)': [],\n",
    "    'Average Memory (GB)': [],\n",
    "    'Average GPU Memory (GB)': [],\n",
    "    'Average FLOPs Estimate (GFLOPs)': [],\n",
    "    'NLU Expert 1 (percent)': [],\n",
    "    'NLU Expert 2 (percent)': [],\n",
    "    'NLU Expert 3 (percent)': [],\n",
    "    'NLU Expert 4 (percent)': [],\n",
    "    'NLU Expert 5 (percent)': [],\n",
    "    'NLU Expert 6 (percent)': [],\n",
    "    'NLU Expert 7 (percent)': [],\n",
    "    'NLU Expert 8 (percent)': [],\n",
    "    'NLG Expert 1 (percent)': [],\n",
    "    'NLG Expert 2 (percent)': [],\n",
    "    'NLG Expert 3 (percent)': [],\n",
    "    'NLG Expert 4 (percent)': [],\n",
    "    'NLG Expert 5 (percent)': [],\n",
    "    'NLG Expert 6 (percent)': [],\n",
    "    'NLG Expert 7 (percent)': [],\n",
    "    'NLG Expert 8 (percent)': [],\n",
    "    'Intent F1-score (Micro)': [],\n",
    "    'Intent F1-score (Macro)': [],\n",
    "    'Intent F1-score (Weighted)': [],\n",
    "    'Intent Accuracy': [],\n",
    "    'Domain Accuracy': [],\n",
    "    'Domain F1-score (Macro)': [],\n",
    "    'BLEU Score': [],\n",
    "    'ROUGE-1 F1': [],\n",
    "    'ROUGE-2 F1': [],\n",
    "    'ROUGE-L F1': [],\n",
    "    'METEOR Score': [],\n",
    "    'Perplexity': [],\n",
    "    'NLU Expert Load Stability (Variance)': [],\n",
    "    'NLG Expert Load Stability (Variance)': []\n",
    "}\n",
    "\n",
    "num_iterations = 5\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "    model = compile_model(model, moe_params)\n",
    "    \n",
    "    evaluator = EvaluationMetrics(model, test_tf_dataset, moe_params, raw_data, word_to_id, id_to_word)\n",
    "\n",
    "    evaluation_results = evaluator.evaluate()\n",
    "    results_table['Evaluation Time (seconds)'].append(evaluation_results['eval_time'])\n",
    "    results_table['Prediction Speed (ms/token)'].append(evaluation_results['pred_speed'])\n",
    "    results_table['Average CPU Usage (percent)'].append(np.mean(evaluator.resource_stats['cpu_usage']) if evaluator.resource_stats['cpu_usage'] else 0)\n",
    "    results_table['Average GPU Usage (percent)'].append(np.mean(evaluator.resource_stats['gpu_load']) if evaluator.resource_stats['gpu_load'] else 0)\n",
    "    results_table['Average Memory (GB)'].append(np.mean(evaluator.resource_stats['memory_used']) if evaluator.resource_stats['memory_used'] else 0)\n",
    "    results_table['Average GPU Memory (GB)'].append(np.mean(evaluator.resource_stats['gpu_memory']) if evaluator.resource_stats['gpu_memory'] else 0)\n",
    "    results_table['Average FLOPs Estimate (GFLOPs)'].append(calculate_flops(model))\n",
    "    nlu_counts = evaluator.nlu_expert_usage_counts\n",
    "    nlg_counts = evaluator.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) if np.sum(nlu_counts) > 0 else 1\n",
    "    total_nlg = np.sum(nlg_counts) if np.sum(nlg_counts) > 0 else 1\n",
    "    results_table['NLU Expert 1 (percent)'].append((nlu_counts[0] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 2 (percent)'].append((nlu_counts[1] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 3 (percent)'].append((nlu_counts[2] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 4 (percent)'].append((nlu_counts[3] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 5 (percent)'].append((nlu_counts[4] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 6 (percent)'].append((nlu_counts[5] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 7 (percent)'].append((nlu_counts[6] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 8 (percent)'].append((nlu_counts[7] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLG Expert 1 (percent)'].append((nlg_counts[0] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 2 (percent)'].append((nlg_counts[1] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 3 (percent)'].append((nlg_counts[2] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 4 (percent)'].append((nlg_counts[3] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 5 (percent)'].append((nlg_counts[4] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 6 (percent)'].append((nlg_counts[5] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 7 (percent)'].append((nlg_counts[6] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 8 (percent)'].append((nlg_counts[7] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['Intent F1-score (Micro)'].append(evaluation_results['intent_f1_micro'])\n",
    "    results_table['Intent F1-score (Macro)'].append(evaluation_results['intent_f1_macro'])\n",
    "    results_table['Intent F1-score (Weighted)'].append(evaluation_results['intent_f1_weighted'])\n",
    "    results_table['Intent Accuracy'].append(evaluation_results['intent_accuracy'])\n",
    "    results_table['Domain Accuracy'].append(evaluation_results['domain_accuracy'])\n",
    "    results_table['Domain F1-score (Macro)'].append(evaluation_results['domain_f1_macro'])\n",
    "    results_table['BLEU Score'].append(evaluation_results['bleu'])\n",
    "    results_table['ROUGE-1 F1'].append(evaluation_results['rouge1'])\n",
    "    results_table['ROUGE-2 F1'].append(evaluation_results['rouge2'])\n",
    "    results_table['ROUGE-L F1'].append(evaluation_results['rougeL'])\n",
    "    results_table['METEOR Score'].append(evaluation_results['meteor'])\n",
    "    results_table['Perplexity'].append(evaluation_results['perplexity'])\n",
    "    results_table['NLU Expert Load Stability (Variance)'].append(np.var(nlu_counts / total_nlu * 100) if total_nlu > 0 else 0)\n",
    "    results_table['NLG Expert Load Stability (Variance)'].append(np.var(nlg_counts / total_nlg * 100) if total_nlg > 0 else 0)\n",
    "\n",
    "for key in results_table:\n",
    "    if results_table[key]:\n",
    "        results_table[key].append(np.mean(results_table[key]))\n",
    "    else:\n",
    "        results_table[key].append(0)\n",
    "\n",
    "final_df = pd.DataFrame(results_table)\n",
    "final_df = final_df.T\n",
    "final_df.columns = [f'Iteration {i+1}' for i in range(num_iterations)] + ['Average']\n",
    "final_df.to_excel('prediction_DeepseekTopPExpertChoic_results.xlsx', index=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE with Adaptive Gating with Expert Choice Balance experiment code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:55:20.283266: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 08:55:20.656588: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 08:55:20.656709: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 08:55:20.724104: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 08:55:20.848866: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 08:55:22.228469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training Iteration 1\n",
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:55:25.345460: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:55:25.549050: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:55:25.549286: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:55:25.550719: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:55:25.550890: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:55:25.551012: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:55:25.628743: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:55:25.628917: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:55:25.629025: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:55:25.629105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/4428 [00:00<?, ?it/s]2025-07-04 08:55:50.030964: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fb32418b870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-07-04 08:55:50.031294: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2025-07-04 08:55:50.060632: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-07-04 08:55:50.114444: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751619350.245696   23276 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 1: 100%|██████████| 4428/4428 [02:48<00:00, 26.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deepseekmoe_adaptive_gating_expert_choice_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_deepseekmoe_adaptive_gating_expert_choice_model/assets\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Iteration 2</th>\n",
       "      <th>Iteration 3</th>\n",
       "      <th>Iteration 4</th>\n",
       "      <th>Iteration 5</th>\n",
       "      <th>Iteration 6</th>\n",
       "      <th>Iteration 7</th>\n",
       "      <th>Iteration 8</th>\n",
       "      <th>Iteration 9</th>\n",
       "      <th>Iteration 10</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Speed (epochs per second)</th>\n",
       "      <td>0.006088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training Time (second/epoch)</th>\n",
       "      <td>164.254727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.425473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Training Time (second/iteration)</th>\n",
       "      <td>169.155386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.915539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computational Resource Usage</th>\n",
       "      <td>49.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>26.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>6.567524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.656752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.529846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.452985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPS Estimate (GFLOPS)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLU Load Distribution Stability</th>\n",
       "      <td>480.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert NLG Load Distribution Stability</th>\n",
       "      <td>246.099410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.609941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 3 (percent)</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 4 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 5 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 6 (percent)</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 7 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLU Expert 8 (percent)</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 3 (percent)</th>\n",
       "      <td>17.835821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.783582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 4 (percent)</th>\n",
       "      <td>11.656716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.165672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 5 (percent)</th>\n",
       "      <td>7.343284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.734328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 6 (percent)</th>\n",
       "      <td>51.388060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.138806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 7 (percent)</th>\n",
       "      <td>6.522388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.652239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average NLG Expert 8 (percent)</th>\n",
       "      <td>5.253731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Entity Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Intent Accuracy</th>\n",
       "      <td>0.746612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.074661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Perplexity</th>\n",
       "      <td>18.530523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.853052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Response Cosine Similarity</th>\n",
       "      <td>0.458661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Iteration 1  Iteration 2  \\\n",
       "Training Speed (epochs per second)                0.006088          0.0   \n",
       "Training Time (second/epoch)                    164.254727          0.0   \n",
       "Total Training Time (second/iteration)          169.155386          0.0   \n",
       "Computational Resource Usage                     49.500000          0.0   \n",
       "Average CPU Usage (percent)                      26.500000          0.0   \n",
       "Average GPU Usage (percent)                      23.000000          0.0   \n",
       "Average Memory (GB)                               6.567524          0.0   \n",
       "Average GPU Memory (GB)                          14.529846          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                   2.377871          0.0   \n",
       "Expert NLU Load Distribution Stability          480.750000          0.0   \n",
       "Expert NLG Load Distribution Stability          246.099410          0.0   \n",
       "Average NLU Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 3 (percent)                    8.000000          0.0   \n",
       "Average NLU Expert 4 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 5 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 6 (percent)                   66.000000          0.0   \n",
       "Average NLU Expert 7 (percent)                    0.000000          0.0   \n",
       "Average NLU Expert 8 (percent)                   26.000000          0.0   \n",
       "Average NLG Expert 1 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 2 (percent)                    0.000000          0.0   \n",
       "Average NLG Expert 3 (percent)                   17.835821          0.0   \n",
       "Average NLG Expert 4 (percent)                   11.656716          0.0   \n",
       "Average NLG Expert 5 (percent)                    7.343284          0.0   \n",
       "Average NLG Expert 6 (percent)                   51.388060          0.0   \n",
       "Average NLG Expert 7 (percent)                    6.522388          0.0   \n",
       "Average NLG Expert 8 (percent)                    5.253731          0.0   \n",
       "Average Validation Entity Accuracy                1.000000          0.0   \n",
       "Average Intent Accuracy                           0.746612          0.0   \n",
       "Average Validation Perplexity                    18.530523          0.0   \n",
       "Average Validation Response Cosine Similarity     0.458661          0.0   \n",
       "\n",
       "                                               Iteration 3  Iteration 4  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 5  Iteration 6  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 7  Iteration 8  \\\n",
       "Training Speed (epochs per second)                     0.0          0.0   \n",
       "Training Time (second/epoch)                           0.0          0.0   \n",
       "Total Training Time (second/iteration)                 0.0          0.0   \n",
       "Computational Resource Usage                           0.0          0.0   \n",
       "Average CPU Usage (percent)                            0.0          0.0   \n",
       "Average GPU Usage (percent)                            0.0          0.0   \n",
       "Average Memory (GB)                                    0.0          0.0   \n",
       "Average GPU Memory (GB)                                0.0          0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0          0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0          0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0          0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0          0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0          0.0   \n",
       "Average Validation Entity Accuracy                     0.0          0.0   \n",
       "Average Intent Accuracy                                0.0          0.0   \n",
       "Average Validation Perplexity                          0.0          0.0   \n",
       "Average Validation Response Cosine Similarity          0.0          0.0   \n",
       "\n",
       "                                               Iteration 9  Iteration 10  \\\n",
       "Training Speed (epochs per second)                     0.0           0.0   \n",
       "Training Time (second/epoch)                           0.0           0.0   \n",
       "Total Training Time (second/iteration)                 0.0           0.0   \n",
       "Computational Resource Usage                           0.0           0.0   \n",
       "Average CPU Usage (percent)                            0.0           0.0   \n",
       "Average GPU Usage (percent)                            0.0           0.0   \n",
       "Average Memory (GB)                                    0.0           0.0   \n",
       "Average GPU Memory (GB)                                0.0           0.0   \n",
       "Average FLOPS Estimate (GFLOPS)                        0.0           0.0   \n",
       "Expert NLU Load Distribution Stability                 0.0           0.0   \n",
       "Expert NLG Load Distribution Stability                 0.0           0.0   \n",
       "Average NLU Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLU Expert 8 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 1 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 2 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 3 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 4 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 5 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 6 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 7 (percent)                         0.0           0.0   \n",
       "Average NLG Expert 8 (percent)                         0.0           0.0   \n",
       "Average Validation Entity Accuracy                     0.0           0.0   \n",
       "Average Intent Accuracy                                0.0           0.0   \n",
       "Average Validation Perplexity                          0.0           0.0   \n",
       "Average Validation Response Cosine Similarity          0.0           0.0   \n",
       "\n",
       "                                                 Average  \n",
       "Training Speed (epochs per second)              0.000609  \n",
       "Training Time (second/epoch)                   16.425473  \n",
       "Total Training Time (second/iteration)         16.915539  \n",
       "Computational Resource Usage                    4.950000  \n",
       "Average CPU Usage (percent)                     2.650000  \n",
       "Average GPU Usage (percent)                     2.300000  \n",
       "Average Memory (GB)                             0.656752  \n",
       "Average GPU Memory (GB)                         1.452985  \n",
       "Average FLOPS Estimate (GFLOPS)                 0.237787  \n",
       "Expert NLU Load Distribution Stability         48.075000  \n",
       "Expert NLG Load Distribution Stability         24.609941  \n",
       "Average NLU Expert 1 (percent)                  0.000000  \n",
       "Average NLU Expert 2 (percent)                  0.000000  \n",
       "Average NLU Expert 3 (percent)                  0.800000  \n",
       "Average NLU Expert 4 (percent)                  0.000000  \n",
       "Average NLU Expert 5 (percent)                  0.000000  \n",
       "Average NLU Expert 6 (percent)                  6.600000  \n",
       "Average NLU Expert 7 (percent)                  0.000000  \n",
       "Average NLU Expert 8 (percent)                  2.600000  \n",
       "Average NLG Expert 1 (percent)                  0.000000  \n",
       "Average NLG Expert 2 (percent)                  0.000000  \n",
       "Average NLG Expert 3 (percent)                  1.783582  \n",
       "Average NLG Expert 4 (percent)                  1.165672  \n",
       "Average NLG Expert 5 (percent)                  0.734328  \n",
       "Average NLG Expert 6 (percent)                  5.138806  \n",
       "Average NLG Expert 7 (percent)                  0.652239  \n",
       "Average NLG Expert 8 (percent)                  0.525373  \n",
       "Average Validation Entity Accuracy              0.100000  \n",
       "Average Intent Accuracy                         0.074661  \n",
       "Average Validation Perplexity                   1.853052  \n",
       "Average Validation Response Cosine Similarity   0.045866  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reset_kernel():\n",
    "    tf.keras.backend.clear_session()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "results_table = {\n",
    "    'Training Speed (epochs per second)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Training Time (second/epoch)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Total Training Time (second/iteration)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Computational Resource Usage': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average CPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average GPU Usage (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average GPU Memory (GB)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average FLOPS Estimate (GFLOPS)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Expert NLU Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Expert NLG Load Distribution Stability': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLU Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 1 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 2 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 3 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 4 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 5 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 6 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 7 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average NLG Expert 8 (percent)': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average Validation Entity Accuracy': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average Intent Accuracy': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average Validation Perplexity': {f'Iteration {i+1}': 0 for i in range(10)},\n",
    "    'Average Validation Response Cosine Similarity': {f'Iteration {i+1}': 0 for i in range(10)}\n",
    "}\n",
    "for key in results_table:\n",
    "    results_table[key]['Average'] = 0\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_deepseekmoe_adaptive_gating_expert_choice_model\"\n",
    "\n",
    "for iteration in range(5):\n",
    "    print(f\"\\nStarting Training Iteration {iteration + 1}\")\n",
    "    reset_kernel()\n",
    "\n",
    "    def load_tf_datasets_from_disk(load_path):\n",
    "        print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "        try:\n",
    "            with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "                loaded_moe_params = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "        element_spec = (\n",
    "            {\n",
    "                'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            },\n",
    "            {\n",
    "                'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "                'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "                'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "            test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "        raw_data = {}\n",
    "        for dataset_name in ['train', 'test']:\n",
    "            path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "            if os.path.exists(path):\n",
    "                with open(path, 'rb') as f:\n",
    "                    raw_data[dataset_name] = pickle.load(f)\n",
    "                print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "            else:\n",
    "                print(f\"Warning: Raw data file {path} not found.\")\n",
    "                raw_data[dataset_name] = []\n",
    "\n",
    "        return {\n",
    "            \"train_dataset\": train_tf_dataset,\n",
    "            \"test_dataset\": test_tf_dataset,\n",
    "            \"moe_params\": loaded_moe_params,\n",
    "            \"raw_data\": raw_data\n",
    "        }\n",
    "\n",
    "    tf_dataset_save_path = \"tf_datasets\"\n",
    "    loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "    train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "    moe_params = loaded_data[\"moe_params\"]\n",
    "    raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "    class DeepSeekMoELayer(layers.Layer):\n",
    "        def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, tau=0.1, name=None):\n",
    "            super(DeepSeekMoELayer, self).__init__(name=name)\n",
    "            self.m = m\n",
    "            self.k_s = k_s\n",
    "            self.total_experts = num_experts * self.m\n",
    "            self.routed_experts = self.total_experts - self.k_s\n",
    "            self.alpha = alpha\n",
    "            self.tau = tau\n",
    "            self.input_dim = input_dim\n",
    "            self.seq_length = None\n",
    "            self.adjusted_expert_dim = expert_dim // self.m\n",
    "\n",
    "            self.shared_experts = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                    layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "                ], name=f'shared_expert_{i}') for i in range(self.k_s)\n",
    "            ]\n",
    "            self.routed_experts_list = [\n",
    "                keras.Sequential([\n",
    "                    layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                    layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "                ], name=f'routed_expert_{i}') for i in range(self.routed_experts)\n",
    "            ]\n",
    "            self.experts = self.shared_experts + self.routed_experts_list\n",
    "            self.gate = layers.Dense(self.routed_experts, activation=None, name='gate_weights')\n",
    "            self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            input_rank = inputs.shape.rank\n",
    "            if input_rank == 3:\n",
    "                batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "                flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "                is_3d = True\n",
    "            else:\n",
    "                batch_size = tf.shape(inputs)[0]\n",
    "                seq_length = 1\n",
    "                flat_inputs = inputs\n",
    "                is_3d = False\n",
    "\n",
    "            flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "            num_tokens = tf.shape(flat_inputs)[0]\n",
    "\n",
    "            shared_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "            for i in range(self.k_s):\n",
    "                shared_output += self.shared_experts[i](flat_inputs)\n",
    "\n",
    "            gate_logits = self.gate(flat_inputs)\n",
    "            gate_weights = tf.nn.softmax(gate_logits, axis=-1)\n",
    "            sorted_weights = tf.sort(gate_weights, axis=-1, direction='DESCENDING')\n",
    "            p1 = sorted_weights[:, 0]\n",
    "            p2 = sorted_weights[:, 1]\n",
    "            use_top_1 = tf.cast(p1 - p2 >= self.tau, tf.float32)\n",
    "            k_per_token = tf.where(use_top_1 > 0, 1, 2)\n",
    "\n",
    "            max_k = 2\n",
    "            top_k_values, top_k_indices = tf.nn.top_k(gate_weights, k=max_k, sorted=True)\n",
    "            top_k_indices = top_k_indices + self.k_s\n",
    "            top_k_weights = top_k_values / (tf.reduce_sum(top_k_values, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "\n",
    "            k_mask = tf.range(max_k, dtype=tf.int32) < tf.expand_dims(k_per_token, -1)\n",
    "            k_mask = tf.cast(k_mask, tf.float32)\n",
    "            top_k_weights = top_k_weights * k_mask\n",
    "            top_k_weights = top_k_weights / (tf.reduce_sum(top_k_weights, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "\n",
    "            expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "            routed_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "            for k in range(max_k):\n",
    "                kth_weights = top_k_weights[:, k]\n",
    "                kth_indices = top_k_indices[:, k]\n",
    "                mask = tf.one_hot(kth_indices, depth=self.total_experts, dtype=tf.float32)\n",
    "                kth_expert_output = tf.reduce_sum(expert_outputs * tf.expand_dims(mask, -1), axis=1)\n",
    "                weighted_kth_output = kth_expert_output * tf.expand_dims(kth_weights, -1)\n",
    "                routed_output += weighted_kth_output\n",
    "\n",
    "            output_flat = shared_output + routed_output + flat_inputs\n",
    "\n",
    "            if is_3d:\n",
    "                output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "                if self.seq_length is not None:\n",
    "                    output.set_shape([None, self.seq_length, self.input_dim])\n",
    "                gate_weights_reshaped = tf.reshape(gate_weights, [batch_size, seq_length, self.routed_experts])\n",
    "                if self.seq_length is not None:\n",
    "                    gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "            else:\n",
    "                output = output_flat\n",
    "                gate_weights_reshaped = gate_weights\n",
    "\n",
    "            f_i = tf.reduce_mean(tf.reduce_sum(tf.one_hot(tf.argmax(gate_weights, axis=-1), depth=self.routed_experts), axis=0))\n",
    "            P_i = tf.reduce_mean(gate_weights, axis=0)\n",
    "            load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "            self.add_loss(load_balancing_loss)\n",
    "            self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "\n",
    "            return output, gate_weights_reshaped\n",
    "\n",
    "        def get_metrics(self):\n",
    "            return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'num_experts': self.total_experts // self.m,\n",
    "                'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "                'input_dim': self.input_dim,\n",
    "                'm': self.m,\n",
    "                'k_s': self.k_s,\n",
    "                'alpha': self.alpha,\n",
    "                'tau': self.tau,\n",
    "                'name': self.name\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class TransformerDecoderLayer(layers.Layer):\n",
    "        def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "            super(TransformerDecoderLayer, self).__init__()\n",
    "            self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "            self.ffn = keras.Sequential([\n",
    "                layers.Dense(dff, activation='relu'),\n",
    "                layers.Dense(d_model)\n",
    "            ])\n",
    "            self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.dropout1 = layers.Dropout(rate)\n",
    "            self.dropout2 = layers.Dropout(rate)\n",
    "            self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "        def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "            if look_ahead_mask is not None:\n",
    "                look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "                look_ahead_mask = 1.0 - look_ahead_mask\n",
    "            attn1_output = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)\n",
    "            attn1 = self.dropout1(attn1_output, training=training)\n",
    "            out1 = self.layernorm1(attn1 + x)\n",
    "            attn2_output = self.mha2(query=out1, value=enc_output, key=enc_output, training=training)\n",
    "            attn2 = self.dropout2(attn2_output, training=training)\n",
    "            out2 = self.layernorm2(attn2 + out1)\n",
    "            ffn_output = self.ffn(out2)\n",
    "            ffn_output = self.dropout3(ffn_output, training=training)\n",
    "            out3 = self.layernorm3(ffn_output + out2)\n",
    "            return out3\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            mha1_config = self.mha1.get_config()\n",
    "            config.update({\n",
    "                'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "                'num_heads': mha1_config['num_heads'],\n",
    "                'dff': self.ffn.layers[0].units,\n",
    "                'rate': self.dropout1.rate\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class MoEModel(models.Model):\n",
    "        def __init__(self, moe_params):\n",
    "            super(MoEModel, self).__init__()\n",
    "            self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "            self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "            self.num_domains = moe_params[\"num_domains\"]\n",
    "            self.num_intents = moe_params[\"num_intents\"]\n",
    "            self.vocab_size = moe_params[\"vocab_size\"]\n",
    "            self.num_experts = moe_params[\"num_experts\"]\n",
    "            self.expert_dim = moe_params[\"expert_dim\"]\n",
    "            self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "            self.num_heads = 4\n",
    "            self.dff = self.embedding_dim * 4\n",
    "            self.dropout_rate = 0.1\n",
    "            self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                                 self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "            self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "            self.embedding = layers.Embedding(\n",
    "                self.vocab_size,\n",
    "                self.embedding_dim,\n",
    "                mask_zero=True,\n",
    "                embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "                name=\"embedding\"\n",
    "            )\n",
    "            self.embedding.build((None,))\n",
    "            with tf.init_scope():\n",
    "                embedding_weights = self.embedding.get_weights()\n",
    "                if embedding_weights and len(embedding_weights) > 0:\n",
    "                    embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                    self.embedding.set_weights(embedding_weights)\n",
    "            self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "            self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "            self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "            self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "            self.moe_nlu = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlu_input_dim, m=2, k_s=2, alpha=0.01, tau=0.1, name='moe_nlu')\n",
    "            self.moe_nlg = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlg_input_dim, m=2, k_s=2, alpha=0.01, tau=0.1, name='moe_nlg')\n",
    "            self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "            self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "            self.response_output = layers.TimeDistributed(layers.Dense(self.vocab_size, activation='softmax'), name='response_output')\n",
    "            self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "\n",
    "        def create_look_ahead_mask(self, size):\n",
    "            mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "            return mask\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "            prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "            decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "            domain_onehot_input = inputs['domain_onehot_input']\n",
    "            turn_id_embedding = inputs['turn_id_embedding']\n",
    "            ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "            pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "            user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "            prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "            decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "            user_enc_out = user_embedded\n",
    "            prev_system_enc_out = prev_system_embedded\n",
    "            look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "            dec_output = decoder_embedded\n",
    "            for i, layer in enumerate(self.decoder_layers):\n",
    "                dec_output = layer(dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask)\n",
    "            decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "            user_attn_out = self.attn_layer(query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), value=user_enc_out, key=user_enc_out, training=training)\n",
    "            prev_system_attn_out = self.attn_layer(query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), value=prev_system_enc_out, key=prev_system_enc_out, training=training)\n",
    "            decoder_attn_out = self.attn_layer(query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), value=decoder_output, key=decoder_output, training=training)\n",
    "            combined_features = layers.Concatenate()([\n",
    "                tf.squeeze(user_attn_out, 1),\n",
    "                tf.squeeze(prev_system_attn_out, 1),\n",
    "                tf.squeeze(decoder_attn_out, 1),\n",
    "                domain_onehot_input,\n",
    "                turn_id_embedding,\n",
    "                ontology_multihot_input\n",
    "            ])\n",
    "            combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "            nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "            nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "            intent_out = self.intent_output(nlu_out)\n",
    "            domain_out = self.domain_output(nlu_out)\n",
    "            intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "            domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "            intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "            domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "            nlg_input = layers.Concatenate(axis=-1)([\n",
    "                decoder_output,\n",
    "                intent_out_tiled,\n",
    "                domain_out_tiled\n",
    "            ])\n",
    "            nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "            nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "            response_out = self.response_output(nlg_out)\n",
    "            return {\n",
    "                'intent_output': intent_out,\n",
    "                'domain_output': domain_out,\n",
    "                'response_output': response_out,\n",
    "                'response_embeddings': decoder_output,\n",
    "                'nlu_gate_weights': nlu_gate_weights,\n",
    "                'nlg_gate_weights': nlg_gate_weights\n",
    "            }\n",
    "\n",
    "        def build_graph(self):\n",
    "            inputs = {\n",
    "                'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "                'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='prev_system_response_tokens'),\n",
    "                'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='decoder_input_tokens'),\n",
    "                'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32, name='domain_onehot_input'),\n",
    "                'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32, name='turn_id_embedding'),\n",
    "                'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32, name='ontology_multihot_input'),\n",
    "            }\n",
    "            return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'moe_params': {\n",
    "                    'embedding_dim': self.embedding_dim,\n",
    "                    'max_seq_length': self.max_seq_length,\n",
    "                    'num_domains': self.num_domains,\n",
    "                    'num_intents': self.num_intents,\n",
    "                    'vocab_size': self.vocab_size,\n",
    "                    'num_experts': self.num_experts,\n",
    "                    'expert_dim': self.expert_dim,\n",
    "                    'turn_id_dim': self.turn_id_dim\n",
    "                }\n",
    "            })\n",
    "            return config\n",
    "\n",
    "    class IntentAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros')\n",
    "            self.total_preds = self.add_weight(name='total_preds', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "            self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "            self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.correct_preds.assign(0.)\n",
    "            self.total_preds.assign(0.)\n",
    "\n",
    "    class IntentPrecision(metrics.Metric):\n",
    "        def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "            self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            predicted_positives = tf.reduce_sum(y_pred)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.predicted_positives.assign(0.)\n",
    "\n",
    "    class IntentRecall(metrics.Metric):\n",
    "        def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.threshold = threshold\n",
    "            self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "            self.actual_positives = self.add_weight(name='ap', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(y_true, tf.float32)\n",
    "            y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "            true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "            actual_positives = tf.reduce_sum(y_true)\n",
    "            self.true_positives.assign_add(true_positives)\n",
    "            self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "        def result(self):\n",
    "            return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.true_positives.assign(0.)\n",
    "            self.actual_positives.assign(0.)\n",
    "\n",
    "    class IntentF1(metrics.Metric):\n",
    "        def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.precision = IntentPrecision(threshold=threshold)\n",
    "            self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "            self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "        def result(self):\n",
    "            p = self.precision.result()\n",
    "            r = self.recall.result()\n",
    "            return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.precision.reset_state()\n",
    "            self.recall.reset_state()\n",
    "\n",
    "    class DomainAccuracy(metrics.Metric):\n",
    "        def __init__(self, name='domain_accuracy', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.accuracy = self.add_weight(name='acc', initializer='zeros')\n",
    "            self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "            pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "            matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "            self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "            self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.accuracy.assign(0.)\n",
    "            self.count.assign(0.)\n",
    "\n",
    "    class Perplexity(metrics.Metric):\n",
    "        def __init__(self, name='perplexity', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "            self.total_loss = self.add_weight(name='total_loss', initializer='zeros')\n",
    "            self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros')\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "            loss = self.cross_entropy(y_true, y_pred)\n",
    "            masked_loss = loss * mask\n",
    "            sum_loss = tf.reduce_sum(masked_loss)\n",
    "            num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "            self.total_loss.assign_add(sum_loss)\n",
    "            self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "        def result(self):\n",
    "            avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "            return tf.exp(avg_loss)\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_loss.assign(0.)\n",
    "            self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "    class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "        def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "            super().__init__(name=name, **kwargs)\n",
    "            self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "            self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "        def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "            y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "            y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "            y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "            cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "            non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "            non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "            masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "            num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "            avg_cosine_sim_per_sample = tf.where(\n",
    "                num_non_zero_tokens > 0,\n",
    "                tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "                tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "            )\n",
    "            self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "            self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "        def result(self):\n",
    "            return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "        def reset_state(self):\n",
    "            self.total_cosine_sim.assign(0.)\n",
    "            self.num_samples.assign(0.)\n",
    "\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "\n",
    "    def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "        flops = 0\n",
    "        functional_model = model.build_graph()\n",
    "\n",
    "        def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "            layer_flops_count = 0\n",
    "\n",
    "            if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "                return 0\n",
    "\n",
    "            if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "                for sub_layer in layer_instance.layers:\n",
    "                    layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "                return layer_flops_count\n",
    "\n",
    "            if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "                return 0\n",
    "\n",
    "            if isinstance(layer_instance, layers.Dense):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                output_dim = layer_instance.output_shape[-1]\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.LSTM):\n",
    "                input_dim = layer_instance.input_shape[-1]\n",
    "                hidden_dim = layer_instance.units\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "                layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "\n",
    "            elif isinstance(layer_instance, layers.Embedding):\n",
    "                pass\n",
    "\n",
    "            elif isinstance(layer_instance, DeepSeekMoELayer):\n",
    "                moe_input_dim = layer_instance.input_dim\n",
    "                effective_batch_size = current_batch_size\n",
    "                if len(layer_instance.input_shape) == 3:\n",
    "                    effective_batch_size *= layer_instance.input_shape[1]\n",
    "                gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "                layer_flops_count += gate_flops\n",
    "                for expert in layer_instance.experts:\n",
    "                    layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "                config = layer_instance.get_config()\n",
    "                num_heads = config['num_heads']\n",
    "                key_dim = config['key_dim']\n",
    "                d_model = num_heads * key_dim\n",
    "                seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "                projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "                attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "                context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "                output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "                layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "\n",
    "            elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "                inner_layer = layer_instance.layer\n",
    "                td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "                layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "\n",
    "            elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "                ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "                layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "\n",
    "            return layer_flops_count\n",
    "\n",
    "        for layer in functional_model.layers:\n",
    "            flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "\n",
    "        return flops / 1e9\n",
    "\n",
    "    def get_gpu_stats():\n",
    "        if nvidia_smi is None:\n",
    "            return 0, 0\n",
    "        try:\n",
    "            nvidia_smi.nvmlInit()\n",
    "            handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "            gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "            mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "            gpu_memory = mem_info.used / (1024 ** 3)\n",
    "            nvidia_smi.nvmlShutdown()\n",
    "            return gpu_load, gpu_memory\n",
    "        except Exception:\n",
    "            return 0, 0\n",
    "\n",
    "    class ResourceMonitor(keras.callbacks.Callback):\n",
    "        def __init__(self, validation_data):\n",
    "            super().__init__()\n",
    "            self.resource_stats = []\n",
    "            self.expert_load_history = []\n",
    "            self.start_time = None\n",
    "            self.epoch_start_time = None\n",
    "            self.flops = None\n",
    "            self.validation_data = validation_data\n",
    "            self.nlu_expert_usage_counts = None\n",
    "            self.nlg_expert_usage_counts = None\n",
    "            self.nlu_gate_weights_history = []\n",
    "            self.nlg_gate_weights_history = []\n",
    "\n",
    "        def on_train_begin(self, logs=None):\n",
    "            self.start_time = time.time()\n",
    "            try:\n",
    "                self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "                self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "                self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "            except Exception:\n",
    "                self.flops = 0\n",
    "\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.epoch_start_time = time.time()\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            self.resource_stats.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_time': epoch_time,\n",
    "                'cpu_usage': cpu_usage,\n",
    "                'memory_used': memory.used / (1024 ** 3),\n",
    "                'memory_total': memory.total / (1024 ** 3),\n",
    "                'gpu_load': gpu_load,\n",
    "                'gpu_memory': gpu_memory,\n",
    "                'loss': logs.get('loss', 0),\n",
    "                'val_loss': logs.get('val_loss', 0),\n",
    "                'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "                'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "                'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "                'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "                'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "                'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "                'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "                'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "                'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "                'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "                'perplexity': logs.get('response_output_perplexity', 0),\n",
    "                'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "                'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "                'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "                'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "            })\n",
    "            if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "                self.expert_load_history.append({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                    'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "                })\n",
    "\n",
    "            sample_size = 100\n",
    "            for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "                inputs, _ = batch\n",
    "                outputs = self.model(inputs, training=False)\n",
    "                nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "                reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "                self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "                nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "                reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "                self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "        def on_train_end(self, logs=None):\n",
    "            self.total_time = time.time() - self.start_time\n",
    "\n",
    "            if len(self.nlu_gate_weights_history) > 0:\n",
    "                all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "                nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "                for expert_id in nlu_top_indices:\n",
    "                    if expert_id < self.model.moe_nlu.total_experts:\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "            if len(self.nlg_gate_weights_history) > 0:\n",
    "                all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "                nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "                for expert_id in nlg_top_indices:\n",
    "                    if expert_id < self.model.moe_nlg.total_experts:\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "        def visualize_expert_load_distribution(self):\n",
    "            total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "            total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "            nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "            nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "            nlu_stability = np.var(nlu_percentages)\n",
    "            nlg_stability = np.var(nlg_percentages)\n",
    "            return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "    class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, d_model, warmup_steps=4000):\n",
    "            super().__init__()\n",
    "            self.d_model = tf.cast(d_model, tf.float32)\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            arg1 = tf.math.rsqrt(step)\n",
    "            arg2 = step * (self.warmup_steps ** -1.5)\n",
    "            return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "        def get_config(self):\n",
    "            return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "    model = MoEModel(moe_params)\n",
    "    embedding_layer = model.embedding\n",
    "\n",
    "    def create_validation_split(dataset, embedding_layer, validation_split=0.2, batch_size=moe_params[\"batch_size\"]):\n",
    "        element_spec = dataset.element_spec\n",
    "        data_list = [(features, targets) for features, targets in dataset.unbatch().as_numpy_iterator()]\n",
    "        if not data_list:\n",
    "            raise ValueError(\"Dataset is empty.\")\n",
    "        if len(data_list) < 2:\n",
    "            raise ValueError(\"Dataset too small to split.\")\n",
    "        train_data, val_data = train_test_split(data_list, test_size=validation_split, shuffle=True)\n",
    "\n",
    "        def create_dataset_from_list(data):\n",
    "            if not data:\n",
    "                raise ValueError(\"Empty data list provided.\")\n",
    "            features = {\n",
    "                'user_utterance_tokens': np.array([d[0]['user_utterance_tokens'] for d in data], dtype=np.int32),\n",
    "                'prev_system_response_tokens': np.array([d[0]['prev_system_response_tokens'] for d in data], dtype=np.int32),\n",
    "                'decoder_input_tokens': np.array([d[0]['decoder_input_tokens'] for d in data], dtype=np.int32),\n",
    "                'domain_onehot_input': np.array([d[0]['domain_onehot_input'] for d in data], dtype=np.float32),\n",
    "                'turn_id_embedding': np.array([d[0]['turn_id_embedding'] for d in data], dtype=np.float32),\n",
    "                'ontology_multihot_input': np.array([d[0]['ontology_multihot_input'] for d in data], dtype=np.float32),\n",
    "            }\n",
    "            response_output = np.array([d[1]['response_output'] for d in data], dtype=np.int32)\n",
    "            response_embeddings = embedding_layer(response_output).numpy()\n",
    "            targets = {\n",
    "                'domain_output': np.array([d[1]['domain_output'] for d in data], dtype=np.int32),\n",
    "                'intent_output': np.array([d[1]['intent_output'] for d in data], dtype=np.float32),\n",
    "                'response_output': response_output,\n",
    "                'response_embeddings': response_embeddings\n",
    "            }\n",
    "            return tf.data.Dataset.from_tensor_slices((features, targets)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        train_dataset = create_dataset_from_list(train_data)\n",
    "        val_dataset = create_dataset_from_list(val_data)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    train_tf_dataset, val_tf_dataset = create_validation_split(train_tf_dataset, embedding_layer)\n",
    "\n",
    "    losses_dict = {\n",
    "        'intent_output': losses.BinaryCrossentropy(),\n",
    "        'domain_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss\n",
    "    }\n",
    "\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={'intent_output': 0.5, 'domain_output': 0.5, 'response_output': 1.0, 'response_embeddings': 1.0}\n",
    "    )\n",
    "\n",
    "    sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "    model(sample_input[0])\n",
    "\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=8, mode='min', restore_best_weights=True)\n",
    "    resource_monitor = ResourceMonitor(val_tf_dataset)\n",
    "\n",
    "    class TQDMProgressBar(callbacks.Callback):\n",
    "        def on_epoch_begin(self, epoch, logs=None):\n",
    "            self.progress_bar = tqdm(total=len(train_tf_dataset), desc=f'Epoch {epoch + 1}')\n",
    "\n",
    "        def on_batch_end(self, batch, logs=None):\n",
    "            self.progress_bar.update(1)\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            self.progress_bar.close()\n",
    "\n",
    "    tqdm_callback = TQDMProgressBar()\n",
    "\n",
    "    history = model.fit(\n",
    "        train_tf_dataset,\n",
    "        epochs=100,\n",
    "        validation_data=val_tf_dataset,\n",
    "        callbacks=[early_stopping, resource_monitor, tqdm_callback],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    expert_stats = resource_monitor.visualize_expert_load_distribution()\n",
    "    stats = resource_monitor.resource_stats\n",
    "    avg_epoch_duration = np.mean([s['epoch_time'] for s in stats]) if stats else 0\n",
    "    total_training_time = resource_monitor.total_time\n",
    "    avg_cpu_usage = np.mean([s['cpu_usage'] for s in stats]) if stats else 0\n",
    "    avg_memory = np.mean([s['memory_used'] for s in stats]) if stats else 0\n",
    "    avg_gpu_load = np.mean([s['gpu_load'] for s in stats]) if stats else 0\n",
    "    avg_gpu_memory = np.mean([s['gpu_memory'] for s in stats]) if stats else 0\n",
    "    avg_flops = resource_monitor.flops if resource_monitor.flops else 0\n",
    "    nlu_counts = resource_monitor.nlu_expert_usage_counts\n",
    "    nlg_counts = resource_monitor.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) or 1\n",
    "    total_nlg = np.sum(nlg_counts) or 1\n",
    "    expert_nlu_percentages = [(nlu_counts[i] / total_nlu) * 100 for i in range(8)]\n",
    "    expert_nlg_percentages = [(nlg_counts[i] / total_nlg) * 100 for i in range(8)]\n",
    "    val_intent_accuracy = np.mean([s['val_intent_accuracy'] for s in stats if s['val_intent_accuracy'] > 0]) if stats else 0\n",
    "    val_entity_accuracy = np.mean([s['val_domain_accuracy'] for s in stats if s['val_domain_accuracy'] > 0]) if stats else 0\n",
    "    val_perplexity = np.mean([s['val_perplexity'] for s in stats if s['val_perplexity'] > 0]) if stats else 0\n",
    "    val_cosine_similarity = np.mean([s['val_cosine_similarity'] for s in stats if s['val_cosine_similarity'] > 0]) if stats and any(s['val_cosine_similarity'] > 0 for s in stats) else 0\n",
    "    \n",
    "    results_table['Training Speed (epochs per second)'][f'Iteration {iteration + 1}'] = 1 / avg_epoch_duration if avg_epoch_duration > 0 else 0\n",
    "    results_table['Training Time (second/epoch)'][f'Iteration {iteration + 1}'] = avg_epoch_duration\n",
    "    results_table['Total Training Time (second/iteration)'][f'Iteration {iteration + 1}'] = total_training_time\n",
    "    results_table['Computational Resource Usage'][f'Iteration {iteration + 1}'] = avg_cpu_usage + avg_gpu_load\n",
    "    results_table['Average CPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_cpu_usage\n",
    "    results_table['Average GPU Usage (percent)'][f'Iteration {iteration + 1}'] = avg_gpu_load\n",
    "    results_table['Average Memory (GB)'][f'Iteration {iteration + 1}'] = avg_memory\n",
    "    results_table['Average GPU Memory (GB)'][f'Iteration {iteration + 1}'] = avg_gpu_memory\n",
    "    results_table['Average FLOPS Estimate (GFLOPS)'][f'Iteration {iteration + 1}'] = avg_flops\n",
    "    results_table['Expert NLU Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlu_stability']\n",
    "    results_table['Expert NLG Load Distribution Stability'][f'Iteration {iteration + 1}'] = expert_stats['nlg_stability']\n",
    "    for i in range(8):\n",
    "        if i < len(expert_stats['nlu_percentages']):\n",
    "            results_table[f'Average NLU Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlu_percentages'][i]\n",
    "        if i < len(expert_stats['nlg_percentages']):\n",
    "            results_table[f'Average NLG Expert {i+1} (percent)'][f'Iteration {iteration + 1}'] = expert_stats['nlg_percentages'][i]\n",
    "    results_table['Average Validation Entity Accuracy'][f'Iteration {iteration + 1}'] = val_entity_accuracy\n",
    "    results_table['Average Intent Accuracy'][f'Iteration {iteration + 1}'] = val_intent_accuracy\n",
    "    results_table['Average Validation Perplexity'][f'Iteration {iteration + 1}'] = val_perplexity\n",
    "    results_table['Average Validation Response Cosine Similarity'][f'Iteration {iteration + 1}'] = val_cosine_similarity\n",
    "\n",
    "    val_loss = min([s['val_loss'] for s in stats]) if stats else float('inf')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        try:\n",
    "            model.save(best_model_path, save_format='tf', include_optimizer=True)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nFailed to save best model to {best_model_path}: {str(e)}\")\n",
    "            \n",
    "for key in results_table:\n",
    "    results_table[key]['Average'] = np.mean([results_table[key][f'Iteration {i+1}'] for i in range(10)])\n",
    "\n",
    "final_result = pd.DataFrame(results_table)\n",
    "final_result = final_result.T\n",
    "final_result.to_excel('training_deepseekmoe_adaptive_gating_expert_choice_result.xlsx', index=True)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeekMoE with Adaptive Gating with Expert Choice Balance evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 08:59:34.141854: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 08:59:34.180462: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 08:59:34.180506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 08:59:34.182729: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 08:59:34.192030: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 08:59:34.894011: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-07-04 08:59:37.809684: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:59:37.957101: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:59:37.957315: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:59:37.958295: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:59:37.958432: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:59:37.958524: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:59:38.029171: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:59:38.029351: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:59:38.029457: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 08:59:38.029857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14401 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mo_e_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  784896    \n",
      "                                                                 \n",
      " embedding (Embedding)       multiple                  8576      \n",
      "                                                                 \n",
      " layer_normalization (Layer  multiple                  256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_layer   multiple                  264576    \n",
      " (TransformerDecoderLayer)                                       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " moe_nlu (DeepSeekMoELayer)  multiple                  244852    \n",
      "                                                                 \n",
      " moe_nlg (DeepSeekMoELayer)  multiple                  87578     \n",
      "                                                                 \n",
      " intent_output (Dense)       multiple                  14446     \n",
      "                                                                 \n",
      " domain_output (Dense)       multiple                  3262      \n",
      "                                                                 \n",
      " response_output (TimeDistr  multiple                  1024044   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " multi_head_attention_2 (Mu  multiple                  66048     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2498534 (9.53 MB)\n",
      "Trainable params: 2498534 (9.53 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1397/1397 [==============================] - 19s 11ms/step - loss: 3.1154 - domain_output_loss: 0.0019 - intent_output_loss: 0.0330 - response_embeddings_loss: 0.8257 - response_output_loss: 2.0229 - domain_output_domain_accuracy: 1.0000 - intent_output_intent_accuracy: 0.7087 - intent_output_intent_precision: 1.0000 - intent_output_intent_recall: 0.6233 - intent_output_intent_f1: 0.7680 - response_embeddings_response_embedding_cosine_similarity: 0.1743 - response_output_perplexity: 30.3974 - moe_nlu_load_balancing_loss: 0.0033 - moe_nlg_load_balancing_loss: 0.2233\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Evaluation Time (seconds)</th>\n",
       "      <td>19.436955</td>\n",
       "      <td>19.436955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prediction Speed (ms/token)</th>\n",
       "      <td>1.996718</td>\n",
       "      <td>1.996718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>22.000072</td>\n",
       "      <td>22.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>1.992842</td>\n",
       "      <td>1.992842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>5.648967</td>\n",
       "      <td>5.648967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.504456</td>\n",
       "      <td>14.504456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPs Estimate (GFLOPs)</th>\n",
       "      <td>2.377871</td>\n",
       "      <td>2.377871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 3 (percent)</th>\n",
       "      <td>10.325698</td>\n",
       "      <td>10.325698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 4 (percent)</th>\n",
       "      <td>35.701503</td>\n",
       "      <td>35.701503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 5 (percent)</th>\n",
       "      <td>4.312813</td>\n",
       "      <td>4.312813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 6 (percent)</th>\n",
       "      <td>30.601288</td>\n",
       "      <td>30.601288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 7 (percent)</th>\n",
       "      <td>0.017895</td>\n",
       "      <td>0.017895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert 8 (percent)</th>\n",
       "      <td>19.040802</td>\n",
       "      <td>19.040802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 1 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 2 (percent)</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 3 (percent)</th>\n",
       "      <td>15.830832</td>\n",
       "      <td>15.830832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 4 (percent)</th>\n",
       "      <td>27.304512</td>\n",
       "      <td>27.304512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 5 (percent)</th>\n",
       "      <td>8.150728</td>\n",
       "      <td>8.150728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 6 (percent)</th>\n",
       "      <td>32.757294</td>\n",
       "      <td>32.757294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 7 (percent)</th>\n",
       "      <td>7.263699</td>\n",
       "      <td>7.263699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert 8 (percent)</th>\n",
       "      <td>8.692935</td>\n",
       "      <td>8.692935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Micro)</th>\n",
       "      <td>0.767959</td>\n",
       "      <td>0.767959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Macro)</th>\n",
       "      <td>0.519699</td>\n",
       "      <td>0.519699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent F1-score (Weighted)</th>\n",
       "      <td>0.677297</td>\n",
       "      <td>0.677297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intent Accuracy</th>\n",
       "      <td>0.708661</td>\n",
       "      <td>0.708661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain F1-score (Macro)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLEU Score</th>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.001012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-1 F1</th>\n",
       "      <td>0.189376</td>\n",
       "      <td>0.189376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-2 F1</th>\n",
       "      <td>0.014427</td>\n",
       "      <td>0.014427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROUGE-L F1</th>\n",
       "      <td>0.151313</td>\n",
       "      <td>0.151313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>METEOR Score</th>\n",
       "      <td>0.139102</td>\n",
       "      <td>0.139102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perplexity</th>\n",
       "      <td>30.397409</td>\n",
       "      <td>30.397409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLU Expert Load Stability (Variance)</th>\n",
       "      <td>181.101129</td>\n",
       "      <td>181.101129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NLG Expert Load Stability (Variance)</th>\n",
       "      <td>126.744343</td>\n",
       "      <td>126.744343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Iteration 1     Average\n",
       "Evaluation Time (seconds)               19.436955   19.436955\n",
       "Prediction Speed (ms/token)              1.996718    1.996718\n",
       "Average CPU Usage (percent)             22.000072   22.000072\n",
       "Average GPU Usage (percent)              1.992842    1.992842\n",
       "Average Memory (GB)                      5.648967    5.648967\n",
       "Average GPU Memory (GB)                 14.504456   14.504456\n",
       "Average FLOPs Estimate (GFLOPs)          2.377871    2.377871\n",
       "NLU Expert 1 (percent)                   0.000000    0.000000\n",
       "NLU Expert 2 (percent)                   0.000000    0.000000\n",
       "NLU Expert 3 (percent)                  10.325698   10.325698\n",
       "NLU Expert 4 (percent)                  35.701503   35.701503\n",
       "NLU Expert 5 (percent)                   4.312813    4.312813\n",
       "NLU Expert 6 (percent)                  30.601288   30.601288\n",
       "NLU Expert 7 (percent)                   0.017895    0.017895\n",
       "NLU Expert 8 (percent)                  19.040802   19.040802\n",
       "NLG Expert 1 (percent)                   0.000000    0.000000\n",
       "NLG Expert 2 (percent)                   0.000000    0.000000\n",
       "NLG Expert 3 (percent)                  15.830832   15.830832\n",
       "NLG Expert 4 (percent)                  27.304512   27.304512\n",
       "NLG Expert 5 (percent)                   8.150728    8.150728\n",
       "NLG Expert 6 (percent)                  32.757294   32.757294\n",
       "NLG Expert 7 (percent)                   7.263699    7.263699\n",
       "NLG Expert 8 (percent)                   8.692935    8.692935\n",
       "Intent F1-score (Micro)                  0.767959    0.767959\n",
       "Intent F1-score (Macro)                  0.519699    0.519699\n",
       "Intent F1-score (Weighted)               0.677297    0.677297\n",
       "Intent Accuracy                          0.708661    0.708661\n",
       "Domain Accuracy                          1.000000    1.000000\n",
       "Domain F1-score (Macro)                  1.000000    1.000000\n",
       "BLEU Score                               0.001012    0.001012\n",
       "ROUGE-1 F1                               0.189376    0.189376\n",
       "ROUGE-2 F1                               0.014427    0.014427\n",
       "ROUGE-L F1                               0.151313    0.151313\n",
       "METEOR Score                             0.139102    0.139102\n",
       "Perplexity                              30.397409   30.397409\n",
       "NLU Expert Load Stability (Variance)   181.101129  181.101129\n",
       "NLG Expert Load Stability (Variance)   126.744343  126.744343"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_tf_datasets_from_disk(load_path):\n",
    "    try:\n",
    "        with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "            loaded_moe_params = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "    element_spec = (\n",
    "        {\n",
    "            'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "        },\n",
    "        {\n",
    "            'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "            'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "        test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "    raw_data = {}\n",
    "    for dataset_name in ['train', 'test']:\n",
    "        path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                raw_data[dataset_name] = pickle.load(f)\n",
    "        else:\n",
    "            raw_data[dataset_name] = []\n",
    "\n",
    "    return {\n",
    "        \"train_dataset\": train_tf_dataset,\n",
    "        \"test_dataset\": test_tf_dataset,\n",
    "        \"moe_params\": loaded_moe_params,\n",
    "        \"raw_data\": raw_data\n",
    "    }\n",
    "\n",
    "tf_dataset_save_path = \"tf_datasets\"\n",
    "loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "test_tf_dataset = loaded_data[\"test_dataset\"]\n",
    "moe_params = loaded_data[\"moe_params\"]\n",
    "raw_data = loaded_data[\"raw_data\"]\n",
    "\n",
    "moe_params[\"num_experts\"] = 4\n",
    "\n",
    "class DeepSeekMoELayer(layers.Layer):\n",
    "    def __init__(self, num_experts, expert_dim, input_dim, m=2, k_s=1, alpha=0.01, tau=0.1, name=None):\n",
    "        super(DeepSeekMoELayer, self).__init__(name=name)\n",
    "        self.m = m\n",
    "        self.k_s = k_s\n",
    "        self.total_experts = num_experts * self.m\n",
    "        self.routed_experts = self.total_experts - self.k_s\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = None\n",
    "        self.adjusted_expert_dim = expert_dim // self.m\n",
    "        self.shared_experts = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "            ], name=f'shared_expert_{i}') for i in range(self.k_s)\n",
    "        ]\n",
    "        self.routed_experts_list = [\n",
    "            keras.Sequential([\n",
    "                layers.Dense(self.adjusted_expert_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "                layers.Dense(input_dim, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "            ], name=f'routed_expert_{i}') for i in range(self.routed_experts)\n",
    "        ]\n",
    "        self.experts = self.shared_experts + self.routed_experts_list\n",
    "        self.gate = layers.Dense(self.routed_experts, activation=None, name='gate_weights')\n",
    "        self.load_balancing_loss_metric = tf.keras.metrics.Mean(name=f'{name}_load_balancing_loss')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_rank = inputs.shape.rank\n",
    "        if input_rank == 3:\n",
    "            batch_size, seq_length = tf.shape(inputs)[0], tf.shape(inputs)[1]\n",
    "            flat_inputs = tf.reshape(inputs, [-1, self.input_dim])\n",
    "            is_3d = True\n",
    "        else:\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            seq_length = 1\n",
    "            flat_inputs = inputs\n",
    "            is_3d = False\n",
    "        flat_inputs = tf.ensure_shape(flat_inputs, [None, self.input_dim])\n",
    "        num_tokens = tf.shape(flat_inputs)[0]\n",
    "        shared_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "        for i in range(self.k_s):\n",
    "            shared_output += self.shared_experts[i](flat_inputs)\n",
    "        gate_logits = self.gate(flat_inputs)\n",
    "        gate_weights = tf.nn.softmax(gate_logits, axis=-1)\n",
    "        sorted_weights = tf.sort(gate_weights, axis=-1, direction='DESCENDING')\n",
    "        p1 = sorted_weights[:, 0]\n",
    "        p2 = sorted_weights[:, 1]\n",
    "        use_top_1 = tf.cast(p1 - p2 >= self.tau, tf.float32)\n",
    "        k_per_token = tf.where(use_top_1 > 0, 1, 2)\n",
    "        max_k = 2\n",
    "        top_k_values, top_k_indices = tf.nn.top_k(gate_weights, k=max_k, sorted=True)\n",
    "        top_k_indices = top_k_indices + self.k_s\n",
    "        top_k_weights = top_k_values / (tf.reduce_sum(top_k_values, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "        k_mask = tf.range(max_k, dtype=tf.int32) < tf.expand_dims(k_per_token, -1)\n",
    "        k_mask = tf.cast(k_mask, tf.float32)\n",
    "        top_k_weights = top_k_weights * k_mask\n",
    "        top_k_weights = top_k_weights / (tf.reduce_sum(top_k_weights, axis=-1, keepdims=True) + tf.keras.backend.epsilon())\n",
    "        expert_outputs = tf.stack([expert(flat_inputs) for expert in self.experts], axis=1)\n",
    "        routed_output = tf.zeros([num_tokens, self.input_dim], dtype=tf.float32)\n",
    "        for k in range(max_k):\n",
    "            kth_weights = top_k_weights[:, k]\n",
    "            kth_indices = top_k_indices[:, k]\n",
    "            mask = tf.one_hot(kth_indices, depth=self.total_experts, dtype=tf.float32)\n",
    "            kth_expert_output = tf.reduce_sum(expert_outputs * tf.expand_dims(mask, -1), axis=1)\n",
    "            weighted_kth_output = kth_expert_output * tf.expand_dims(kth_weights, -1)\n",
    "            routed_output += weighted_kth_output\n",
    "        output_flat = shared_output + routed_output + flat_inputs\n",
    "        if is_3d:\n",
    "            output = tf.reshape(output_flat, [batch_size, seq_length, self.input_dim])\n",
    "            if self.seq_length is not None:\n",
    "                output.set_shape([None, self.seq_length, self.input_dim])\n",
    "            gate_weights_reshaped = tf.reshape(gate_weights, [batch_size, seq_length, self.routed_experts])\n",
    "            if self.seq_length is not None:\n",
    "                gate_weights_reshaped.set_shape([None, self.seq_length, self.routed_experts])\n",
    "        else:\n",
    "            output = output_flat\n",
    "            gate_weights_reshaped = gate_weights\n",
    "        f_i = tf.reduce_mean(tf.reduce_sum(tf.one_hot(tf.argmax(gate_weights, axis=-1), depth=self.routed_experts), axis=0))\n",
    "        P_i = tf.reduce_mean(gate_weights, axis=0)\n",
    "        load_balancing_loss = self.alpha * tf.reduce_sum(f_i * P_i)\n",
    "        self.add_loss(load_balancing_loss)\n",
    "        self.load_balancing_loss_metric.update_state(load_balancing_loss)\n",
    "        return output, gate_weights_reshaped\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return {f'{self.name}_load_balancing_loss': self.load_balancing_loss_metric}\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_experts': self.total_experts // self.m,\n",
    "            'expert_dim': self.adjusted_expert_dim * self.m,\n",
    "            'input_dim': self.input_dim,\n",
    "            'm': self.m,\n",
    "            'k_s': self.k_s,\n",
    "            'alpha': self.alpha,\n",
    "            'tau': self.tau,\n",
    "            'name': self.name\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class TransformerDecoderLayer(layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        self.dropout3 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask=None):\n",
    "        if look_ahead_mask is not None:\n",
    "            look_ahead_mask = tf.cast(look_ahead_mask, tf.float32)\n",
    "            look_ahead_mask = 1.0 - look_ahead_mask\n",
    "        attn1_output = self.mha1(query=x, value=x, key=x, attention_mask=look_ahead_mask, training=training)\n",
    "        attn1 = self.dropout1(attn1_output, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        attn2_output = self.mha2(query=out1, value=enc_output, key=enc_output, training=training)\n",
    "        attn2 = self.dropout2(attn2_output, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        return out3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        mha1_config = self.mha1.get_config()\n",
    "        config.update({\n",
    "            'd_model': mha1_config['key_dim'] * mha1_config['num_heads'],\n",
    "            'num_heads': mha1_config['num_heads'],\n",
    "            'dff': self.ffn.layers[0].units,\n",
    "            'rate': self.dropout1.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class MoEModel(models.Model):\n",
    "    def __init__(self, moe_params):\n",
    "        super(MoEModel, self).__init__()\n",
    "        self.embedding_dim = moe_params[\"embedding_dim\"]\n",
    "        self.max_seq_length = moe_params[\"max_seq_length\"]\n",
    "        self.num_domains = moe_params[\"num_domains\"]\n",
    "        self.num_intents = moe_params[\"num_intents\"]\n",
    "        self.vocab_size = moe_params[\"vocab_size\"]\n",
    "        self.num_experts = moe_params[\"num_experts\"]\n",
    "        self.expert_dim = moe_params[\"expert_dim\"]\n",
    "        self.turn_id_dim = moe_params[\"turn_id_dim\"]\n",
    "        self.num_heads = 4\n",
    "        self.dff = self.embedding_dim * 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.nlu_input_dim = (self.embedding_dim + self.embedding_dim + self.embedding_dim + \n",
    "                             self.num_domains + self.turn_id_dim + self.num_intents)\n",
    "        self.nlg_input_dim = self.embedding_dim + self.num_intents + self.num_domains\n",
    "        self.embedding = layers.Embedding(\n",
    "            self.vocab_size,\n",
    "            self.embedding_dim,\n",
    "            mask_zero=True,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "            name=\"embedding\"\n",
    "        )\n",
    "        self.embedding.build((None,))\n",
    "        with tf.init_scope():\n",
    "            embedding_weights = self.embedding.get_weights()\n",
    "            if embedding_weights and len(embedding_weights) > 0:\n",
    "                embedding_weights[0][0] = tf.zeros((self.embedding_dim,))\n",
    "                self.embedding.set_weights(embedding_weights)\n",
    "        self.pos_encoding = layers.Embedding(self.max_seq_length, self.embedding_dim)\n",
    "        self.embedding_norm = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.decoder_layers = [TransformerDecoderLayer(self.embedding_dim, self.num_heads, self.dff, self.dropout_rate) for _ in range(1)]\n",
    "        self.decoder_dropout = layers.Dropout(self.dropout_rate)\n",
    "        self.moe_nlu = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlu_input_dim, m=2, k_s=2, alpha=0.01, tau=0.1, name='moe_nlu')\n",
    "        self.moe_nlg = DeepSeekMoELayer(num_experts=self.num_experts, expert_dim=self.expert_dim, input_dim=self.nlg_input_dim, m=2, k_s=2, alpha=0.01, tau=0.1, name='moe_nlg')\n",
    "        self.intent_output = layers.Dense(self.num_intents, activation='sigmoid', name='intent_output')\n",
    "        self.domain_output = layers.Dense(self.num_domains, activation='softmax', name='domain_output')\n",
    "        self.response_output = layers.TimeDistributed(layers.Dense(self.vocab_size, activation='softmax'), name='response_output')\n",
    "        self.attn_layer = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embedding_dim // self.num_heads)\n",
    "\n",
    "    def create_look_ahead_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        user_utterance_tokens = inputs['user_utterance_tokens']\n",
    "        prev_system_response_tokens = inputs['prev_system_response_tokens']\n",
    "        decoder_input_tokens = inputs['decoder_input_tokens']\n",
    "        domain_onehot_input = inputs['domain_onehot_input']\n",
    "        turn_id_embedding = inputs['turn_id_embedding']\n",
    "        ontology_multihot_input = inputs['ontology_multihot_input']\n",
    "        pos_enc = self.pos_encoding(tf.range(self.max_seq_length))\n",
    "        user_embedded = self.embedding_norm(self.embedding(user_utterance_tokens) + pos_enc)\n",
    "        prev_system_embedded = self.embedding_norm(self.embedding(prev_system_response_tokens) + pos_enc)\n",
    "        decoder_embedded = self.embedding_norm(self.embedding(decoder_input_tokens) + pos_enc)\n",
    "        user_enc_out = user_embedded\n",
    "        prev_system_enc_out = prev_system_embedded\n",
    "        look_ahead_mask = self.create_look_ahead_mask(self.max_seq_length)\n",
    "        dec_output = decoder_embedded\n",
    "        for i, layer in enumerate(self.decoder_layers):\n",
    "            dec_output = layer(dec_output, prev_system_enc_out, training=training, look_ahead_mask=look_ahead_mask)\n",
    "        decoder_output = self.decoder_dropout(dec_output, training=training)\n",
    "        user_attn_out = self.attn_layer(query=tf.reduce_mean(user_enc_out, axis=1, keepdims=True), value=user_enc_out, key=user_enc_out, training=training)\n",
    "        prev_system_attn_out = self.attn_layer(query=tf.reduce_mean(prev_system_enc_out, axis=1, keepdims=True), value=prev_system_enc_out, key=prev_system_enc_out, training=training)\n",
    "        decoder_attn_out = self.attn_layer(query=tf.reduce_mean(decoder_output, axis=1, keepdims=True), value=decoder_output, key=decoder_output, training=training)\n",
    "        combined_features = layers.Concatenate()([\n",
    "            tf.squeeze(user_attn_out, 1),\n",
    "            tf.squeeze(prev_system_attn_out, 1),\n",
    "            tf.squeeze(decoder_attn_out, 1),\n",
    "            domain_onehot_input,\n",
    "            turn_id_embedding,\n",
    "            ontology_multihot_input\n",
    "        ])\n",
    "        combined_features = tf.ensure_shape(combined_features, [None, self.nlu_input_dim])\n",
    "        nlu_out, nlu_gate_weights = self.moe_nlu(combined_features)\n",
    "        nlu_out = tf.ensure_shape(nlu_out, [None, self.nlu_input_dim])\n",
    "        intent_out = self.intent_output(nlu_out)\n",
    "        domain_out = self.domain_output(nlu_out)\n",
    "        intent_out_expanded = tf.expand_dims(intent_out, axis=1)\n",
    "        domain_out_expanded = tf.expand_dims(domain_out, axis=1)\n",
    "        intent_out_tiled = tf.tile(intent_out_expanded, [1, self.max_seq_length, 1])\n",
    "        domain_out_tiled = tf.tile(domain_out_expanded, [1, self.max_seq_length, 1])\n",
    "        nlg_input = layers.Concatenate(axis=-1)([\n",
    "            decoder_output,\n",
    "            intent_out_tiled,\n",
    "            domain_out_tiled\n",
    "        ])\n",
    "        nlg_input = tf.ensure_shape(nlg_input, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        nlg_out, nlg_gate_weights = self.moe_nlg(nlg_input)\n",
    "        nlg_out = tf.ensure_shape(nlg_out, [None, self.max_seq_length, self.nlg_input_dim])\n",
    "        response_out = self.response_output(nlg_out)\n",
    "        return {\n",
    "            'intent_output': intent_out,\n",
    "            'domain_output': domain_out,\n",
    "            'response_output': response_out,\n",
    "            'response_embeddings': decoder_output,\n",
    "            'nlu_gate_weights': nlu_gate_weights,\n",
    "            'nlg_gate_weights': nlg_gate_weights\n",
    "        }\n",
    "\n",
    "    def build_graph(self):\n",
    "        inputs = {\n",
    "            'user_utterance_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='user_utterance_tokens'),\n",
    "            'prev_system_response_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='prev_system_response_tokens'),\n",
    "            'decoder_input_tokens': tf.keras.Input(shape=(self.max_seq_length,), dtype=tf.int32, name='decoder_input_tokens'),\n",
    "            'domain_onehot_input': tf.keras.Input(shape=(self.num_domains,), dtype=tf.float32, name='domain_onehot_input'),\n",
    "            'turn_id_embedding': tf.keras.Input(shape=(self.turn_id_dim,), dtype=tf.float32, name='turn_id_embedding'),\n",
    "            'ontology_multihot_input': tf.keras.Input(shape=(self.num_intents,), dtype=tf.float32, name='ontology_multihot_input'),\n",
    "        }\n",
    "        return tf.keras.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'moe_params': {\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'max_seq_length': self.max_seq_length,\n",
    "                'num_domains': self.num_domains,\n",
    "                'num_intents': self.num_intents,\n",
    "                'vocab_size': self.vocab_size,\n",
    "                'num_experts': self.num_experts,\n",
    "                'expert_dim': self.expert_dim,\n",
    "                'turn_id_dim': self.turn_id_dim\n",
    "            }\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class IntentAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='intent_accuracy', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.correct_preds = self.add_weight(name='correct_preds', initializer='zeros')\n",
    "        self.total_preds = self.add_weight(name='total_preds', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        correct_batch = tf.reduce_all(tf.equal(y_true, y_pred), axis=-1)\n",
    "        self.correct_preds.assign_add(tf.reduce_sum(tf.cast(correct_batch, tf.float32)))\n",
    "        self.total_preds.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct_preds / (self.total_preds + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.correct_preds.assign(0.)\n",
    "        self.total_preds.assign(0.)\n",
    "\n",
    "class IntentPrecision(metrics.Metric):\n",
    "    def __init__(self, name='intent_precision', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.predicted_positives = self.add_weight(name='pp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        predicted_positives = tf.reduce_sum(y_pred)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.predicted_positives.assign_add(predicted_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.predicted_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.predicted_positives.assign(0.)\n",
    "\n",
    "class IntentRecall(metrics.Metric):\n",
    "    def __init__(self, name='intent_recall', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.actual_positives = self.add_weight(name='ap', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred > self.threshold, tf.float32)\n",
    "        true_positives = tf.reduce_sum(y_true * y_pred)\n",
    "        actual_positives = tf.reduce_sum(y_true)\n",
    "        self.true_positives.assign_add(true_positives)\n",
    "        self.actual_positives.assign_add(actual_positives)\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives / (self.actual_positives + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.true_positives.assign(0.)\n",
    "        self.actual_positives.assign(0.)\n",
    "\n",
    "class IntentF1(metrics.Metric):\n",
    "    def __init__(self, name='intent_f1', threshold=0.5, **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = IntentPrecision(threshold=threshold)\n",
    "        self.recall = IntentRecall(threshold=threshold)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * (p * r) / (p + r + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()\n",
    "\n",
    "class DomainAccuracy(metrics.Metric):\n",
    "    def __init__(self, name='domain_accuracy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.accuracy = self.add_weight(name='acc', initializer='zeros')\n",
    "        self.count = self.add_weight(name='count', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int64)\n",
    "        pred_labels = tf.argmax(y_pred, axis=1, output_type=tf.int64)\n",
    "        matches = tf.cast(tf.equal(y_true, pred_labels), tf.float32)\n",
    "        self.accuracy.assign_add(tf.reduce_sum(matches))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.accuracy / (self.count + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.accuracy.assign(0.)\n",
    "        self.count.assign(0.)\n",
    "\n",
    "class Perplexity(metrics.Metric):\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "        self.total_loss = self.add_weight(name='total_loss', initializer='zeros')\n",
    "        self.total_non_padding_tokens = self.add_weight(name='total_non_padding_tokens', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        mask = tf.cast(y_true != 0, dtype=tf.float32)\n",
    "        loss = self.cross_entropy(y_true, y_pred)\n",
    "        masked_loss = loss * mask\n",
    "        sum_loss = tf.reduce_sum(masked_loss)\n",
    "        num_non_padding_tokens = tf.reduce_sum(mask)\n",
    "        self.total_loss.assign_add(sum_loss)\n",
    "        self.total_non_padding_tokens.assign_add(num_non_padding_tokens)\n",
    "\n",
    "    def result(self):\n",
    "        avg_loss = self.total_loss / (self.total_non_padding_tokens + tf.keras.backend.epsilon())\n",
    "        return tf.exp(avg_loss)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_loss.assign(0.)\n",
    "        self.total_non_padding_tokens.assign(0.)\n",
    "\n",
    "class ResponseEmbeddingCosineSimilarity(metrics.Metric):\n",
    "    def __init__(self, name='response_embedding_cosine_similarity', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.total_cosine_sim = self.add_weight(name='total_cosine_sim', initializer='zeros', dtype=tf.float32)\n",
    "        self.num_samples = self.add_weight(name='num_samples', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm_val = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm_val = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true_norm = y_true / (y_true_norm_val + epsilon)\n",
    "        y_pred_norm = y_pred / (y_pred_norm_val + epsilon)\n",
    "        cosine_sim_per_token = tf.reduce_sum(y_true_norm * y_pred_norm, axis=-1)\n",
    "        non_padding_mask = tf.cast(y_true_norm_val > epsilon, tf.float32) * tf.cast(y_pred_norm_val > epsilon, tf.float32)\n",
    "        non_padding_mask = tf.squeeze(non_padding_mask, axis=-1)\n",
    "        masked_cosine_sim = cosine_sim_per_token * non_padding_mask\n",
    "        num_non_zero_tokens = tf.reduce_sum(non_padding_mask, axis=-1)\n",
    "        avg_cosine_sim_per_sample = tf.where(\n",
    "            num_non_zero_tokens > 0,\n",
    "            tf.reduce_sum(masked_cosine_sim, axis=-1) / num_non_zero_tokens,\n",
    "            tf.zeros_like(num_non_zero_tokens, dtype=tf.float32)\n",
    "        )\n",
    "        self.total_cosine_sim.assign_add(tf.reduce_sum(avg_cosine_sim_per_sample))\n",
    "        self.num_samples.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total_cosine_sim / (self.num_samples + tf.keras.backend.epsilon())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.total_cosine_sim.assign(0.)\n",
    "        self.num_samples.assign(0.)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    epsilon = tf.keras.backend.epsilon()\n",
    "    y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "    y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "    y_true = y_true / (y_true_norm + epsilon)\n",
    "    y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "    cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    positive_loss = 1.0 - cosine_sim\n",
    "    mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "    masked_loss = positive_loss * mask\n",
    "    total_loss = tf.reduce_sum(masked_loss)\n",
    "    num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "    return total_loss / num_tokens\n",
    "\n",
    "def calculate_flops(model, batch_size=moe_params[\"batch_size\"]):\n",
    "    flops = 0\n",
    "    functional_model = model.build_graph()\n",
    "    def _calculate_layer_flops(layer_instance, current_batch_size, current_seq_length):\n",
    "        layer_flops_count = 0\n",
    "        if isinstance(layer_instance, (tf.keras.layers.InputLayer, type(tf.keras.Input(shape=())))):\n",
    "            return 0\n",
    "        if hasattr(layer_instance, 'layers') and isinstance(layer_instance.layers, (list, tuple)):\n",
    "            for sub_layer in layer_instance.layers:\n",
    "                layer_flops_count += _calculate_layer_flops(sub_layer, current_batch_size, current_seq_length)\n",
    "            return layer_flops_count\n",
    "        if not hasattr(layer_instance, 'input_shape') or layer_instance.input_shape is None:\n",
    "            return 0\n",
    "        if isinstance(layer_instance, layers.Dense):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            output_dim = layer_instance.output_shape[-1]\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            layer_flops_count += 2 * input_dim * output_dim * effective_batch_size\n",
    "        elif isinstance(layer_instance, layers.LSTM):\n",
    "            input_dim = layer_instance.input_shape[-1]\n",
    "            hidden_dim = layer_instance.units\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else 1\n",
    "            layer_flops_count += 8 * (input_dim + hidden_dim) * hidden_dim * seq_length * current_batch_size\n",
    "        elif isinstance(layer_instance, layers.Embedding):\n",
    "            pass\n",
    "        elif isinstance(layer_instance, DeepSeekMoELayer):\n",
    "            moe_input_dim = layer_instance.input_dim\n",
    "            effective_batch_size = current_batch_size\n",
    "            if len(layer_instance.input_shape) == 3:\n",
    "                effective_batch_size *= layer_instance.input_shape[1]\n",
    "            gate_flops = 2 * moe_input_dim * layer_instance.routed_experts * effective_batch_size\n",
    "            layer_flops_count += gate_flops\n",
    "            for expert in layer_instance.experts:\n",
    "                layer_flops_count += _calculate_layer_flops(expert, effective_batch_size, 1)\n",
    "        elif isinstance(layer_instance, layers.MultiHeadAttention):\n",
    "            config = layer_instance.get_config()\n",
    "            num_heads = config['num_heads']\n",
    "            key_dim = config['key_dim']\n",
    "            d_model = num_heads * key_dim\n",
    "            seq_length = layer_instance.input_shape[1] if len(layer_instance.input_shape) > 1 else current_seq_length\n",
    "            projection_flops = 3 * (2 * d_model * d_model) * seq_length * current_batch_size\n",
    "            attn_score_flops = num_heads * (2 * seq_length * key_dim * seq_length) * current_batch_size\n",
    "            context_flops = num_heads * (2 * seq_length * seq_length * key_dim) * current_batch_size\n",
    "            output_projection_flops = 2 * d_model * d_model * seq_length * current_batch_size\n",
    "            layer_flops_count += projection_flops + attn_score_flops + context_flops + output_projection_flops\n",
    "        elif isinstance(layer_instance, layers.TimeDistributed):\n",
    "            inner_layer = layer_instance.layer\n",
    "            td_effective_batch_size = current_batch_size * layer_instance.input_shape[1] if len(layer_instance.input_shape) == 3 else current_batch_size\n",
    "            layer_flops_count += _calculate_layer_flops(inner_layer, td_effective_batch_size, 1)\n",
    "        elif isinstance(layer_instance, TransformerDecoderLayer):\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha1, current_batch_size, model.max_seq_length)\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.mha2, current_batch_size, model.max_seq_length)\n",
    "            ffn_effective_batch_size = current_batch_size * model.max_seq_length\n",
    "            layer_flops_count += _calculate_layer_flops(layer_instance.ffn, ffn_effective_batch_size, 1)\n",
    "        return layer_flops_count\n",
    "    for layer in functional_model.layers:\n",
    "        flops += _calculate_layer_flops(layer, batch_size, model.max_seq_length)\n",
    "    return flops / 1e9\n",
    "\n",
    "def get_gpu_stats():\n",
    "    if nvidia_smi is None:\n",
    "        return 0, 0\n",
    "    try:\n",
    "        nvidia_smi.nvmlInit()\n",
    "        handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "        gpu_load = nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu\n",
    "        mem_info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "        gpu_memory = mem_info.used / (1024 ** 3)\n",
    "        nvidia_smi.nvmlShutdown()\n",
    "        return gpu_load, gpu_memory\n",
    "    except Exception:\n",
    "        return 0, 0\n",
    "\n",
    "class ResourceMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        super().__init__()\n",
    "        self.resource_stats = []\n",
    "        self.expert_load_history = []\n",
    "        self.start_time = None\n",
    "        self.epoch_start_time = None\n",
    "        self.flops = None\n",
    "        self.validation_data = validation_data\n",
    "        self.nlu_expert_usage_counts = None\n",
    "        self.nlg_expert_usage_counts = None\n",
    "        self.nlu_gate_weights_history = []\n",
    "        self.nlg_gate_weights_history = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.start_time = time.time()\n",
    "        try:\n",
    "            self.flops = calculate_flops(self.model, batch_size=moe_params[\"batch_size\"])\n",
    "            self.nlu_expert_usage_counts = np.zeros(self.model.moe_nlu.total_experts)\n",
    "            self.nlg_expert_usage_counts = np.zeros(self.model.moe_nlg.total_experts)\n",
    "        except Exception:\n",
    "            self.flops = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory()\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.resource_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'epoch_time': epoch_time,\n",
    "            'cpu_usage': cpu_usage,\n",
    "            'memory_used': memory.used / (1024 ** 3),\n",
    "            'memory_total': memory.total / (1024 ** 3),\n",
    "            'gpu_load': gpu_load,\n",
    "            'gpu_memory': gpu_memory,\n",
    "            'loss': logs.get('loss', 0),\n",
    "            'val_loss': logs.get('val_loss', 0),\n",
    "            'intent_accuracy': logs.get('intent_output_intent_accuracy', 0),\n",
    "            'val_intent_accuracy': logs.get('val_intent_output_intent_accuracy', 0),\n",
    "            'intent_precision': logs.get('intent_output_intent_precision', 0),\n",
    "            'val_intent_precision': logs.get('val_intent_output_intent_precision', 0),\n",
    "            'intent_recall': logs.get('intent_output_intent_recall', 0),\n",
    "            'val_intent_recall': logs.get('val_intent_output_intent_recall', 0),\n",
    "            'intent_f1': logs.get('intent_output_intent_f1', 0),\n",
    "            'val_intent_f1': logs.get('val_intent_output_intent_f1', 0),\n",
    "            'domain_accuracy': logs.get('domain_output_domain_accuracy', 0),\n",
    "            'val_domain_accuracy': logs.get('val_domain_output_domain_accuracy', 0),\n",
    "            'perplexity': logs.get('response_output_perplexity', 0),\n",
    "            'val_perplexity': logs.get('val_response_output_perplexity', 0),\n",
    "            'cosine_similarity': logs.get('response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'val_cosine_similarity': logs.get('val_response_embeddings_response_embedding_cosine_similarity', 0),\n",
    "            'nlu_load_balancing_loss': logs.get('moe_nlu_load_balancing_loss', 0),\n",
    "            'nlg_load_balancing_loss': logs.get('moe_nlg_load_balancing_loss', 0),\n",
    "        })\n",
    "        if 'moe_nlu_load_balancing_loss' in logs or 'moe_nlg_load_balancing_loss' in logs:\n",
    "            self.expert_load_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'nlu_expert_load_balance_loss': float(logs.get('moe_nlu_load_balancing_loss', 0)),\n",
    "                'nlg_expert_load_balance_loss': float(logs.get('moe_nlg_load_balancing_loss', 0))\n",
    "            })\n",
    "        sample_size = 100\n",
    "        for batch in self.validation_data.take(sample_size // moe_params[\"batch_size\"]):\n",
    "            inputs, _ = batch\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            nlu_gate_weights = outputs['nlu_gate_weights']\n",
    "            reshaped_nlu_weights = tf.reshape(nlu_gate_weights, [-1, self.model.moe_nlu.routed_experts])\n",
    "            self.nlu_gate_weights_history.append(reshaped_nlu_weights.numpy())\n",
    "            nlg_gate_weights = outputs['nlg_gate_weights']\n",
    "            reshaped_nlg_weights = tf.reshape(nlg_gate_weights, [-1, self.model.moe_nlg.routed_experts])\n",
    "            self.nlg_gate_weights_history.append(reshaped_nlg_weights.numpy())\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.total_time = time.time() - self.start_time\n",
    "        if len(self.nlu_gate_weights_history) > 0:\n",
    "            all_nlu_weights = np.concatenate(self.nlu_gate_weights_history, axis=0)\n",
    "            nlu_top_indices = np.argmax(all_nlu_weights, axis=1) + self.model.moe_nlu.k_s\n",
    "            for expert_id in nlu_top_indices:\n",
    "                if expert_id < self.model.moe_nlu.total_experts:\n",
    "                    self.nlu_expert_usage_counts[expert_id] += 1\n",
    "        if len(self.nlg_gate_weights_history) > 0:\n",
    "            all_nlg_weights = np.concatenate(self.nlg_gate_weights_history, axis=0)\n",
    "            nlg_top_indices = np.argmax(all_nlg_weights, axis=1) + self.model.moe_nlg.k_s\n",
    "            for expert_id in nlg_top_indices:\n",
    "                if expert_id < self.model.moe_nlg.total_experts:\n",
    "                    self.nlg_expert_usage_counts[expert_id] += 1\n",
    "\n",
    "    def visualize_expert_load_distribution(self):\n",
    "        total_nlu = np.sum(self.nlu_expert_usage_counts) or 1\n",
    "        total_nlg = np.sum(self.nlg_expert_usage_counts) or 1\n",
    "        nlu_percentages = (self.nlu_expert_usage_counts / total_nlu) * 100\n",
    "        nlg_percentages = (self.nlg_expert_usage_counts / total_nlg) * 100\n",
    "        nlu_stability = np.var(nlu_percentages)\n",
    "        nlg_stability = np.var(nlg_percentages)\n",
    "        return {'nlu_percentages': nlu_percentages, 'nlg_percentages': nlg_percentages, 'nlu_stability': nlu_stability, 'nlg_stability': nlg_stability}\n",
    "\n",
    "class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "try:\n",
    "    possible_paths = [\n",
    "        os.path.join(\"preprocessor_models\", \"preprocessor_params.json\"),\n",
    "        os.path.join(\"tf_datasets\", \"preprocessor_params.json\"),\n",
    "        \"preprocessor_params.json\"\n",
    "    ]\n",
    "    vocab_loaded = False\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                preprocessor_params = json.load(f)\n",
    "            word_to_id = {k: int(v) for k, v in preprocessor_params['word_to_id'].items()}\n",
    "            id_to_word = {int(k): v for k, v in preprocessor_params['id_to_word'].items()}\n",
    "            vocab_loaded = True\n",
    "            break\n",
    "    if not vocab_loaded:\n",
    "        raise FileNotFoundError(\"Vocabulary file not found in any location\")\n",
    "except Exception as e:\n",
    "    word_to_id = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    id_to_word = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
    "\n",
    "model_save_path = \"best_deepseekmoe_adaptive_gating_expert_choice_model\"\n",
    "custom_objects = {\n",
    "    \"DeepSeekMoELayer\": DeepSeekMoELayer,\n",
    "    \"CustomLearningRateSchedule\": CustomLearningRateSchedule,\n",
    "    \"MoEModel\": MoEModel,\n",
    "    \"TransformerDecoderLayer\": TransformerDecoderLayer,\n",
    "    \"IntentAccuracy\": IntentAccuracy,\n",
    "    \"IntentPrecision\": IntentPrecision,\n",
    "    \"IntentRecall\": IntentRecall,\n",
    "    \"IntentF1\": IntentF1,\n",
    "    \"DomainAccuracy\": DomainAccuracy,\n",
    "    \"Perplexity\": Perplexity,\n",
    "    \"ResponseEmbeddingCosineSimilarity\": ResponseEmbeddingCosineSimilarity\n",
    "}\n",
    "try:\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "except Exception as e:\n",
    "    raise\n",
    "\n",
    "def compile_model(model, moe_params):\n",
    "    learning_rate = CustomLearningRateSchedule(moe_params[\"embedding_dim\"])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "    def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_true_norm = tf.norm(y_true, axis=-1, keepdims=True)\n",
    "        y_pred_norm = tf.norm(y_pred, axis=-1, keepdims=True)\n",
    "        y_true = y_true / (y_true_norm + epsilon)\n",
    "        y_pred = y_pred / (y_pred_norm + epsilon)\n",
    "        cosine_sim = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        positive_loss = 1.0 - cosine_sim\n",
    "        mask = tf.cast(tf.reduce_sum(tf.abs(y_true), axis=-1) > 0, dtype=tf.float32)\n",
    "        masked_loss = positive_loss * mask\n",
    "        total_loss = tf.reduce_sum(masked_loss)\n",
    "        num_tokens = tf.reduce_sum(mask) + tf.keras.backend.epsilon()\n",
    "        return total_loss / num_tokens\n",
    "    losses_dict = {\n",
    "        'intent_output': tf.keras.losses.BinaryCrossentropy(),\n",
    "        'domain_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_output': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        'response_embeddings': contrastive_loss,\n",
    "    }\n",
    "    metrics_dict = {\n",
    "        'intent_output': [IntentAccuracy(), IntentPrecision(), IntentRecall(), IntentF1()],\n",
    "        'domain_output': [DomainAccuracy()],\n",
    "        'response_output': [Perplexity()],\n",
    "        'response_embeddings': [ResponseEmbeddingCosineSimilarity()]\n",
    "    }\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=losses_dict,\n",
    "        metrics=metrics_dict,\n",
    "        loss_weights={\n",
    "            'intent_output': 0.5,\n",
    "            'domain_output': 0.5,\n",
    "            'response_output': 1.0,\n",
    "            'response_embeddings': 1.0\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = compile_model(model, moe_params)\n",
    "sample_input = next(iter(train_tf_dataset.take(1)))\n",
    "model(sample_input[0])\n",
    "model.summary()\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self, model, test_dataset, moe_params, raw_data, word_to_id, id_to_word):\n",
    "        self.model = model\n",
    "        self.test_dataset = test_dataset\n",
    "        self.moe_params = moe_params\n",
    "        self.raw_data = raw_data\n",
    "        self.word_to_id = word_to_id\n",
    "        self.id_to_word = id_to_word\n",
    "        self.nlu_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.nlg_expert_usage_counts = np.zeros(moe_params[\"num_experts\"] * moe_params.get(\"m\", 2)) if \"num_experts\" in moe_params else np.zeros(1)\n",
    "        self.resource_stats = {\n",
    "            'cpu_usage': [],\n",
    "            'memory_used': [],\n",
    "            'gpu_load': [],\n",
    "            'gpu_memory': [],\n",
    "            'batch_times': []\n",
    "        }\n",
    "        self.special_tokens = {'<pad>', '<sos>', '<eos>', '<unk>'}\n",
    "        self.nlu_top_k = model.get_layer('moe_nlu').get_config().get('top_k', 2)\n",
    "        self.nlg_top_k = model.get_layer('moe_nlg').get_config().get('top_k', 2)\n",
    "\n",
    "    def evaluate(self):\n",
    "        def add_response_embeddings(features, targets):\n",
    "            response_tokens = targets['response_output']\n",
    "            response_embeddings = self.model.embedding(response_tokens)\n",
    "            targets['response_embeddings'] = response_embeddings\n",
    "            return features, targets\n",
    "        test_dataset_with_embeddings = self.test_dataset.map(add_response_embeddings, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        start_time = time.time()\n",
    "        test_results = self.model.evaluate(test_dataset_with_embeddings, verbose=1)\n",
    "        eval_time = time.time() - start_time\n",
    "        metric_names = self.model.metrics_names\n",
    "        perplexity_value = None\n",
    "        for name, value in zip(metric_names, test_results):\n",
    "            if name == 'response_output_perplexity':\n",
    "                perplexity_value = value\n",
    "                break\n",
    "        if perplexity_value is None:\n",
    "            perplexity_value = 0.0\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory().used / (1024 ** 3)\n",
    "        gpu_load, gpu_memory = get_gpu_stats()\n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "        total_tokens = 0\n",
    "        total_time = 0\n",
    "        num_batches = 0\n",
    "        test_dataset_small = test_dataset_with_embeddings.unbatch().batch(moe_params[\"batch_size\"])\n",
    "        all_true_intents = []\n",
    "        all_pred_intents = []\n",
    "        all_true_domains = []\n",
    "        all_pred_domains = []\n",
    "        for batch_idx, batch in enumerate(test_dataset_small):\n",
    "            inputs, targets = batch\n",
    "            response_tokens = targets['response_output'].numpy()\n",
    "            non_padding_mask = response_tokens != 0\n",
    "            total_tokens += np.sum(non_padding_mask)\n",
    "            start_time = time.time()\n",
    "            outputs = self.model(inputs, training=False)\n",
    "            batch_time = time.time() - start_time\n",
    "            total_time += batch_time\n",
    "            num_batches += tf.shape(inputs['user_utterance_tokens'])[0]\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory = psutil.virtual_memory()\n",
    "            gpu_load, gpu_memory = get_gpu_stats()\n",
    "            self.resource_stats['cpu_usage'].append(cpu_usage)\n",
    "            self.resource_stats['memory_used'].append(memory.used / (1024 ** 3))\n",
    "            self.resource_stats['gpu_load'].append(gpu_load)\n",
    "            self.resource_stats['gpu_memory'].append(gpu_memory)\n",
    "            self.resource_stats['batch_times'].append(batch_time)\n",
    "            nlu_gate_weights = outputs.get('nlu_gate_weights', tf.zeros([0, self.model.moe_nlu.routed_experts])).numpy()\n",
    "            nlu_top_indices = np.argsort(-nlu_gate_weights, axis=-1)[:, :self.nlu_top_k] + self.model.moe_nlu.k_s\n",
    "            for idx in range(self.nlu_top_k):\n",
    "                expert_ids = nlu_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlu_expert_usage_counts):\n",
    "                        self.nlu_expert_usage_counts[expert_id] += 1\n",
    "            nlg_gate_weights = outputs.get('nlg_gate_weights', tf.zeros([0, self.model.moe_nlg.routed_experts])).numpy()\n",
    "            if len(nlg_gate_weights.shape) == 3:\n",
    "                nlg_gate_weights = nlg_gate_weights.reshape(-1, nlg_gate_weights.shape[-1])\n",
    "            nlg_top_indices = np.argsort(-nlg_gate_weights, axis=-1)[:, :self.nlg_top_k] + self.model.moe_nlg.k_s\n",
    "            for idx in range(self.nlg_top_k):\n",
    "                expert_ids = nlg_top_indices[:, idx]\n",
    "                for expert_id in expert_ids:\n",
    "                    if expert_id < len(self.nlg_expert_usage_counts):\n",
    "                        self.nlg_expert_usage_counts[expert_id] += 1\n",
    "            true_intents = targets.get('intent_output', np.zeros((moe_params[\"batch_size\"], self.model.num_intents))).numpy()\n",
    "            pred_intents = (outputs.get('intent_output', np.zeros_like(true_intents)).numpy() > 0.5).astype(np.int32)\n",
    "            all_true_intents.append(true_intents)\n",
    "            all_pred_intents.append(pred_intents)\n",
    "            true_domains = targets.get('domain_output', np.zeros((moe_params[\"batch_size\"], 1))).numpy().flatten()\n",
    "            pred_domains = np.argmax(outputs.get('domain_output', np.zeros((moe_params[\"batch_size\"], self.model.num_domains))).numpy(), axis=-1)\n",
    "            all_true_domains.append(true_domains)\n",
    "            all_pred_domains.append(pred_domains)\n",
    "            if batch_idx % 50 == 0:\n",
    "                gc.collect()\n",
    "                tf.keras.backend.clear_session()\n",
    "        avg_time_per_token = (total_time / total_tokens) * 1000 if total_tokens > 0 else 0\n",
    "        avg_cpu = np.mean(self.resource_stats['cpu_usage']) if self.resource_stats['cpu_usage'] else 0\n",
    "        avg_memory = np.mean(self.resource_stats['memory_used']) if self.resource_stats['memory_used'] else 0\n",
    "        avg_gpu_load = np.mean(self.resource_stats['gpu_load']) if self.resource_stats['gpu_load'] else 0\n",
    "        avg_gpu_memory = np.mean(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        peak_gpu_memory = np.max(self.resource_stats['gpu_memory']) if self.resource_stats['gpu_memory'] else 0\n",
    "        flops = calculate_flops(self.model, batch_size=self.moe_params.get(\"batch_size\", moe_params[\"batch_size\"]))\n",
    "        all_true_intents = np.vstack(all_true_intents)\n",
    "        all_pred_intents = np.vstack(all_pred_intents)\n",
    "        all_true_domains = np.concatenate(all_true_domains)\n",
    "        all_pred_domains = np.concatenate(all_pred_domains)\n",
    "        intent_f1_score = f1_score(all_true_intents, all_pred_intents, average='micro')\n",
    "        intent_f1_score_macro = f1_score(all_true_intents, all_pred_intents, average='macro')\n",
    "        intent_f1_score_weighted = f1_score(all_true_intents, all_pred_intents, average='weighted')\n",
    "        intent_accuracy_score = accuracy_score(all_true_intents, all_pred_intents)\n",
    "        domain_accuracy_score = accuracy_score(all_true_domains, all_pred_domains)\n",
    "        domain_f1_score_macro = f1_score(all_true_domains, all_pred_domains, average='macro')\n",
    "        hypotheses = []\n",
    "        references = []\n",
    "        meteor_scores = []\n",
    "        rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        domain_metrics = defaultdict(lambda: {\n",
    "            'bleu': [],\n",
    "            'rouge1': [],\n",
    "            'rouge2': [],\n",
    "            'rougeL': [],\n",
    "            'meteor': [],\n",
    "            'count': 0\n",
    "        })\n",
    "        if not self.raw_data.get('test'):\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "            domain_metrics = {}\n",
    "        else:\n",
    "            for i, batch in enumerate(test_dataset_small):\n",
    "                if i >= len(self.raw_data.get('test', [])):\n",
    "                    break\n",
    "                try:\n",
    "                    inputs, targets = batch\n",
    "                    with tf.device('/CPU:0'):\n",
    "                        outputs = self.model(inputs, training=False)\n",
    "                    domain_id = targets['domain_output'].numpy()[0][0]\n",
    "                    domain_name = f\"domain_{domain_id}\"\n",
    "                    response_probs = outputs['response_output'].numpy()\n",
    "                    generated_tokens = np.argmax(response_probs, axis=-1)[0]\n",
    "                    true_tokens = targets['response_output'].numpy()[0]\n",
    "                    raw_item = self.raw_data['test'][i]\n",
    "                    ref_text = raw_item.get('raw_next_system_response', '')\n",
    "                    if not ref_text:\n",
    "                        continue\n",
    "                    gen_text = self.tokens_to_text(generated_tokens)\n",
    "                    ref_tokens = self.tokens_to_text(true_tokens)\n",
    "                    if not gen_text.strip() or not ref_text.strip():\n",
    "                        continue\n",
    "                    hypotheses.append(gen_text)\n",
    "                    references.append([ref_text])\n",
    "                    ref_tokens = word_tokenize(ref_text.lower())\n",
    "                    gen_tokens = word_tokenize(gen_text.lower())\n",
    "                    try:\n",
    "                        meteor = meteor_score([ref_tokens], gen_tokens)\n",
    "                        meteor_scores.append(meteor)\n",
    "                        domain_metrics[domain_name]['meteor'].append(meteor)\n",
    "                    except Exception as e:\n",
    "                        meteor_scores.append(0)\n",
    "                        domain_metrics[domain_name]['meteor'].append(0)\n",
    "                    try:\n",
    "                        rouge_scores = rouge_scorer_instance.score(ref_text, gen_text)\n",
    "                        domain_metrics[domain_name]['rouge1'].append(rouge_scores['rouge1'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rouge2'].append(rouge_scores['rouge2'].fmeasure)\n",
    "                        domain_metrics[domain_name]['rougeL'].append(rouge_scores['rougeL'].fmeasure)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                    domain_metrics[domain_name]['count'] += 1\n",
    "                    if i % 50 == 0:\n",
    "                        gc.collect()\n",
    "                        tf.keras.backend.clear_session()\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        if hypotheses and references:\n",
    "            hypotheses_tok = []\n",
    "            references_tok = []\n",
    "            for hyp, ref_list in zip(hypotheses, references):\n",
    "                if not hyp or not ref_list[0]:\n",
    "                    continue\n",
    "                hyp_tokens = word_tokenize(hyp.lower())\n",
    "                ref_tokens = [word_tokenize(ref.lower()) for ref in ref_list]\n",
    "                hypotheses_tok.append(hyp_tokens)\n",
    "                references_tok.append(ref_tokens)\n",
    "            chencherry = SmoothingFunction()\n",
    "            try:\n",
    "                bleu_score = corpus_bleu(references_tok, hypotheses_tok, smoothing_function=chencherry.method1)\n",
    "            except Exception as e:\n",
    "                bleu_score = 0.0\n",
    "            rouge_scores = []\n",
    "            for hyp, ref in zip(hypotheses, references):\n",
    "                if hyp and ref[0]:\n",
    "                    try:\n",
    "                        rouge_scores.append(rouge_scorer_instance.score(ref[0], hyp))\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "            avg_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores]) if rouge_scores else 0.0\n",
    "            avg_meteor = np.mean(meteor_scores) if meteor_scores else 0.0\n",
    "        else:\n",
    "            bleu_score = avg_rouge1 = avg_rouge2 = avg_rougeL = avg_meteor = 0.0\n",
    "        return {\n",
    "            'intent_f1_micro': intent_f1_score,\n",
    "            'intent_f1_macro': intent_f1_score_macro,\n",
    "            'intent_f1_weighted': intent_f1_score_weighted,\n",
    "            'intent_accuracy': intent_accuracy_score,\n",
    "            'domain_accuracy': domain_accuracy_score,\n",
    "            'domain_f1_macro': domain_f1_score_macro,\n",
    "            'bleu': bleu_score,\n",
    "            'rouge1': avg_rouge1,\n",
    "            'rouge2': avg_rouge2,\n",
    "            'rougeL': avg_rougeL,\n",
    "            'meteor': avg_meteor,\n",
    "            'perplexity': perplexity_value,\n",
    "            'pred_speed': avg_time_per_token,\n",
    "            'domain_metrics': domain_metrics,\n",
    "            'eval_time': eval_time\n",
    "        }\n",
    "    \n",
    "    def tokens_to_text(self, tokens):\n",
    "        words = []\n",
    "        for token in tokens:\n",
    "            if token == 0:\n",
    "                continue\n",
    "            word = self.id_to_word.get(token, '<unk>')\n",
    "            if word not in self.special_tokens:\n",
    "                words.append(word)\n",
    "        return \" \".join(words).strip()\n",
    "\n",
    "results_table = {\n",
    "    'Evaluation Time (seconds)': [],\n",
    "    'Prediction Speed (ms/token)': [],\n",
    "    'Average CPU Usage (percent)': [],\n",
    "    'Average GPU Usage (percent)': [],\n",
    "    'Average Memory (GB)': [],\n",
    "    'Average GPU Memory (GB)': [],\n",
    "    'Average FLOPs Estimate (GFLOPs)': [],\n",
    "    'NLU Expert 1 (percent)': [],\n",
    "    'NLU Expert 2 (percent)': [],\n",
    "    'NLU Expert 3 (percent)': [],\n",
    "    'NLU Expert 4 (percent)': [],\n",
    "    'NLU Expert 5 (percent)': [],\n",
    "    'NLU Expert 6 (percent)': [],\n",
    "    'NLU Expert 7 (percent)': [],\n",
    "    'NLU Expert 8 (percent)': [],\n",
    "    'NLG Expert 1 (percent)': [],\n",
    "    'NLG Expert 2 (percent)': [],\n",
    "    'NLG Expert 3 (percent)': [],\n",
    "    'NLG Expert 4 (percent)': [],\n",
    "    'NLG Expert 5 (percent)': [],\n",
    "    'NLG Expert 6 (percent)': [],\n",
    "    'NLG Expert 7 (percent)': [],\n",
    "    'NLG Expert 8 (percent)': [],\n",
    "    'Intent F1-score (Micro)': [],\n",
    "    'Intent F1-score (Macro)': [],\n",
    "    'Intent F1-score (Weighted)': [],\n",
    "    'Intent Accuracy': [],\n",
    "    'Domain Accuracy': [],\n",
    "    'Domain F1-score (Macro)': [],\n",
    "    'BLEU Score': [],\n",
    "    'ROUGE-1 F1': [],\n",
    "    'ROUGE-2 F1': [],\n",
    "    'ROUGE-L F1': [],\n",
    "    'METEOR Score': [],\n",
    "    'Perplexity': [],\n",
    "    'NLU Expert Load Stability (Variance)': [],\n",
    "    'NLG Expert Load Stability (Variance)': []\n",
    "}\n",
    "\n",
    "num_iterations = 5\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    model = tf.keras.models.load_model(model_save_path, custom_objects=custom_objects, compile=False)\n",
    "    model = compile_model(model, moe_params)\n",
    "    evaluator = EvaluationMetrics(model, test_tf_dataset, moe_params, raw_data, word_to_id, id_to_word)\n",
    "    evaluation_results = evaluator.evaluate()\n",
    "    results_table['Evaluation Time (seconds)'].append(evaluation_results['eval_time'])\n",
    "    results_table['Prediction Speed (ms/token)'].append(evaluation_results['pred_speed'])\n",
    "    results_table['Average CPU Usage (percent)'].append(np.mean(evaluator.resource_stats['cpu_usage']) if evaluator.resource_stats['cpu_usage'] else 0)\n",
    "    results_table['Average GPU Usage (percent)'].append(np.mean(evaluator.resource_stats['gpu_load']) if evaluator.resource_stats['gpu_load'] else 0)\n",
    "    results_table['Average Memory (GB)'].append(np.mean(evaluator.resource_stats['memory_used']) if evaluator.resource_stats['memory_used'] else 0)\n",
    "    results_table['Average GPU Memory (GB)'].append(np.mean(evaluator.resource_stats['gpu_memory']) if evaluator.resource_stats['gpu_memory'] else 0)\n",
    "    results_table['Average FLOPs Estimate (GFLOPs)'].append(calculate_flops(model))\n",
    "    nlu_counts = evaluator.nlu_expert_usage_counts\n",
    "    nlg_counts = evaluator.nlg_expert_usage_counts\n",
    "    total_nlu = np.sum(nlu_counts) if np.sum(nlu_counts) > 0 else 1\n",
    "    total_nlg = np.sum(nlg_counts) if np.sum(nlg_counts) > 0 else 1\n",
    "    results_table['NLU Expert 1 (percent)'].append((nlu_counts[0] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 2 (percent)'].append((nlu_counts[1] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 3 (percent)'].append((nlu_counts[2] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 4 (percent)'].append((nlu_counts[3] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 5 (percent)'].append((nlu_counts[4] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 6 (percent)'].append((nlu_counts[5] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 7 (percent)'].append((nlu_counts[6] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLU Expert 8 (percent)'].append((nlu_counts[7] / total_nlu * 100 if total_nlu > 0 else 0))\n",
    "    results_table['NLG Expert 1 (percent)'].append((nlg_counts[0] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 2 (percent)'].append((nlg_counts[1] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 3 (percent)'].append((nlg_counts[2] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 4 (percent)'].append((nlg_counts[3] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 5 (percent)'].append((nlg_counts[4] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 6 (percent)'].append((nlg_counts[5] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 7 (percent)'].append((nlg_counts[6] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['NLG Expert 8 (percent)'].append((nlg_counts[7] / total_nlg * 100 if total_nlg > 0 else 0))\n",
    "    results_table['Intent F1-score (Micro)'].append(evaluation_results['intent_f1_micro'])\n",
    "    results_table['Intent F1-score (Macro)'].append(evaluation_results['intent_f1_macro'])\n",
    "    results_table['Intent F1-score (Weighted)'].append(evaluation_results['intent_f1_weighted'])\n",
    "    results_table['Intent Accuracy'].append(evaluation_results['intent_accuracy'])\n",
    "    results_table['Domain Accuracy'].append(evaluation_results['domain_accuracy'])\n",
    "    results_table['Domain F1-score (Macro)'].append(evaluation_results['domain_f1_macro'])\n",
    "    results_table['BLEU Score'].append(evaluation_results['bleu'])\n",
    "    results_table['ROUGE-1 F1'].append(evaluation_results['rouge1'])\n",
    "    results_table['ROUGE-2 F1'].append(evaluation_results['rouge2'])\n",
    "    results_table['ROUGE-L F1'].append(evaluation_results['rougeL'])\n",
    "    results_table['METEOR Score'].append(evaluation_results['meteor'])\n",
    "    results_table['Perplexity'].append(evaluation_results['perplexity'])\n",
    "    results_table['NLU Expert Load Stability (Variance)'].append(np.var(nlu_counts / total_nlu * 100) if total_nlu > 0 else 0)\n",
    "    results_table['NLG Expert Load Stability (Variance)'].append(np.var(nlg_counts / total_nlg * 100) if total_nlg > 0 else 0)\n",
    "\n",
    "for key in results_table:\n",
    "    if results_table[key]:\n",
    "        results_table[key].append(np.mean(results_table[key]))\n",
    "    else:\n",
    "        results_table[key].append(0)\n",
    "\n",
    "final_df = pd.DataFrame(results_table)\n",
    "final_df = final_df.T\n",
    "final_df.columns = [f'Iteration {i+1}' for i in range(num_iterations)] + ['Average']\n",
    "final_df.to_excel('prediction_deepseekmoe_adaptive_gating_expert_choice_results.xlsx', index=False)\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
