{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 12:53:29,164] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 12:53:33.543579: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-26 12:53:34.166014: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-26 12:53:34.166150: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-26 12:53:34.256742: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-26 12:53:34.462734: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-26 12:53:36.423700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- System Information ---\n",
      "Operating System: Linux 5.19.0-45-generic\n",
      "Python Version: 3.11.7 (main, Dec  8 2023, 18:56:58) [GCC 11.4.0]\n",
      "--- CPU Information ---\n",
      "CPU: x86_64\n",
      "Physical Cores: 8\n",
      "Logical Threads: 8\n",
      "--- Memory Information ---\n",
      "Total Memory: 44.08 GB\n",
      "Available Memory: 18.54 GB\n",
      "--- GPU Information ---\n",
      "GPU 1: NVIDIA RTX A4000, Driver Version: 550.144.03\n",
      "CUDA Version: 12.4     |\n",
      "--- DeepSpeed Information ---\n",
      "DeepSpeed Version: 0.10.3\n",
      "--- TensorFlow Information ---\n",
      "TensorFlow Version: 2.15.0\n",
      "Mon May 26 12:53:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               Off |   00000000:00:05.0 Off |                  Off |\n",
      "| 41%   49C    P2             38W /  140W |   14591MiB /  16376MiB |     17%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "import psutil\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "def get_os_info():\n",
    "    \"\"\"Retrieves operating system information.\"\"\"\n",
    "    os_name = platform.system()\n",
    "    os_version = platform.release()\n",
    "    return os_name, os_version\n",
    "\n",
    "def get_python_info():\n",
    "    \"\"\"Retrieves Python version information.\"\"\"\n",
    "    python_version = sys.version\n",
    "    return python_version\n",
    "\n",
    "def get_cpu_info():\n",
    "    \"\"\"Retrieves CPU information.\"\"\"\n",
    "    cpu_name = platform.processor()\n",
    "    cpu_cores = psutil.cpu_count(logical=False) \n",
    "    cpu_threads = psutil.cpu_count(logical=True) \n",
    "    return cpu_name, cpu_cores, cpu_threads\n",
    "\n",
    "def get_memory_info():\n",
    "    \"\"\"Retrieves memory information.\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    total_memory_gb = mem.total / (1024 ** 3) \n",
    "    available_memory_gb = mem.available / (1024 ** 3)\n",
    "    return total_memory_gb, available_memory_gb\n",
    "\n",
    "def get_gpu_info():\n",
    "    \"\"\"Retrieves GPU information using nvidia-smi command.\"\"\"\n",
    "    try:\n",
    "        subprocess.run([\"nvidia-smi\", \"-h\"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        result = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,driver_version\", \"--format=csv,noheader\"],\n",
    "                                 capture_output=True, text=True, check=True)\n",
    "        cuda_result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True, check=True)\n",
    "        cuda_version_line = next((line for line in cuda_result.stdout.split('\\n') if \"CUDA Version\" in line), None)\n",
    "        cuda_version = cuda_version_line.split(\":\")[-1].strip() if cuda_version_line else \"N/A\"\n",
    "\n",
    "        gpus = []\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            name, driver_version = line.split(', ')\n",
    "            gpus.append({'name': name, 'driver_version': driver_version})\n",
    "        return gpus, cuda_version\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return \"N/A\", \"N/A\"\n",
    "\n",
    "def check_deepspeed():\n",
    "    \"\"\"Checks for DeepSpeed installation and version.\"\"\"\n",
    "    deepspeed_spec = importlib.util.find_spec(\"deepspeed\")\n",
    "    if deepspeed_spec is not None:\n",
    "        try:\n",
    "            import deepspeed\n",
    "            return deepspeed.__version__\n",
    "        except ImportError:\n",
    "            return \"Installed, but version cannot be determined\"\n",
    "    else:\n",
    "        return \"Not Installed\"\n",
    "\n",
    "def check_tensorflow():\n",
    "    \"\"\"Checks for TensorFlow installation and version.\"\"\"\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        return tf.__version__\n",
    "    except ImportError:\n",
    "        return \"Not Installed\"\n",
    "\n",
    "def main():\n",
    "    \"\"\"Prints system information.\"\"\"\n",
    "    os_name, os_version = get_os_info()\n",
    "    python_version = get_python_info()\n",
    "    cpu_name, cpu_cores, cpu_threads = get_cpu_info()\n",
    "    total_memory_gb, available_memory_gb = get_memory_info()\n",
    "    gpus, cuda_version = get_gpu_info()\n",
    "    deepspeed_version = check_deepspeed()\n",
    "    tensorflow_version = check_tensorflow()\n",
    "\n",
    "    print(\"--- System Information ---\")\n",
    "    print(f\"Operating System: {os_name} {os_version}\")\n",
    "    print(f\"Python Version: {python_version}\")\n",
    "    print(\"--- CPU Information ---\")\n",
    "    print(f\"CPU: {cpu_name}\")\n",
    "    print(f\"Physical Cores: {cpu_cores}\")\n",
    "    print(f\"Logical Threads: {cpu_threads}\")\n",
    "    print(\"--- Memory Information ---\")\n",
    "    print(f\"Total Memory: {total_memory_gb:.2f} GB\")\n",
    "    print(f\"Available Memory: {available_memory_gb:.2f} GB\")\n",
    "    print(\"--- GPU Information ---\")\n",
    "    if gpus == \"N/A\":\n",
    "        print(\"GPU: N/A (No NVIDIA GPU or nvidia-smi not found)\")\n",
    "        print(f\"CUDA Version: N/A\")\n",
    "    else:\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"GPU {i + 1}: {gpu['name']}, Driver Version: {gpu['driver_version']}\")\n",
    "        print(f\"CUDA Version: {cuda_version}\")\n",
    "    print(\"--- DeepSpeed Information ---\")\n",
    "    print(f\"DeepSpeed Version: {deepspeed_version}\")\n",
    "    print(\"--- TensorFlow Information ---\")\n",
    "    print(f\"TensorFlow Version: {tensorflow_version}\")\n",
    "main()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322M\t/notebooks/MultiWOZ-coref\n",
      "9.9M\t/notebooks/.ipynb_checkpoints\n",
      "341M\t/notebooks\n"
     ]
    }
   ],
   "source": [
    "! du -h --max-depth=1 /notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rm -rf /notebooks/.Trash-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MultiWOZ-coref'...\n",
      "remote: Enumerating objects: 60, done.\u001b[K\n",
      "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
      "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
      "remote: Total 60 (delta 19), reused 4 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (60/60), 29.40 MiB | 14.42 MiB/s, done.\n",
      "Resolving deltas: 100% (19/19), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lexmen318/MultiWOZ-coref.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully opened the zip file: MultiWOZ-coref/MultiWOZ2_3.zip\n",
      "\n",
      "Extracted all files to: MultiWOZ-coref\n",
      "\n",
      "Contents of the extracted directory:\n",
      "- appendix_new.pdf\n",
      "- README.md\n",
      "- MultiWOZ2_3.zip\n",
      "- .git\n",
      "- MultiWOZ2_3\n"
     ]
    }
   ],
   "source": [
    "zip_file_path = \"MultiWOZ-coref/MultiWOZ2_3.zip\"\n",
    "extraction_path = \"MultiWOZ-coref\"\n",
    "\n",
    "zip_ref = zipfile.ZipFile(zip_file_path, 'r')\n",
    "print(f\"Successfully opened the zip file: {zip_file_path}\")\n",
    "\n",
    "os.makedirs(extraction_path, exist_ok=True)\n",
    "\n",
    "zip_ref.extractall(extraction_path)\n",
    "print(f\"\\nExtracted all files to: {extraction_path}\")\n",
    "\n",
    "print(\"\\nContents of the extracted directory:\")\n",
    "for item in os.listdir(extraction_path):\n",
    "    print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.11/dist-packages (0.8.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nlpaug in /usr/local/lib/python3.11/dist-packages (1.1.11)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (1.26.3)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.31.0)\n",
      "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (4.7.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2023.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.22.0->nlpaug) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.22.0->nlpaug) (2020.6.20)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: dateparser in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2025.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2024.11.6)\n",
      "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser) (5.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.0->dateparser) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: CurrencyConverter in /usr/local/lib/python3.11/dist-packages (0.18.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: dateparser in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: word2number in /usr/local/lib/python3.11/dist-packages (1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2025.2)\n",
      "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.11/dist-packages (from dateparser) (2024.11.6)\n",
      "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.11/dist-packages (from dateparser) (5.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.0->dateparser) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: contractions in /usr/local/lib/python3.11/dist-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.11/dist-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (0.3.3)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.11/dist-packages (from textsearch>=0.0.21->contractions) (2.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.11.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (6.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: GPUtil in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.66.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.11/dist-packages (7.352.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (12.0.0)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml) (12.575.51)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 05:54:34.189923: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-04 05:54:34.680307: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-04 05:54:34.680418: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-04 05:54:34.765220: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-04 05:54:34.932189: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-04 05:54:36.439998: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install pyspellchecker\n",
    "!pip install nlpaug\n",
    "!pip install dateparser\n",
    "!pip install CurrencyConverter\n",
    "!pip install dateparser word2number\n",
    "!pip install contractions\n",
    "!pip install gensim\n",
    "!pip install GPUtil\n",
    "!pip install rouge_score\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install pynvml\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import zipfile\n",
    "import contractions\n",
    "import dateparser\n",
    "from word2number import w2n\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, Features, Value, Sequence, ClassLabel\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dir = \"MultiWOZ-coref/MultiWOZ2_3\"\n",
    "data_file = os.path.join(dataset_dir, \"data.json\")\n",
    "ontology_file = os.path.join(dataset_dir, \"ontology.json\")\n",
    "dialogue_acts_file = os.path.join(dataset_dir, \"dialogue_acts.json\")\n",
    "\n",
    "with open(data_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "with open(ontology_file, 'r') as f:\n",
    "    ontology = json.load(f)\n",
    "with open(dialogue_acts_file, 'r') as f:\n",
    "    dialogue_acts = json.load(f)\n",
    "\n",
    "def get_primary_domain(dialogue):\n",
    "    \"\"\"Extract the primary domain from dialogue structure.\"\"\"\n",
    "    if \"new_goal\" in dialogue and dialogue[\"new_goal\"]:\n",
    "        for domain in dialogue[\"new_goal\"]:\n",
    "            if domain != \"user_action\":\n",
    "                return domain\n",
    "    \n",
    "    if \"goal\" in dialogue and dialogue[\"goal\"]:\n",
    "        for domain in dialogue[\"goal\"]:\n",
    "            if domain != \"user_action\" and isinstance(dialogue[\"goal\"][domain], dict):\n",
    "                return domain\n",
    "    domains = set()\n",
    "    for turn in dialogue.get(\"log\", []):\n",
    "        metadata = turn.get(\"metadata\", {})\n",
    "        for domain in metadata:\n",
    "            if domain != \"user_action\" and isinstance(metadata[domain], dict):\n",
    "                domains.add(domain)\n",
    "    return domains.pop() if domains else \"unknown\"\n",
    "\n",
    "raw_data = [\n",
    "    {\"dialogue_id\": dialogue_id, \"dialogue\": dialogue, \"domain\": get_primary_domain(dialogue)}\n",
    "    for dialogue_id, dialogue in data.items()\n",
    "]\n",
    "\n",
    "train_data_raw, test_data_raw = train_test_split(\n",
    "    raw_data,\n",
    "    test_size=0.2,\n",
    "    stratify=[d[\"domain\"] for d in raw_data],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Taking samples from each dataset ---\n",
      "Sampled train_dataset size: 1600\n",
      "Sampled test_dataset size: 400\n"
     ]
    }
   ],
   "source": [
    "def get_samples(dataset, num_samples):\n",
    "    if not dataset:\n",
    "        print(\"Warning: Dataset is empty. Returning an empty list.\")\n",
    "        return []\n",
    "    if not isinstance(dataset, (list, tuple)):\n",
    "        try:\n",
    "            dataset_list = list(dataset)\n",
    "        except TypeError:\n",
    "            print(\"Error: Dataset could not be converted to a list for sampling.\")\n",
    "            return []\n",
    "    else:\n",
    "        dataset_list = list(dataset)\n",
    "    domain_counts = {}\n",
    "    for item in dataset_list:\n",
    "        if 'domain' in item:\n",
    "            domain = item['domain']\n",
    "            domain_counts[domain] = domain_counts.get(domain, 0) + 1\n",
    "        else:\n",
    "            print(f\"Warning: Item {item} does not have a 'domain' key. Skipping for stratification.\")\n",
    "    total_items = len(dataset_list)\n",
    "    if total_items == 0:\n",
    "        print(\"Warning: Dataset has no items. Returning an empty list.\")\n",
    "        return []\n",
    "    actual_samples_to_take = min(num_samples, total_items)\n",
    "    if actual_samples_to_take < num_samples:\n",
    "        print(f\"Warning: Dataset size ({total_items}) is less than requested samples ({num_samples}). Taking {actual_samples_to_take} samples.\")\n",
    "    sampled_data = []\n",
    "    random.seed(42)  \n",
    "    items_by_domain = {domain: [] for domain in domain_counts}\n",
    "    for item in dataset_list:\n",
    "        if 'domain' in item:\n",
    "            items_by_domain[item['domain']].append(item)\n",
    "    for domain, count in domain_counts.items():\n",
    "        domain_proportion = count / total_items\n",
    "        num_domain_samples = round(actual_samples_to_take * domain_proportion)\n",
    "        num_domain_samples = min(num_domain_samples, len(items_by_domain[domain]))\n",
    "        sampled_domain_items = random.sample(items_by_domain[domain], num_domain_samples)\n",
    "        sampled_data.extend(sampled_domain_items)\n",
    "    if len(sampled_data) < actual_samples_to_take:\n",
    "        remaining_items = [item for item in dataset_list if item not in sampled_data]\n",
    "        num_to_add = actual_samples_to_take - len(sampled_data)\n",
    "        if remaining_items and num_to_add > 0:\n",
    "            sampled_data.extend(random.sample(remaining_items, min(num_to_add, len(remaining_items))))\n",
    "    elif len(sampled_data) > actual_samples_to_take:\n",
    "        num_to_remove = len(sampled_data) - actual_samples_to_take\n",
    "        for _ in range(num_to_remove):\n",
    "            sampled_data.pop(random.randrange(len(sampled_data)))\n",
    "    random.shuffle(sampled_data)\n",
    "    \n",
    "    return sampled_data\n",
    "\n",
    "print(f\"\\n--- Taking samples from each dataset ---\")\n",
    "\n",
    "train_data_raw = get_samples(train_data_raw, 1600)\n",
    "print(f\"Sampled train_dataset size: {len(train_data_raw)}\")\n",
    "\n",
    "test_data_raw = get_samples(test_data_raw, 400)\n",
    "print(f\"Sampled test_dataset size: {len(test_data_raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training example:\n",
      "{'dialogue_id': 'MUL0185.json', 'domain': 'restaurant', 'dialogue': [{'state': '{}', 'system_response': '', 'turn_id': 0, 'user_utterance': 'Yes I need a cheap restaurant in the Cambridge area on the north side of town . What do you suggest ?'}, {'state': '{}', 'system_response': 'thank you I will go to royal spice .', 'turn_id': 1, 'user_utterance': ''}, {'state': '{}', 'system_response': '', 'turn_id': 2, 'user_utterance': 'Okay can you book me a table for Saturday at 19:45 for 3 people ? I would also like the reference number for the booking .'}, {'state': '{}', 'system_response': 'I have successfully booked your reservation . Your reference number is 7XYDZ38V . Can I help you with anything else ?', 'turn_id': 3, 'user_utterance': ''}, {'state': '{}', 'system_response': '', 'turn_id': 4, 'user_utterance': 'Yes , I also need a place to stay . I would prefer at least 4 stars and free parking .'}, {'state': '{}', 'system_response': 'We have 19 entries that match your search . Do you have any further preferences ?', 'turn_id': 5, 'user_utterance': ''}, {'state': '{}', 'system_response': '', 'turn_id': 6, 'user_utterance': 'I would like the place to have the same price range as the restaurant , so cheap .'}, {'state': '{}', 'system_response': 'The worth house is a hotel that is also on the north side of town and it is cheap . Would you like to book a room there ?', 'turn_id': 7, 'user_utterance': ''}, {'state': '{}', 'system_response': '', 'turn_id': 8, 'user_utterance': 'I would prefer one that has free parking and a star rating of 4 .'}, {'state': '{}', 'system_response': 'The worth house does meet your stated criteria .', 'turn_id': 9, 'user_utterance': ''}, {'state': '{}', 'system_response': '', 'turn_id': 10, 'user_utterance': 'Okay , could you also tell me the area of the hotel ?'}, {'state': '{}', 'system_response': 'Absolutely , the Worth House is located in the North Area . Would you like a reservation ?', 'turn_id': 11, 'user_utterance': ''}, {'state': '{}', 'system_response': '', 'turn_id': 12, 'user_utterance': \"Yes , that 's perfect . Thank you for all of your help !\"}, {'state': '{}', 'system_response': 'What day would you like the reservation for ?', 'turn_id': 13, 'user_utterance': ''}, {'state': '{}', 'system_response': '', 'turn_id': 14, 'user_utterance': 'Saturday at 19:45'}, {'state': '{}', 'system_response': 'For how many people and how many nights would you like that hotel reservation ?', 'turn_id': 15, 'user_utterance': ''}, {'state': '{}', 'system_response': '', 'turn_id': 16, 'user_utterance': 'Actually , I will go ahead and make a reservation for a later time . But , thank you for all the info . I think that is all I needed .'}, {'state': '{}', 'system_response': 'If there is nothing else , have a good day .', 'turn_id': 17, 'user_utterance': ''}], 'dialogue_acts': {'0': {}, '1': {}, '10': {}, '11': {}, '12': {}, '13': {}, '14': {}, '15': {}, '16': {}, '17': {}, '18': None, '19': None, '2': {}, '20': None, '21': None, '22': None, '23': None, '24': None, '25': None, '26': None, '27': None, '28': None, '29': None, '3': {}, '30': None, '31': None, '32': None, '33': None, '34': None, '35': None, '36': None, '37': None, '38': None, '39': None, '4': {}, '40': None, '41': None, '42': None, '43': None, '5': {}, '6': {}, '7': {}, '8': {}, '9': {}}, 'ontology': {'attraction-area': None, 'attraction-name': None, 'attraction-type': None, 'hospital-department': None, 'hotel-area': None, 'hotel-book day': None, 'hotel-book people': None, 'hotel-book stay': None, 'hotel-internet': None, 'hotel-name': None, 'hotel-parking': None, 'hotel-pricerange': None, 'hotel-stars': None, 'hotel-type': None, 'restaurant-area': ['north', 'east', 'dontcare', 'south', 'centre', 'west', 'east|south'], 'restaurant-book day': ['sunday|thursday', 'wednesday', 'saturday', 'sunday', 'saturday|thursday', 'friday', 'dontcare', 'monday', 'thursday', 'tuesday'], 'restaurant-book people': ['2', '7', '8', '5', '1', '6', '4|7', '3', '4'], 'restaurant-book time': ['12:00', '13:30', '10:00', '21:00', '7pm', '1345', '19:30', '21:45', '15:00', '01:00', '16:30', '14:15', '03:00', '10:45', '18:30', '15:15', '17:00|16:00', '4pm', '15:30|16:30', '9', '14:00', '17:45', '09:45', '1715', '16:45', '12:45', '1330', '20:45', '11:00', '12:15', '10:30', '16:15', '15:45', '11:30', '20:00', '19:45', '10:48', '13:45', '18:00|12:15', '18:15', '06:30', '20:30', '11:30|12:30', '17:30', '12:30', '09:15', '14:45', 'dontcare', '13:00', '14:30', '20:15', '10:15', '09:30', '18:00', '14:40', '09:00', '17:00', '16:00', '17:30|16:30', '13:15', '1430', '1145', '8pm', '08:45', '11:15', '22:00', '18:45', '19:15', '17:15', '15:30', '19:00', '11:45'], 'restaurant-food': ['north american>indian', 'portugese', 'polynesian', 'austrian', 'greek', 'asian', 'modern global', 'korean', 'danish', 'chinese', 'bistro', 'afternoon tea', 'kosher|british', 'tuscan', 'indonesian', 'spanish|portuguese', 'north african', 'north indian', 'the americas', 'italian', 'romanian', 'welsh', 'french', 'modern american', 'new zealand', 'sri lankan', 'traditional american', 'thai', 'fusion', 'molecular gastronomy', 'brazilian|portuguese', 'south african', 'european', 'kosher', 'jamaican>chinese', 'modern english', 'italian|indian', 'mexican', 'turkish', 'australian|indian', 'halal', 'german', 'dojo noodle bar', 'thai and chinese', 'international', 'australian', 'indian|african', 'chinese|mexican', 'american', 'north american', 'eastern european', 'caribbean>indian', 'afghan', 'seafood', 'moroccan', 'russian', 'latin american', 'creative', 'eritrean', 'unusual', 'caribbean', 'spanish', 'sushi', 'vegetarian', 'corsica', 'barbeque>modern european', 'light bites', 'world', 'barbeque', 'vietnamese', 'dontcare', 'african', 'south indian', 'panasian', 'persian', 'modern eclectic', 'gastropub', 'japanese', 'cuban', 'middle eastern', 'canapes', 'lebanese', 'jamaican', 'northern european', 'basque', 'scottish', 'christmas', 'irish', 'indian', 'belgian', 'british', 'hungarian', 'catalan', 'english', 'crossover', 'mediterranean', 'malaysian', 'swiss', 'swedish', 'polish', 'brazilian', 'traditional', 'steakhouse', 'singaporean', 'asian oriental', 'venetian', 'scandinavian', 'cantonese', 'modern european'], 'restaurant-name': ['copper kettle', 'taj tandoori', 'charlie chan', 'one seven', 'dif', 'eraina and michaelhouse cafe', 'mahal of cambridge', 'restaurant two two', 'curry prince|rajmahal', 'golden curry', 'missing sock', 'frankie and bennys', 'kymmoy', 'cam', 'dojo noodle bar|j restaurant', 'the bedouin', 'restaurant alimentum', 'gourmet burger kitchen', 'dojo noodle  bar|j restaurant', 'la margherita', 'golden house', 'chiquito', 'darrys cookhouse and wine shop', 'scudamores punt', 'eraina', 'the varsity restaurant', 'pizza hut fenditton', 'saint johns chop house', 'curry garden', 'sala thong|bangkok city', 'jinling noodle bar', 'sitar tandoori', 'pizza hut', 'the Nirala', 'city stop restaurant', 'de luca cucina and bar riverside brasserie', 'rice boat', 'wise buddha', 'restaurant one seven', 'meze bar restaurant', 'yipee noodle bar', 'little seoul', 'rajmahal', 'ali baba', 'limehouse', 'the grafton hotel', 'barbakan', 'sesame restaurant and bar', 'golden wok', 'pizza hut city centre', 'binh', 'hotel du vin and bistro', 'lan hong house', 'tandoori Palace', 'alimentum', 'the oak bistro', 'anatolia', 'ian hong house', 'two two and cote', 'curry queen', 'backstreet bistro', 'mahal', 'la mimosa', 'shanghai family restaurant', 'molecular gastronomy', 'efes', 'cote', 'good luck', 'cafe uno', 'oak bistro', 'european', 'saffron brasserie', 'gardenia', 'de luca cucina and bar', 'two two', 'ashley hotel', 'the hotpot', 'michaelhouse cafe', 'yu garden', 'gourmet formal kitchen', 'lovel', 'efes restaurant', 'the peking restaurant: ', 'river bar steakhouse and grill', 'j restaurant', 'the slug and lettuce', 'yippee noodle bar', 'meze bar', 'rice house', 'clowns cafe', 'no', 'hobsons house', 'dojo noodle bar', 'the kohinoor', 'royal standard', 'bloomsbury restaurant', 'graffiti', 'el shaddia guesthouse', 'worth house', 'shiraz', 'the golden house', 'shanghai', 'don pasquale pizzeria', 'tandoori palace', 'autumn house', 'panahar', 'the peking', 'wagamama', 'tandoori', 'nusha', 'the gandhi', 'nandos city centre', 'slug and lettuce', 'sitar', 'alex', 'meghna', 'cambridge chop house', 'the missing sock', 'primavera', 'the meze bar', 'travellers rest', 'south', 'curry king', 'pipasha restaurant', 'cambridge punter', 'saigon city', 'bedouin', 'pizza express', 'ask', 'broughton house gallery', 'tang chinese', 'curry prince', 'dontcare', 'charlie', 'nirala', 'nandos', 'cotto', 'parkside pools', 'galleria', 'adden', 'funky', 'maharajah tandoori restaurant', 'nil', 'the cow pizza kitchen and bar', 'bridge', 'grafton hotel', 'india west', 'cow pizza kitchen and bar', 'midsummer house restaurant', 'cambridge be', 'the lucky star', 'hakka', 'prezzo', 'cambridge lodge restaurant', 'sala thong', 'Kohinoor', 'zizzi cambridge', 'pizza hut cherry hinton', '4 kings parade city centre', 'golden wok|nirala', 'nus', 'fitzbillies restaurant', 'lucky star', 'la tasca', 'loch fyne', 'cityr', 'the gardenia', 'bangkok city', 'chiquito restaurant bar', 'anatolia and efes restaurant', 'ugly duckling', 'cocum', 'hk fusion', 'stazione restaurant and coffee bar', 'restaurant 22', 'grafton hotel restaurant', 'the maharajah tandoor', 'the alex', 'thanh binh', 'the river bar steakhouse and grill', 'india house', 'peking restaurant', 'hotpot', 'kohinoor', 'la raza', 'da vinci pizzeria', 'pizza hut cherry hinton|alimentum', 'royal spice', 'riverside brasserie', 'kitchen and bar'], 'restaurant-pricerange': ['expensive', 'cheap', 'moderate', 'dontcare', 'moderate|cheap'], 'taxi-arriveBy': None, 'taxi-departure': None, 'taxi-destination': None, 'taxi-leaveAt': None, 'train-arriveBy': None, 'train-book people': None, 'train-day': None, 'train-departure': None, 'train-destination': None, 'train-leaveAt': None}}\n"
     ]
    }
   ],
   "source": [
    "def normalize_time_in_text(text):\n",
    "    def time_repl_hhmm_ampm(match):\n",
    "        try:\n",
    "            parsed_time = dateparser.parse(match.group(0))\n",
    "            return parsed_time.strftime('%H:%M') if parsed_time else match.group(0)\n",
    "        except:\n",
    "            return match.group(0)\n",
    "    text = re.sub(r'\\b(\\d{1,2}:\\d{2})\\s*(am|pm)\\b', time_repl_hhmm_ampm, text, flags=re.IGNORECASE)\n",
    "\n",
    "    def time_repl_hh_ampm(match):\n",
    "        try:\n",
    "            parsed_time = dateparser.parse(match.group(0))\n",
    "            return parsed_time.strftime('%H:%M') if parsed_time else match.group(0)\n",
    "        except:\n",
    "            return match.group(0)\n",
    "    text = re.sub(r'\\b(\\d{1,2})\\s*(am|pm)\\b', time_repl_hh_ampm, text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bnoon\\b', '12:00', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bmidday\\b', '12:00', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bmidnight\\b', '00:00', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# --- Currency Normalization ---\n",
    "CURRENCY_SYMBOLS_MAP = {\n",
    "    '£': 'GBP',\n",
    "    '$': 'USD',\n",
    "    '€': 'EUR',\n",
    "    '¥': 'JPY',\n",
    "    '₹': 'INR',\n",
    "}\n",
    "CURRENCY_KEYWORDS_MAP = {\n",
    "    'pound': 'GBP', 'pounds': 'GBP', 'quid': 'GBP', 'gbp': 'GBP', 'sterling': 'GBP',\n",
    "    'dollar': 'USD', 'dollars': 'USD', 'buck': 'USD', 'bucks': 'USD', 'usd': 'USD',\n",
    "    'euro': 'EUR', 'euros': 'EUR', 'eur': 'EUR',\n",
    "    'yen': 'JPY', 'jpy': 'JPY',\n",
    "    'rupee': 'INR', 'rupees': 'INR', 'inr': 'INR',\n",
    "}\n",
    "ALL_CURRENCY_KEYWORDS_SORTED = sorted(CURRENCY_KEYWORDS_MAP.keys(), key=len, reverse=True)\n",
    "ALL_CURRENCY_SYMBOLS_SORTED = sorted(CURRENCY_SYMBOLS_MAP.keys(), key=len, reverse=True)\n",
    "NUMBER_WORDS_FOR_REGEX = (\n",
    "    r\"zero|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|\"\n",
    "    r\"thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|\"\n",
    "    r\"thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion\"\n",
    ")\n",
    "COMPLEX_NUMBER_WORDS_PATTERN = rf\"\\b((?:{NUMBER_WORDS_FOR_REGEX})(?:\\s+(?:and\\s+)?(?:{NUMBER_WORDS_FOR_REGEX}))*)\\b\"\n",
    "\n",
    "def normalize_currency_in_text(text):\n",
    "    \"\"\"\n",
    "    Normalizes currency expressions in a string.\n",
    "    Examples: \"twenty pounds\" -> \"20 GBP\", \"£20\" -> \"20 GBP\"\n",
    "    \"\"\"\n",
    "    def word_num_keyword_replacer(match):\n",
    "        num_word_str = match.group(1)\n",
    "        currency_key_str = match.group(2).lower()\n",
    "        try:\n",
    "            num_val = w2n.word_to_num(num_word_str)\n",
    "            currency_code = CURRENCY_KEYWORDS_MAP.get(currency_key_str, CURRENCY_KEYWORDS_MAP.get(currency_key_str.rstrip('s'), currency_key_str.upper()))\n",
    "            return f\"{num_val} {currency_code.upper()}\"\n",
    "        except ValueError:\n",
    "            return match.group(0)\n",
    "\n",
    "    currency_keywords_regex_part = \"|\".join([re.escape(k) for k in ALL_CURRENCY_KEYWORDS_SORTED])\n",
    "    pattern_word_num_then_keyword = rf\"({COMPLEX_NUMBER_WORDS_PATTERN})\\s+({currency_keywords_regex_part})\\b\"\n",
    "    text = re.sub(pattern_word_num_then_keyword, word_num_keyword_replacer, text, flags=re.IGNORECASE)\n",
    "\n",
    "    for symbol in ALL_CURRENCY_SYMBOLS_SORTED:\n",
    "        code = CURRENCY_SYMBOLS_MAP[symbol]\n",
    "        text = re.sub(rf'{re.escape(symbol)}\\s*(\\d+\\.?\\d*)', rf'\\1 {code}', text)\n",
    "        text = re.sub(rf'(\\d+\\.?\\d*)\\s*{re.escape(symbol)}', rf'\\1 {code}', text)\n",
    "\n",
    "    for keyword in ALL_CURRENCY_KEYWORDS_SORTED:\n",
    "        code = CURRENCY_KEYWORDS_MAP[keyword]\n",
    "        text = re.sub(rf'(\\d+\\.?\\d*)\\s+{re.escape(keyword)}\\b', rf'\\1 {code}', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(rf'\\b{re.escape(keyword)}\\s+(\\d+\\.?\\d*)', rf'\\1 {code}', text, flags=re.IGNORECASE)\n",
    "\n",
    "    all_target_codes = set(CURRENCY_SYMBOLS_MAP.values()) | set(CURRENCY_KEYWORDS_MAP.values())\n",
    "    for code_val in all_target_codes:\n",
    "        text = re.sub(rf'\\b{re.escape(code_val)}\\b', code_val.upper(), text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'(\\d)([A-Z]{3}\\b)', r'\\1 \\2', text)\n",
    "    text = re.sub(r'(\\b[A-Z]{3})(\\d)', r'\\1 \\2', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def flatten_dialogue(dialogue_data):\n",
    "    flattened_turns = []\n",
    "    turns = dialogue_data.get(\"log\", [])\n",
    "    for turn_idx, turn in enumerate(turns):\n",
    "        raw_text = turn.get(\"text\", \"\")\n",
    "        normalized_text = normalize_time_in_text(raw_text)\n",
    "        normalized_text = normalize_currency_in_text(normalized_text)\n",
    "\n",
    "        is_user_turn = turn_idx % 2 == 0  # User turns are even in 0-based index\n",
    "        \n",
    "        turn_data = {\n",
    "            \"turn_id\": turn_idx,\n",
    "            \"user_utterance\": normalized_text if is_user_turn else \"\",\n",
    "            \"system_response\": \"\" if is_user_turn else normalized_text,\n",
    "            \"state\": turn.get(\"metadata\", {}).get(\"state\", {})\n",
    "        }\n",
    "        \n",
    "        if isinstance(turn_data[\"state\"], dict):\n",
    "            turn_data[\"state\"] = json.dumps(turn_data[\"state\"])\n",
    "        \n",
    "        flattened_turns.append(turn_data)\n",
    "    return flattened_turns\n",
    "\n",
    "def normalize_dialogue_acts(dialogue_acts, dialogue_data):\n",
    "    normalized_acts = {}\n",
    "    for dialogue_id, turns_data in dialogue_data.items():\n",
    "        acts = dialogue_acts.get(dialogue_id, {})\n",
    "        turns = turns_data.get(\"log\", [])\n",
    "        normalized_turn_acts = {}\n",
    "        \n",
    "        for turn_idx in range(len(turns)):\n",
    "            turn_key = str(turn_idx)\n",
    "            turn_acts = acts.get(turn_key, {})\n",
    "            processed_acts = {}\n",
    "            for act_type, act_values in turn_acts.items():\n",
    "                if isinstance(act_values, dict):\n",
    "                    processed_acts[act_type] = [f\"{k}:{v}\" for k, v in act_values.items()]\n",
    "                elif isinstance(act_values, list):\n",
    "                    flat_values = []\n",
    "                    for item in act_values:\n",
    "                        if isinstance(item, list):\n",
    "                            flat_values.extend([str(x) for x in item])\n",
    "                        else:\n",
    "                            flat_values.append(str(item))\n",
    "                    processed_acts[act_type] = flat_values\n",
    "                else:\n",
    "                    processed_acts[act_type] = [str(act_values)]\n",
    "            \n",
    "            normalized_turn_acts[turn_key] = processed_acts\n",
    "        \n",
    "        normalized_acts[dialogue_id] = normalized_turn_acts\n",
    "    return normalized_acts\n",
    "\n",
    "def process_data(raw_data_subset, dialogue_acts, ontology):\n",
    "    \"\"\"Process a subset of the data (train or test) with normalization and flattening.\"\"\"\n",
    "    normalized_dialogue_acts = normalize_dialogue_acts(dialogue_acts, \n",
    "                                                     {d[\"dialogue_id\"]: d[\"dialogue\"] for d in raw_data_subset})\n",
    "    processed_data = []\n",
    "    \n",
    "    for item in raw_data_subset:\n",
    "        dialogue_id = item[\"dialogue_id\"]\n",
    "        dialogue = item[\"dialogue\"]\n",
    "        domain = item[\"domain\"]\n",
    "        domain_ontology = {\n",
    "            slot: values \n",
    "            for slot, values in ontology.items() \n",
    "            if slot.startswith(f\"{domain}-\")\n",
    "        }\n",
    "        \n",
    "        processed_data.append({\n",
    "            \"dialogue_id\": dialogue_id,\n",
    "            \"domain\": domain,\n",
    "            \"dialogue\": flatten_dialogue(dialogue),\n",
    "            \"dialogue_acts\": normalized_dialogue_acts[dialogue_id],\n",
    "            \"ontology\": domain_ontology\n",
    "        })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "train_data_processed = process_data(train_data_raw, dialogue_acts, ontology)\n",
    "test_data_processed = process_data(test_data_raw, dialogue_acts, ontology)\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data_processed)\n",
    "test_dataset = Dataset.from_list(test_data_processed)\n",
    "\n",
    "print(\"First training example:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display dialogue after clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Randomly Checking 2 Dialogues (First 2 Turns) from Train ---\n",
      "\n",
      "Dialogue ID: SNG0238.json\n",
      "Domain: hospital\n",
      "First 2 turns:\n",
      "  Turn 0:\n",
      "    User: Could you find me a hospital in town ?\n",
      "    System: \n",
      "  Turn 1:\n",
      "    User: \n",
      "    System: Yes , Addenbrookes Hospital is in your area , would you like me to book you an appointment ?\n",
      "\n",
      "Dialogue ID: PMUL3693.json\n",
      "Domain: attraction\n",
      "First 2 turns:\n",
      "  Turn 0:\n",
      "    User: Hello , can you recommend any theatres in the Centre of town , please ?\n",
      "    System: \n",
      "  Turn 1:\n",
      "    User: \n",
      "    System: I really love The Cambridge Corn Exchange located on Wheeler Street . I do n't have admission information , but you can call them at 01223357851 .\n",
      "\n",
      "--- Randomly Checking 2 Dialogues (First 2 Turns) from Test ---\n",
      "\n",
      "Dialogue ID: PMUL4431.json\n",
      "Domain: restaurant\n",
      "First 2 turns:\n",
      "  Turn 0:\n",
      "    User: I am traveling to Cambridge and looking forward to try local restaurants .\n",
      "    System: \n",
      "  Turn 1:\n",
      "    User: \n",
      "    System: We have lots to explore ! I can help you find one , if you 'd like .\n",
      "\n",
      "Dialogue ID: PMUL1174.json\n",
      "Domain: train\n",
      "First 2 turns:\n",
      "  Turn 0:\n",
      "    User: I 'm looking for a train that leaves Cambridge on Wednesday please .\n",
      "    System: \n",
      "  Turn 1:\n",
      "    User: \n",
      "    System: Where are you traveling to , and at what time will you be traveling ?\n"
     ]
    }
   ],
   "source": [
    "def randomly_check_first_dialogue_turns(split_name, split_data, num_samples=2, num_turns=2):\n",
    "    print(f\"\\n--- Randomly Checking {num_samples} Dialogues (First {num_turns} Turns) from {split_name} ---\")\n",
    "    if not split_data:\n",
    "        print(f\"No data in {split_name} split.\")\n",
    "        return\n",
    "    if not isinstance(split_data, (list, tuple)):\n",
    "        split_data = list(split_data)\n",
    "    random_dialogues = random.sample(split_data, min(num_samples, len(split_data)))\n",
    "    for dialogue in random_dialogues:\n",
    "        dialogue_id = dialogue[\"dialogue_id\"]\n",
    "        turns = dialogue[\"dialogue\"]\n",
    "        print(f\"\\nDialogue ID: {dialogue_id}\")\n",
    "        print(f\"Domain: {dialogue.get('domain', 'unknown')}\")\n",
    "        print(f\"First {min(num_turns, len(turns))} turns:\")\n",
    "        for i in range(min(num_turns, len(turns))):\n",
    "            turn = turns[i]\n",
    "            print(f\"  Turn {turn['turn_id']}:\")\n",
    "            print(f\"    User: {turn['user_utterance']}\")\n",
    "            print(f\"    System: {turn['system_response']}\")\n",
    "\n",
    "randomly_check_first_dialogue_turns(\"Train\", train_dataset)\n",
    "randomly_check_first_dialogue_turns(\"Test\", test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0565d2ef8d34b419a3d6ff233eef31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7535181a8cdf481eb95bbe9928c48f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Randomly Checking 2 Dialogues (First 2 Turns) from Train ---\n",
      "\n",
      "Dialogue ID: SNG0462.json\n",
      "Domain: restaurant\n",
      "First 2 turns:\n",
      "  Turn 0:\n",
      "    User: hi ! can you give me some information on the royal spice restaurant ?\n",
      "    System: \n",
      "  Turn 1:\n",
      "    User: \n",
      "    System: of course ! it 's a cheap indian restaurant in the north at victoria avenue chesteron cb41eh . the phone number is 01733553355 . may i help with anything else ?\n",
      "\n",
      "Dialogue ID: MUL0244.json\n",
      "Domain: train\n",
      "First 2 turns:\n",
      "  Turn 0:\n",
      "    User: yes , i would like to book a train that is leaving monday , and is going to cambridge .\n",
      "    System: \n",
      "  Turn 1:\n",
      "    User: \n",
      "    System: there are 202 trains to cambridge on monday . can you tell me your departure station and the time you  would like to travel ?\n",
      "\n",
      "--- Randomly Checking 2 Dialogues (First 2 Turns) from Test ---\n",
      "\n",
      "Dialogue ID: WOZ20573.json\n",
      "Domain: restaurant\n",
      "First 2 turns:\n",
      "  Turn 0:\n",
      "    User: hello . can you help me find the address of an inexpensive restaurant in the south part of town ?\n",
      "    System: \n",
      "  Turn 1:\n",
      "    User: \n",
      "    System: there are two restaurants that are in the cheap price range and in the south part of town . would you like portuguese or chinese food ?\n",
      "\n",
      "Dialogue ID: PMUL4745.json\n",
      "Domain: attraction\n",
      "First 2 turns:\n",
      "  Turn 0:\n",
      "    User: please give me some information on byard art\n",
      "    System: \n",
      "  Turn 1:\n",
      "    User: \n",
      "    System: it is a museum in south with free admission . what else can i help you with ?\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = contractions.fix(text)\n",
    "    return text\n",
    "def preprocess_ontology(ontology):\n",
    "    if isinstance(ontology, list):\n",
    "        return [preprocess_text(item) for item in ontology]\n",
    "    elif isinstance(ontology, dict):\n",
    "        return {preprocess_text(k): v for k, v in ontology.items()}\n",
    "    return ontology\n",
    "def preprocess_split_lowercase_contraction(dataset):\n",
    "    processed_dataset = dataset.map(\n",
    "        lambda example: {\n",
    "            \"dialogue_id\": example[\"dialogue_id\"],\n",
    "            \"domain\": preprocess_text(example[\"domain\"]),\n",
    "            \"dialogue\": [\n",
    "                {\n",
    "                    \"turn_id\": turn[\"turn_id\"],\n",
    "                    \"user_utterance\": preprocess_text(turn[\"user_utterance\"]),\n",
    "                    \"system_response\": preprocess_text(turn[\"system_response\"]),\n",
    "                    \"state\": turn[\"state\"]\n",
    "                    # Removed \"dialogue_acts\" since it's not a turn-level field\n",
    "                }\n",
    "                for turn in example[\"dialogue\"]\n",
    "            ],\n",
    "            \"dialogue_acts\": example[\"dialogue_acts\"],  # Preserve top-level dialogue_acts\n",
    "            \"ontology\": preprocess_ontology(example[\"ontology\"])\n",
    "        },\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    return processed_dataset\n",
    "\n",
    "train_dataset = preprocess_split_lowercase_contraction(train_dataset)\n",
    "test_dataset = preprocess_split_lowercase_contraction(test_dataset)\n",
    "\n",
    "def randomly_check_first_dialogue_turns(split_name, split_data, num_samples=2, num_turns=2):\n",
    "    print(f\"\\n--- Randomly Checking {num_samples} Dialogues (First {num_turns} Turns) from {split_name} ---\")\n",
    "    if not split_data:\n",
    "        print(f\"No data in {split_name} split.\")\n",
    "        return\n",
    "    if not isinstance(split_data, (list, tuple)):\n",
    "        split_data = list(split_data)\n",
    "    random_dialogues = random.sample(split_data, min(num_samples, len(split_data)))\n",
    "    for dialogue in random_dialogues:\n",
    "        dialogue_id = dialogue[\"dialogue_id\"]\n",
    "        turns = dialogue[\"dialogue\"]\n",
    "        print(f\"\\nDialogue ID: {dialogue_id}\")\n",
    "        print(f\"Domain: {dialogue.get('domain', 'unknown')}\")\n",
    "        print(f\"First {min(num_turns, len(turns))} turns:\")\n",
    "        for i in range(min(num_turns, len(turns))):\n",
    "            turn = turns[i]\n",
    "            print(f\"  Turn {turn['turn_id']}:\")\n",
    "            print(f\"    User: {turn['user_utterance']}\")\n",
    "            print(f\"    System: {turn['system_response']}\")\n",
    "\n",
    "randomly_check_first_dialogue_turns(\"Train\", train_dataset)\n",
    "randomly_check_first_dialogue_turns(\"Test\", test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counts number of words in each sentence in each dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence in train_dataset: 67 words\n",
      "Longest sentence in test_dataset: 56 words\n",
      "\n",
      "Summary of longest sentence lengths:\n",
      "train_dataset: 67 words\n",
      "test_dataset: 56 words\n"
     ]
    }
   ],
   "source": [
    "def get_longest_sentence_word_length(datasets: Dict[str, List[Dict[str, Any]]]) -> Dict[str, int]:\n",
    "    longest_sentence_lengths = {}\n",
    "\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        max_words_in_sentence = 0\n",
    "        \n",
    "        for dialogue_entry in dataset:\n",
    "            dialogue_turns = dialogue_entry.get('dialogue', [])\n",
    "            \n",
    "            for turn in dialogue_turns:\n",
    "                user_utterance = turn.get('user_utterance', '').strip()\n",
    "                system_response = turn.get('system_response', '').strip()\n",
    "\n",
    "                # Process user utterance\n",
    "                if user_utterance:\n",
    "                    # Simple tokenization by splitting on whitespace\n",
    "                    words = user_utterance.split()\n",
    "                    max_words_in_sentence = max(max_words_in_sentence, len(words))\n",
    "\n",
    "                # Process system response\n",
    "                if system_response:\n",
    "                    # Simple tokenization by splitting on whitespace\n",
    "                    words = system_response.split()\n",
    "                    max_words_in_sentence = max(max_words_in_sentence, len(words))\n",
    "        \n",
    "        longest_sentence_lengths[dataset_name] = max_words_in_sentence\n",
    "        print(f\"Longest sentence in {dataset_name}: {max_words_in_sentence} words\")\n",
    "\n",
    "    return longest_sentence_lengths\n",
    "all_datasets = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'test_dataset': test_dataset,\n",
    "}\n",
    "longest_lengths = get_longest_sentence_word_length(all_datasets)\n",
    "print(\"\\nSummary of longest sentence lengths:\")\n",
    "for dataset_name, length in longest_lengths.items():\n",
    "    print(f\"{dataset_name}: {length} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the number of turns in dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset: train_dataset\n",
      "Checking dataset: test_dataset\n",
      "\n",
      "The longest turn_id found across all datasets is: 43\n"
     ]
    }
   ],
   "source": [
    "def find_longest_turn_id(all_datasets):\n",
    "    max_turn_id = 0\n",
    "\n",
    "    for dataset_name, dialogues_dataset in all_datasets.items():\n",
    "        print(f\"Checking dataset: {dataset_name}\")\n",
    "        for dialogue in dialogues_dataset:\n",
    "            try:\n",
    "                for turn in dialogue['dialogue']:\n",
    "                    if 'turn_id' in turn:\n",
    "                        max_turn_id = max(max_turn_id, turn['turn_id'])\n",
    "            except KeyError as e:\n",
    "                print(f\"Warning: Skipping dialogue in {dataset_name} due to missing key: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred while processing dialogue in {dataset_name}: {e}\")\n",
    "                continue\n",
    "    return max_turn_id\n",
    "\n",
    "all_datasets_dummy = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'test_dataset': test_dataset\n",
    "}\n",
    "longest_turn_id = find_longest_turn_id(all_datasets_dummy)\n",
    "print(f\"\\nThe longest turn_id found across all datasets is: {longest_turn_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count number of unique intents and entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Intents: 31\n",
      "Intents: {'attraction-name', 'restaurant-book day', 'taxi-leaveat', 'restaurant-name', 'restaurant-book people', 'taxi-destination', 'hotel-stars', 'hotel-area', 'restaurant-food', 'hotel-pricerange', 'hospital-department', 'hotel-name', 'taxi-arriveby', 'restaurant-area', 'train-destination', 'train-arriveby', 'hotel-parking', 'train-day', 'hotel-book stay', 'train-leaveat', 'restaurant-book time', 'train-book people', 'train-departure', 'restaurant-pricerange', 'taxi-departure', 'hotel-book day', 'attraction-type', 'hotel-book people', 'hotel-internet', 'hotel-type', 'attraction-area'}\n",
      "Number of Unique Entities: 7\n",
      "Entities: {'restaurant', 'hotel', 'hospital', 'police', 'attraction', 'train', 'taxi'}\n"
     ]
    }
   ],
   "source": [
    "def count_intents_and_entities(dataset):\n",
    "    intents = set()\n",
    "    entities = set()\n",
    "    \n",
    "    for example in dataset:\n",
    "        intents.update(example[\"ontology\"])\n",
    "        entities.add(example[\"domain\"])\n",
    "    \n",
    "    return intents, entities\n",
    "\n",
    "all_intents = set()\n",
    "all_entities = set()\n",
    "for dataset in [train_dataset, test_dataset]:\n",
    "    intents, entities = count_intents_and_entities(dataset)\n",
    "    all_intents.update(intents)\n",
    "    all_entities.update(entities)\n",
    "\n",
    "print(f\"Number of Unique Intents: {len(all_intents)}\")\n",
    "print(f\"Intents: {all_intents}\")\n",
    "print(f\"Number of Unique Entities: {len(all_entities)}\")\n",
    "print(f\"Entities: {all_entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text representation and split features and target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Preprocessed Datasets and Preprocessor Models ---\n",
      "Collecting vocabulary, domains, and slots from dataset...\n",
      "Fitting Word2Vec model...\n",
      "Preprocessor fitted. Vocab size: 6132, Num domains: 7, Num intents (slots): 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 06:01:26.856091: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:01:27.163710: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:01:27.163911: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:01:27.180293: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:01:27.180490: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:01:27.180607: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:01:27.286947: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:01:27.288183: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:01:27.288326: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-07-04 06:01:27.288905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13087 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Dataset for train saved to tf_datasets/train\n",
      "Raw data for train saved to tf_datasets/train_raw_data.pkl\n",
      "TensorFlow Dataset for test saved to tf_datasets/test\n",
      "Raw data for test saved to tf_datasets/test_raw_data.pkl\n",
      "MoE parameters saved to tf_datasets/moe_params.json\n",
      "Preprocessor saved to preprocessor_models\n",
      "\n",
      "--- Loading Preprocessed Datasets and Preprocessor Models ---\n",
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n",
      "Loaded train dataset from tf_datasets/train\n",
      "Loaded test dataset from tf_datasets/test\n",
      "Loaded calibration_train dataset from tf_datasets/calibration_train\n",
      "Loaded calibration_val dataset from tf_datasets/calibration_val\n",
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n",
      "Loaded raw data for calibration_train from tf_datasets/calibration_train_raw_data.pkl\n",
      "Loaded raw data for calibration_val from tf_datasets/calibration_val_raw_data.pkl\n",
      "TensorFlow Datasets and raw data loaded successfully.\n",
      "Preprocessor loaded from preprocessor_models\n",
      "\n",
      "--- Verifying Loaded Data ---\n",
      "Loaded MoE Parameters: {\n",
      "    \"embedding_dim\": 128,\n",
      "    \"max_seq_length\": 67,\n",
      "    \"turn_id_dim\": 43,\n",
      "    \"num_experts\": 4,\n",
      "    \"expert_dim\": 64,\n",
      "    \"hidden_dim\": 32,\n",
      "    \"vocab_size\": 6132,\n",
      "    \"num_entities\": 7,\n",
      "    \"num_intents\": 31,\n",
      "    \"num_domains\": 7,\n",
      "    \"batch_size\": 2,\n",
      "    \"shuffle_buffer_size\": 5000,\n",
      "    \"w2v_window\": 10,\n",
      "    \"w2v_min_count\": 3,\n",
      "    \"w2v_sg\": 1,\n",
      "    \"w2v_epochs\": 20,\n",
      "    \"w2v_negative\": 5\n",
      "}\n",
      "\n",
      "Sample from Loaded Train Dataset:\n",
      "Features keys: dict_keys(['user_utterance_tokens', 'prev_system_response_tokens', 'decoder_input_tokens', 'domain_onehot_input', 'turn_id_embedding', 'ontology_multihot_input'])\n",
      "Targets keys: dict_keys(['domain_classification_output', 'intent_classification_output', 'response_generation_output'])\n",
      "User Utterance Tokens shape: (2, 67)\n",
      "Response Generation Output shape: (2, 67)\n",
      "\n",
      "Loaded Preprocessor Stats:\n",
      "Vocab size: 6132\n",
      "Num domains: 7\n",
      "Num intents (slots): 31\n",
      "Sample word_to_id (first 5): {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3, '!': 4}\n",
      "\n",
      "Raw Data Sample (Train, first item):\n",
      "{'raw_next_system_response': 'thank you i will go to royal spice .', 'dialogue_id': 'MUL0185.json', 'turn_id': 0, 'raw_user_utterance': 'yes i need a cheap restaurant in the cambridge area on the north side of town . what do you suggest ?', 'raw_domain': 'restaurant', 'raw_entities': [(12, 12, 'restaurant-area', 'north'), (4, 4, 'restaurant-pricerange', 'cheap')]}\n"
     ]
    }
   ],
   "source": [
    "moe_params = {\n",
    "    \"embedding_dim\": 128,\n",
    "    \"max_seq_length\": 67,\n",
    "    \"turn_id_dim\": 43,\n",
    "    \"num_experts\": 4,\n",
    "    \"expert_dim\": 64,\n",
    "    \"hidden_dim\": 32,\n",
    "    \"vocab_size\": None,\n",
    "    \"num_entities\": None,\n",
    "    \"num_intents\": None,\n",
    "    \"num_domains\": None,\n",
    "    \"batch_size\": 2,\n",
    "    \"shuffle_buffer_size\": 5000,\n",
    "    \"w2v_window\": 10,\n",
    "    \"w2v_min_count\": 3,\n",
    "    \"w2v_sg\": 1,\n",
    "    \"w2v_epochs\": 20,\n",
    "    \"w2v_negative\": 5\n",
    "}\n",
    "\n",
    "class DialoguePreprocessor:\n",
    "    def __init__(self, moe_parameters):\n",
    "        self.max_seq_length = moe_parameters[\"max_seq_length\"]\n",
    "        self.embedding_dim = moe_parameters[\"embedding_dim\"]\n",
    "        self.w2v_window = moe_parameters[\"w2v_window\"]\n",
    "        self.w2v_min_count = moe_parameters[\"w2v_min_count\"]\n",
    "        self.w2v_sg = moe_parameters[\"w2v_sg\"]\n",
    "        self.w2v_epochs = moe_parameters[\"w2v_epochs\"]\n",
    "        self.w2v_negative = moe_parameters[\"w2v_negative\"]\n",
    "\n",
    "        self.word_vectors = None\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        self.domain_encoder = LabelEncoder()\n",
    "        self.vocab_size = 0\n",
    "        self.num_domains = 0\n",
    "        self.num_intents = 0\n",
    "        self.slot_to_id = {}\n",
    "        self.id_to_slot = {}\n",
    "        self.slot_values = {}\n",
    "        self.unique_domains = []\n",
    "        self.unique_slots = []\n",
    "        self.PAD_TOKEN = '<pad>'\n",
    "        self.SOS_TOKEN = '<sos>'\n",
    "        self.EOS_TOKEN = '<eos>'\n",
    "        self.UNK_TOKEN = '<unk>'\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def fit(self, dialogues_dataset):\n",
    "        all_words = set()\n",
    "        all_domains = []\n",
    "        all_slots = set()\n",
    "        tokenized_sentences_for_w2v = []\n",
    "\n",
    "        # Initialize basic vocabulary\n",
    "        self.word_to_id = {\n",
    "            self.PAD_TOKEN: 0,\n",
    "            self.SOS_TOKEN: 1,\n",
    "            self.EOS_TOKEN: 2,\n",
    "            self.UNK_TOKEN: 3\n",
    "        }\n",
    "        current_id = 4\n",
    "\n",
    "        if not dialogues_dataset:\n",
    "            print(\"Warning: dialogues_dataset is empty. Initializing with minimal vocabulary.\")\n",
    "        else:\n",
    "            print(\"Collecting vocabulary, domains, and slots from dataset...\")\n",
    "            for dialogue in dialogues_dataset:\n",
    "                try:\n",
    "                    if 'domain' not in dialogue:\n",
    "                        print(f\"Skipping dialogue {dialogue.get('dialogue_id', 'N/A')}: Missing 'domain' key.\")\n",
    "                        continue\n",
    "                    all_domains.append(dialogue['domain'])\n",
    "\n",
    "                    if 'ontology' not in dialogue:\n",
    "                        print(f\"Skipping dialogue {dialogue.get('dialogue_id', 'N/A')}: Missing 'ontology' key.\")\n",
    "                        continue\n",
    "                    ontology = dialogue['ontology']\n",
    "                    for slot, values in ontology.items():\n",
    "                        if values:\n",
    "                            all_slots.add(slot)\n",
    "                            self.slot_values[slot] = set(values) if isinstance(values, list) else set()\n",
    "\n",
    "                    if 'dialogue' not in dialogue:\n",
    "                        print(f\"Skipping dialogue {dialogue.get('dialogue_id', 'N/A')}: Missing 'dialogue' key.\")\n",
    "                        continue\n",
    "                    for turn in dialogue['dialogue']:\n",
    "                        if 'user_utterance' in turn and turn['user_utterance']:\n",
    "                            tokens = word_tokenize(turn['user_utterance'].lower())\n",
    "                            all_words.update(tokens)\n",
    "                            tokenized_sentences_for_w2v.append(tokens)\n",
    "                        if 'system_response' in turn and turn['system_response']:\n",
    "                            tokens = word_tokenize(turn['system_response'].lower())\n",
    "                            all_words.update(tokens)\n",
    "                            tokenized_sentences_for_w2v.append(tokens)\n",
    "                except KeyError as e:\n",
    "                    print(f\"Skipping dialogue due to missing key during fit: {e}\")\n",
    "                    continue\n",
    "\n",
    "            all_words_list = sorted(list(all_words))\n",
    "            for word in all_words_list:\n",
    "                if word not in self.word_to_id:\n",
    "                    self.word_to_id[word] = current_id\n",
    "                    current_id += 1\n",
    "\n",
    "        self.id_to_word = {v: k for k, v in self.word_to_id.items()}\n",
    "        self.vocab_size = len(self.word_to_id)\n",
    "\n",
    "        if tokenized_sentences_for_w2v:\n",
    "            print(\"Fitting Word2Vec model...\")\n",
    "            self.word_vectors = Word2Vec(\n",
    "                sentences=tokenized_sentences_for_w2v,\n",
    "                vector_size=self.embedding_dim,\n",
    "                window=self.w2v_window,\n",
    "                min_count=self.w2v_min_count,\n",
    "                workers=4,\n",
    "                sg=self.w2v_sg,\n",
    "                epochs=self.w2v_epochs,\n",
    "                negative=self.w2v_negative\n",
    "            )\n",
    "        else:\n",
    "            print(\"Warning: No tokenized sentences for Word2Vec. Word vectors will not be initialized.\")\n",
    "\n",
    "        self.unique_domains = sorted(list(set(all_domains)))\n",
    "        if self.unique_domains:\n",
    "            self.domain_encoder.fit(self.unique_domains)\n",
    "            self.num_domains = len(self.domain_encoder.classes_)\n",
    "            self.unique_domains = self.domain_encoder.classes_.tolist()\n",
    "        else:\n",
    "            print(\"Warning: No domains found in the dataset.\")\n",
    "            self.num_domains = 0\n",
    "            self.domain_encoder.fit(['dummy'])\n",
    "        self.unique_slots = sorted(list(all_slots))\n",
    "        if self.unique_slots:\n",
    "            self.slot_to_id = {slot: idx for idx, slot in enumerate(self.unique_slots)}\n",
    "            self.id_to_slot = {idx: slot for slot, idx in self.slot_to_id.items()}\n",
    "            self.num_intents = len(self.slot_to_id)\n",
    "        else:\n",
    "            print(\"Warning: No slots found in the dataset.\")\n",
    "            self.num_intents = 0\n",
    "\n",
    "        self.is_fitted = True\n",
    "        print(f\"Preprocessor fitted. Vocab size: {self.vocab_size}, Num domains: {self.num_domains}, Num intents (slots): {self.num_intents}\")\n",
    "\n",
    "    def _text_to_token_ids(self, text, add_sos_eos=False):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        ids = []\n",
    "        if add_sos_eos:\n",
    "            ids.append(self.word_to_id[self.SOS_TOKEN])\n",
    "        for token in tokens:\n",
    "            ids.append(self.word_to_id.get(token, self.word_to_id[self.UNK_TOKEN]))\n",
    "        if add_sos_eos:\n",
    "            ids.append(self.word_to_id[self.EOS_TOKEN])\n",
    "        if len(ids) < self.max_seq_length:\n",
    "            padding = [self.word_to_id[self.PAD_TOKEN]] * (self.max_seq_length - len(ids))\n",
    "            ids.extend(padding)\n",
    "        else:\n",
    "            ids = ids[:self.max_seq_length]\n",
    "        return np.array(ids, dtype=np.int32), tokens\n",
    "\n",
    "    def _get_turn_id_embedding(self, turn_id, turn_id_dim):\n",
    "        embedding = np.zeros(turn_id_dim, dtype=np.float32)\n",
    "        if turn_id_dim > 0:\n",
    "            embedding[0] = float(turn_id) / 40.0\n",
    "        return embedding\n",
    "\n",
    "    def _extract_entities_and_intents(self, utterance, tokens, ontology):\n",
    "        active_slots = np.zeros(self.num_intents, dtype=np.float32)\n",
    "        entities = []\n",
    "\n",
    "        for slot, values in ontology.items():\n",
    "            if not values:\n",
    "                continue\n",
    "            slot_id = self.slot_to_id.get(slot, -1)\n",
    "            if slot_id == -1:\n",
    "                continue\n",
    "\n",
    "            for value in values:\n",
    "                value_tokens = word_tokenize(value.lower())\n",
    "                value_len = len(value_tokens)\n",
    "                for i in range(len(tokens) - value_len + 1):\n",
    "                    if tokens[i:i + value_len] == value_tokens:\n",
    "                        active_slots[slot_id] = 1.0\n",
    "                        entities.append((i, i + value_len - 1, slot, value))\n",
    "                        break\n",
    "\n",
    "        return active_slots, entities\n",
    "\n",
    "    def preprocess(self, dialogues_dataset):\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Preprocessor has not been fitted. Call .fit() first.\")\n",
    "\n",
    "        processed_turns = []\n",
    "        for dialogue in dialogues_dataset:\n",
    "            try:\n",
    "                dialogue_domain_label = self.domain_encoder.transform([dialogue['domain']])[0]\n",
    "                dialogue_domain_onehot_input = tf.keras.utils.to_categorical(\n",
    "                    dialogue_domain_label, num_classes=self.num_domains\n",
    "                ).astype(np.float32)\n",
    "\n",
    "                prev_system_response_tokens, _ = self._text_to_token_ids(self.SOS_TOKEN, add_sos_eos=False)\n",
    "\n",
    "                for i in range(0, len(dialogue['dialogue']), 2):\n",
    "                    user_turn = dialogue['dialogue'][i]\n",
    "                    if 'user_utterance' not in user_turn or not user_turn['user_utterance']:\n",
    "                        print(f\"Skipping user turn {user_turn.get('turn_id', 'N/A')} in dialogue {dialogue.get('dialogue_id', 'N/A')} due to missing or empty 'user_utterance'.\")\n",
    "                        continue\n",
    "\n",
    "                    user_utterance_tokens, tokens = self._text_to_token_ids(user_turn['user_utterance'])\n",
    "                    turn_id_embedding = self._get_turn_id_embedding(user_turn['turn_id'], moe_params[\"turn_id_dim\"])\n",
    "\n",
    "                    active_slots, entities = self._extract_entities_and_intents(user_turn['user_utterance'], tokens, dialogue['ontology'])\n",
    "\n",
    "                    next_system_response_text = \"\"\n",
    "                    if (i + 1) < len(dialogue['dialogue']):\n",
    "                        next_system_turn = dialogue['dialogue'][i + 1]\n",
    "                        if 'system_response' in next_system_turn and next_system_turn['system_response']:\n",
    "                            next_system_response_text = next_system_turn['system_response']\n",
    "\n",
    "                    full_next_system_response_tokens, _ = self._text_to_token_ids(next_system_response_text, add_sos_eos=True)\n",
    "                    decoder_input_tokens_for_generation = np.copy(full_next_system_response_tokens)\n",
    "                    decoder_input_tokens_for_generation = np.roll(decoder_input_tokens_for_generation, shift=1)\n",
    "                    decoder_input_tokens_for_generation[0] = self.word_to_id[self.SOS_TOKEN]\n",
    "                    response_generation_target_tokens = np.copy(full_next_system_response_tokens)\n",
    "\n",
    "                    features = {\n",
    "                        'user_utterance_tokens': user_utterance_tokens,\n",
    "                        'prev_system_response_tokens': prev_system_response_tokens,\n",
    "                        'decoder_input_tokens': decoder_input_tokens_for_generation,\n",
    "                        'domain_onehot_input': dialogue_domain_onehot_input,\n",
    "                        'turn_id_embedding': turn_id_embedding,\n",
    "                        'ontology_multihot_input': active_slots,\n",
    "                    }\n",
    "\n",
    "                    targets = {\n",
    "                        'domain_classification_output': np.array([dialogue_domain_label], dtype=np.int32),\n",
    "                        'intent_classification_output': active_slots,\n",
    "                        'response_generation_output': response_generation_target_tokens,\n",
    "                        'entities_output': entities\n",
    "                    }\n",
    "\n",
    "                    raw_data = {\n",
    "                        'raw_next_system_response': next_system_response_text,\n",
    "                        'dialogue_id': dialogue['dialogue_id'],\n",
    "                        'turn_id': user_turn['turn_id'],\n",
    "                        'raw_user_utterance': user_turn['user_utterance'],\n",
    "                        'raw_domain': dialogue['domain'],\n",
    "                        'raw_entities': entities\n",
    "                    }\n",
    "\n",
    "                    processed_turns.append({\n",
    "                        'features': features,\n",
    "                        'targets': targets,\n",
    "                        'raw_data': raw_data\n",
    "                    })\n",
    "\n",
    "                    prev_system_response_tokens, _ = self._text_to_token_ids(next_system_response_text, add_sos_eos=False)\n",
    "\n",
    "            except KeyError as e:\n",
    "                print(f\"Warning: Skipping dialogue {dialogue.get('dialogue_id', 'N/A')} due to missing key: {e}\")\n",
    "                continue\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: Data error in dialogue {dialogue.get('dialogue_id', 'N/A')}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return processed_turns\n",
    "\n",
    "    def save(self, path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        with open(os.path.join(path, \"domain_encoder.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.domain_encoder, f)\n",
    "        if self.word_vectors:\n",
    "            self.word_vectors.wv.save(os.path.join(path, \"word_vectors.kv\"))\n",
    "        params_to_save = {\n",
    "            'max_seq_length': self.max_seq_length,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'num_domains': self.num_domains,\n",
    "            'num_intents': self.num_intents,\n",
    "            'unique_domains': self.unique_domains,\n",
    "            'unique_slots': self.unique_slots,\n",
    "            'word_to_id': {str(k): v for k, v in self.word_to_id.items()},\n",
    "            'id_to_word': {str(k): v for k, v in self.id_to_word.items()},\n",
    "            'slot_to_id': {str(k): v for k, v in self.slot_to_id.items()},\n",
    "            'id_to_slot': {str(k): v for k, v in self.id_to_slot.items()},\n",
    "            'slot_values': {k: list(v) for k, v in self.slot_values.items()},\n",
    "            'PAD_TOKEN': self.PAD_TOKEN,\n",
    "            'SOS_TOKEN': self.SOS_TOKEN,\n",
    "            'EOS_TOKEN': self.EOS_TOKEN,\n",
    "            'UNK_TOKEN': self.UNK_TOKEN,\n",
    "            'w2v_window': self.w2v_window,\n",
    "            'w2v_min_count': self.w2v_min_count,\n",
    "            'w2v_sg': self.w2v_sg,\n",
    "            'w2v_epochs': self.w2v_epochs,\n",
    "            'w2v_negative': self.w2v_negative,\n",
    "            'is_fitted': self.is_fitted\n",
    "        }\n",
    "        with open(os.path.join(path, \"preprocessor_params.json\"), \"w\") as f:\n",
    "            json.dump(params_to_save, f)\n",
    "        print(f\"Preprocessor saved to {path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        with open(os.path.join(path, \"preprocessor_params.json\"), \"r\") as f:\n",
    "            params = json.load(f)\n",
    "        dummy_moe_params = {\n",
    "            \"max_seq_length\": params['max_seq_length'],\n",
    "            \"embedding_dim\": params['embedding_dim'],\n",
    "            \"turn_id_dim\": moe_params[\"turn_id_dim\"],\n",
    "            \"w2v_window\": params['w2v_window'],\n",
    "            \"w2v_min_count\": params['w2v_min_count'],\n",
    "            \"w2v_sg\": params['w2v_sg'],\n",
    "            \"w2v_epochs\": params['w2v_epochs'],\n",
    "            \"w2v_negative\": params['w2v_negative']\n",
    "        }\n",
    "        preprocessor = cls(dummy_moe_params)\n",
    "        with open(os.path.join(path, \"domain_encoder.pkl\"), \"rb\") as f:\n",
    "            preprocessor.domain_encoder = pickle.load(f)\n",
    "        if os.path.exists(os.path.join(path, \"word_vectors.kv\")):\n",
    "            preprocessor.word_vectors = KeyedVectors.load(os.path.join(path, \"word_vectors.kv\"))\n",
    "        else:\n",
    "            print(f\"Warning: word_vectors.kv not found at {os.path.join(path, 'word_vectors.kv')}\")\n",
    "            preprocessor.word_vectors = None\n",
    "        preprocessor.vocab_size = params['vocab_size']\n",
    "        preprocessor.num_domains = params['num_domains']\n",
    "        preprocessor.num_intents = params['num_intents']\n",
    "        preprocessor.unique_domains = params['unique_domains']\n",
    "        preprocessor.unique_slots = params['unique_slots']\n",
    "        preprocessor.word_to_id = {str(k): v for k, v in params['word_to_id'].items()}\n",
    "        preprocessor.id_to_word = {int(k): v for k, v in params['id_to_word'].items()}\n",
    "        preprocessor.slot_to_id = {str(k): v for k, v in params['slot_to_id'].items()}\n",
    "        preprocessor.id_to_slot = {int(k): v for k, v in params['id_to_slot'].items()}\n",
    "        preprocessor.slot_values = {k: set(v) for k, v in params['slot_values'].items()}\n",
    "        preprocessor.PAD_TOKEN = params['PAD_TOKEN']\n",
    "        preprocessor.SOS_TOKEN = params['SOS_TOKEN']\n",
    "        preprocessor.EOS_TOKEN = params['EOS_TOKEN']\n",
    "        preprocessor.UNK_TOKEN = params['UNK_TOKEN']\n",
    "        preprocessor.is_fitted = params.get('is_fitted', False)\n",
    "        return preprocessor\n",
    "\n",
    "def create_and_save_tf_dataset(processed_data, batch_size, shuffle_buffer_size, dataset_name, save_path, moe_params_for_spec):\n",
    "    if not processed_data:\n",
    "        print(f\"Warning: No processed data for {dataset_name}. Returning an empty dataset.\")\n",
    "        element_spec = (\n",
    "            {\n",
    "                'user_utterance_tokens': tf.TensorSpec(shape=(moe_params_for_spec[\"max_seq_length\"],), dtype=tf.int32),\n",
    "                'prev_system_response_tokens': tf.TensorSpec(shape=(moe_params_for_spec[\"max_seq_length\"],), dtype=tf.int32),\n",
    "                'decoder_input_tokens': tf.TensorSpec(shape=(moe_params_for_spec[\"max_seq_length\"],), dtype=tf.int32),\n",
    "                'domain_onehot_input': tf.TensorSpec(shape=(moe_params_for_spec[\"num_domains\"],), dtype=tf.float32),\n",
    "                'turn_id_embedding': tf.TensorSpec(shape=(moe_params_for_spec[\"turn_id_dim\"],), dtype=tf.float32),\n",
    "                'ontology_multihot_input': tf.TensorSpec(shape=(moe_params_for_spec[\"num_intents\"],), dtype=tf.float32),\n",
    "            },\n",
    "            {\n",
    "                'domain_classification_output': tf.TensorSpec(shape=(1,), dtype=tf.int32),\n",
    "                'intent_classification_output': tf.TensorSpec(shape=(moe_params_for_spec[\"num_intents\"],), dtype=tf.float32),\n",
    "                'response_generation_output': tf.TensorSpec(shape=(moe_params_for_spec[\"max_seq_length\"],), dtype=tf.int32),\n",
    "            }\n",
    "        )\n",
    "        return tf.data.Dataset.from_generator(lambda: [], output_signature=element_spec), []\n",
    "\n",
    "    features_list = defaultdict(list)\n",
    "    targets_list = defaultdict(list)\n",
    "    raw_data_list = []\n",
    "\n",
    "    for item in processed_data:\n",
    "        for key, value in item['features'].items():\n",
    "            features_list[key].append(value)\n",
    "        for key, value in item['targets'].items():\n",
    "            if key != 'entities_output':\n",
    "                targets_list[key].append(value)\n",
    "        raw_data_list.append(item['raw_data'])\n",
    "\n",
    "    features_np = {k: np.array(v) for k, v in features_list.items()}\n",
    "    targets_np = {k: np.array(v) for k, v in targets_list.items()}\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features_np, targets_np))\n",
    "\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset_dir = os.path.join(save_path, dataset_name)\n",
    "    tf.data.Dataset.save(dataset, dataset_dir)\n",
    "    print(f\"TensorFlow Dataset for {dataset_name} saved to {dataset_dir}\")\n",
    "\n",
    "    with open(os.path.join(save_path, f'{dataset_name}_raw_data.pkl'), 'wb') as f:\n",
    "        pickle.dump(raw_data_list, f)\n",
    "    print(f\"Raw data for {dataset_name} saved to {os.path.join(save_path, f'{dataset_name}_raw_data.pkl')}\")\n",
    "\n",
    "    return dataset, raw_data_list\n",
    "\n",
    "def load_tf_datasets_from_disk(load_path):\n",
    "    print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "    with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "        loaded_moe_params = json.load(f)\n",
    "\n",
    "    element_spec = (\n",
    "        {\n",
    "            'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"],), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"],), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"],), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"],), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"],), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"],), dtype=tf.float32),\n",
    "        },\n",
    "        {\n",
    "            'domain_classification_output': tf.TensorSpec(shape=(None, 1,), dtype=tf.int32),\n",
    "            'intent_classification_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"],), dtype=tf.float32),\n",
    "            'response_generation_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"],), dtype=tf.int32)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    datasets = {}\n",
    "    for dataset_name in [\"train\", \"test\", \"calibration_train\", \"calibration_val\"]:\n",
    "        try:\n",
    "            datasets[dataset_name] = tf.data.Dataset.load(os.path.join(load_path, dataset_name), element_spec=element_spec)\n",
    "            print(f\"Loaded {dataset_name} dataset from {os.path.join(load_path, dataset_name)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load {dataset_name} dataset: {e}\")\n",
    "            datasets[dataset_name] = tf.data.Dataset.from_generator(lambda: [], output_signature=element_spec)\n",
    "\n",
    "    raw_data_paths = {\n",
    "        \"train\": os.path.join(load_path, 'train_raw_data.pkl'),\n",
    "        \"test\": os.path.join(load_path, 'test_raw_data.pkl'),\n",
    "        \"calibration_train\": os.path.join(load_path, 'calibration_train_raw_data.pkl'),\n",
    "        \"calibration_val\": os.path.join(load_path, 'calibration_val_raw_data.pkl')\n",
    "    }\n",
    "\n",
    "    raw_data = {}\n",
    "    for key, path in raw_data_paths.items():\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                raw_data[key] = pickle.load(f)\n",
    "            print(f\"Loaded raw data for {key} from {path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Raw data file {path} not found.\")\n",
    "            raw_data[key] = []\n",
    "\n",
    "    print(\"TensorFlow Datasets and raw data loaded successfully.\")\n",
    "    return {\n",
    "        \"train_dataset\": datasets[\"train\"],\n",
    "        \"test_dataset\": datasets[\"test\"],\n",
    "        \"calibration_train_dataset\": datasets[\"calibration_train\"],\n",
    "        \"calibration_val_dataset\": datasets[\"calibration_val\"],\n",
    "        \"moe_params\": loaded_moe_params,\n",
    "        \"raw_data\": raw_data\n",
    "    }\n",
    "\n",
    "print(\"\\n--- Saving Preprocessed Datasets and Preprocessor Models ---\")\n",
    "\n",
    "preprocessor = DialoguePreprocessor(moe_params)\n",
    "preprocessor.fit(train_dataset)\n",
    "\n",
    "moe_params[\"vocab_size\"] = preprocessor.vocab_size\n",
    "moe_params[\"num_domains\"] = preprocessor.num_domains\n",
    "moe_params[\"num_intents\"] = preprocessor.num_intents\n",
    "moe_params[\"num_entities\"] = preprocessor.num_domains\n",
    "\n",
    "processed_train_data = preprocessor.preprocess(train_dataset)\n",
    "processed_test_data = preprocessor.preprocess(test_dataset)\n",
    "\n",
    "tf_dataset_save_path = \"tf_datasets\"\n",
    "os.makedirs(tf_dataset_save_path, exist_ok=True)\n",
    "\n",
    "train_tf_dataset, train_raw_data = create_and_save_tf_dataset(\n",
    "    processed_train_data, moe_params[\"batch_size\"], moe_params[\"shuffle_buffer_size\"],\n",
    "    \"train\", tf_dataset_save_path, moe_params\n",
    ")\n",
    "test_tf_dataset, test_raw_data = create_and_save_tf_dataset(\n",
    "    processed_test_data, moe_params[\"batch_size\"], None,\n",
    "    \"test\", tf_dataset_save_path, moe_params\n",
    ")\n",
    "\n",
    "with open(os.path.join(tf_dataset_save_path, \"moe_params.json\"), \"w\") as f:\n",
    "    json.dump(moe_params, f, indent=4)\n",
    "print(f\"MoE parameters saved to {tf_dataset_save_path}/moe_params.json\")\n",
    "\n",
    "preprocessor_save_path = \"preprocessor_models\"\n",
    "preprocessor.save(preprocessor_save_path)\n",
    "\n",
    "print(\"\\n--- Loading Preprocessed Datasets and Preprocessor Models ---\")\n",
    "\n",
    "loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "\n",
    "loaded_preprocessor = DialoguePreprocessor.load(preprocessor_save_path)\n",
    "print(f\"Preprocessor loaded from {preprocessor_save_path}\")\n",
    "\n",
    "print(\"\\n--- Verifying Loaded Data ---\")\n",
    "print(f\"Loaded MoE Parameters: {json.dumps(loaded_data['moe_params'], indent=4)}\")\n",
    "\n",
    "for features, targets in loaded_data[\"train_dataset\"].take(1):\n",
    "    print(\"\\nSample from Loaded Train Dataset:\")\n",
    "    print(\"Features keys:\", features.keys())\n",
    "    print(\"Targets keys:\", targets.keys())\n",
    "    print(\"User Utterance Tokens shape:\", features['user_utterance_tokens'].shape)\n",
    "    print(\"Response Generation Output shape:\", targets['response_generation_output'].shape)\n",
    "\n",
    "print(\"\\nLoaded Preprocessor Stats:\")\n",
    "print(f\"Vocab size: {loaded_preprocessor.vocab_size}\")\n",
    "print(f\"Num domains: {loaded_preprocessor.num_domains}\")\n",
    "print(f\"Num intents (slots): {loaded_preprocessor.num_intents}\")\n",
    "print(f\"Sample word_to_id (first 5): {dict(list(loaded_preprocessor.word_to_id.items())[:5])}\")\n",
    "\n",
    "print(\"\\nRaw Data Sample (Train, first item):\")\n",
    "if loaded_data[\"raw_data\"][\"train\"]:\n",
    "    print(loaded_data[\"raw_data\"][\"train\"][0])\n",
    "else:\n",
    "    print(\"No raw data available for train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load text representation parameters and preprocessed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading TensorFlow Datasets and MoE parameters from tf_datasets...\n",
      "Loaded raw data for train from tf_datasets/train_raw_data.pkl\n",
      "Loaded raw data for test from tf_datasets/test_raw_data.pkl\n"
     ]
    }
   ],
   "source": [
    "def load_tf_datasets_from_disk(load_path):\n",
    "    print(f\"\\nLoading TensorFlow Datasets and MoE parameters from {load_path}...\")\n",
    "    try:\n",
    "        with open(os.path.join(load_path, \"moe_params.json\"), \"r\") as f:\n",
    "            loaded_moe_params = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"moe_params.json not found in {load_path}\")\n",
    "\n",
    "    element_spec = (\n",
    "        {\n",
    "            'user_utterance_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'prev_system_response_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'decoder_input_tokens': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32),\n",
    "            'domain_onehot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_domains\"]), dtype=tf.float32),\n",
    "            'turn_id_embedding': tf.TensorSpec(shape=(None, loaded_moe_params[\"turn_id_dim\"]), dtype=tf.float32),\n",
    "            'ontology_multihot_input': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "        },\n",
    "        {\n",
    "            'domain_output': tf.TensorSpec(shape=(None, 1), dtype=tf.int32),\n",
    "            'intent_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"num_intents\"]), dtype=tf.float32),\n",
    "            'response_output': tf.TensorSpec(shape=(None, loaded_moe_params[\"max_seq_length\"]), dtype=tf.int32)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"train\"), element_spec=element_spec)\n",
    "        test_tf_dataset = tf.data.Dataset.load(os.path.join(load_path, \"test\"), element_spec=element_spec)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load datasets from {load_path}: {str(e)}\")\n",
    "\n",
    "    raw_data = {}\n",
    "    for dataset_name in ['train', 'test']:\n",
    "        path = os.path.join(load_path, f'{dataset_name}_raw_data.pkl')\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'rb') as f:\n",
    "                raw_data[dataset_name] = pickle.load(f)\n",
    "            print(f\"Loaded raw data for {dataset_name} from {path}\")\n",
    "        else:\n",
    "            print(f\"Warning: Raw data file {path} not found.\")\n",
    "            raw_data[dataset_name] = []\n",
    "\n",
    "    return {\n",
    "        \"train_dataset\": train_tf_dataset,\n",
    "        \"test_dataset\": test_tf_dataset,\n",
    "        \"moe_params\": loaded_moe_params,\n",
    "        \"raw_data\": raw_data\n",
    "    }\n",
    "\n",
    "tf_dataset_save_path = \"tf_datasets\"\n",
    "loaded_data = load_tf_datasets_from_disk(tf_dataset_save_path)\n",
    "train_tf_dataset = loaded_data[\"train_dataset\"]\n",
    "test_tf_dataset = loaded_data[\"test_dataset\"]\n",
    "moe_params = loaded_data[\"moe_params\"]\n",
    "raw_data = loaded_data[\"raw_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
