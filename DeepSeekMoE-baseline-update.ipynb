{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### README\n",
    "\n",
    "***I seperate to run each part in this notebook due to my computational resource limitation. It start with initial training then restart kernel, hyperparameter tuning then restart kernel, training experiment then restart kernel, and evaluation experiment then restart kernel by every process start with the same preprocess pipeline.*** \n",
    "\n",
    "***Remark that I have runned all of the pipeline on few number of sample, epoch, and iteration before upload on GitHub.***\n",
    "\n",
    "***In this notebook I have set them back to values for experiment in the paper (sample, epoch, and number of experiment iteration).***\n",
    "\n",
    "***I have runned all of the pipeline with values for experiment in the paper (sample, epoch, and number of experiment iteration), and it's ok.***\n",
    "\n",
    "***Any things that I can help you, please tell me.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSeek Model on HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***[Original Model Link!](https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat/tree/main)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***[Model Architecture Link!](https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat/blob/main/modeling_deepseek.py)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***[Model Configuration Link!](https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat/blob/main/configuration_deepseek.py)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'MultiWOZ-coref'...\n",
      "remote: Enumerating objects: 60, done.\u001b[K\n",
      "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
      "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
      "remote: Total 60 (delta 19), reused 4 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (60/60), 29.40 MiB | 44.40 MiB/s, done.\n",
      "Resolving deltas: 100% (19/19), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lexmen318/MultiWOZ-coref.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pyspellchecker\n",
    "!pip install nlpaug\n",
    "!pip install dateparser\n",
    "!pip install CurrencyConverter\n",
    "!pip install dateparser word2number\n",
    "!pip install contractions\n",
    "!pip install gensim\n",
    "!pip install GPUtil\n",
    "!pip install rouge_score\n",
    "!pip install nvidia-ml-py3\n",
    "!pip install pynvml\n",
    "!pip install rouge-score\n",
    "!pip install rouge\n",
    "!pip install optuna\n",
    "!pip install evaluate\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 12:34:32.578812: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-17 12:34:32.578918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-17 12:34:32.580368: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-17 12:34:32.588742: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-17 12:34:33.359410: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.11/dist-packages/nvidia_smi.py:42: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  from pynvml import *\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "import importlib.util\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "import shutil\n",
    "import math\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "import psutil\n",
    "import GPUtil\n",
    "import gc\n",
    "import re\n",
    "from collections import Counter\n",
    "from math import sqrt, log, exp\n",
    "from tqdm.auto import tqdm\n",
    "import optuna\n",
    "import contractions\n",
    "import dateparser\n",
    "from word2number import w2n\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, Features, Value, Sequence, ClassLabel\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import warnings\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, optimizers, losses, metrics, callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "import uuid\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders\n",
    "import logging\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "import zipfile\n",
    "import random\n",
    "import nvidia_smi\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Set the TF logging level to ERROR or higher\n",
    "# 1 = INFO, 2 = WARNING, 3 = ERROR\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-17 12:34:37,556] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "--- System Information ---\n",
      "Operating System: Linux 5.19.0-45-generic\n",
      "Python Version: 3.11.7 (main, Dec  8 2023, 18:56:58) [GCC 11.4.0]\n",
      "--- CPU Information ---\n",
      "CPU: x86_64\n",
      "Physical Cores: 8\n",
      "Logical Threads: 8\n",
      "--- Memory Information ---\n",
      "Total Memory: 44.08 GB\n",
      "Available Memory: 40.67 GB\n",
      "--- GPU Information ---\n",
      "GPU 1: NVIDIA RTX A4000, Driver Version: 550.144.03\n",
      "CUDA Version: 12.4     |\n",
      "--- DeepSpeed Information ---\n",
      "DeepSpeed Version: 0.10.3\n",
      "--- TensorFlow Information ---\n",
      "TensorFlow Version: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "def get_os_info():\n",
    "    \"\"\"Retrieves operating system information.\"\"\"\n",
    "    os_name = platform.system()\n",
    "    os_version = platform.release()\n",
    "    return os_name, os_version\n",
    "\n",
    "def get_python_info():\n",
    "    \"\"\"Retrieves Python version information.\"\"\"\n",
    "    python_version = sys.version\n",
    "    return python_version\n",
    "\n",
    "def get_cpu_info():\n",
    "    \"\"\"Retrieves CPU information.\"\"\"\n",
    "    cpu_name = platform.processor()\n",
    "    cpu_cores = psutil.cpu_count(logical=False) \n",
    "    cpu_threads = psutil.cpu_count(logical=True) \n",
    "    return cpu_name, cpu_cores, cpu_threads\n",
    "\n",
    "def get_memory_info():\n",
    "    \"\"\"Retrieves memory information.\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    total_memory_gb = mem.total / (1024 ** 3) \n",
    "    available_memory_gb = mem.available / (1024 ** 3)\n",
    "    return total_memory_gb, available_memory_gb\n",
    "\n",
    "def get_gpu_info():\n",
    "    \"\"\"Retrieves GPU information using nvidia-smi command.\"\"\"\n",
    "    try:\n",
    "        subprocess.run([\"nvidia-smi\", \"-h\"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        result = subprocess.run([\"nvidia-smi\", \"--query-gpu=name,driver_version\", \"--format=csv,noheader\"],\n",
    "                                 capture_output=True, text=True, check=True)\n",
    "        cuda_result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True, check=True)\n",
    "        cuda_version_line = next((line for line in cuda_result.stdout.split('\\n') if \"CUDA Version\" in line), None)\n",
    "        cuda_version = cuda_version_line.split(\":\")[-1].strip() if cuda_version_line else \"N/A\"\n",
    "\n",
    "        gpus = []\n",
    "        for line in result.stdout.strip().split('\\n'):\n",
    "            name, driver_version = line.split(', ')\n",
    "            gpus.append({'name': name, 'driver_version': driver_version})\n",
    "        return gpus, cuda_version\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return \"N/A\", \"N/A\"\n",
    "\n",
    "def check_deepspeed():\n",
    "    \"\"\"Checks for DeepSpeed installation and version.\"\"\"\n",
    "    deepspeed_spec = importlib.util.find_spec(\"deepspeed\")\n",
    "    if deepspeed_spec is not None:\n",
    "        try:\n",
    "            import deepspeed\n",
    "            return deepspeed.__version__\n",
    "        except ImportError:\n",
    "            return \"Installed, but version cannot be determined\"\n",
    "    else:\n",
    "        return \"Not Installed\"\n",
    "\n",
    "def check_tensorflow():\n",
    "    \"\"\"Checks for TensorFlow installation and version.\"\"\"\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        return tf.__version__\n",
    "    except ImportError:\n",
    "        return \"Not Installed\"\n",
    "\n",
    "os_name, os_version = get_os_info()\n",
    "python_version = get_python_info()\n",
    "cpu_name, cpu_cores, cpu_threads = get_cpu_info()\n",
    "total_memory_gb, available_memory_gb = get_memory_info()\n",
    "gpus, cuda_version = get_gpu_info()\n",
    "deepspeed_version = check_deepspeed()\n",
    "tensorflow_version = check_tensorflow()\n",
    "\n",
    "print(\"--- System Information ---\")\n",
    "print(f\"Operating System: {os_name} {os_version}\")\n",
    "print(f\"Python Version: {python_version}\")\n",
    "print(\"--- CPU Information ---\")\n",
    "print(f\"CPU: {cpu_name}\")\n",
    "print(f\"Physical Cores: {cpu_cores}\")\n",
    "print(f\"Logical Threads: {cpu_threads}\")\n",
    "print(\"--- Memory Information ---\")\n",
    "print(f\"Total Memory: {total_memory_gb:.2f} GB\")\n",
    "print(f\"Available Memory: {available_memory_gb:.2f} GB\")\n",
    "print(\"--- GPU Information ---\")\n",
    "if gpus == \"N/A\":\n",
    "    print(\"GPU: N/A (No NVIDIA GPU or nvidia-smi not found)\")\n",
    "    print(f\"CUDA Version: N/A\")\n",
    "else:\n",
    "    for i, gpu in enumerate(gpus):\n",
    "        print(f\"GPU {i + 1}: {gpu['name']}, Driver Version: {gpu['driver_version']}\")\n",
    "    print(f\"CUDA Version: {cuda_version}\")\n",
    "print(\"--- DeepSpeed Information ---\")\n",
    "print(f\"DeepSpeed Version: {deepspeed_version}\")\n",
    "print(\"--- TensorFlow Information ---\")\n",
    "print(f\"TensorFlow Version: {tensorflow_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory if it doesn't exist: MultiWOZ-coref-extract...\n",
      "Data files found in destination\n",
      "Load dataset...\n",
      "Load dataset sucessful\n"
     ]
    }
   ],
   "source": [
    "# Path for downloaded zip file of MultiWOZ2_3 dataset\n",
    "zip_file_path = \"MultiWOZ-coref/MultiWOZ2_3.zip\"\n",
    "\n",
    "# Removes the '.zip' extension from the file path to get a directory name.\n",
    "destination_dir = \"MultiWOZ-coref-extract\"\n",
    "\n",
    "# Define expected final data paths\n",
    "dataset_dir = os.path.join(destination_dir, \"MultiWOZ2_3\")\n",
    "data_file = os.path.join(dataset_dir, \"data.json\")\n",
    "dialogue_acts_file = os.path.join(dataset_dir, \"dialogue_acts.json\")\n",
    "ontology_file = os.path.join(dataset_dir, \"ontology.json\")\n",
    "\n",
    "# Check if all required files are present (zip and data files)\n",
    "required_files = {\n",
    "    'zip': zip_file_path,\n",
    "    'data': data_file,\n",
    "    'ontology': ontology_file,\n",
    "    'dialogue_acts': dialogue_acts_file\n",
    "}\n",
    "\n",
    "print(f\"Creating directory if it doesn't exist: {destination_dir}...\")\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# Extract if any data files are missing\n",
    "data_files = {k: v for k, v in required_files.items() if k != 'zip'}\n",
    "if any(not os.path.exists(path) for path in data_files.values()):\n",
    "    print(f\"Extracting {zip_file_path} to {destination_dir}\")\n",
    "    # Use 'with' statement for clean memory\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination_dir)\n",
    "    print(\"Extraction successful!\")\n",
    "else:\n",
    "    print(\"Data files found in destination\")\n",
    "\n",
    "# Load files assuming they exist and are correctly formatted\n",
    "print(\"Load dataset...\")\n",
    "loaded_data = {\n",
    "    key: json.load(open(path, 'r')) if os.path.exists(path) else {}\n",
    "    for key, path in data_files.items()\n",
    "}\n",
    "\n",
    "# Assign loaded data or None if loading failed (empty dict)\n",
    "data = loaded_data.get('data')\n",
    "ontology = loaded_data.get('ontology')\n",
    "dialogue_acts = loaded_data.get('dialogue_acts')\n",
    "print(\"Load dataset sucessful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get domain for dialogue\n",
    "def get_primary_domain(dialogue):\n",
    "    \"\"\"Extract the primary domain from dialogue structure.\"\"\"\n",
    "    if \"new_goal\" in dialogue and dialogue[\"new_goal\"]:\n",
    "        for domain in dialogue[\"new_goal\"]:\n",
    "            if domain != \"user_action\":\n",
    "                return domain\n",
    "    \n",
    "    if \"goal\" in dialogue and dialogue[\"goal\"]:\n",
    "        for domain in dialogue[\"goal\"]:\n",
    "            if domain != \"user_action\" and isinstance(dialogue[\"goal\"][domain], dict):\n",
    "                return domain\n",
    "            \n",
    "    domains = set()\n",
    "    for turn in dialogue.get(\"log\", []):\n",
    "        metadata = turn.get(\"metadata\", {})\n",
    "        for domain in metadata:\n",
    "            if domain != \"user_action\" and isinstance(metadata[domain], dict):\n",
    "                domains.add(domain)\n",
    "                \n",
    "    return domains.pop() if domains else \"unknown\"\n",
    "\n",
    "def normalize_dialogue_id(dialogue_id):\n",
    "    \"\"\"Normalize dialogue ID to match between data and dialogue acts.\"\"\"\n",
    "    if dialogue_id.endswith('.json'):\n",
    "        return dialogue_id[:-5]\n",
    "    return dialogue_id\n",
    "\n",
    "# Process and merge data with dialogue acts\n",
    "raw_data = []\n",
    "\n",
    "for dialogue_id, dialogue in data.items():\n",
    "    normalized_id = normalize_dialogue_id(dialogue_id)\n",
    "    acts = dialogue_acts.get(normalized_id, {})\n",
    "    processed_acts = {}\n",
    "    for turn_key, turn_acts in acts.items():\n",
    "        if turn_acts == \"No Annotation\":\n",
    "            processed_acts[str(turn_key)] = {}\n",
    "        else:\n",
    "            processed_acts[str(turn_key)] = turn_acts\n",
    "    \n",
    "    raw_data.append({\n",
    "        \"dialogue_id\": dialogue_id,\n",
    "        \"dialogue\": dialogue,\n",
    "        \"domain\": get_primary_domain(dialogue),\n",
    "        \"dialogue_acts\": processed_acts,\n",
    "        \"ontology\": ontology\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Randomly Checking 2 Dialogues (First 4 Turns) from Raw Data \n",
      "\n",
      "Dialogue ID: SNG01661.json\n",
      "Domain: train\n",
      "First 4 turns:\n",
      "    Ontology for domain 'train': {\n",
      "  \"train-book people\": [\n",
      "    \"2\",\n",
      "    \"8\",\n",
      "    \"7\",\n",
      "    \"15\",\n",
      "    \"5\",\n",
      "    \"1\",\n",
      "    \"dontcare\",\n",
      "    \"6\",\n",
      "    \"3\",\n",
      "    \"9\",\n",
      "    \"4\",\n",
      "    \"10\"\n",
      "  ],\n",
      "  \"train-arriveBy\": [\n",
      "    \"19:57\",\n",
      "    \"12:00\",\n",
      "    \"19:30\",\n",
      "    \"11:54\",\n",
      "    \"05:30\",\n",
      "    \"16:30\",\n",
      "    \"18:30\",\n",
      "    \"06:45\",\n",
      "    \"19:58\",\n",
      "    \"23:27\",\n",
      "    \"16:06\",\n",
      "    \"07:35\",\n",
      "    \"08:30\",\n",
      "    \"12:07\",\n",
      "    \"10:30\",\n",
      "    \"12:06\",\n",
      "    \"15:45\",\n",
      "    \"18:35\",\n",
      "    \"18:07\",\n",
      "    \"10:32\",\n",
      "    \"06:43\",\n",
      "    \"8\",\n",
      "    \"20:38\",\n",
      "    \"18:23\",\n",
      "    \"20:06\",\n",
      "    \"09:01\",\n",
      "    \"20:08\",\n",
      "    \"19:54\",\n",
      "    \"10:15\",\n",
      "    \"09:30\",\n",
      "    \"11:52\",\n",
      "    \"18:00\",\n",
      "    \"07:30\",\n",
      "    \"08:07\",\n",
      "    \"16:00\",\n",
      "    \"08:56\",\n",
      "    \"17:23\",\n",
      "    \"14:07\",\n",
      "    \"10:08\",\n",
      "    \"11:00\",\n",
      "    \"08:44\",\n",
      "    \"21:45\",\n",
      "    \"15:00\",\n",
      "    \"10:54\",\n",
      "    \"17:51\",\n",
      "    \"03:00\",\n",
      "    \"10:45\",\n",
      "    \"16:08\",\n",
      "    \"13:06\",\n",
      "    \"12:45\",\n",
      "    \"20:45\",\n",
      "    \"22:01\",\n",
      "    \"12:15\",\n",
      "    \"18:03\",\n",
      "    \"23:00\",\n",
      "    \"11:30\",\n",
      "    \"11:06\",\n",
      "    \"20:00\",\n",
      "    \"07:15\",\n",
      "    \"15:06\",\n",
      "    \"19:45\",\n",
      "    \"19:32\",\n",
      "    \"07:08\",\n",
      "    \"14:45\",\n",
      "    \"15:54\",\n",
      "    \"21:20\",\n",
      "    \"20:15\",\n",
      "    \"17:00\",\n",
      "    \"02:30\",\n",
      "    \"21:06\",\n",
      "    \"14:24\",\n",
      "    \"15:01\",\n",
      "    \"11:15\",\n",
      "    \"19:15\",\n",
      "    \"11:32\",\n",
      "    \"07:44\",\n",
      "    \"22:06\",\n",
      "    \"13:30\",\n",
      "    \"10:00\",\n",
      "    \"21:00\",\n",
      "    \"13:17\",\n",
      "    \"06:55\",\n",
      "    \"05:52\",\n",
      "    \"12:43\",\n",
      "    \"15:15\",\n",
      "    \"17:45\",\n",
      "    \"10:23\",\n",
      "    \"09:45\",\n",
      "    \"06:01\",\n",
      "    \"15:07\",\n",
      "    \"10:07\",\n",
      "    \"13:52\",\n",
      "    \"17:58\",\n",
      "    \"16:15\",\n",
      "    \"21:30\",\n",
      "    \"13:51\",\n",
      "    \"15:24\",\n",
      "    \"02:00\",\n",
      "    \"19:27\",\n",
      "    \"09:06\",\n",
      "    \"13:38\",\n",
      "    \"08:00\",\n",
      "    \"09:32\",\n",
      "    \"dontcare\",\n",
      "    \"13:00\",\n",
      "    \"14:43\",\n",
      "    \"20:20\",\n",
      "    \"06:07\",\n",
      "    \"09:00\",\n",
      "    \"10:43\",\n",
      "    \"14:01\",\n",
      "    \"08:45\",\n",
      "    \"16:32\",\n",
      "    \"22:00\",\n",
      "    \"17:15\",\n",
      "    \"20:07\",\n",
      "    \"15:30\",\n",
      "    \"08:15\",\n",
      "    \"18:10\",\n",
      "    \"12:30\",\n",
      "    \"11:45\",\n",
      "    \"06:15\",\n",
      "    \"1100\",\n",
      "    \"18:32\",\n",
      "    \"13:03\",\n",
      "    \"08:54\",\n",
      "    \"21:15\",\n",
      "    \"13:32\",\n",
      "    \"16:23\",\n",
      "    \"14:15\",\n",
      "    \"23:30\",\n",
      "    \"14:00\",\n",
      "    \"16:45\",\n",
      "    \"20:54\",\n",
      "    \"12:08\",\n",
      "    \"07:00\",\n",
      "    \"16:58\",\n",
      "    \"07:52\",\n",
      "    \"21:08\",\n",
      "    \"19:51\",\n",
      "    \"13:45\",\n",
      "    \"18:15\",\n",
      "    \"20:30\",\n",
      "    \"06:30\",\n",
      "    \"19:52\",\n",
      "    \"16:07\",\n",
      "    \"17:30\",\n",
      "    \"1545\",\n",
      "    \"09:15\",\n",
      "    \"21:51\",\n",
      "    \"14:30\",\n",
      "    \"22:07\",\n",
      "    \"13:15\",\n",
      "    \"16:44\",\n",
      "    \"08:23\",\n",
      "    \"18:45\",\n",
      "    \"19:00\"\n",
      "  ],\n",
      "  \"train-day\": [\n",
      "    \"saturday\",\n",
      "    \"wednesday\",\n",
      "    \"sunday\",\n",
      "    \"friday\",\n",
      "    \"dontcare\",\n",
      "    \"monday\",\n",
      "    \"thursday\",\n",
      "    \"tuesday\"\n",
      "  ],\n",
      "  \"train-departure\": [\n",
      "    \"huntingdon\",\n",
      "    \"panahar\",\n",
      "    \"london liverpool street\",\n",
      "    \"leicester\",\n",
      "    \"brookshite\",\n",
      "    \"aylesbray lodge guest\",\n",
      "    \"broxbourne\",\n",
      "    \"london kings cross\",\n",
      "    \"cambridge\",\n",
      "    \"birmingham new street\",\n",
      "    \"camboats\",\n",
      "    \"peterborough\",\n",
      "    \"cineworld\",\n",
      "    \"alpha-milton\",\n",
      "    \"cafe uno\",\n",
      "    \"city hall\",\n",
      "    \"stansted airport\",\n",
      "    \"liverpool\",\n",
      "    \"hamilton lodge\",\n",
      "    \"dontcare\",\n",
      "    \"duxford\",\n",
      "    \"wandlebury country park\",\n",
      "    \"stevenage\",\n",
      "    \"ely\",\n",
      "    \"norwich\",\n",
      "    \"london\",\n",
      "    \"east london\",\n",
      "    \"london liverpool\",\n",
      "    \"bishops stortford\",\n",
      "    \"kings lynn\",\n",
      "    \"stratford\"\n",
      "  ],\n",
      "  \"train-destination\": [\n",
      "    \"copper kettle\",\n",
      "    \"city centre north\",\n",
      "    \"london liverpool street\",\n",
      "    \"leicester\",\n",
      "    \"norway\",\n",
      "    \"centre\",\n",
      "    \"broxbourne\",\n",
      "    \"gourmet burger kitchen\",\n",
      "    \"london kings cross\",\n",
      "    \"cambridge\",\n",
      "    \"birmingham new street\",\n",
      "    \"glastonbury\",\n",
      "    \"peterborough\",\n",
      "    \"huntingdon marriott hotel\",\n",
      "    \"stansted airport\",\n",
      "    \"liverpool\",\n",
      "    \"curry prince\",\n",
      "    \"dontcare\",\n",
      "    \"stevenage\",\n",
      "    \"ely\",\n",
      "    \"norwich\",\n",
      "    \"liverpool street\",\n",
      "    \"london\",\n",
      "    \"bournemouth\",\n",
      "    \"huntington marriott\",\n",
      "    \"bishops stortford\",\n",
      "    \"kings lynn\"\n",
      "  ],\n",
      "  \"train-leaveAt\": [\n",
      "    \"12:00\",\n",
      "    \"13:40\",\n",
      "    \"19:30\",\n",
      "    \"11:54\",\n",
      "    \"05:30\",\n",
      "    \"16:30\",\n",
      "    \"09:39\",\n",
      "    \"morning\",\n",
      "    \"15:39\",\n",
      "    \"18:30\",\n",
      "    \"06:45\",\n",
      "    \"20:19\",\n",
      "    \"07:35\",\n",
      "    \"08:11\",\n",
      "    \"08:30\",\n",
      "    \"10:19\",\n",
      "    \"08:16\",\n",
      "    \"10:30\",\n",
      "    \"15:45\",\n",
      "    \"11:48\",\n",
      "    \"06:09\",\n",
      "    \"10:32\",\n",
      "    \"07:54\",\n",
      "    \"19:50\",\n",
      "    \"11:29\",\n",
      "    \"19:54\",\n",
      "    \"10:15\",\n",
      "    \"09:30\",\n",
      "    \"13:54\",\n",
      "    \"18:00\",\n",
      "    \"06:40\",\n",
      "    \"08:10\",\n",
      "    \"16:00\",\n",
      "    \"07:40\",\n",
      "    \"1145\",\n",
      "    \"07:21\",\n",
      "    \"05:29\",\n",
      "    \"13:29\",\n",
      "    \"11:01\",\n",
      "    \"05:15\",\n",
      "    \"11:11\",\n",
      "    \"11:00\",\n",
      "    \"13:11\",\n",
      "    \"14:09\",\n",
      "    \"21:45\",\n",
      "    \"15:00\",\n",
      "    \"19:35\",\n",
      "    \"05:17\",\n",
      "    \"10:45\",\n",
      "    \"05:24\",\n",
      "    \"12:45\",\n",
      "    \"05:54\",\n",
      "    \"20:45\",\n",
      "    \"22:01\",\n",
      "    \"07:45\",\n",
      "    \"12:15\",\n",
      "    \"15:11\",\n",
      "    \"15:50\",\n",
      "    \"12:48\",\n",
      "    \"08:52\",\n",
      "    \"09:19\",\n",
      "    \"11:30\",\n",
      "    \"20:00\",\n",
      "    \"07:15\",\n",
      "    \"19:45\",\n",
      "    \"07:27\",\n",
      "    \"09:11\",\n",
      "    \"afternoon\",\n",
      "    \"17:59\",\n",
      "    \"14:45\",\n",
      "    \"05:11\",\n",
      "    \"15:54\",\n",
      "    \"20:15\",\n",
      "    \"10\",\n",
      "    \"05:01\",\n",
      "    \"13:36\",\n",
      "    \"14:40\",\n",
      "    \"845\",\n",
      "    \"17:00\",\n",
      "    \"09:17\",\n",
      "    \"05:00\",\n",
      "    \"18:06\",\n",
      "    \"15:01\",\n",
      "    \"05:16\",\n",
      "    \"11:15\",\n",
      "    \"09:29\",\n",
      "    \"19:15\",\n",
      "    \"17:50\",\n",
      "    \"11:17\",\n",
      "    \"05:45\",\n",
      "    \"18:09\",\n",
      "    \"15:17\",\n",
      "    \"17:29\",\n",
      "    \"13:30\",\n",
      "    \"10:00\",\n",
      "    \"21:00\",\n",
      "    \"11:35\",\n",
      "    \"10:21\",\n",
      "    \"13:17\",\n",
      "    \"21:11\",\n",
      "    \"15:15\",\n",
      "    \"10:36\",\n",
      "    \"19:40\",\n",
      "    \"17:45\",\n",
      "    \"08:01\",\n",
      "    \"09:45\",\n",
      "    \"06:01\",\n",
      "    \"20:21\",\n",
      "    \"16:15\",\n",
      "    \"after lunch\",\n",
      "    \"21:30\",\n",
      "    \"12:26\",\n",
      "    \"06:00\",\n",
      "    \"05:36\",\n",
      "    \"1532\",\n",
      "    \"15:24\",\n",
      "    \"02:00\",\n",
      "    \"21:50\",\n",
      "    \"18:21\",\n",
      "    \"17:01\",\n",
      "    \"05:40\",\n",
      "    \"12:19\",\n",
      "    \"06:10\",\n",
      "    \"08:00\",\n",
      "    \"09:54\",\n",
      "    \"dontcare\",\n",
      "    \"13:00\",\n",
      "    \"5:45pm\",\n",
      "    \"13:24\",\n",
      "    \"17:16\",\n",
      "    \"09:00\",\n",
      "    \"09:23\",\n",
      "    \"14:01\",\n",
      "    \"1329\",\n",
      "    \"08:45\",\n",
      "    \"13:39\",\n",
      "    \"22:00\",\n",
      "    \"17:11\",\n",
      "    \"17:15\",\n",
      "    \"15:30\",\n",
      "    \"08:15\",\n",
      "    \"07:17\",\n",
      "    \"12:32\",\n",
      "    \"01:44\",\n",
      "    \"12:30\",\n",
      "    \"11:45\",\n",
      "    \"after 16:30\",\n",
      "    \"00:00\",\n",
      "    \"13:01\",\n",
      "    \"21:15\",\n",
      "    \"09:50\",\n",
      "    \"19:17\",\n",
      "    \"13:32\",\n",
      "    \"09:59\",\n",
      "    \"14:15\",\n",
      "    \"19:48\",\n",
      "    \"18:46\",\n",
      "    \"08:32\",\n",
      "    \"08:08\",\n",
      "    \"9\",\n",
      "    \"14:00\",\n",
      "    \"15:29\",\n",
      "    \"20:40\",\n",
      "    \"08:06\",\n",
      "    \"17:21\",\n",
      "    \"16:45\",\n",
      "    \"11:50\",\n",
      "    \"14:19\",\n",
      "    \"07:00\",\n",
      "    \"13:50\",\n",
      "    \"21:39\",\n",
      "    \"11:39\",\n",
      "    \"09:40\",\n",
      "    \"11:21\",\n",
      "    \"929\",\n",
      "    \"08:09\",\n",
      "    \"13:45\",\n",
      "    \"20:01\",\n",
      "    \"18:15\",\n",
      "    \"20:30\",\n",
      "    \"06:30\",\n",
      "    \"10:11\",\n",
      "    \"12:56\",\n",
      "    \"21:01\",\n",
      "    \"22:09\",\n",
      "    \"17:30\",\n",
      "    \"1545\",\n",
      "    \"09:15\",\n",
      "    \"21:29\",\n",
      "    \"14:30\",\n",
      "    \"19:16\",\n",
      "    \"07:01\",\n",
      "    \"13:15\",\n",
      "    \"19:11\",\n",
      "    \"05:50\",\n",
      "    \"18:40\",\n",
      "    \"05:35\",\n",
      "    \"14:21\",\n",
      "    \"18:45\",\n",
      "    \"15:16\",\n",
      "    \"18:36\",\n",
      "    \"19:00\",\n",
      "    \"20:36\"\n",
      "  ]\n",
      "}\n",
      "  Turn 0 (User):\n",
      "    Text: I am looking for a train to Cambridge .\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: What time do you want to leave ?\n",
      "    Dialogue Acts: Train-Request(['Leave', '?'])\n",
      "  Turn 2 (User):\n",
      "    Text: I would like to leave after 15:15\n",
      "    Dialogue Acts: Train-Request(['Depart', '?'])\n",
      "  Turn 3 (System):\n",
      "    Text: I 'd love to help ! Where are you departing from ?\n",
      "    Dialogue Acts: Train-Inform(['Arrive', '17:54'], ['Time', '105 minutes'], ['Leave', '16:09'], ['Ticket', '37.80'])\n",
      "\n",
      "Dialogue ID: SNG0767.json\n",
      "Domain: hotel\n",
      "First 4 turns:\n",
      "    Ontology for domain 'hotel': {\n",
      "  \"hotel-book day\": [\n",
      "    \"saturday\",\n",
      "    \"wednesday\",\n",
      "    \"sunday\",\n",
      "    \"friday\",\n",
      "    \"saturday|tuesday\",\n",
      "    \"wednesday|friday\",\n",
      "    \"dontcare\",\n",
      "    \"monday<thursday\",\n",
      "    \"monday\",\n",
      "    \"thursday\",\n",
      "    \"tuesday\",\n",
      "    \"friday>tuesday\",\n",
      "    \"sunday>monday\"\n",
      "  ],\n",
      "  \"hotel-book people\": [\n",
      "    \"2\",\n",
      "    \"7\",\n",
      "    \"8\",\n",
      "    \"5\",\n",
      "    \"1\",\n",
      "    \"6\",\n",
      "    \"3\",\n",
      "    \"4\"\n",
      "  ],\n",
      "  \"hotel-book stay\": [\n",
      "    \"2\",\n",
      "    \"8\",\n",
      "    \"7\",\n",
      "    \"5\",\n",
      "    \"5|4\",\n",
      "    \"1\",\n",
      "    \"6\",\n",
      "    \"3\",\n",
      "    \"3|1\",\n",
      "    \"4\"\n",
      "  ],\n",
      "  \"hotel-area\": [\n",
      "    \"north\",\n",
      "    \"east\",\n",
      "    \"south\",\n",
      "    \"dontcare\",\n",
      "    \"centre\",\n",
      "    \"west\",\n",
      "    \"west|centre\"\n",
      "  ],\n",
      "  \"hotel-internet\": [\n",
      "    \"free\",\n",
      "    \"dontcare\",\n",
      "    \"no\",\n",
      "    \"yes\"\n",
      "  ],\n",
      "  \"hotel-name\": [\n",
      "    \"a and b guest house\",\n",
      "    \"aylesbray lodge guest house\",\n",
      "    \"aylesbray lodge guest house|rosas bed and breakfast\",\n",
      "    \"la margherit\",\n",
      "    \"leverton house\",\n",
      "    \"bridge guest house|aylesbray lodge guest house\",\n",
      "    \"eraina\",\n",
      "    \"holiday inn\",\n",
      "    \"yes\",\n",
      "    \"huntingdon marriott hotel\",\n",
      "    \"acorn guesthouse\",\n",
      "    \"university arms hotel\",\n",
      "    \"wankworth house\",\n",
      "    \"archway house\",\n",
      "    \"cambridge belfry\",\n",
      "    \"home from home\",\n",
      "    \"the allenbell|autumn house|leverton house\",\n",
      "    \"limehouse\",\n",
      "    \"super 5\",\n",
      "    \"anatolia\",\n",
      "    \"huntingdon marriott hotel|university arms hotel\",\n",
      "    \"bridge guest house\",\n",
      "    \"finches b and b\",\n",
      "    \"a and b guesthouse\",\n",
      "    \"the cambridge belfry\",\n",
      "    \"cote\",\n",
      "    \"sleeperz\",\n",
      "    \"gonville hotel|the lensfield hotel\",\n",
      "    \"doubletree by hilton cambridge\",\n",
      "    \"carolina bed and breakfast\",\n",
      "    \"the alpha-milton|the hamilton lodge\",\n",
      "    \"alexander bed and breakfast|university arms hotel\",\n",
      "    \"ashley hotel\",\n",
      "    \"allenbell\",\n",
      "    \"ashley hotel|lovell lodge\",\n",
      "    \"acorn guest house\",\n",
      "    \"no\",\n",
      "    \"the worth house\",\n",
      "    \"city centre north b and b\",\n",
      "    \"hobsons house\",\n",
      "    \"huntingdon marriott hotel|cambridge belfry\",\n",
      "    \"el shaddia guesthouse\",\n",
      "    \"worth house\",\n",
      "    \"lensfield hotel\",\n",
      "    \"kirkwood\",\n",
      "    \"cambridge belfray\",\n",
      "    \"autumn house\",\n",
      "    \"alexander bed and breakfast|el shaddai\",\n",
      "    \"nusha\",\n",
      "    \"warkworth house\",\n",
      "    \"wandlebury coutn\",\n",
      "    \"lan hon\",\n",
      "    \"rosas bed and breakfast\",\n",
      "    \"city centre b & b|el shaddai\",\n",
      "    \"whale\",\n",
      "    \"belfy hotel\",\n",
      "    \"north b and b\",\n",
      "    \"warkworth house|autumn house\",\n",
      "    \"hamilton lodge\",\n",
      "    \"the ashley hotel|lovell lodge\",\n",
      "    \"dontcare\",\n",
      "    \"express by holiday inn cambridge\",\n",
      "    \"cityroomz\",\n",
      "    \"cherr\",\n",
      "    \"alexander|el shaddai|gonville hotel|university arms hotl\",\n",
      "    \"alpha milton guest house\",\n",
      "    \"city centre north b and b|el shaddai\",\n",
      "    \"holiday inn cambridge\",\n",
      "    \"alpha-milton guest house\",\n",
      "    \"gonville hotel\",\n",
      "    \"kirkwood house\",\n",
      "    \"marriot hotel\",\n",
      "    \"the gonvile hotel\",\n",
      "    \"avalon\",\n",
      "    \"acorn place\",\n",
      "    \"finches bed and breakfast\",\n",
      "    \"arbury lodge guesthouse\",\n",
      "    \"nus\",\n",
      "    \"the acorn guest house\",\n",
      "    \"alesbray lodge guesthouse\",\n",
      "    \"ashely hotel|lovell lodge\",\n",
      "    \"alexander bed and breakfast\",\n",
      "    \"allenbell|autumn house|leverton house\",\n",
      "    \"allenbell|alexander bed and breakfast\",\n",
      "    \"NOT(hamilton lodge)\",\n",
      "    \"sou\",\n",
      "    \"lovell lodge\",\n",
      "    \"wartworth\",\n",
      "    \"ashley hotel|lovell lodge|cityroomz\"\n",
      "  ],\n",
      "  \"hotel-parking\": [\n",
      "    \"dontcare\",\n",
      "    \"free\",\n",
      "    \"no\",\n",
      "    \"yes\"\n",
      "  ],\n",
      "  \"hotel-pricerange\": [\n",
      "    \"expensive\",\n",
      "    \"cheap\",\n",
      "    \"moderate\",\n",
      "    \"cheap>moderate\",\n",
      "    \"dontcare\",\n",
      "    \"cheap|moderate\",\n",
      "    \"moderate|cheap\",\n",
      "    \"$100\"\n",
      "  ],\n",
      "  \"hotel-stars\": [\n",
      "    \"2\",\n",
      "    \"4|5\",\n",
      "    \"0\",\n",
      "    \"5\",\n",
      "    \"1\",\n",
      "    \"dontcare\",\n",
      "    \"3|4\",\n",
      "    \"3\",\n",
      "    \"4\"\n",
      "  ],\n",
      "  \"hotel-type\": [\n",
      "    \"guesthouse\",\n",
      "    \"bed and breakfast\",\n",
      "    \"dontcare\",\n",
      "    \"hotel|guesthouse\",\n",
      "    \"hotel\"\n",
      "  ]\n",
      "}\n",
      "  Turn 0 (User):\n",
      "    Text: I kind of need some help finding a nice hotel in the north part of town .\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: There are two hotels in the north part of town in the moderate price range , does that price range work for you ?\n",
      "    Dialogue Acts: Hotel-Inform(['Area', 'north'], ['Choice', 'two'], ['Price', 'moderate'])\n",
      "  Turn 2 (User):\n",
      "    Text: Do either of those have 4 stars ?\n",
      "    Dialogue Acts: Hotel-Request(['Area', '?'], ['Stars', '?']); Hotel-Inform(['Stars', '2'])\n",
      "  Turn 3 (System):\n",
      "    Text: I 'm sorry , they are both 2 star hotels . Would you like to try a different area or star number ?\n",
      "    Dialogue Acts: Hotel-Request(['Price', '?']); Hotel-Inform(['Price', 'moderate'], ['Price', 'cheap'], ['Choice', 'several'], ['Choice', 'one'])\n"
     ]
    }
   ],
   "source": [
    "def format_dialogue_acts(acts_dict):\n",
    "    \"\"\"Format dialogue acts for better display.\"\"\"\n",
    "    if not acts_dict:\n",
    "        return \"No acts available\"\n",
    "    formatted = []\n",
    "    for act_type, slots in acts_dict.items():\n",
    "        slot_strs = []\n",
    "        for slot in slots:\n",
    "            if ':' in slot:\n",
    "                key, value = slot.split(':', 1)\n",
    "                slot_strs.append(f\"{key}={value}\")\n",
    "            else:\n",
    "                slot_strs.append(str(slot))\n",
    "        formatted.append(f\"{act_type}({', '.join(slot_strs)})\")\n",
    "    return \"; \".join(formatted)\n",
    "\n",
    "def randomly_check_first_dialogue_turns_with_acts_and_ontology(split_name, split_data, num_samples=2, num_turns=4):\n",
    "    \"\"\"\n",
    "    Randomly checks a few dialogues and displays the dialogue text, acts, and ontology.\n",
    "    \"\"\"\n",
    "    print(f\"\\nRandomly Checking {num_samples} Dialogues (First {num_turns} Turns) from {split_name} \")\n",
    "    if not split_data:\n",
    "        print(f\"No data in {split_name} split.\")\n",
    "        return\n",
    "    \n",
    "    if not isinstance(split_data, (list, tuple)):\n",
    "        split_data = list(split_data)\n",
    "    \n",
    "    if len(split_data) == 0:\n",
    "        print(f\"No data in {split_name} split after conversion.\")\n",
    "        return\n",
    "        \n",
    "    random_dialogues = random.sample(split_data, min(num_samples, len(split_data)))\n",
    "    \n",
    "    for dialogue_item in random_dialogues:\n",
    "        dialogue_id = dialogue_item[\"dialogue_id\"]\n",
    "        raw_dialogue_object = dialogue_item[\"dialogue\"]\n",
    "        if isinstance(raw_dialogue_object, list):\n",
    "            turns = raw_dialogue_object\n",
    "        elif isinstance(raw_dialogue_object, dict):\n",
    "            turns = raw_dialogue_object.get(\"log\", [])\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected dialogue format for dialogue ID {dialogue_id}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nDialogue ID: {dialogue_id}\")\n",
    "        print(f\"Domain: {dialogue_item.get('domain', 'unknown')}\")\n",
    "        print(f\"First {min(num_turns, len(turns))} turns:\")\n",
    "        \n",
    "        dialogue_acts_for_dialogue = dialogue_item.get(\"dialogue_acts\", {})\n",
    "        domain = dialogue_item.get('domain', 'unknown')\n",
    "        \n",
    "        domain_ontology = {\n",
    "            slot: values\n",
    "            for slot, values in dialogue_item.get(\"ontology\", {}).items()\n",
    "            if slot.startswith(f\"{domain}-\")\n",
    "        }\n",
    "\n",
    "        print(f\"    Ontology for domain '{domain}': {json.dumps(domain_ontology, indent=2)}\")\n",
    "\n",
    "        for i in range(min(num_turns, len(turns))):\n",
    "            turn = turns[i]\n",
    "            # In the raw data, the 'text' key is at this level\n",
    "            text_content = turn.get('text', '')\n",
    "            # use the loop index for the turn id\n",
    "            turn_idx = i                                      \n",
    "            turn_key = str(turn_idx)\n",
    "\n",
    "            turn_type = \"User\" if turn_idx % 2 == 0 else \"System\"\n",
    "            print(f\"  Turn {turn_idx} ({turn_type}):\")\n",
    "            \n",
    "            # Use the text content from the turn dictionary\n",
    "            print(f\"    Text: {text_content}\")\n",
    "            \n",
    "            turn_acts_data = dialogue_acts_for_dialogue.get(turn_key, {})\n",
    "            print(f\"    Dialogue Acts: {format_dialogue_acts(turn_acts_data)}\")\n",
    "            \n",
    "randomly_check_first_dialogue_turns_with_acts_and_ontology(\"Raw Data\", raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled train_dataset size: 1600\n",
      "Sampled test_dataset size: 400\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "train_data_raw, test_data_raw = train_test_split(\n",
    "    raw_data,\n",
    "    test_size=0.2,\n",
    "    stratify=[d[\"domain\"] for d in raw_data],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Get sample of the dataset function\n",
    "def get_samples(dataset, num_samples):\n",
    "    if not dataset:\n",
    "        print(\"Warning: Dataset is empty. Returning an empty list.\")\n",
    "        return []\n",
    "    if not isinstance(dataset, (list, tuple)):\n",
    "        try:\n",
    "            dataset_list = list(dataset)\n",
    "        except TypeError:\n",
    "            print(\"Error: Dataset could not be converted to a list for sampling.\")\n",
    "            return []\n",
    "    else:\n",
    "        dataset_list = list(dataset)\n",
    "    domain_counts = {}\n",
    "    \n",
    "    for item in dataset_list:\n",
    "        if 'domain' in item:\n",
    "            domain = item['domain']\n",
    "            domain_counts[domain] = domain_counts.get(domain, 0) + 1\n",
    "        else:\n",
    "            print(f\"Warning: Item {item} does not have a 'domain' key. Skipping for stratification.\")\n",
    "    total_items = len(dataset_list)\n",
    "    if total_items == 0:\n",
    "        print(\"Warning: Dataset has no items. Returning an empty list.\")\n",
    "        return []\n",
    "    actual_samples_to_take = min(num_samples, total_items)\n",
    "    if actual_samples_to_take < num_samples:\n",
    "        print(f\"Warning: Dataset size ({total_items}) is less than requested samples ({num_samples}). Taking {actual_samples_to_take} samples.\")\n",
    "    sampled_data = []\n",
    "    random.seed(42)\n",
    "    items_by_domain = {domain: [] for domain in domain_counts}\n",
    "    \n",
    "    for item in dataset_list:\n",
    "        if 'domain' in item:\n",
    "            items_by_domain[item['domain']].append(item)\n",
    "            \n",
    "    for domain, count in domain_counts.items():\n",
    "        domain_proportion = count / total_items\n",
    "        num_domain_samples = round(actual_samples_to_take * domain_proportion)\n",
    "        num_domain_samples = min(num_domain_samples, len(items_by_domain[domain]))\n",
    "        sampled_domain_items = random.sample(items_by_domain[domain], num_domain_samples)\n",
    "        sampled_data.extend(sampled_domain_items)\n",
    "        \n",
    "    if len(sampled_data) < actual_samples_to_take:\n",
    "        remaining_items = [item for item in dataset_list if item not in sampled_data]\n",
    "        num_to_add = actual_samples_to_take - len(sampled_data)\n",
    "        if remaining_items and num_to_add > 0:\n",
    "            sampled_data.extend(random.sample(remaining_items, min(num_to_add, len(remaining_items))))\n",
    "    elif len(sampled_data) > actual_samples_to_take:\n",
    "        num_to_remove = len(sampled_data) - actual_samples_to_take\n",
    "        for _ in range(num_to_remove):\n",
    "            sampled_data.pop(random.randrange(len(sampled_data)))\n",
    "            \n",
    "    random.shuffle(sampled_data)\n",
    "    \n",
    "    return sampled_data\n",
    "\n",
    "train_data_raw = get_samples(train_data_raw, 1600)\n",
    "print(f\"Sampled train_dataset size: {len(train_data_raw)}\")\n",
    "\n",
    "test_data_raw = get_samples(test_data_raw, 400)\n",
    "print(f\"Sampled test_dataset size: {len(test_data_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Randomly Checking 2 Dialogues (First 4 Turns) from Train \n",
      "\n",
      "Dialogue ID: SNG0238.json\n",
      "Domain: hospital\n",
      "First 4 turns:\n",
      "    Ontology for domain 'hospital': {\n",
      "  \"hospital-department\": [\n",
      "    \"antenatal\",\n",
      "    \"childrens surgical and medicine\",\n",
      "    \"haematology and haematological oncology\",\n",
      "    \"infectious diseases\",\n",
      "    \"medical decisions unit\",\n",
      "    \"teenage cancer trust unit\",\n",
      "    \"transitional care\",\n",
      "    \"john farman intensive care unit\",\n",
      "    \"urology\",\n",
      "    \"intermediate dependancy area\",\n",
      "    \"respiratory medicine\",\n",
      "    \"neonatal unit\",\n",
      "    \"inpatient occupational therapy\",\n",
      "    \"gynaecology\",\n",
      "    \"medicine for the elderly\",\n",
      "    \"neurology\",\n",
      "    \"trauma high dependency unit\",\n",
      "    \"cardiology\",\n",
      "    \"cardiology and coronary care unit\",\n",
      "    \"hepatobillary and gastrointestinal surgery regional referral centre\",\n",
      "    \"acute medical assessment unit\",\n",
      "    \"gastroenterology\",\n",
      "    \"oral and maxillofacial surgery and ent\",\n",
      "    \"hepatology\",\n",
      "    \"acute medicine for the elderly\",\n",
      "    \"transplant high dependency unit\",\n",
      "    \"plastic and vascular surgery plastics\",\n",
      "    \"clinical research facility\",\n",
      "    \"infusion services\",\n",
      "    \"cambridge eye unit\",\n",
      "    \"dontcare\",\n",
      "    \"coronary care unit\",\n",
      "    \"neurology neurosurgery\",\n",
      "    \"paediatric clinic\",\n",
      "    \"oncology\",\n",
      "    \"diabetes and endocrinology\",\n",
      "    \"neurosciences\",\n",
      "    \"clinical decisions unit\",\n",
      "    \"surgery\",\n",
      "    \"psychiatry\",\n",
      "    \"paediatric intensive care unit\",\n",
      "    \"emergency department\",\n",
      "    \"haematology\",\n",
      "    \"childrens oncology and haematology\",\n",
      "    \"neurosciences critical care unit\",\n",
      "    \"paediatric day unit\",\n",
      "    \"trauma and orthopaedics\",\n",
      "    \"haematology day unit\"\n",
      "  ]\n",
      "}\n",
      "  Turn 0 (User):\n",
      "    Text: Could you find me a hospital in town ?\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: Yes , Addenbrookes Hospital is in your area , would you like me to book you an appointment ?\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 2 (User):\n",
      "    Text: Do they have a transitional care department ? I would also like to know the postcode and phone number .\n",
      "    Dialogue Acts: Hospital-Inform(['Phone', '01223254668'], ['Department', 'transitional care'], ['Post', 'CB20QQ'])\n",
      "  Turn 3 (System):\n",
      "    Text: Yes they have transitional care .   Their post code is CB20QQ and the phone is 01223254668 .   Would you like me to book that for you ?\n",
      "    Dialogue Acts: general-bye(['none', 'none'])\n",
      "\n",
      "Dialogue ID: PMUL3693.json\n",
      "Domain: attraction\n",
      "First 4 turns:\n",
      "    Ontology for domain 'attraction': {\n",
      "  \"attraction-area\": [\n",
      "    \"north\",\n",
      "    \"centre|west\",\n",
      "    \"east\",\n",
      "    \"south\",\n",
      "    \"centre\",\n",
      "    \"west\",\n",
      "    \"dontcare\"\n",
      "  ],\n",
      "  \"attraction-name\": [\n",
      "    \"cambridge arts theater\",\n",
      "    \"scudamores punting co\",\n",
      "    \"trinity college\",\n",
      "    \"history of science museum\",\n",
      "    \"old schools\",\n",
      "    \"christs college|saint catharines\",\n",
      "    \"aylesbray lodge guest house\",\n",
      "    \"scott polar\",\n",
      "    \"churchills college\",\n",
      "    \"museum of archaelogy and anthropology\",\n",
      "    \"cambridge artworks\",\n",
      "    \"pembroke college\",\n",
      "    \"milton country park\",\n",
      "    \"regency gallery\",\n",
      "    \"cambridge contemporary art museum\",\n",
      "    \"little saint marys church\",\n",
      "    \"sheeps green and lammas land park fen causeway\",\n",
      "    \"the junction\",\n",
      "    \"college\",\n",
      "    \"peoples portraits exhibition at girton college\",\n",
      "    \"st catharines college\",\n",
      "    \"kings college|hughes hall\",\n",
      "    \"cambridge and country folk museum\",\n",
      "    \"cineworld cinema\",\n",
      "    \"cambridge arts theatre\",\n",
      "    \"great saint marys church\",\n",
      "    \"university arms hotel\",\n",
      "    \"wandlebury country park\",\n",
      "    \"soul tree nightclub\",\n",
      "    \"churchill\",\n",
      "    \"archway house\",\n",
      "    \"salsa\",\n",
      "    \"home from home\",\n",
      "    \"sheeps green\",\n",
      "    \"scudamores punti co|the cambridge punter\",\n",
      "    \"sidney sussex|gonville and caius\",\n",
      "    \"christs church\",\n",
      "    \"riverboat georgina\",\n",
      "    \"cherry hinton village centre\",\n",
      "    \"mumford theatre\",\n",
      "    \"museum\",\n",
      "    \"castle galleries|primavera\",\n",
      "    \"ruskin gallery\",\n",
      "    \"christs college|downing college\",\n",
      "    \"cambridge corn exchange\",\n",
      "    \"lammas land park\",\n",
      "    \"museum of archaelogy\",\n",
      "    \"king hedges learner pool\",\n",
      "    \"whipple museum of the history of science\",\n",
      "    \"camboats\",\n",
      "    \"scudamores punting co|the cambridge punter\",\n",
      "    \"abbey pool and astroturf\",\n",
      "    \"the wandlebury\",\n",
      "    \"abbey pool and astroturf pitch\",\n",
      "    \"the cambridge corn exchange\",\n",
      "    \"cafe uno\",\n",
      "    \"cherry hinton water play\",\n",
      "    \"cafe jello museum\",\n",
      "    \"saint catharines college\",\n",
      "    \"abbey pool\",\n",
      "    \"museum of classical archaeology\",\n",
      "    \"hughes hall\",\n",
      "    \"whale of a time\",\n",
      "    \"kambar\",\n",
      "    \"vue cinema\",\n",
      "    \"the place\",\n",
      "    \"queens college\",\n",
      "    \"scudamore\",\n",
      "    \"ABC Theatre\",\n",
      "    \"museum of archaelogy and anthropology|fitzwilliam museum\",\n",
      "    \"st christs college\",\n",
      "    \"corn cambridge exchange\",\n",
      "    \"holy trinity church\",\n",
      "    \"the fez club\",\n",
      "    \"christ college\",\n",
      "    \"funky fun house\",\n",
      "    \"lynne strover gallery\",\n",
      "    \"city\",\n",
      "    \"cherry hinton hall\",\n",
      "    \"scudamores punting co|cambridge punters\",\n",
      "    \"fitzwilliam museum\",\n",
      "    \"hobsons house\",\n",
      "    \"club salsa\",\n",
      "    \"tenpin\",\n",
      "    \"worth house\",\n",
      "    \"jesus college\",\n",
      "    \"kings hedges learner pool\",\n",
      "    \"cambridge botanic gardens\",\n",
      "    \"camboats|funky fun house\",\n",
      "    \"emmanuel college\",\n",
      "    \"saint barnabas press gallery\",\n",
      "    \"nusha\",\n",
      "    \"clare hall\",\n",
      "    \"soultree\",\n",
      "    \"boat\",\n",
      "    \"cherry hinton water play park\",\n",
      "    \"county folk museum\",\n",
      "    \"botanic gardens\",\n",
      "    \"sidney sussex college\",\n",
      "    \"the man on the moon\",\n",
      "    \"whale of time\",\n",
      "    \"adc\",\n",
      "    \"man on the moon\",\n",
      "    \"primavera\",\n",
      "    \"jesus green\",\n",
      "    \"cambridge punter\",\n",
      "    \"cherry hinton hall and grounds\",\n",
      "    \"the cambridge artworks\",\n",
      "    \"whippple museum\",\n",
      "    \"free\",\n",
      "    \"cherry hinton water park\",\n",
      "    \"broughton house gallery\",\n",
      "    \"williams art and antiques\",\n",
      "    \"cambridge book and print gallery\",\n",
      "    \"cambridge and county folk museum\",\n",
      "    \"all saints church\",\n",
      "    \"dontcare\",\n",
      "    \"cambridge temporary art\",\n",
      "    \"parkside pools\",\n",
      "    \"nusha|tenpin\",\n",
      "    \"gonville hotel\",\n",
      "    \"pizza\",\n",
      "    \"cambridge university botanic gardens\",\n",
      "    \"castle galleries\",\n",
      "    \"scott polar museum\",\n",
      "    \"jesus green outdoor pool\",\n",
      "    \"cinema cinema\",\n",
      "    \"kettles yard\",\n",
      "    \"clare college\",\n",
      "    \"place\",\n",
      "    \"cambridge\",\n",
      "    \"peoples portraits exhibition\",\n",
      "    \"churchhill college\",\n",
      "    \"byard art\",\n",
      "    \"Cambridge university botanic gardens\",\n",
      "    \"bed\",\n",
      "    \"saint johns college\",\n",
      "    \"bangkok city\",\n",
      "    \"gallery\",\n",
      "    \"tenpin|nusha\",\n",
      "    \"trinity street college\",\n",
      "    \"queens\",\n",
      "    \"museum of archaelogy and anthropogy\",\n",
      "    \"kings college\",\n",
      "    \"adc theatre\",\n",
      "    \"magdalene college\",\n",
      "    \"cambridge contemporary art\",\n",
      "    \"contemporary art museum\",\n",
      "    \"caf\\u008e jello galery\",\n",
      "    \"gonville and caius college\",\n",
      "    \"the cambridge arts theatre\",\n",
      "    \"the castle galleries\",\n",
      "    \"thanh\",\n",
      "    \"corpus christi\",\n",
      "    \"cambridge museum of technology\",\n",
      "    \"the churchill college\",\n",
      "    \"kohinoor\",\n",
      "    \"downing college\",\n",
      "    \"cafe jello gallery\",\n",
      "    \"older churches\",\n",
      "    \"school\",\n",
      "    \"ballare\",\n",
      "    \"cambride and country folk museum\",\n",
      "    \"gallery at twelve a high street\"\n",
      "  ],\n",
      "  \"attraction-type\": [\n",
      "    \"sports\",\n",
      "    \"entertainment|cinemas|museums|theatres\",\n",
      "    \"church\",\n",
      "    \"museum\",\n",
      "    \"theatre\",\n",
      "    \"boat\",\n",
      "    \"architecture\",\n",
      "    \"churchills college\",\n",
      "    \"multiple sports|theatre\",\n",
      "    \"park|boat\",\n",
      "    \"theater\",\n",
      "    \"cinema\",\n",
      "    \"camboats\",\n",
      "    \"concert\",\n",
      "    \"night club\",\n",
      "    \"park\",\n",
      "    \"concerthall\",\n",
      "    \"boating\",\n",
      "    \"hiking|historical\",\n",
      "    \"college\",\n",
      "    \"hotel\",\n",
      "    \"gallery\",\n",
      "    \"entertainment\",\n",
      "    \"dontcare\",\n",
      "    \"museum kettles yard\",\n",
      "    \"concerthall|boat\",\n",
      "    \"museum|nightclub\",\n",
      "    \"special\",\n",
      "    \"swimming pool\",\n",
      "    \"gastropub\",\n",
      "    \"outdoor\",\n",
      "    \"pool\",\n",
      "    \"multiple sports\"\n",
      "  ]\n",
      "}\n",
      "  Turn 0 (User):\n",
      "    Text: Hello , can you recommend any theatres in the Centre of town , please ?\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: I really love The Cambridge Corn Exchange located on Wheeler Street .   I do n't have admission information , but you can call them at 01223357851 .\n",
      "    Dialogue Acts: Attraction-Inform(['Phone', '01223357851'], ['Fee', \"do n't have admission information\"], ['Name', 'Cambridge Corn Exchange'], ['Addr', 'Wheeler Street'])\n",
      "  Turn 2 (User):\n",
      "    Text: You do n't have what it costs to get in ? I need the entrance fee if you would please\n",
      "    Dialogue Acts: Attraction-Inform(['Fee', 'do not have the entrance fee'], ['Phone', '01223357851'])\n",
      "  Turn 3 (System):\n",
      "    Text: I 'm sorry we do not have the entrance fee , only the phone number which is 01223357851 .\n",
      "    Dialogue Acts: Hotel-Request(['Price', '?'], ['Area', '?']); Hotel-Inform(['Choice', '29'])\n",
      "\n",
      " Randomly Checking 2 Dialogues (First 4 Turns) from Test \n",
      "\n",
      "Dialogue ID: PMUL4431.json\n",
      "Domain: restaurant\n",
      "First 4 turns:\n",
      "    Ontology for domain 'restaurant': {\n",
      "  \"restaurant-book day\": [\n",
      "    \"sunday|thursday\",\n",
      "    \"wednesday\",\n",
      "    \"saturday\",\n",
      "    \"sunday\",\n",
      "    \"saturday|thursday\",\n",
      "    \"friday\",\n",
      "    \"dontcare\",\n",
      "    \"monday\",\n",
      "    \"thursday\",\n",
      "    \"tuesday\"\n",
      "  ],\n",
      "  \"restaurant-book people\": [\n",
      "    \"2\",\n",
      "    \"7\",\n",
      "    \"8\",\n",
      "    \"5\",\n",
      "    \"1\",\n",
      "    \"6\",\n",
      "    \"4|7\",\n",
      "    \"3\",\n",
      "    \"4\"\n",
      "  ],\n",
      "  \"restaurant-book time\": [\n",
      "    \"12:00\",\n",
      "    \"13:30\",\n",
      "    \"10:00\",\n",
      "    \"21:00\",\n",
      "    \"7pm\",\n",
      "    \"1345\",\n",
      "    \"19:30\",\n",
      "    \"21:45\",\n",
      "    \"15:00\",\n",
      "    \"01:00\",\n",
      "    \"16:30\",\n",
      "    \"14:15\",\n",
      "    \"03:00\",\n",
      "    \"10:45\",\n",
      "    \"18:30\",\n",
      "    \"15:15\",\n",
      "    \"17:00|16:00\",\n",
      "    \"4pm\",\n",
      "    \"15:30|16:30\",\n",
      "    \"9\",\n",
      "    \"14:00\",\n",
      "    \"17:45\",\n",
      "    \"09:45\",\n",
      "    \"1715\",\n",
      "    \"16:45\",\n",
      "    \"12:45\",\n",
      "    \"1330\",\n",
      "    \"20:45\",\n",
      "    \"11:00\",\n",
      "    \"12:15\",\n",
      "    \"10:30\",\n",
      "    \"16:15\",\n",
      "    \"15:45\",\n",
      "    \"11:30\",\n",
      "    \"20:00\",\n",
      "    \"19:45\",\n",
      "    \"10:48\",\n",
      "    \"13:45\",\n",
      "    \"18:00|12:15\",\n",
      "    \"18:15\",\n",
      "    \"06:30\",\n",
      "    \"20:30\",\n",
      "    \"11:30|12:30\",\n",
      "    \"17:30\",\n",
      "    \"12:30\",\n",
      "    \"09:15\",\n",
      "    \"14:45\",\n",
      "    \"dontcare\",\n",
      "    \"13:00\",\n",
      "    \"14:30\",\n",
      "    \"20:15\",\n",
      "    \"10:15\",\n",
      "    \"09:30\",\n",
      "    \"18:00\",\n",
      "    \"14:40\",\n",
      "    \"09:00\",\n",
      "    \"17:00\",\n",
      "    \"16:00\",\n",
      "    \"17:30|16:30\",\n",
      "    \"13:15\",\n",
      "    \"1430\",\n",
      "    \"1145\",\n",
      "    \"8pm\",\n",
      "    \"08:45\",\n",
      "    \"11:15\",\n",
      "    \"22:00\",\n",
      "    \"18:45\",\n",
      "    \"19:15\",\n",
      "    \"17:15\",\n",
      "    \"15:30\",\n",
      "    \"19:00\",\n",
      "    \"11:45\"\n",
      "  ],\n",
      "  \"restaurant-area\": [\n",
      "    \"north\",\n",
      "    \"east\",\n",
      "    \"dontcare\",\n",
      "    \"south\",\n",
      "    \"centre\",\n",
      "    \"west\",\n",
      "    \"east|south\"\n",
      "  ],\n",
      "  \"restaurant-food\": [\n",
      "    \"north american>indian\",\n",
      "    \"portugese\",\n",
      "    \"polynesian\",\n",
      "    \"austrian\",\n",
      "    \"greek\",\n",
      "    \"asian\",\n",
      "    \"modern global\",\n",
      "    \"korean\",\n",
      "    \"danish\",\n",
      "    \"chinese\",\n",
      "    \"bistro\",\n",
      "    \"afternoon tea\",\n",
      "    \"kosher|british\",\n",
      "    \"tuscan\",\n",
      "    \"indonesian\",\n",
      "    \"spanish|portuguese\",\n",
      "    \"north african\",\n",
      "    \"north indian\",\n",
      "    \"the americas\",\n",
      "    \"italian\",\n",
      "    \"romanian\",\n",
      "    \"welsh\",\n",
      "    \"french\",\n",
      "    \"modern american\",\n",
      "    \"new zealand\",\n",
      "    \"sri lankan\",\n",
      "    \"traditional american\",\n",
      "    \"thai\",\n",
      "    \"fusion\",\n",
      "    \"molecular gastronomy\",\n",
      "    \"brazilian|portuguese\",\n",
      "    \"south african\",\n",
      "    \"european\",\n",
      "    \"kosher\",\n",
      "    \"jamaican>chinese\",\n",
      "    \"modern english\",\n",
      "    \"italian|indian\",\n",
      "    \"mexican\",\n",
      "    \"turkish\",\n",
      "    \"australian|indian\",\n",
      "    \"halal\",\n",
      "    \"german\",\n",
      "    \"dojo noodle bar\",\n",
      "    \"thai and chinese\",\n",
      "    \"international\",\n",
      "    \"australian\",\n",
      "    \"indian|african\",\n",
      "    \"chinese|mexican\",\n",
      "    \"american\",\n",
      "    \"north american\",\n",
      "    \"eastern european\",\n",
      "    \"caribbean>indian\",\n",
      "    \"afghan\",\n",
      "    \"seafood\",\n",
      "    \"moroccan\",\n",
      "    \"russian\",\n",
      "    \"latin american\",\n",
      "    \"creative\",\n",
      "    \"eritrean\",\n",
      "    \"unusual\",\n",
      "    \"caribbean\",\n",
      "    \"spanish\",\n",
      "    \"sushi\",\n",
      "    \"vegetarian\",\n",
      "    \"corsica\",\n",
      "    \"barbeque>modern european\",\n",
      "    \"light bites\",\n",
      "    \"world\",\n",
      "    \"barbeque\",\n",
      "    \"vietnamese\",\n",
      "    \"dontcare\",\n",
      "    \"african\",\n",
      "    \"south indian\",\n",
      "    \"panasian\",\n",
      "    \"persian\",\n",
      "    \"modern eclectic\",\n",
      "    \"gastropub\",\n",
      "    \"japanese\",\n",
      "    \"cuban\",\n",
      "    \"middle eastern\",\n",
      "    \"canapes\",\n",
      "    \"lebanese\",\n",
      "    \"jamaican\",\n",
      "    \"northern european\",\n",
      "    \"basque\",\n",
      "    \"scottish\",\n",
      "    \"christmas\",\n",
      "    \"irish\",\n",
      "    \"indian\",\n",
      "    \"belgian\",\n",
      "    \"british\",\n",
      "    \"hungarian\",\n",
      "    \"catalan\",\n",
      "    \"english\",\n",
      "    \"crossover\",\n",
      "    \"mediterranean\",\n",
      "    \"malaysian\",\n",
      "    \"swiss\",\n",
      "    \"swedish\",\n",
      "    \"polish\",\n",
      "    \"brazilian\",\n",
      "    \"traditional\",\n",
      "    \"steakhouse\",\n",
      "    \"singaporean\",\n",
      "    \"asian oriental\",\n",
      "    \"venetian\",\n",
      "    \"scandinavian\",\n",
      "    \"cantonese\",\n",
      "    \"modern european\"\n",
      "  ],\n",
      "  \"restaurant-name\": [\n",
      "    \"copper kettle\",\n",
      "    \"taj tandoori\",\n",
      "    \"charlie chan\",\n",
      "    \"one seven\",\n",
      "    \"dif\",\n",
      "    \"eraina and michaelhouse cafe\",\n",
      "    \"mahal of cambridge\",\n",
      "    \"restaurant two two\",\n",
      "    \"curry prince|rajmahal\",\n",
      "    \"golden curry\",\n",
      "    \"missing sock\",\n",
      "    \"frankie and bennys\",\n",
      "    \"kymmoy\",\n",
      "    \"cam\",\n",
      "    \"dojo noodle bar|j restaurant\",\n",
      "    \"the bedouin\",\n",
      "    \"restaurant alimentum\",\n",
      "    \"gourmet burger kitchen\",\n",
      "    \"dojo noodle  bar|j restaurant\",\n",
      "    \"la margherita\",\n",
      "    \"golden house\",\n",
      "    \"chiquito\",\n",
      "    \"darrys cookhouse and wine shop\",\n",
      "    \"scudamores punt\",\n",
      "    \"eraina\",\n",
      "    \"the varsity restaurant\",\n",
      "    \"pizza hut fenditton\",\n",
      "    \"saint johns chop house\",\n",
      "    \"curry garden\",\n",
      "    \"sala thong|bangkok city\",\n",
      "    \"jinling noodle bar\",\n",
      "    \"sitar tandoori\",\n",
      "    \"pizza hut\",\n",
      "    \"the Nirala\",\n",
      "    \"city stop restaurant\",\n",
      "    \"de luca cucina and bar riverside brasserie\",\n",
      "    \"rice boat\",\n",
      "    \"wise buddha\",\n",
      "    \"restaurant one seven\",\n",
      "    \"meze bar restaurant\",\n",
      "    \"yipee noodle bar\",\n",
      "    \"little seoul\",\n",
      "    \"rajmahal\",\n",
      "    \"ali baba\",\n",
      "    \"limehouse\",\n",
      "    \"the grafton hotel\",\n",
      "    \"barbakan\",\n",
      "    \"sesame restaurant and bar\",\n",
      "    \"golden wok\",\n",
      "    \"pizza hut city centre\",\n",
      "    \"binh\",\n",
      "    \"hotel du vin and bistro\",\n",
      "    \"lan hong house\",\n",
      "    \"tandoori Palace\",\n",
      "    \"alimentum\",\n",
      "    \"the oak bistro\",\n",
      "    \"anatolia\",\n",
      "    \"ian hong house\",\n",
      "    \"two two and cote\",\n",
      "    \"curry queen\",\n",
      "    \"backstreet bistro\",\n",
      "    \"mahal\",\n",
      "    \"la mimosa\",\n",
      "    \"shanghai family restaurant\",\n",
      "    \"molecular gastronomy\",\n",
      "    \"efes\",\n",
      "    \"cote\",\n",
      "    \"good luck\",\n",
      "    \"cafe uno\",\n",
      "    \"oak bistro\",\n",
      "    \"european\",\n",
      "    \"saffron brasserie\",\n",
      "    \"gardenia\",\n",
      "    \"de luca cucina and bar\",\n",
      "    \"two two\",\n",
      "    \"ashley hotel\",\n",
      "    \"the hotpot\",\n",
      "    \"michaelhouse cafe\",\n",
      "    \"yu garden\",\n",
      "    \"gourmet formal kitchen\",\n",
      "    \"lovel\",\n",
      "    \"efes restaurant\",\n",
      "    \"the peking restaurant: \",\n",
      "    \"river bar steakhouse and grill\",\n",
      "    \"j restaurant\",\n",
      "    \"the slug and lettuce\",\n",
      "    \"yippee noodle bar\",\n",
      "    \"meze bar\",\n",
      "    \"rice house\",\n",
      "    \"clowns cafe\",\n",
      "    \"no\",\n",
      "    \"hobsons house\",\n",
      "    \"dojo noodle bar\",\n",
      "    \"the kohinoor\",\n",
      "    \"royal standard\",\n",
      "    \"bloomsbury restaurant\",\n",
      "    \"graffiti\",\n",
      "    \"el shaddia guesthouse\",\n",
      "    \"worth house\",\n",
      "    \"shiraz\",\n",
      "    \"the golden house\",\n",
      "    \"shanghai\",\n",
      "    \"don pasquale pizzeria\",\n",
      "    \"tandoori palace\",\n",
      "    \"autumn house\",\n",
      "    \"panahar\",\n",
      "    \"the peking\",\n",
      "    \"wagamama\",\n",
      "    \"tandoori\",\n",
      "    \"nusha\",\n",
      "    \"the gandhi\",\n",
      "    \"nandos city centre\",\n",
      "    \"slug and lettuce\",\n",
      "    \"sitar\",\n",
      "    \"alex\",\n",
      "    \"meghna\",\n",
      "    \"cambridge chop house\",\n",
      "    \"the missing sock\",\n",
      "    \"primavera\",\n",
      "    \"the meze bar\",\n",
      "    \"travellers rest\",\n",
      "    \"south\",\n",
      "    \"curry king\",\n",
      "    \"pipasha restaurant\",\n",
      "    \"cambridge punter\",\n",
      "    \"saigon city\",\n",
      "    \"bedouin\",\n",
      "    \"pizza express\",\n",
      "    \"ask\",\n",
      "    \"broughton house gallery\",\n",
      "    \"tang chinese\",\n",
      "    \"curry prince\",\n",
      "    \"dontcare\",\n",
      "    \"charlie\",\n",
      "    \"nirala\",\n",
      "    \"nandos\",\n",
      "    \"cotto\",\n",
      "    \"parkside pools\",\n",
      "    \"galleria\",\n",
      "    \"adden\",\n",
      "    \"funky\",\n",
      "    \"maharajah tandoori restaurant\",\n",
      "    \"nil\",\n",
      "    \"the cow pizza kitchen and bar\",\n",
      "    \"bridge\",\n",
      "    \"grafton hotel\",\n",
      "    \"india west\",\n",
      "    \"cow pizza kitchen and bar\",\n",
      "    \"midsummer house restaurant\",\n",
      "    \"cambridge be\",\n",
      "    \"the lucky star\",\n",
      "    \"hakka\",\n",
      "    \"prezzo\",\n",
      "    \"cambridge lodge restaurant\",\n",
      "    \"sala thong\",\n",
      "    \"Kohinoor\",\n",
      "    \"zizzi cambridge\",\n",
      "    \"pizza hut cherry hinton\",\n",
      "    \"4 kings parade city centre\",\n",
      "    \"golden wok|nirala\",\n",
      "    \"nus\",\n",
      "    \"fitzbillies restaurant\",\n",
      "    \"lucky star\",\n",
      "    \"la tasca\",\n",
      "    \"loch fyne\",\n",
      "    \"cityr\",\n",
      "    \"the gardenia\",\n",
      "    \"bangkok city\",\n",
      "    \"chiquito restaurant bar\",\n",
      "    \"anatolia and efes restaurant\",\n",
      "    \"ugly duckling\",\n",
      "    \"cocum\",\n",
      "    \"hk fusion\",\n",
      "    \"stazione restaurant and coffee bar\",\n",
      "    \"restaurant 22\",\n",
      "    \"grafton hotel restaurant\",\n",
      "    \"the maharajah tandoor\",\n",
      "    \"the alex\",\n",
      "    \"thanh binh\",\n",
      "    \"the river bar steakhouse and grill\",\n",
      "    \"india house\",\n",
      "    \"peking restaurant\",\n",
      "    \"hotpot\",\n",
      "    \"kohinoor\",\n",
      "    \"la raza\",\n",
      "    \"da vinci pizzeria\",\n",
      "    \"pizza hut cherry hinton|alimentum\",\n",
      "    \"royal spice\",\n",
      "    \"riverside brasserie\",\n",
      "    \"kitchen and bar\"\n",
      "  ],\n",
      "  \"restaurant-pricerange\": [\n",
      "    \"expensive\",\n",
      "    \"cheap\",\n",
      "    \"moderate\",\n",
      "    \"dontcare\",\n",
      "    \"moderate|cheap\"\n",
      "  ]\n",
      "}\n",
      "  Turn 0 (User):\n",
      "    Text: I am   traveling to Cambridge and looking forward to try local restaurants .\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: We have lots to explore !   I can help you find one , if you 'd like .\n",
      "    Dialogue Acts: Restaurant-Inform(['Choice', 'lots'])\n",
      "  Turn 2 (User):\n",
      "    Text: Great ! The restaurant should serve scandinavian food and should be in the east .\n",
      "    Dialogue Acts: Restaurant-Request(['Food', '?']); Restaurant-NoOffer(['Food', 'Scandinavian'])\n",
      "  Turn 3 (System):\n",
      "    Text: I 'm sorry there are no restaurants serving Scandinavian food . Is there another type of food you might like to try ?\n",
      "    Dialogue Acts: Restaurant-Inform(['Choice', '4'], ['Price', 'moderately priced'], ['Price', 'expensive'], ['Food', 'indian'])\n",
      "\n",
      "Dialogue ID: PMUL1174.json\n",
      "Domain: train\n",
      "First 4 turns:\n",
      "    Ontology for domain 'train': {\n",
      "  \"train-book people\": [\n",
      "    \"2\",\n",
      "    \"8\",\n",
      "    \"7\",\n",
      "    \"15\",\n",
      "    \"5\",\n",
      "    \"1\",\n",
      "    \"dontcare\",\n",
      "    \"6\",\n",
      "    \"3\",\n",
      "    \"9\",\n",
      "    \"4\",\n",
      "    \"10\"\n",
      "  ],\n",
      "  \"train-arriveBy\": [\n",
      "    \"19:57\",\n",
      "    \"12:00\",\n",
      "    \"19:30\",\n",
      "    \"11:54\",\n",
      "    \"05:30\",\n",
      "    \"16:30\",\n",
      "    \"18:30\",\n",
      "    \"06:45\",\n",
      "    \"19:58\",\n",
      "    \"23:27\",\n",
      "    \"16:06\",\n",
      "    \"07:35\",\n",
      "    \"08:30\",\n",
      "    \"12:07\",\n",
      "    \"10:30\",\n",
      "    \"12:06\",\n",
      "    \"15:45\",\n",
      "    \"18:35\",\n",
      "    \"18:07\",\n",
      "    \"10:32\",\n",
      "    \"06:43\",\n",
      "    \"8\",\n",
      "    \"20:38\",\n",
      "    \"18:23\",\n",
      "    \"20:06\",\n",
      "    \"09:01\",\n",
      "    \"20:08\",\n",
      "    \"19:54\",\n",
      "    \"10:15\",\n",
      "    \"09:30\",\n",
      "    \"11:52\",\n",
      "    \"18:00\",\n",
      "    \"07:30\",\n",
      "    \"08:07\",\n",
      "    \"16:00\",\n",
      "    \"08:56\",\n",
      "    \"17:23\",\n",
      "    \"14:07\",\n",
      "    \"10:08\",\n",
      "    \"11:00\",\n",
      "    \"08:44\",\n",
      "    \"21:45\",\n",
      "    \"15:00\",\n",
      "    \"10:54\",\n",
      "    \"17:51\",\n",
      "    \"03:00\",\n",
      "    \"10:45\",\n",
      "    \"16:08\",\n",
      "    \"13:06\",\n",
      "    \"12:45\",\n",
      "    \"20:45\",\n",
      "    \"22:01\",\n",
      "    \"12:15\",\n",
      "    \"18:03\",\n",
      "    \"23:00\",\n",
      "    \"11:30\",\n",
      "    \"11:06\",\n",
      "    \"20:00\",\n",
      "    \"07:15\",\n",
      "    \"15:06\",\n",
      "    \"19:45\",\n",
      "    \"19:32\",\n",
      "    \"07:08\",\n",
      "    \"14:45\",\n",
      "    \"15:54\",\n",
      "    \"21:20\",\n",
      "    \"20:15\",\n",
      "    \"17:00\",\n",
      "    \"02:30\",\n",
      "    \"21:06\",\n",
      "    \"14:24\",\n",
      "    \"15:01\",\n",
      "    \"11:15\",\n",
      "    \"19:15\",\n",
      "    \"11:32\",\n",
      "    \"07:44\",\n",
      "    \"22:06\",\n",
      "    \"13:30\",\n",
      "    \"10:00\",\n",
      "    \"21:00\",\n",
      "    \"13:17\",\n",
      "    \"06:55\",\n",
      "    \"05:52\",\n",
      "    \"12:43\",\n",
      "    \"15:15\",\n",
      "    \"17:45\",\n",
      "    \"10:23\",\n",
      "    \"09:45\",\n",
      "    \"06:01\",\n",
      "    \"15:07\",\n",
      "    \"10:07\",\n",
      "    \"13:52\",\n",
      "    \"17:58\",\n",
      "    \"16:15\",\n",
      "    \"21:30\",\n",
      "    \"13:51\",\n",
      "    \"15:24\",\n",
      "    \"02:00\",\n",
      "    \"19:27\",\n",
      "    \"09:06\",\n",
      "    \"13:38\",\n",
      "    \"08:00\",\n",
      "    \"09:32\",\n",
      "    \"dontcare\",\n",
      "    \"13:00\",\n",
      "    \"14:43\",\n",
      "    \"20:20\",\n",
      "    \"06:07\",\n",
      "    \"09:00\",\n",
      "    \"10:43\",\n",
      "    \"14:01\",\n",
      "    \"08:45\",\n",
      "    \"16:32\",\n",
      "    \"22:00\",\n",
      "    \"17:15\",\n",
      "    \"20:07\",\n",
      "    \"15:30\",\n",
      "    \"08:15\",\n",
      "    \"18:10\",\n",
      "    \"12:30\",\n",
      "    \"11:45\",\n",
      "    \"06:15\",\n",
      "    \"1100\",\n",
      "    \"18:32\",\n",
      "    \"13:03\",\n",
      "    \"08:54\",\n",
      "    \"21:15\",\n",
      "    \"13:32\",\n",
      "    \"16:23\",\n",
      "    \"14:15\",\n",
      "    \"23:30\",\n",
      "    \"14:00\",\n",
      "    \"16:45\",\n",
      "    \"20:54\",\n",
      "    \"12:08\",\n",
      "    \"07:00\",\n",
      "    \"16:58\",\n",
      "    \"07:52\",\n",
      "    \"21:08\",\n",
      "    \"19:51\",\n",
      "    \"13:45\",\n",
      "    \"18:15\",\n",
      "    \"20:30\",\n",
      "    \"06:30\",\n",
      "    \"19:52\",\n",
      "    \"16:07\",\n",
      "    \"17:30\",\n",
      "    \"1545\",\n",
      "    \"09:15\",\n",
      "    \"21:51\",\n",
      "    \"14:30\",\n",
      "    \"22:07\",\n",
      "    \"13:15\",\n",
      "    \"16:44\",\n",
      "    \"08:23\",\n",
      "    \"18:45\",\n",
      "    \"19:00\"\n",
      "  ],\n",
      "  \"train-day\": [\n",
      "    \"saturday\",\n",
      "    \"wednesday\",\n",
      "    \"sunday\",\n",
      "    \"friday\",\n",
      "    \"dontcare\",\n",
      "    \"monday\",\n",
      "    \"thursday\",\n",
      "    \"tuesday\"\n",
      "  ],\n",
      "  \"train-departure\": [\n",
      "    \"huntingdon\",\n",
      "    \"panahar\",\n",
      "    \"london liverpool street\",\n",
      "    \"leicester\",\n",
      "    \"brookshite\",\n",
      "    \"aylesbray lodge guest\",\n",
      "    \"broxbourne\",\n",
      "    \"london kings cross\",\n",
      "    \"cambridge\",\n",
      "    \"birmingham new street\",\n",
      "    \"camboats\",\n",
      "    \"peterborough\",\n",
      "    \"cineworld\",\n",
      "    \"alpha-milton\",\n",
      "    \"cafe uno\",\n",
      "    \"city hall\",\n",
      "    \"stansted airport\",\n",
      "    \"liverpool\",\n",
      "    \"hamilton lodge\",\n",
      "    \"dontcare\",\n",
      "    \"duxford\",\n",
      "    \"wandlebury country park\",\n",
      "    \"stevenage\",\n",
      "    \"ely\",\n",
      "    \"norwich\",\n",
      "    \"london\",\n",
      "    \"east london\",\n",
      "    \"london liverpool\",\n",
      "    \"bishops stortford\",\n",
      "    \"kings lynn\",\n",
      "    \"stratford\"\n",
      "  ],\n",
      "  \"train-destination\": [\n",
      "    \"copper kettle\",\n",
      "    \"city centre north\",\n",
      "    \"london liverpool street\",\n",
      "    \"leicester\",\n",
      "    \"norway\",\n",
      "    \"centre\",\n",
      "    \"broxbourne\",\n",
      "    \"gourmet burger kitchen\",\n",
      "    \"london kings cross\",\n",
      "    \"cambridge\",\n",
      "    \"birmingham new street\",\n",
      "    \"glastonbury\",\n",
      "    \"peterborough\",\n",
      "    \"huntingdon marriott hotel\",\n",
      "    \"stansted airport\",\n",
      "    \"liverpool\",\n",
      "    \"curry prince\",\n",
      "    \"dontcare\",\n",
      "    \"stevenage\",\n",
      "    \"ely\",\n",
      "    \"norwich\",\n",
      "    \"liverpool street\",\n",
      "    \"london\",\n",
      "    \"bournemouth\",\n",
      "    \"huntington marriott\",\n",
      "    \"bishops stortford\",\n",
      "    \"kings lynn\"\n",
      "  ],\n",
      "  \"train-leaveAt\": [\n",
      "    \"12:00\",\n",
      "    \"13:40\",\n",
      "    \"19:30\",\n",
      "    \"11:54\",\n",
      "    \"05:30\",\n",
      "    \"16:30\",\n",
      "    \"09:39\",\n",
      "    \"morning\",\n",
      "    \"15:39\",\n",
      "    \"18:30\",\n",
      "    \"06:45\",\n",
      "    \"20:19\",\n",
      "    \"07:35\",\n",
      "    \"08:11\",\n",
      "    \"08:30\",\n",
      "    \"10:19\",\n",
      "    \"08:16\",\n",
      "    \"10:30\",\n",
      "    \"15:45\",\n",
      "    \"11:48\",\n",
      "    \"06:09\",\n",
      "    \"10:32\",\n",
      "    \"07:54\",\n",
      "    \"19:50\",\n",
      "    \"11:29\",\n",
      "    \"19:54\",\n",
      "    \"10:15\",\n",
      "    \"09:30\",\n",
      "    \"13:54\",\n",
      "    \"18:00\",\n",
      "    \"06:40\",\n",
      "    \"08:10\",\n",
      "    \"16:00\",\n",
      "    \"07:40\",\n",
      "    \"1145\",\n",
      "    \"07:21\",\n",
      "    \"05:29\",\n",
      "    \"13:29\",\n",
      "    \"11:01\",\n",
      "    \"05:15\",\n",
      "    \"11:11\",\n",
      "    \"11:00\",\n",
      "    \"13:11\",\n",
      "    \"14:09\",\n",
      "    \"21:45\",\n",
      "    \"15:00\",\n",
      "    \"19:35\",\n",
      "    \"05:17\",\n",
      "    \"10:45\",\n",
      "    \"05:24\",\n",
      "    \"12:45\",\n",
      "    \"05:54\",\n",
      "    \"20:45\",\n",
      "    \"22:01\",\n",
      "    \"07:45\",\n",
      "    \"12:15\",\n",
      "    \"15:11\",\n",
      "    \"15:50\",\n",
      "    \"12:48\",\n",
      "    \"08:52\",\n",
      "    \"09:19\",\n",
      "    \"11:30\",\n",
      "    \"20:00\",\n",
      "    \"07:15\",\n",
      "    \"19:45\",\n",
      "    \"07:27\",\n",
      "    \"09:11\",\n",
      "    \"afternoon\",\n",
      "    \"17:59\",\n",
      "    \"14:45\",\n",
      "    \"05:11\",\n",
      "    \"15:54\",\n",
      "    \"20:15\",\n",
      "    \"10\",\n",
      "    \"05:01\",\n",
      "    \"13:36\",\n",
      "    \"14:40\",\n",
      "    \"845\",\n",
      "    \"17:00\",\n",
      "    \"09:17\",\n",
      "    \"05:00\",\n",
      "    \"18:06\",\n",
      "    \"15:01\",\n",
      "    \"05:16\",\n",
      "    \"11:15\",\n",
      "    \"09:29\",\n",
      "    \"19:15\",\n",
      "    \"17:50\",\n",
      "    \"11:17\",\n",
      "    \"05:45\",\n",
      "    \"18:09\",\n",
      "    \"15:17\",\n",
      "    \"17:29\",\n",
      "    \"13:30\",\n",
      "    \"10:00\",\n",
      "    \"21:00\",\n",
      "    \"11:35\",\n",
      "    \"10:21\",\n",
      "    \"13:17\",\n",
      "    \"21:11\",\n",
      "    \"15:15\",\n",
      "    \"10:36\",\n",
      "    \"19:40\",\n",
      "    \"17:45\",\n",
      "    \"08:01\",\n",
      "    \"09:45\",\n",
      "    \"06:01\",\n",
      "    \"20:21\",\n",
      "    \"16:15\",\n",
      "    \"after lunch\",\n",
      "    \"21:30\",\n",
      "    \"12:26\",\n",
      "    \"06:00\",\n",
      "    \"05:36\",\n",
      "    \"1532\",\n",
      "    \"15:24\",\n",
      "    \"02:00\",\n",
      "    \"21:50\",\n",
      "    \"18:21\",\n",
      "    \"17:01\",\n",
      "    \"05:40\",\n",
      "    \"12:19\",\n",
      "    \"06:10\",\n",
      "    \"08:00\",\n",
      "    \"09:54\",\n",
      "    \"dontcare\",\n",
      "    \"13:00\",\n",
      "    \"5:45pm\",\n",
      "    \"13:24\",\n",
      "    \"17:16\",\n",
      "    \"09:00\",\n",
      "    \"09:23\",\n",
      "    \"14:01\",\n",
      "    \"1329\",\n",
      "    \"08:45\",\n",
      "    \"13:39\",\n",
      "    \"22:00\",\n",
      "    \"17:11\",\n",
      "    \"17:15\",\n",
      "    \"15:30\",\n",
      "    \"08:15\",\n",
      "    \"07:17\",\n",
      "    \"12:32\",\n",
      "    \"01:44\",\n",
      "    \"12:30\",\n",
      "    \"11:45\",\n",
      "    \"after 16:30\",\n",
      "    \"00:00\",\n",
      "    \"13:01\",\n",
      "    \"21:15\",\n",
      "    \"09:50\",\n",
      "    \"19:17\",\n",
      "    \"13:32\",\n",
      "    \"09:59\",\n",
      "    \"14:15\",\n",
      "    \"19:48\",\n",
      "    \"18:46\",\n",
      "    \"08:32\",\n",
      "    \"08:08\",\n",
      "    \"9\",\n",
      "    \"14:00\",\n",
      "    \"15:29\",\n",
      "    \"20:40\",\n",
      "    \"08:06\",\n",
      "    \"17:21\",\n",
      "    \"16:45\",\n",
      "    \"11:50\",\n",
      "    \"14:19\",\n",
      "    \"07:00\",\n",
      "    \"13:50\",\n",
      "    \"21:39\",\n",
      "    \"11:39\",\n",
      "    \"09:40\",\n",
      "    \"11:21\",\n",
      "    \"929\",\n",
      "    \"08:09\",\n",
      "    \"13:45\",\n",
      "    \"20:01\",\n",
      "    \"18:15\",\n",
      "    \"20:30\",\n",
      "    \"06:30\",\n",
      "    \"10:11\",\n",
      "    \"12:56\",\n",
      "    \"21:01\",\n",
      "    \"22:09\",\n",
      "    \"17:30\",\n",
      "    \"1545\",\n",
      "    \"09:15\",\n",
      "    \"21:29\",\n",
      "    \"14:30\",\n",
      "    \"19:16\",\n",
      "    \"07:01\",\n",
      "    \"13:15\",\n",
      "    \"19:11\",\n",
      "    \"05:50\",\n",
      "    \"18:40\",\n",
      "    \"05:35\",\n",
      "    \"14:21\",\n",
      "    \"18:45\",\n",
      "    \"15:16\",\n",
      "    \"18:36\",\n",
      "    \"19:00\",\n",
      "    \"20:36\"\n",
      "  ]\n",
      "}\n",
      "  Turn 0 (User):\n",
      "    Text: I 'm looking for a train that leaves Cambridge on Wednesday please .\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: Where are you traveling to , and at what time will you be traveling ?\n",
      "    Dialogue Acts: Train-Request(['Arrive', '?'], ['Dest', '?'])\n",
      "  Turn 2 (User):\n",
      "    Text: I 'm travelling to London Liverpool Street and I 'll be departing after 17:00 .\n",
      "    Dialogue Acts: Train-Inform(['Leave', '17:59'], ['Id', 'TR4757']); Train-OfferBook(['none', 'none'])\n",
      "  Turn 3 (System):\n",
      "    Text: TR4757 leaves at 17:59 .   Can I book seats for you ?\n",
      "    Dialogue Acts: Train-Inform(['Id', 'TR4757'], ['Arrive', '19:27']); general-reqmore(['none', 'none'])\n"
     ]
    }
   ],
   "source": [
    "def format_dialogue_acts(acts_dict):\n",
    "    \"\"\"Format dialogue acts for better display.\"\"\"\n",
    "    if not acts_dict:\n",
    "        return \"No acts available\"\n",
    "    formatted = []\n",
    "    for act_type, slots in acts_dict.items():\n",
    "        slot_strs = []\n",
    "        for slot in slots:\n",
    "            if ':' in slot:\n",
    "                key, value = slot.split(':', 1)\n",
    "                slot_strs.append(f\"{key}={value}\")\n",
    "            else:\n",
    "                slot_strs.append(str(slot))\n",
    "        formatted.append(f\"{act_type}({', '.join(slot_strs)})\")\n",
    "    return \"; \".join(formatted)\n",
    "\n",
    "def randomly_check_first_dialogue_turns_with_acts_and_ontology(split_name, split_data, num_samples=2, num_turns=4):\n",
    "    \"\"\"\n",
    "    Randomly checks a few dialogues and displays the dialogue text, acts, and ontology.\n",
    "    \"\"\"\n",
    "    print(f\"\\n Randomly Checking {num_samples} Dialogues (First {num_turns} Turns) from {split_name} \")\n",
    "    if not split_data:\n",
    "        print(f\"No data in {split_name} split.\")\n",
    "        return\n",
    "    \n",
    "    if not isinstance(split_data, (list, tuple)):\n",
    "        split_data = list(split_data)\n",
    "    \n",
    "    if len(split_data) == 0:\n",
    "        print(f\"No data in {split_name} split after conversion.\")\n",
    "        return\n",
    "        \n",
    "    random_dialogues = random.sample(split_data, min(num_samples, len(split_data)))\n",
    "    \n",
    "    for dialogue_item in random_dialogues:\n",
    "        dialogue_id = dialogue_item[\"dialogue_id\"]\n",
    "        raw_dialogue_object = dialogue_item[\"dialogue\"]\n",
    "        if isinstance(raw_dialogue_object, list):\n",
    "            turns = raw_dialogue_object\n",
    "        elif isinstance(raw_dialogue_object, dict):\n",
    "            turns = raw_dialogue_object.get(\"log\", [])\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected dialogue format for dialogue ID {dialogue_id}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nDialogue ID: {dialogue_id}\")\n",
    "        print(f\"Domain: {dialogue_item.get('domain', 'unknown')}\")\n",
    "        print(f\"First {min(num_turns, len(turns))} turns:\")\n",
    "        \n",
    "        dialogue_acts_for_dialogue = dialogue_item.get(\"dialogue_acts\", {})\n",
    "        domain = dialogue_item.get('domain', 'unknown')\n",
    "        \n",
    "        domain_ontology = {\n",
    "            slot: values\n",
    "            for slot, values in dialogue_item.get(\"ontology\", {}).items()\n",
    "            if slot.startswith(f\"{domain}-\")\n",
    "        }\n",
    "\n",
    "        print(f\"    Ontology for domain '{domain}': {json.dumps(domain_ontology, indent=2)}\")\n",
    "\n",
    "        for i in range(min(num_turns, len(turns))):\n",
    "            turn = turns[i]\n",
    "            # In the raw data, the 'text' key is at this level\n",
    "            text_content = turn.get('text', '')\n",
    "            # use the loop index for the turn id\n",
    "            turn_idx = i \n",
    "            turn_key = str(turn_idx)\n",
    "\n",
    "            turn_type = \"User\" if turn_idx % 2 == 0 else \"System\"\n",
    "            print(f\"  Turn {turn_idx} ({turn_type}):\")\n",
    "            \n",
    "            # Use the text content from the turn dictionary\n",
    "            print(f\"    Text: {text_content}\")\n",
    "            \n",
    "            turn_acts_data = dialogue_acts_for_dialogue.get(turn_key, {})\n",
    "            print(f\"    Dialogue Acts: {format_dialogue_acts(turn_acts_data)}\")\n",
    "\n",
    "\n",
    "randomly_check_first_dialogue_turns_with_acts_and_ontology(\"Train\", train_data_raw)\n",
    "randomly_check_first_dialogue_turns_with_acts_and_ontology(\"Test\", test_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Dialougue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Randomly Checking 2 Dialogues (First 2 Turns) from Train \n",
      "\n",
      "Dialogue ID: SNG0462.json\n",
      "Domain: restaurant\n",
      "First 2 turns:\n",
      "  Turn 0:\n",
      "    User: Hi ! Can you give me some information on the Royal Spice restaurant ?\n",
      "  Turn 1:\n",
      "    System: Of course ! It 's a cheap Indian restaurant in the north at Victoria Avenue Chesteron cb41eh . The phone number is 01733553355 . May I help with anything else ?\n",
      "\n",
      "Dialogue ID: MUL0244.json\n",
      "Domain: train\n",
      "First 2 turns:\n",
      "  Turn 0:\n",
      "    User: Yes , I would like to book a train that is leaving Monday , and is going to Cambridge .\n",
      "  Turn 1:\n",
      "    System: There are 202 trains to cambridge on monday . Can you tell me your departure station and the time you 'd like to travel ?\n",
      "\n",
      " Randomly Checking 2 Dialogues (First 2 Turns) from Test \n",
      "\n",
      "Dialogue ID: WOZ20573.json\n",
      "Domain: restaurant\n",
      "First 2 turns:\n",
      "  Turn 0:\n",
      "    User: Hello . Can you help me find the address of an inexpensive restaurant in the south part of town ?\n",
      "  Turn 1:\n",
      "    System: There are two restaurants that are in the cheap price range and in the south part of town . Would you like Portuguese or Chinese food ?\n",
      "\n",
      "Dialogue ID: PMUL4745.json\n",
      "Domain: attraction\n",
      "First 2 turns:\n",
      "  Turn 0:\n",
      "    User: Please give me some information on byard art\n",
      "  Turn 1:\n",
      "    System: It is a museum in south with free admission .   What else can I help you with ?\n"
     ]
    }
   ],
   "source": [
    "def randomly_check_first_dialogue_turns(split_name, split_data, num_samples=2, num_turns=2):\n",
    "    print(f\"\\n Randomly Checking {num_samples} Dialogues (First {num_turns} Turns) from {split_name} \")\n",
    "    if not split_data:\n",
    "        print(f\"No data in {split_name} split.\")\n",
    "        return\n",
    "    \n",
    "    if not isinstance(split_data, (list, tuple)):\n",
    "        split_data = list(split_data)\n",
    "    \n",
    "    if len(split_data) == 0:\n",
    "        print(f\"No data in {split_name} split after conversion.\")\n",
    "        return\n",
    "        \n",
    "    random_dialogues = random.sample(split_data, min(num_samples, len(split_data)))\n",
    "    \n",
    "    for dialogue in random_dialogues:\n",
    "        dialogue_id = dialogue[\"dialogue_id\"]\n",
    "        turns = dialogue[\"dialogue\"].get(\"log\", []) \n",
    "        \n",
    "        print(f\"\\nDialogue ID: {dialogue_id}\")\n",
    "        print(f\"Domain: {dialogue.get('domain', 'unknown')}\")\n",
    "        print(f\"First {min(num_turns, len(turns))} turns:\")\n",
    "        \n",
    "        for i in range(min(num_turns, len(turns))):\n",
    "            turn = turns[i]\n",
    "            print(f\"  Turn {turn['turn_id']}:\")\n",
    "            print(f\"    User: {turn['text']}\" if turn['turn_id'] % 2 == 0 else f\"    System: {turn['text']}\")\n",
    "\n",
    "\n",
    "randomly_check_first_dialogue_turns(\"Train\", train_data_raw)\n",
    "randomly_check_first_dialogue_turns(\"Test\", test_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Randomly Checking 2 Dialogues (First 4 Turns) from Train \n",
      "\n",
      "Dialogue ID: SNG1089.json\n",
      "Domain: attraction\n",
      "First 4 turns:\n",
      "    Ontology for domain 'attraction': {\n",
      "  \"attraction-area\": [\n",
      "    \"north\",\n",
      "    \"centre|west\",\n",
      "    \"east\",\n",
      "    \"south\",\n",
      "    \"centre\",\n",
      "    \"west\",\n",
      "    \"dontcare\"\n",
      "  ],\n",
      "  \"attraction-name\": [\n",
      "    \"cambridge arts theater\",\n",
      "    \"scudamores punting co\",\n",
      "    \"trinity college\",\n",
      "    \"history of science museum\",\n",
      "    \"old schools\",\n",
      "    \"christs college|saint catharines\",\n",
      "    \"aylesbray lodge guest house\",\n",
      "    \"scott polar\",\n",
      "    \"churchills college\",\n",
      "    \"museum of archaelogy and anthropology\",\n",
      "    \"cambridge artworks\",\n",
      "    \"pembroke college\",\n",
      "    \"milton country park\",\n",
      "    \"regency gallery\",\n",
      "    \"cambridge contemporary art museum\",\n",
      "    \"little saint marys church\",\n",
      "    \"sheeps green and lammas land park fen causeway\",\n",
      "    \"the junction\",\n",
      "    \"college\",\n",
      "    \"peoples portraits exhibition at girton college\",\n",
      "    \"st catharines college\",\n",
      "    \"kings college|hughes hall\",\n",
      "    \"cambridge and country folk museum\",\n",
      "    \"cineworld cinema\",\n",
      "    \"cambridge arts theatre\",\n",
      "    \"great saint marys church\",\n",
      "    \"university arms hotel\",\n",
      "    \"wandlebury country park\",\n",
      "    \"soul tree nightclub\",\n",
      "    \"churchill\",\n",
      "    \"archway house\",\n",
      "    \"salsa\",\n",
      "    \"home from home\",\n",
      "    \"sheeps green\",\n",
      "    \"scudamores punti co|the cambridge punter\",\n",
      "    \"sidney sussex|gonville and caius\",\n",
      "    \"christs church\",\n",
      "    \"riverboat georgina\",\n",
      "    \"cherry hinton village centre\",\n",
      "    \"mumford theatre\",\n",
      "    \"museum\",\n",
      "    \"castle galleries|primavera\",\n",
      "    \"ruskin gallery\",\n",
      "    \"christs college|downing college\",\n",
      "    \"cambridge corn exchange\",\n",
      "    \"lammas land park\",\n",
      "    \"museum of archaelogy\",\n",
      "    \"king hedges learner pool\",\n",
      "    \"whipple museum of the history of science\",\n",
      "    \"camboats\",\n",
      "    \"scudamores punting co|the cambridge punter\",\n",
      "    \"abbey pool and astroturf\",\n",
      "    \"the wandlebury\",\n",
      "    \"abbey pool and astroturf pitch\",\n",
      "    \"the cambridge corn exchange\",\n",
      "    \"cafe uno\",\n",
      "    \"cherry hinton water play\",\n",
      "    \"cafe jello museum\",\n",
      "    \"saint catharines college\",\n",
      "    \"abbey pool\",\n",
      "    \"museum of classical archaeology\",\n",
      "    \"hughes hall\",\n",
      "    \"whale of a time\",\n",
      "    \"kambar\",\n",
      "    \"vue cinema\",\n",
      "    \"the place\",\n",
      "    \"queens college\",\n",
      "    \"scudamore\",\n",
      "    \"ABC Theatre\",\n",
      "    \"museum of archaelogy and anthropology|fitzwilliam museum\",\n",
      "    \"st christs college\",\n",
      "    \"corn cambridge exchange\",\n",
      "    \"holy trinity church\",\n",
      "    \"the fez club\",\n",
      "    \"christ college\",\n",
      "    \"funky fun house\",\n",
      "    \"lynne strover gallery\",\n",
      "    \"city\",\n",
      "    \"cherry hinton hall\",\n",
      "    \"scudamores punting co|cambridge punters\",\n",
      "    \"fitzwilliam museum\",\n",
      "    \"hobsons house\",\n",
      "    \"club salsa\",\n",
      "    \"tenpin\",\n",
      "    \"worth house\",\n",
      "    \"jesus college\",\n",
      "    \"kings hedges learner pool\",\n",
      "    \"cambridge botanic gardens\",\n",
      "    \"camboats|funky fun house\",\n",
      "    \"emmanuel college\",\n",
      "    \"saint barnabas press gallery\",\n",
      "    \"nusha\",\n",
      "    \"clare hall\",\n",
      "    \"soultree\",\n",
      "    \"boat\",\n",
      "    \"cherry hinton water play park\",\n",
      "    \"county folk museum\",\n",
      "    \"botanic gardens\",\n",
      "    \"sidney sussex college\",\n",
      "    \"the man on the moon\",\n",
      "    \"whale of time\",\n",
      "    \"adc\",\n",
      "    \"man on the moon\",\n",
      "    \"primavera\",\n",
      "    \"jesus green\",\n",
      "    \"cambridge punter\",\n",
      "    \"cherry hinton hall and grounds\",\n",
      "    \"the cambridge artworks\",\n",
      "    \"whippple museum\",\n",
      "    \"free\",\n",
      "    \"cherry hinton water park\",\n",
      "    \"broughton house gallery\",\n",
      "    \"williams art and antiques\",\n",
      "    \"cambridge book and print gallery\",\n",
      "    \"cambridge and county folk museum\",\n",
      "    \"all saints church\",\n",
      "    \"dontcare\",\n",
      "    \"cambridge temporary art\",\n",
      "    \"parkside pools\",\n",
      "    \"nusha|tenpin\",\n",
      "    \"gonville hotel\",\n",
      "    \"pizza\",\n",
      "    \"cambridge university botanic gardens\",\n",
      "    \"castle galleries\",\n",
      "    \"scott polar museum\",\n",
      "    \"jesus green outdoor pool\",\n",
      "    \"cinema cinema\",\n",
      "    \"kettles yard\",\n",
      "    \"clare college\",\n",
      "    \"place\",\n",
      "    \"cambridge\",\n",
      "    \"peoples portraits exhibition\",\n",
      "    \"churchhill college\",\n",
      "    \"byard art\",\n",
      "    \"Cambridge university botanic gardens\",\n",
      "    \"bed\",\n",
      "    \"saint johns college\",\n",
      "    \"bangkok city\",\n",
      "    \"gallery\",\n",
      "    \"tenpin|nusha\",\n",
      "    \"trinity street college\",\n",
      "    \"queens\",\n",
      "    \"museum of archaelogy and anthropogy\",\n",
      "    \"kings college\",\n",
      "    \"adc theatre\",\n",
      "    \"magdalene college\",\n",
      "    \"cambridge contemporary art\",\n",
      "    \"contemporary art museum\",\n",
      "    \"caf\\u008e jello galery\",\n",
      "    \"gonville and caius college\",\n",
      "    \"the cambridge arts theatre\",\n",
      "    \"the castle galleries\",\n",
      "    \"thanh\",\n",
      "    \"corpus christi\",\n",
      "    \"cambridge museum of technology\",\n",
      "    \"the churchill college\",\n",
      "    \"kohinoor\",\n",
      "    \"downing college\",\n",
      "    \"cafe jello gallery\",\n",
      "    \"older churches\",\n",
      "    \"school\",\n",
      "    \"ballare\",\n",
      "    \"cambride and country folk museum\",\n",
      "    \"gallery at twelve a high street\"\n",
      "  ],\n",
      "  \"attraction-type\": [\n",
      "    \"sports\",\n",
      "    \"entertainment|cinemas|museums|theatres\",\n",
      "    \"church\",\n",
      "    \"museum\",\n",
      "    \"theatre\",\n",
      "    \"boat\",\n",
      "    \"architecture\",\n",
      "    \"churchills college\",\n",
      "    \"multiple sports|theatre\",\n",
      "    \"park|boat\",\n",
      "    \"theater\",\n",
      "    \"cinema\",\n",
      "    \"camboats\",\n",
      "    \"concert\",\n",
      "    \"night club\",\n",
      "    \"park\",\n",
      "    \"concerthall\",\n",
      "    \"boating\",\n",
      "    \"hiking|historical\",\n",
      "    \"college\",\n",
      "    \"hotel\",\n",
      "    \"gallery\",\n",
      "    \"entertainment\",\n",
      "    \"dontcare\",\n",
      "    \"museum kettles yard\",\n",
      "    \"concerthall|boat\",\n",
      "    \"museum|nightclub\",\n",
      "    \"special\",\n",
      "    \"swimming pool\",\n",
      "    \"gastropub\",\n",
      "    \"outdoor\",\n",
      "    \"pool\",\n",
      "    \"multiple sports\"\n",
      "  ]\n",
      "}\n",
      "  Turn 0 (User):\n",
      "    Text: I 'm looking for a park in the south part of town .\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: I have 2 parks to choose from . Both are free to get in . There is Sheep 's Green and Lammas Land Park and Wandlebury Country Park . Which do you prefer ?\n",
      "    Dialogue Acts: Attraction-Select(['none', 'none']); Attraction-Inform(['Fee', 'free'], ['Type', 'parks'], ['Name', \"There is Sheep 's Green and Lammas Land Park\"], ['Name', 'Wandlebury Country Park'], ['Choice', '2'])\n",
      "  Turn 2 (User):\n",
      "    Text: What 's the address for Wandlebury ? May I have their phone number , too ?\n",
      "    Dialogue Acts: general-reqmore(['none', 'none']); Attraction-Inform(['Name', 'Wandlebury Country Park'], ['Phone', '01223243830'], ['Addr', 'Wandlebury Ring , Gog Magog Hills'], ['Addr', 'Babraham'])\n",
      "  Turn 3 (System):\n",
      "    Text: Wandlebury Country Park is located on the Wandlebury Ring , Gog Magog Hills , in Babraham .   Their phone number is 01223243830 .   Is there anything else I can assist you with today ?\n",
      "    Dialogue Acts: general-bye(['none', 'none'])\n",
      "\n",
      "Dialogue ID: PMUL2215.json\n",
      "Domain: attraction\n",
      "First 4 turns:\n",
      "    Ontology for domain 'attraction': {\n",
      "  \"attraction-area\": [\n",
      "    \"north\",\n",
      "    \"centre|west\",\n",
      "    \"east\",\n",
      "    \"south\",\n",
      "    \"centre\",\n",
      "    \"west\",\n",
      "    \"dontcare\"\n",
      "  ],\n",
      "  \"attraction-name\": [\n",
      "    \"cambridge arts theater\",\n",
      "    \"scudamores punting co\",\n",
      "    \"trinity college\",\n",
      "    \"history of science museum\",\n",
      "    \"old schools\",\n",
      "    \"christs college|saint catharines\",\n",
      "    \"aylesbray lodge guest house\",\n",
      "    \"scott polar\",\n",
      "    \"churchills college\",\n",
      "    \"museum of archaelogy and anthropology\",\n",
      "    \"cambridge artworks\",\n",
      "    \"pembroke college\",\n",
      "    \"milton country park\",\n",
      "    \"regency gallery\",\n",
      "    \"cambridge contemporary art museum\",\n",
      "    \"little saint marys church\",\n",
      "    \"sheeps green and lammas land park fen causeway\",\n",
      "    \"the junction\",\n",
      "    \"college\",\n",
      "    \"peoples portraits exhibition at girton college\",\n",
      "    \"st catharines college\",\n",
      "    \"kings college|hughes hall\",\n",
      "    \"cambridge and country folk museum\",\n",
      "    \"cineworld cinema\",\n",
      "    \"cambridge arts theatre\",\n",
      "    \"great saint marys church\",\n",
      "    \"university arms hotel\",\n",
      "    \"wandlebury country park\",\n",
      "    \"soul tree nightclub\",\n",
      "    \"churchill\",\n",
      "    \"archway house\",\n",
      "    \"salsa\",\n",
      "    \"home from home\",\n",
      "    \"sheeps green\",\n",
      "    \"scudamores punti co|the cambridge punter\",\n",
      "    \"sidney sussex|gonville and caius\",\n",
      "    \"christs church\",\n",
      "    \"riverboat georgina\",\n",
      "    \"cherry hinton village centre\",\n",
      "    \"mumford theatre\",\n",
      "    \"museum\",\n",
      "    \"castle galleries|primavera\",\n",
      "    \"ruskin gallery\",\n",
      "    \"christs college|downing college\",\n",
      "    \"cambridge corn exchange\",\n",
      "    \"lammas land park\",\n",
      "    \"museum of archaelogy\",\n",
      "    \"king hedges learner pool\",\n",
      "    \"whipple museum of the history of science\",\n",
      "    \"camboats\",\n",
      "    \"scudamores punting co|the cambridge punter\",\n",
      "    \"abbey pool and astroturf\",\n",
      "    \"the wandlebury\",\n",
      "    \"abbey pool and astroturf pitch\",\n",
      "    \"the cambridge corn exchange\",\n",
      "    \"cafe uno\",\n",
      "    \"cherry hinton water play\",\n",
      "    \"cafe jello museum\",\n",
      "    \"saint catharines college\",\n",
      "    \"abbey pool\",\n",
      "    \"museum of classical archaeology\",\n",
      "    \"hughes hall\",\n",
      "    \"whale of a time\",\n",
      "    \"kambar\",\n",
      "    \"vue cinema\",\n",
      "    \"the place\",\n",
      "    \"queens college\",\n",
      "    \"scudamore\",\n",
      "    \"ABC Theatre\",\n",
      "    \"museum of archaelogy and anthropology|fitzwilliam museum\",\n",
      "    \"st christs college\",\n",
      "    \"corn cambridge exchange\",\n",
      "    \"holy trinity church\",\n",
      "    \"the fez club\",\n",
      "    \"christ college\",\n",
      "    \"funky fun house\",\n",
      "    \"lynne strover gallery\",\n",
      "    \"city\",\n",
      "    \"cherry hinton hall\",\n",
      "    \"scudamores punting co|cambridge punters\",\n",
      "    \"fitzwilliam museum\",\n",
      "    \"hobsons house\",\n",
      "    \"club salsa\",\n",
      "    \"tenpin\",\n",
      "    \"worth house\",\n",
      "    \"jesus college\",\n",
      "    \"kings hedges learner pool\",\n",
      "    \"cambridge botanic gardens\",\n",
      "    \"camboats|funky fun house\",\n",
      "    \"emmanuel college\",\n",
      "    \"saint barnabas press gallery\",\n",
      "    \"nusha\",\n",
      "    \"clare hall\",\n",
      "    \"soultree\",\n",
      "    \"boat\",\n",
      "    \"cherry hinton water play park\",\n",
      "    \"county folk museum\",\n",
      "    \"botanic gardens\",\n",
      "    \"sidney sussex college\",\n",
      "    \"the man on the moon\",\n",
      "    \"whale of time\",\n",
      "    \"adc\",\n",
      "    \"man on the moon\",\n",
      "    \"primavera\",\n",
      "    \"jesus green\",\n",
      "    \"cambridge punter\",\n",
      "    \"cherry hinton hall and grounds\",\n",
      "    \"the cambridge artworks\",\n",
      "    \"whippple museum\",\n",
      "    \"free\",\n",
      "    \"cherry hinton water park\",\n",
      "    \"broughton house gallery\",\n",
      "    \"williams art and antiques\",\n",
      "    \"cambridge book and print gallery\",\n",
      "    \"cambridge and county folk museum\",\n",
      "    \"all saints church\",\n",
      "    \"dontcare\",\n",
      "    \"cambridge temporary art\",\n",
      "    \"parkside pools\",\n",
      "    \"nusha|tenpin\",\n",
      "    \"gonville hotel\",\n",
      "    \"pizza\",\n",
      "    \"cambridge university botanic gardens\",\n",
      "    \"castle galleries\",\n",
      "    \"scott polar museum\",\n",
      "    \"jesus green outdoor pool\",\n",
      "    \"cinema cinema\",\n",
      "    \"kettles yard\",\n",
      "    \"clare college\",\n",
      "    \"place\",\n",
      "    \"cambridge\",\n",
      "    \"peoples portraits exhibition\",\n",
      "    \"churchhill college\",\n",
      "    \"byard art\",\n",
      "    \"Cambridge university botanic gardens\",\n",
      "    \"bed\",\n",
      "    \"saint johns college\",\n",
      "    \"bangkok city\",\n",
      "    \"gallery\",\n",
      "    \"tenpin|nusha\",\n",
      "    \"trinity street college\",\n",
      "    \"queens\",\n",
      "    \"museum of archaelogy and anthropogy\",\n",
      "    \"kings college\",\n",
      "    \"adc theatre\",\n",
      "    \"magdalene college\",\n",
      "    \"cambridge contemporary art\",\n",
      "    \"contemporary art museum\",\n",
      "    \"caf\\u008e jello galery\",\n",
      "    \"gonville and caius college\",\n",
      "    \"the cambridge arts theatre\",\n",
      "    \"the castle galleries\",\n",
      "    \"thanh\",\n",
      "    \"corpus christi\",\n",
      "    \"cambridge museum of technology\",\n",
      "    \"the churchill college\",\n",
      "    \"kohinoor\",\n",
      "    \"downing college\",\n",
      "    \"cafe jello gallery\",\n",
      "    \"older churches\",\n",
      "    \"school\",\n",
      "    \"ballare\",\n",
      "    \"cambride and country folk museum\",\n",
      "    \"gallery at twelve a high street\"\n",
      "  ],\n",
      "  \"attraction-type\": [\n",
      "    \"sports\",\n",
      "    \"entertainment|cinemas|museums|theatres\",\n",
      "    \"church\",\n",
      "    \"museum\",\n",
      "    \"theatre\",\n",
      "    \"boat\",\n",
      "    \"architecture\",\n",
      "    \"churchills college\",\n",
      "    \"multiple sports|theatre\",\n",
      "    \"park|boat\",\n",
      "    \"theater\",\n",
      "    \"cinema\",\n",
      "    \"camboats\",\n",
      "    \"concert\",\n",
      "    \"night club\",\n",
      "    \"park\",\n",
      "    \"concerthall\",\n",
      "    \"boating\",\n",
      "    \"hiking|historical\",\n",
      "    \"college\",\n",
      "    \"hotel\",\n",
      "    \"gallery\",\n",
      "    \"entertainment\",\n",
      "    \"dontcare\",\n",
      "    \"museum kettles yard\",\n",
      "    \"concerthall|boat\",\n",
      "    \"museum|nightclub\",\n",
      "    \"special\",\n",
      "    \"swimming pool\",\n",
      "    \"gastropub\",\n",
      "    \"outdoor\",\n",
      "    \"pool\",\n",
      "    \"multiple sports\"\n",
      "  ]\n",
      "}\n",
      "  Turn 0 (User):\n",
      "    Text: Hi , I am looking forward to eat at your local restaurants on my upcoming trip .   I have those planned , but could use some help with places to go in town .\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: Do you have any food type , price range , and location preferences ?\n",
      "    Dialogue Acts: Restaurant-Request(['Area', '?'], ['Food', '?'], ['Price', '?'])\n",
      "  Turn 2 (User):\n",
      "    Text: You know what , I change my mind . Let 's look for an attraction for entertainment in the centre of town .\n",
      "    Dialogue Acts: Attraction-Inform(['Choice', 'lots of']); Attraction-Request(['Type', '?'], ['Name', '?'])\n",
      "  Turn 3 (System):\n",
      "    Text: There are lots of attractions is there anything specific you are looking for ?\n",
      "    Dialogue Acts: Attraction-Request(['Area', '?']); Attraction-NoOffer(['none', 'none'])\n",
      "\n",
      " Randomly Checking 2 Dialogues (First 4 Turns) from Test \n",
      "\n",
      "Dialogue ID: MUL2311.json\n",
      "Domain: attraction\n",
      "First 4 turns:\n",
      "    Ontology for domain 'attraction': {\n",
      "  \"attraction-area\": [\n",
      "    \"north\",\n",
      "    \"centre|west\",\n",
      "    \"east\",\n",
      "    \"south\",\n",
      "    \"centre\",\n",
      "    \"west\",\n",
      "    \"dontcare\"\n",
      "  ],\n",
      "  \"attraction-name\": [\n",
      "    \"cambridge arts theater\",\n",
      "    \"scudamores punting co\",\n",
      "    \"trinity college\",\n",
      "    \"history of science museum\",\n",
      "    \"old schools\",\n",
      "    \"christs college|saint catharines\",\n",
      "    \"aylesbray lodge guest house\",\n",
      "    \"scott polar\",\n",
      "    \"churchills college\",\n",
      "    \"museum of archaelogy and anthropology\",\n",
      "    \"cambridge artworks\",\n",
      "    \"pembroke college\",\n",
      "    \"milton country park\",\n",
      "    \"regency gallery\",\n",
      "    \"cambridge contemporary art museum\",\n",
      "    \"little saint marys church\",\n",
      "    \"sheeps green and lammas land park fen causeway\",\n",
      "    \"the junction\",\n",
      "    \"college\",\n",
      "    \"peoples portraits exhibition at girton college\",\n",
      "    \"st catharines college\",\n",
      "    \"kings college|hughes hall\",\n",
      "    \"cambridge and country folk museum\",\n",
      "    \"cineworld cinema\",\n",
      "    \"cambridge arts theatre\",\n",
      "    \"great saint marys church\",\n",
      "    \"university arms hotel\",\n",
      "    \"wandlebury country park\",\n",
      "    \"soul tree nightclub\",\n",
      "    \"churchill\",\n",
      "    \"archway house\",\n",
      "    \"salsa\",\n",
      "    \"home from home\",\n",
      "    \"sheeps green\",\n",
      "    \"scudamores punti co|the cambridge punter\",\n",
      "    \"sidney sussex|gonville and caius\",\n",
      "    \"christs church\",\n",
      "    \"riverboat georgina\",\n",
      "    \"cherry hinton village centre\",\n",
      "    \"mumford theatre\",\n",
      "    \"museum\",\n",
      "    \"castle galleries|primavera\",\n",
      "    \"ruskin gallery\",\n",
      "    \"christs college|downing college\",\n",
      "    \"cambridge corn exchange\",\n",
      "    \"lammas land park\",\n",
      "    \"museum of archaelogy\",\n",
      "    \"king hedges learner pool\",\n",
      "    \"whipple museum of the history of science\",\n",
      "    \"camboats\",\n",
      "    \"scudamores punting co|the cambridge punter\",\n",
      "    \"abbey pool and astroturf\",\n",
      "    \"the wandlebury\",\n",
      "    \"abbey pool and astroturf pitch\",\n",
      "    \"the cambridge corn exchange\",\n",
      "    \"cafe uno\",\n",
      "    \"cherry hinton water play\",\n",
      "    \"cafe jello museum\",\n",
      "    \"saint catharines college\",\n",
      "    \"abbey pool\",\n",
      "    \"museum of classical archaeology\",\n",
      "    \"hughes hall\",\n",
      "    \"whale of a time\",\n",
      "    \"kambar\",\n",
      "    \"vue cinema\",\n",
      "    \"the place\",\n",
      "    \"queens college\",\n",
      "    \"scudamore\",\n",
      "    \"ABC Theatre\",\n",
      "    \"museum of archaelogy and anthropology|fitzwilliam museum\",\n",
      "    \"st christs college\",\n",
      "    \"corn cambridge exchange\",\n",
      "    \"holy trinity church\",\n",
      "    \"the fez club\",\n",
      "    \"christ college\",\n",
      "    \"funky fun house\",\n",
      "    \"lynne strover gallery\",\n",
      "    \"city\",\n",
      "    \"cherry hinton hall\",\n",
      "    \"scudamores punting co|cambridge punters\",\n",
      "    \"fitzwilliam museum\",\n",
      "    \"hobsons house\",\n",
      "    \"club salsa\",\n",
      "    \"tenpin\",\n",
      "    \"worth house\",\n",
      "    \"jesus college\",\n",
      "    \"kings hedges learner pool\",\n",
      "    \"cambridge botanic gardens\",\n",
      "    \"camboats|funky fun house\",\n",
      "    \"emmanuel college\",\n",
      "    \"saint barnabas press gallery\",\n",
      "    \"nusha\",\n",
      "    \"clare hall\",\n",
      "    \"soultree\",\n",
      "    \"boat\",\n",
      "    \"cherry hinton water play park\",\n",
      "    \"county folk museum\",\n",
      "    \"botanic gardens\",\n",
      "    \"sidney sussex college\",\n",
      "    \"the man on the moon\",\n",
      "    \"whale of time\",\n",
      "    \"adc\",\n",
      "    \"man on the moon\",\n",
      "    \"primavera\",\n",
      "    \"jesus green\",\n",
      "    \"cambridge punter\",\n",
      "    \"cherry hinton hall and grounds\",\n",
      "    \"the cambridge artworks\",\n",
      "    \"whippple museum\",\n",
      "    \"free\",\n",
      "    \"cherry hinton water park\",\n",
      "    \"broughton house gallery\",\n",
      "    \"williams art and antiques\",\n",
      "    \"cambridge book and print gallery\",\n",
      "    \"cambridge and county folk museum\",\n",
      "    \"all saints church\",\n",
      "    \"dontcare\",\n",
      "    \"cambridge temporary art\",\n",
      "    \"parkside pools\",\n",
      "    \"nusha|tenpin\",\n",
      "    \"gonville hotel\",\n",
      "    \"pizza\",\n",
      "    \"cambridge university botanic gardens\",\n",
      "    \"castle galleries\",\n",
      "    \"scott polar museum\",\n",
      "    \"jesus green outdoor pool\",\n",
      "    \"cinema cinema\",\n",
      "    \"kettles yard\",\n",
      "    \"clare college\",\n",
      "    \"place\",\n",
      "    \"cambridge\",\n",
      "    \"peoples portraits exhibition\",\n",
      "    \"churchhill college\",\n",
      "    \"byard art\",\n",
      "    \"Cambridge university botanic gardens\",\n",
      "    \"bed\",\n",
      "    \"saint johns college\",\n",
      "    \"bangkok city\",\n",
      "    \"gallery\",\n",
      "    \"tenpin|nusha\",\n",
      "    \"trinity street college\",\n",
      "    \"queens\",\n",
      "    \"museum of archaelogy and anthropogy\",\n",
      "    \"kings college\",\n",
      "    \"adc theatre\",\n",
      "    \"magdalene college\",\n",
      "    \"cambridge contemporary art\",\n",
      "    \"contemporary art museum\",\n",
      "    \"caf\\u008e jello galery\",\n",
      "    \"gonville and caius college\",\n",
      "    \"the cambridge arts theatre\",\n",
      "    \"the castle galleries\",\n",
      "    \"thanh\",\n",
      "    \"corpus christi\",\n",
      "    \"cambridge museum of technology\",\n",
      "    \"the churchill college\",\n",
      "    \"kohinoor\",\n",
      "    \"downing college\",\n",
      "    \"cafe jello gallery\",\n",
      "    \"older churches\",\n",
      "    \"school\",\n",
      "    \"ballare\",\n",
      "    \"cambride and country folk museum\",\n",
      "    \"gallery at twelve a high street\"\n",
      "  ],\n",
      "  \"attraction-type\": [\n",
      "    \"sports\",\n",
      "    \"entertainment|cinemas|museums|theatres\",\n",
      "    \"church\",\n",
      "    \"museum\",\n",
      "    \"theatre\",\n",
      "    \"boat\",\n",
      "    \"architecture\",\n",
      "    \"churchills college\",\n",
      "    \"multiple sports|theatre\",\n",
      "    \"park|boat\",\n",
      "    \"theater\",\n",
      "    \"cinema\",\n",
      "    \"camboats\",\n",
      "    \"concert\",\n",
      "    \"night club\",\n",
      "    \"park\",\n",
      "    \"concerthall\",\n",
      "    \"boating\",\n",
      "    \"hiking|historical\",\n",
      "    \"college\",\n",
      "    \"hotel\",\n",
      "    \"gallery\",\n",
      "    \"entertainment\",\n",
      "    \"dontcare\",\n",
      "    \"museum kettles yard\",\n",
      "    \"concerthall|boat\",\n",
      "    \"museum|nightclub\",\n",
      "    \"special\",\n",
      "    \"swimming pool\",\n",
      "    \"gastropub\",\n",
      "    \"outdoor\",\n",
      "    \"pool\",\n",
      "    \"multiple sports\"\n",
      "  ]\n",
      "}\n",
      "  Turn 0 (User):\n",
      "    Text: Yes , could you give me some suggestions on places to go to in the centre of town ?\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: Castle Galleries is a really nice museum .   It is located at Unit su43 , Grande Arcade , Saint Andrews Street , postcode cb23bj .   It is my personal favorite .\n",
      "    Dialogue Acts: Attraction-Recommend(['Type', 'museum'], ['Name', 'Castle Galleries']); general-greet(['none', 'none']); Attraction-Inform(['Post', 'cb23bj'], ['Addr', 'Unit su43 , Grande Arcade , Saint Andrews Street'])\n",
      "  Turn 2 (User):\n",
      "    Text: That 's all I need thanks .\n",
      "    Dialogue Acts: general-bye(['none', 'none']); general-welcome(['none', 'none'])\n",
      "  Turn 3 (System):\n",
      "    Text: Thank you for contacting Cambridge TownInfo Centre , have a great day !\n",
      "    Dialogue Acts: Booking-Inform(['Name', 'Royal Standard'])\n",
      "\n",
      "Dialogue ID: PMUL0099.json\n",
      "Domain: restaurant\n",
      "First 4 turns:\n",
      "    Ontology for domain 'restaurant': {\n",
      "  \"restaurant-book day\": [\n",
      "    \"sunday|thursday\",\n",
      "    \"wednesday\",\n",
      "    \"saturday\",\n",
      "    \"sunday\",\n",
      "    \"saturday|thursday\",\n",
      "    \"friday\",\n",
      "    \"dontcare\",\n",
      "    \"monday\",\n",
      "    \"thursday\",\n",
      "    \"tuesday\"\n",
      "  ],\n",
      "  \"restaurant-book people\": [\n",
      "    \"2\",\n",
      "    \"7\",\n",
      "    \"8\",\n",
      "    \"5\",\n",
      "    \"1\",\n",
      "    \"6\",\n",
      "    \"4|7\",\n",
      "    \"3\",\n",
      "    \"4\"\n",
      "  ],\n",
      "  \"restaurant-book time\": [\n",
      "    \"12:00\",\n",
      "    \"13:30\",\n",
      "    \"10:00\",\n",
      "    \"21:00\",\n",
      "    \"7pm\",\n",
      "    \"1345\",\n",
      "    \"19:30\",\n",
      "    \"21:45\",\n",
      "    \"15:00\",\n",
      "    \"01:00\",\n",
      "    \"16:30\",\n",
      "    \"14:15\",\n",
      "    \"03:00\",\n",
      "    \"10:45\",\n",
      "    \"18:30\",\n",
      "    \"15:15\",\n",
      "    \"17:00|16:00\",\n",
      "    \"4pm\",\n",
      "    \"15:30|16:30\",\n",
      "    \"9\",\n",
      "    \"14:00\",\n",
      "    \"17:45\",\n",
      "    \"09:45\",\n",
      "    \"1715\",\n",
      "    \"16:45\",\n",
      "    \"12:45\",\n",
      "    \"1330\",\n",
      "    \"20:45\",\n",
      "    \"11:00\",\n",
      "    \"12:15\",\n",
      "    \"10:30\",\n",
      "    \"16:15\",\n",
      "    \"15:45\",\n",
      "    \"11:30\",\n",
      "    \"20:00\",\n",
      "    \"19:45\",\n",
      "    \"10:48\",\n",
      "    \"13:45\",\n",
      "    \"18:00|12:15\",\n",
      "    \"18:15\",\n",
      "    \"06:30\",\n",
      "    \"20:30\",\n",
      "    \"11:30|12:30\",\n",
      "    \"17:30\",\n",
      "    \"12:30\",\n",
      "    \"09:15\",\n",
      "    \"14:45\",\n",
      "    \"dontcare\",\n",
      "    \"13:00\",\n",
      "    \"14:30\",\n",
      "    \"20:15\",\n",
      "    \"10:15\",\n",
      "    \"09:30\",\n",
      "    \"18:00\",\n",
      "    \"14:40\",\n",
      "    \"09:00\",\n",
      "    \"17:00\",\n",
      "    \"16:00\",\n",
      "    \"17:30|16:30\",\n",
      "    \"13:15\",\n",
      "    \"1430\",\n",
      "    \"1145\",\n",
      "    \"8pm\",\n",
      "    \"08:45\",\n",
      "    \"11:15\",\n",
      "    \"22:00\",\n",
      "    \"18:45\",\n",
      "    \"19:15\",\n",
      "    \"17:15\",\n",
      "    \"15:30\",\n",
      "    \"19:00\",\n",
      "    \"11:45\"\n",
      "  ],\n",
      "  \"restaurant-area\": [\n",
      "    \"north\",\n",
      "    \"east\",\n",
      "    \"dontcare\",\n",
      "    \"south\",\n",
      "    \"centre\",\n",
      "    \"west\",\n",
      "    \"east|south\"\n",
      "  ],\n",
      "  \"restaurant-food\": [\n",
      "    \"north american>indian\",\n",
      "    \"portugese\",\n",
      "    \"polynesian\",\n",
      "    \"austrian\",\n",
      "    \"greek\",\n",
      "    \"asian\",\n",
      "    \"modern global\",\n",
      "    \"korean\",\n",
      "    \"danish\",\n",
      "    \"chinese\",\n",
      "    \"bistro\",\n",
      "    \"afternoon tea\",\n",
      "    \"kosher|british\",\n",
      "    \"tuscan\",\n",
      "    \"indonesian\",\n",
      "    \"spanish|portuguese\",\n",
      "    \"north african\",\n",
      "    \"north indian\",\n",
      "    \"the americas\",\n",
      "    \"italian\",\n",
      "    \"romanian\",\n",
      "    \"welsh\",\n",
      "    \"french\",\n",
      "    \"modern american\",\n",
      "    \"new zealand\",\n",
      "    \"sri lankan\",\n",
      "    \"traditional american\",\n",
      "    \"thai\",\n",
      "    \"fusion\",\n",
      "    \"molecular gastronomy\",\n",
      "    \"brazilian|portuguese\",\n",
      "    \"south african\",\n",
      "    \"european\",\n",
      "    \"kosher\",\n",
      "    \"jamaican>chinese\",\n",
      "    \"modern english\",\n",
      "    \"italian|indian\",\n",
      "    \"mexican\",\n",
      "    \"turkish\",\n",
      "    \"australian|indian\",\n",
      "    \"halal\",\n",
      "    \"german\",\n",
      "    \"dojo noodle bar\",\n",
      "    \"thai and chinese\",\n",
      "    \"international\",\n",
      "    \"australian\",\n",
      "    \"indian|african\",\n",
      "    \"chinese|mexican\",\n",
      "    \"american\",\n",
      "    \"north american\",\n",
      "    \"eastern european\",\n",
      "    \"caribbean>indian\",\n",
      "    \"afghan\",\n",
      "    \"seafood\",\n",
      "    \"moroccan\",\n",
      "    \"russian\",\n",
      "    \"latin american\",\n",
      "    \"creative\",\n",
      "    \"eritrean\",\n",
      "    \"unusual\",\n",
      "    \"caribbean\",\n",
      "    \"spanish\",\n",
      "    \"sushi\",\n",
      "    \"vegetarian\",\n",
      "    \"corsica\",\n",
      "    \"barbeque>modern european\",\n",
      "    \"light bites\",\n",
      "    \"world\",\n",
      "    \"barbeque\",\n",
      "    \"vietnamese\",\n",
      "    \"dontcare\",\n",
      "    \"african\",\n",
      "    \"south indian\",\n",
      "    \"panasian\",\n",
      "    \"persian\",\n",
      "    \"modern eclectic\",\n",
      "    \"gastropub\",\n",
      "    \"japanese\",\n",
      "    \"cuban\",\n",
      "    \"middle eastern\",\n",
      "    \"canapes\",\n",
      "    \"lebanese\",\n",
      "    \"jamaican\",\n",
      "    \"northern european\",\n",
      "    \"basque\",\n",
      "    \"scottish\",\n",
      "    \"christmas\",\n",
      "    \"irish\",\n",
      "    \"indian\",\n",
      "    \"belgian\",\n",
      "    \"british\",\n",
      "    \"hungarian\",\n",
      "    \"catalan\",\n",
      "    \"english\",\n",
      "    \"crossover\",\n",
      "    \"mediterranean\",\n",
      "    \"malaysian\",\n",
      "    \"swiss\",\n",
      "    \"swedish\",\n",
      "    \"polish\",\n",
      "    \"brazilian\",\n",
      "    \"traditional\",\n",
      "    \"steakhouse\",\n",
      "    \"singaporean\",\n",
      "    \"asian oriental\",\n",
      "    \"venetian\",\n",
      "    \"scandinavian\",\n",
      "    \"cantonese\",\n",
      "    \"modern european\"\n",
      "  ],\n",
      "  \"restaurant-name\": [\n",
      "    \"copper kettle\",\n",
      "    \"taj tandoori\",\n",
      "    \"charlie chan\",\n",
      "    \"one seven\",\n",
      "    \"dif\",\n",
      "    \"eraina and michaelhouse cafe\",\n",
      "    \"mahal of cambridge\",\n",
      "    \"restaurant two two\",\n",
      "    \"curry prince|rajmahal\",\n",
      "    \"golden curry\",\n",
      "    \"missing sock\",\n",
      "    \"frankie and bennys\",\n",
      "    \"kymmoy\",\n",
      "    \"cam\",\n",
      "    \"dojo noodle bar|j restaurant\",\n",
      "    \"the bedouin\",\n",
      "    \"restaurant alimentum\",\n",
      "    \"gourmet burger kitchen\",\n",
      "    \"dojo noodle  bar|j restaurant\",\n",
      "    \"la margherita\",\n",
      "    \"golden house\",\n",
      "    \"chiquito\",\n",
      "    \"darrys cookhouse and wine shop\",\n",
      "    \"scudamores punt\",\n",
      "    \"eraina\",\n",
      "    \"the varsity restaurant\",\n",
      "    \"pizza hut fenditton\",\n",
      "    \"saint johns chop house\",\n",
      "    \"curry garden\",\n",
      "    \"sala thong|bangkok city\",\n",
      "    \"jinling noodle bar\",\n",
      "    \"sitar tandoori\",\n",
      "    \"pizza hut\",\n",
      "    \"the Nirala\",\n",
      "    \"city stop restaurant\",\n",
      "    \"de luca cucina and bar riverside brasserie\",\n",
      "    \"rice boat\",\n",
      "    \"wise buddha\",\n",
      "    \"restaurant one seven\",\n",
      "    \"meze bar restaurant\",\n",
      "    \"yipee noodle bar\",\n",
      "    \"little seoul\",\n",
      "    \"rajmahal\",\n",
      "    \"ali baba\",\n",
      "    \"limehouse\",\n",
      "    \"the grafton hotel\",\n",
      "    \"barbakan\",\n",
      "    \"sesame restaurant and bar\",\n",
      "    \"golden wok\",\n",
      "    \"pizza hut city centre\",\n",
      "    \"binh\",\n",
      "    \"hotel du vin and bistro\",\n",
      "    \"lan hong house\",\n",
      "    \"tandoori Palace\",\n",
      "    \"alimentum\",\n",
      "    \"the oak bistro\",\n",
      "    \"anatolia\",\n",
      "    \"ian hong house\",\n",
      "    \"two two and cote\",\n",
      "    \"curry queen\",\n",
      "    \"backstreet bistro\",\n",
      "    \"mahal\",\n",
      "    \"la mimosa\",\n",
      "    \"shanghai family restaurant\",\n",
      "    \"molecular gastronomy\",\n",
      "    \"efes\",\n",
      "    \"cote\",\n",
      "    \"good luck\",\n",
      "    \"cafe uno\",\n",
      "    \"oak bistro\",\n",
      "    \"european\",\n",
      "    \"saffron brasserie\",\n",
      "    \"gardenia\",\n",
      "    \"de luca cucina and bar\",\n",
      "    \"two two\",\n",
      "    \"ashley hotel\",\n",
      "    \"the hotpot\",\n",
      "    \"michaelhouse cafe\",\n",
      "    \"yu garden\",\n",
      "    \"gourmet formal kitchen\",\n",
      "    \"lovel\",\n",
      "    \"efes restaurant\",\n",
      "    \"the peking restaurant: \",\n",
      "    \"river bar steakhouse and grill\",\n",
      "    \"j restaurant\",\n",
      "    \"the slug and lettuce\",\n",
      "    \"yippee noodle bar\",\n",
      "    \"meze bar\",\n",
      "    \"rice house\",\n",
      "    \"clowns cafe\",\n",
      "    \"no\",\n",
      "    \"hobsons house\",\n",
      "    \"dojo noodle bar\",\n",
      "    \"the kohinoor\",\n",
      "    \"royal standard\",\n",
      "    \"bloomsbury restaurant\",\n",
      "    \"graffiti\",\n",
      "    \"el shaddia guesthouse\",\n",
      "    \"worth house\",\n",
      "    \"shiraz\",\n",
      "    \"the golden house\",\n",
      "    \"shanghai\",\n",
      "    \"don pasquale pizzeria\",\n",
      "    \"tandoori palace\",\n",
      "    \"autumn house\",\n",
      "    \"panahar\",\n",
      "    \"the peking\",\n",
      "    \"wagamama\",\n",
      "    \"tandoori\",\n",
      "    \"nusha\",\n",
      "    \"the gandhi\",\n",
      "    \"nandos city centre\",\n",
      "    \"slug and lettuce\",\n",
      "    \"sitar\",\n",
      "    \"alex\",\n",
      "    \"meghna\",\n",
      "    \"cambridge chop house\",\n",
      "    \"the missing sock\",\n",
      "    \"primavera\",\n",
      "    \"the meze bar\",\n",
      "    \"travellers rest\",\n",
      "    \"south\",\n",
      "    \"curry king\",\n",
      "    \"pipasha restaurant\",\n",
      "    \"cambridge punter\",\n",
      "    \"saigon city\",\n",
      "    \"bedouin\",\n",
      "    \"pizza express\",\n",
      "    \"ask\",\n",
      "    \"broughton house gallery\",\n",
      "    \"tang chinese\",\n",
      "    \"curry prince\",\n",
      "    \"dontcare\",\n",
      "    \"charlie\",\n",
      "    \"nirala\",\n",
      "    \"nandos\",\n",
      "    \"cotto\",\n",
      "    \"parkside pools\",\n",
      "    \"galleria\",\n",
      "    \"adden\",\n",
      "    \"funky\",\n",
      "    \"maharajah tandoori restaurant\",\n",
      "    \"nil\",\n",
      "    \"the cow pizza kitchen and bar\",\n",
      "    \"bridge\",\n",
      "    \"grafton hotel\",\n",
      "    \"india west\",\n",
      "    \"cow pizza kitchen and bar\",\n",
      "    \"midsummer house restaurant\",\n",
      "    \"cambridge be\",\n",
      "    \"the lucky star\",\n",
      "    \"hakka\",\n",
      "    \"prezzo\",\n",
      "    \"cambridge lodge restaurant\",\n",
      "    \"sala thong\",\n",
      "    \"Kohinoor\",\n",
      "    \"zizzi cambridge\",\n",
      "    \"pizza hut cherry hinton\",\n",
      "    \"4 kings parade city centre\",\n",
      "    \"golden wok|nirala\",\n",
      "    \"nus\",\n",
      "    \"fitzbillies restaurant\",\n",
      "    \"lucky star\",\n",
      "    \"la tasca\",\n",
      "    \"loch fyne\",\n",
      "    \"cityr\",\n",
      "    \"the gardenia\",\n",
      "    \"bangkok city\",\n",
      "    \"chiquito restaurant bar\",\n",
      "    \"anatolia and efes restaurant\",\n",
      "    \"ugly duckling\",\n",
      "    \"cocum\",\n",
      "    \"hk fusion\",\n",
      "    \"stazione restaurant and coffee bar\",\n",
      "    \"restaurant 22\",\n",
      "    \"grafton hotel restaurant\",\n",
      "    \"the maharajah tandoor\",\n",
      "    \"the alex\",\n",
      "    \"thanh binh\",\n",
      "    \"the river bar steakhouse and grill\",\n",
      "    \"india house\",\n",
      "    \"peking restaurant\",\n",
      "    \"hotpot\",\n",
      "    \"kohinoor\",\n",
      "    \"la raza\",\n",
      "    \"da vinci pizzeria\",\n",
      "    \"pizza hut cherry hinton|alimentum\",\n",
      "    \"royal spice\",\n",
      "    \"riverside brasserie\",\n",
      "    \"kitchen and bar\"\n",
      "  ],\n",
      "  \"restaurant-pricerange\": [\n",
      "    \"expensive\",\n",
      "    \"cheap\",\n",
      "    \"moderate\",\n",
      "    \"dontcare\",\n",
      "    \"moderate|cheap\"\n",
      "  ]\n",
      "}\n",
      "  Turn 0 (User):\n",
      "    Text: I would like to find an expensive restaurant in the north part of Cambridge .\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: Sure what type of food are you wanting to eat ?\n",
      "    Dialogue Acts: Restaurant-Request(['Food', '?'])\n",
      "  Turn 2 (User):\n",
      "    Text: How about chinese food ?\n",
      "    Dialogue Acts: Restaurant-Select(['none', 'none']); Restaurant-Inform(['Choice', '2'], ['Name', 'Hakka'], ['Name', 'the hotpot'])\n",
      "  Turn 3 (System):\n",
      "    Text: There 2 of them - the Hakka and the hotpot . Which one would you like to try ?\n",
      "    Dialogue Acts: general-reqmore(['none', 'none']); Booking-Book(['Ref', '48WF5G7X'])\n"
     ]
    }
   ],
   "source": [
    "def format_dialogue_acts(acts_dict):\n",
    "    \"\"\"Format dialogue acts for better display.\"\"\"\n",
    "    if not acts_dict:\n",
    "        return \"No acts available\"\n",
    "    formatted = []\n",
    "    for act_type, slots in acts_dict.items():\n",
    "        slot_strs = []\n",
    "        for slot in slots:\n",
    "            if ':' in slot:\n",
    "                key, value = slot.split(':', 1)\n",
    "                slot_strs.append(f\"{key}={value}\")\n",
    "            else:\n",
    "                slot_strs.append(str(slot))\n",
    "        formatted.append(f\"{act_type}({', '.join(slot_strs)})\")\n",
    "    return \"; \".join(formatted)\n",
    "\n",
    "def randomly_check_first_dialogue_turns_with_acts_and_ontology(split_name, split_data, num_samples=2, num_turns=4):\n",
    "    \"\"\"\n",
    "    Randomly checks a few dialogues and displays the dialogue text, acts, and ontology.\n",
    "    \"\"\"\n",
    "    print(f\"\\n Randomly Checking {num_samples} Dialogues (First {num_turns} Turns) from {split_name} \")\n",
    "    if not split_data:\n",
    "        print(f\"No data in {split_name} split.\")\n",
    "        return\n",
    "    \n",
    "    if not isinstance(split_data, (list, tuple)):\n",
    "        split_data = list(split_data)\n",
    "    \n",
    "    if len(split_data) == 0:\n",
    "        print(f\"No data in {split_name} split after conversion.\")\n",
    "        return\n",
    "        \n",
    "    random_dialogues = random.sample(split_data, min(num_samples, len(split_data)))\n",
    "    \n",
    "    for dialogue_item in random_dialogues:\n",
    "        dialogue_id = dialogue_item[\"dialogue_id\"]\n",
    "        raw_dialogue_object = dialogue_item[\"dialogue\"]\n",
    "        if isinstance(raw_dialogue_object, list):\n",
    "            turns = raw_dialogue_object\n",
    "        elif isinstance(raw_dialogue_object, dict):\n",
    "            turns = raw_dialogue_object.get(\"log\", [])\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected dialogue format for dialogue ID {dialogue_id}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nDialogue ID: {dialogue_id}\")\n",
    "        print(f\"Domain: {dialogue_item.get('domain', 'unknown')}\")\n",
    "        print(f\"First {min(num_turns, len(turns))} turns:\")\n",
    "        \n",
    "        # Dialogue acts and ontology are still accessed from the top-level item\n",
    "        dialogue_acts_for_dialogue = dialogue_item.get(\"dialogue_acts\", {})\n",
    "        domain = dialogue_item.get('domain', 'unknown')\n",
    "        \n",
    "        domain_ontology = {\n",
    "            slot: values\n",
    "            for slot, values in dialogue_item.get(\"ontology\", {}).items()\n",
    "            if slot.startswith(f\"{domain}-\")\n",
    "        }\n",
    "\n",
    "        print(f\"    Ontology for domain '{domain}': {json.dumps(domain_ontology, indent=2)}\")\n",
    "\n",
    "        for i in range(min(num_turns, len(turns))):\n",
    "            turn = turns[i]\n",
    "            # In the raw data, the 'text' key is at this level\n",
    "            text_content = turn.get('text', '')\n",
    "            # use the loop index for the turn id\n",
    "            turn_idx = i\n",
    "            turn_key = str(turn_idx)\n",
    "\n",
    "            turn_type = \"User\" if turn_idx % 2 == 0 else \"System\"\n",
    "            print(f\"  Turn {turn_idx} ({turn_type}):\")\n",
    "            \n",
    "            # Use the text content from the turn dictionary\n",
    "            print(f\"    Text: {text_content}\")\n",
    "            \n",
    "            turn_acts_data = dialogue_acts_for_dialogue.get(turn_key, {})\n",
    "            print(f\"    Dialogue Acts: {format_dialogue_acts(turn_acts_data)}\")\n",
    "\n",
    "randomly_check_first_dialogue_turns_with_acts_and_ontology(\"Train\", train_data_raw)\n",
    "randomly_check_first_dialogue_turns_with_acts_and_ontology(\"Test\", test_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts number of words in each sentence in each dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence in train_dataset: 67 words\n",
      "Longest sentence in test_dataset: 56 words\n",
      "\n",
      "Summary of longest sentence lengths:\n",
      "train_dataset: 67 words\n",
      "test_dataset: 56 words\n"
     ]
    }
   ],
   "source": [
    "def get_longest_sentence_word_length(datasets: Dict[str, List[Dict[str, Any]]]) -> Dict[str, int]:\n",
    "    longest_sentence_lengths = {}\n",
    "    \n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        max_words_in_sentence = 0\n",
    "        \n",
    "        for dialogue_entry in dataset:\n",
    "            # The 'dialogue' entry is a dictionary containing a 'log' key, which is a list of turns.\n",
    "            dialogue_log = dialogue_entry.get('dialogue', {}).get('log', [])\n",
    "            \n",
    "            for turn in dialogue_log:\n",
    "                # The turn itself is a dictionary with a 'text' key.\n",
    "                sentence_text = turn.get('text', '').strip()\n",
    "                \n",
    "                if sentence_text:\n",
    "                    words = sentence_text.split()\n",
    "                    max_words_in_sentence = max(max_words_in_sentence, len(words))\n",
    "        \n",
    "        longest_sentence_lengths[dataset_name] = max_words_in_sentence\n",
    "        print(f\"Longest sentence in {dataset_name}: {max_words_in_sentence} words\")\n",
    "    \n",
    "    return longest_sentence_lengths\n",
    "\n",
    "all_datasets = {\n",
    "    'train_dataset': train_data_raw,\n",
    "    'test_dataset': test_data_raw,\n",
    "}\n",
    "\n",
    "longest_lengths = get_longest_sentence_word_length(all_datasets)\n",
    "\n",
    "print(\"\\nSummary of longest sentence lengths:\")\n",
    "for dataset_name, length in longest_lengths.items():\n",
    "    print(f\"{dataset_name}: {length} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the number of turns in dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset: train_dataset\n",
      "Checking dataset: test_dataset\n",
      "\n",
      "The longest turn_id found across all datasets is: 43\n"
     ]
    }
   ],
   "source": [
    "def find_longest_turn_id(all_datasets):\n",
    "    max_turn_id = 0\n",
    "\n",
    "    for dataset_name, dialogues_dataset in all_datasets.items():\n",
    "        print(f\"Checking dataset: {dataset_name}\")\n",
    "        for dialogue in dialogues_dataset:\n",
    "            # The list of turns is under the 'log' key inside the 'dialogue' dictionary\n",
    "            dialogue_turns = dialogue.get('dialogue', {}).get('log', [])\n",
    "            \n",
    "            for turn in dialogue_turns:\n",
    "                # The 'turn_id' is an integer,if turn_id is an integer\n",
    "                if 'turn_id' in turn:\n",
    "                    max_turn_id = max(max_turn_id, turn['turn_id'])\n",
    "    \n",
    "    return max_turn_id\n",
    "\n",
    "all_datasets = {\n",
    "    'train_dataset': train_data_raw,\n",
    "    'test_dataset': test_data_raw\n",
    "}\n",
    "\n",
    "longest_turn_id = find_longest_turn_id(all_datasets)\n",
    "print(f\"\\nThe longest turn_id found across all datasets is: {longest_turn_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing Datasets\n",
      "Preprocessed train_data size: 1600\n",
      "Preprocessed test_data size: 400\n",
      "\n",
      " Randomly Checking 2 Dialogues (First 4 Turns) from Processed Train \n",
      "\n",
      "Dialogue ID: SNG0668.json\n",
      "Domain: restaurant\n",
      "    Ontology for domain 'restaurant': {\n",
      "  \"restaurant-book day\": [\n",
      "    \"sunday|thursday\",\n",
      "    \"wednesday\",\n",
      "    \"saturday\",\n",
      "    \"sunday\",\n",
      "    \"saturday|thursday\",\n",
      "    \"friday\",\n",
      "    \"dontcare\",\n",
      "    \"monday\",\n",
      "    \"thursday\",\n",
      "    \"tuesday\"\n",
      "  ],\n",
      "  \"restaurant-book people\": [\n",
      "    \"2\",\n",
      "    \"7\",\n",
      "    \"8\",\n",
      "    \"5\",\n",
      "    \"1\",\n",
      "    \"6\",\n",
      "    \"4|7\",\n",
      "    \"3\",\n",
      "    \"4\"\n",
      "  ],\n",
      "  \"restaurant-book time\": [\n",
      "    \"12:00\",\n",
      "    \"13:30\",\n",
      "    \"10:00\",\n",
      "    \"21:00\",\n",
      "    \"7pm\",\n",
      "    \"1345\",\n",
      "    \"19:30\",\n",
      "    \"21:45\",\n",
      "    \"15:00\",\n",
      "    \"01:00\",\n",
      "    \"16:30\",\n",
      "    \"14:15\",\n",
      "    \"03:00\",\n",
      "    \"10:45\",\n",
      "    \"18:30\",\n",
      "    \"15:15\",\n",
      "    \"17:00|16:00\",\n",
      "    \"4pm\",\n",
      "    \"15:30|16:30\",\n",
      "    \"9\",\n",
      "    \"14:00\",\n",
      "    \"17:45\",\n",
      "    \"09:45\",\n",
      "    \"1715\",\n",
      "    \"16:45\",\n",
      "    \"12:45\",\n",
      "    \"1330\",\n",
      "    \"20:45\",\n",
      "    \"11:00\",\n",
      "    \"12:15\",\n",
      "    \"10:30\",\n",
      "    \"16:15\",\n",
      "    \"15:45\",\n",
      "    \"11:30\",\n",
      "    \"20:00\",\n",
      "    \"19:45\",\n",
      "    \"10:48\",\n",
      "    \"13:45\",\n",
      "    \"18:00|12:15\",\n",
      "    \"18:15\",\n",
      "    \"06:30\",\n",
      "    \"20:30\",\n",
      "    \"11:30|12:30\",\n",
      "    \"17:30\",\n",
      "    \"12:30\",\n",
      "    \"09:15\",\n",
      "    \"14:45\",\n",
      "    \"dontcare\",\n",
      "    \"13:00\",\n",
      "    \"14:30\",\n",
      "    \"20:15\",\n",
      "    \"10:15\",\n",
      "    \"09:30\",\n",
      "    \"18:00\",\n",
      "    \"14:40\",\n",
      "    \"09:00\",\n",
      "    \"17:00\",\n",
      "    \"16:00\",\n",
      "    \"17:30|16:30\",\n",
      "    \"13:15\",\n",
      "    \"1430\",\n",
      "    \"1145\",\n",
      "    \"8pm\",\n",
      "    \"08:45\",\n",
      "    \"11:15\",\n",
      "    \"22:00\",\n",
      "    \"18:45\",\n",
      "    \"19:15\",\n",
      "    \"17:15\",\n",
      "    \"15:30\",\n",
      "    \"19:00\",\n",
      "    \"11:45\"\n",
      "  ],\n",
      "  \"restaurant-area\": [\n",
      "    \"north\",\n",
      "    \"east\",\n",
      "    \"dontcare\",\n",
      "    \"south\",\n",
      "    \"centre\",\n",
      "    \"west\",\n",
      "    \"east|south\"\n",
      "  ],\n",
      "  \"restaurant-food\": [\n",
      "    \"north american>indian\",\n",
      "    \"portugese\",\n",
      "    \"polynesian\",\n",
      "    \"austrian\",\n",
      "    \"greek\",\n",
      "    \"asian\",\n",
      "    \"modern global\",\n",
      "    \"korean\",\n",
      "    \"danish\",\n",
      "    \"chinese\",\n",
      "    \"bistro\",\n",
      "    \"afternoon tea\",\n",
      "    \"kosher|british\",\n",
      "    \"tuscan\",\n",
      "    \"indonesian\",\n",
      "    \"spanish|portuguese\",\n",
      "    \"north african\",\n",
      "    \"north indian\",\n",
      "    \"the americas\",\n",
      "    \"italian\",\n",
      "    \"romanian\",\n",
      "    \"welsh\",\n",
      "    \"french\",\n",
      "    \"modern american\",\n",
      "    \"new zealand\",\n",
      "    \"sri lankan\",\n",
      "    \"traditional american\",\n",
      "    \"thai\",\n",
      "    \"fusion\",\n",
      "    \"molecular gastronomy\",\n",
      "    \"brazilian|portuguese\",\n",
      "    \"south african\",\n",
      "    \"european\",\n",
      "    \"kosher\",\n",
      "    \"jamaican>chinese\",\n",
      "    \"modern english\",\n",
      "    \"italian|indian\",\n",
      "    \"mexican\",\n",
      "    \"turkish\",\n",
      "    \"australian|indian\",\n",
      "    \"halal\",\n",
      "    \"german\",\n",
      "    \"dojo noodle bar\",\n",
      "    \"thai and chinese\",\n",
      "    \"international\",\n",
      "    \"australian\",\n",
      "    \"indian|african\",\n",
      "    \"chinese|mexican\",\n",
      "    \"american\",\n",
      "    \"north american\",\n",
      "    \"eastern european\",\n",
      "    \"caribbean>indian\",\n",
      "    \"afghan\",\n",
      "    \"seafood\",\n",
      "    \"moroccan\",\n",
      "    \"russian\",\n",
      "    \"latin american\",\n",
      "    \"creative\",\n",
      "    \"eritrean\",\n",
      "    \"unusual\",\n",
      "    \"caribbean\",\n",
      "    \"spanish\",\n",
      "    \"sushi\",\n",
      "    \"vegetarian\",\n",
      "    \"corsica\",\n",
      "    \"barbeque>modern european\",\n",
      "    \"light bites\",\n",
      "    \"world\",\n",
      "    \"barbeque\",\n",
      "    \"vietnamese\",\n",
      "    \"dontcare\",\n",
      "    \"african\",\n",
      "    \"south indian\",\n",
      "    \"panasian\",\n",
      "    \"persian\",\n",
      "    \"modern eclectic\",\n",
      "    \"gastropub\",\n",
      "    \"japanese\",\n",
      "    \"cuban\",\n",
      "    \"middle eastern\",\n",
      "    \"canapes\",\n",
      "    \"lebanese\",\n",
      "    \"jamaican\",\n",
      "    \"northern european\",\n",
      "    \"basque\",\n",
      "    \"scottish\",\n",
      "    \"christmas\",\n",
      "    \"irish\",\n",
      "    \"indian\",\n",
      "    \"belgian\",\n",
      "    \"british\",\n",
      "    \"hungarian\",\n",
      "    \"catalan\",\n",
      "    \"english\",\n",
      "    \"crossover\",\n",
      "    \"mediterranean\",\n",
      "    \"malaysian\",\n",
      "    \"swiss\",\n",
      "    \"swedish\",\n",
      "    \"polish\",\n",
      "    \"brazilian\",\n",
      "    \"traditional\",\n",
      "    \"steakhouse\",\n",
      "    \"singaporean\",\n",
      "    \"asian oriental\",\n",
      "    \"venetian\",\n",
      "    \"scandinavian\",\n",
      "    \"cantonese\",\n",
      "    \"modern european\"\n",
      "  ],\n",
      "  \"restaurant-name\": [\n",
      "    \"copper kettle\",\n",
      "    \"taj tandoori\",\n",
      "    \"charlie chan\",\n",
      "    \"one seven\",\n",
      "    \"dif\",\n",
      "    \"eraina and michaelhouse cafe\",\n",
      "    \"mahal of cambridge\",\n",
      "    \"restaurant two two\",\n",
      "    \"curry prince|rajmahal\",\n",
      "    \"golden curry\",\n",
      "    \"missing sock\",\n",
      "    \"frankie and bennys\",\n",
      "    \"kymmoy\",\n",
      "    \"cam\",\n",
      "    \"dojo noodle bar|j restaurant\",\n",
      "    \"the bedouin\",\n",
      "    \"restaurant alimentum\",\n",
      "    \"gourmet burger kitchen\",\n",
      "    \"dojo noodle  bar|j restaurant\",\n",
      "    \"la margherita\",\n",
      "    \"golden house\",\n",
      "    \"chiquito\",\n",
      "    \"darrys cookhouse and wine shop\",\n",
      "    \"scudamores punt\",\n",
      "    \"eraina\",\n",
      "    \"the varsity restaurant\",\n",
      "    \"pizza hut fenditton\",\n",
      "    \"saint johns chop house\",\n",
      "    \"curry garden\",\n",
      "    \"sala thong|bangkok city\",\n",
      "    \"jinling noodle bar\",\n",
      "    \"sitar tandoori\",\n",
      "    \"pizza hut\",\n",
      "    \"the Nirala\",\n",
      "    \"city stop restaurant\",\n",
      "    \"de luca cucina and bar riverside brasserie\",\n",
      "    \"rice boat\",\n",
      "    \"wise buddha\",\n",
      "    \"restaurant one seven\",\n",
      "    \"meze bar restaurant\",\n",
      "    \"yipee noodle bar\",\n",
      "    \"little seoul\",\n",
      "    \"rajmahal\",\n",
      "    \"ali baba\",\n",
      "    \"limehouse\",\n",
      "    \"the grafton hotel\",\n",
      "    \"barbakan\",\n",
      "    \"sesame restaurant and bar\",\n",
      "    \"golden wok\",\n",
      "    \"pizza hut city centre\",\n",
      "    \"binh\",\n",
      "    \"hotel du vin and bistro\",\n",
      "    \"lan hong house\",\n",
      "    \"tandoori Palace\",\n",
      "    \"alimentum\",\n",
      "    \"the oak bistro\",\n",
      "    \"anatolia\",\n",
      "    \"ian hong house\",\n",
      "    \"two two and cote\",\n",
      "    \"curry queen\",\n",
      "    \"backstreet bistro\",\n",
      "    \"mahal\",\n",
      "    \"la mimosa\",\n",
      "    \"shanghai family restaurant\",\n",
      "    \"molecular gastronomy\",\n",
      "    \"efes\",\n",
      "    \"cote\",\n",
      "    \"good luck\",\n",
      "    \"cafe uno\",\n",
      "    \"oak bistro\",\n",
      "    \"european\",\n",
      "    \"saffron brasserie\",\n",
      "    \"gardenia\",\n",
      "    \"de luca cucina and bar\",\n",
      "    \"two two\",\n",
      "    \"ashley hotel\",\n",
      "    \"the hotpot\",\n",
      "    \"michaelhouse cafe\",\n",
      "    \"yu garden\",\n",
      "    \"gourmet formal kitchen\",\n",
      "    \"lovel\",\n",
      "    \"efes restaurant\",\n",
      "    \"the peking restaurant: \",\n",
      "    \"river bar steakhouse and grill\",\n",
      "    \"j restaurant\",\n",
      "    \"the slug and lettuce\",\n",
      "    \"yippee noodle bar\",\n",
      "    \"meze bar\",\n",
      "    \"rice house\",\n",
      "    \"clowns cafe\",\n",
      "    \"no\",\n",
      "    \"hobsons house\",\n",
      "    \"dojo noodle bar\",\n",
      "    \"the kohinoor\",\n",
      "    \"royal standard\",\n",
      "    \"bloomsbury restaurant\",\n",
      "    \"graffiti\",\n",
      "    \"el shaddia guesthouse\",\n",
      "    \"worth house\",\n",
      "    \"shiraz\",\n",
      "    \"the golden house\",\n",
      "    \"shanghai\",\n",
      "    \"don pasquale pizzeria\",\n",
      "    \"tandoori palace\",\n",
      "    \"autumn house\",\n",
      "    \"panahar\",\n",
      "    \"the peking\",\n",
      "    \"wagamama\",\n",
      "    \"tandoori\",\n",
      "    \"nusha\",\n",
      "    \"the gandhi\",\n",
      "    \"nandos city centre\",\n",
      "    \"slug and lettuce\",\n",
      "    \"sitar\",\n",
      "    \"alex\",\n",
      "    \"meghna\",\n",
      "    \"cambridge chop house\",\n",
      "    \"the missing sock\",\n",
      "    \"primavera\",\n",
      "    \"the meze bar\",\n",
      "    \"travellers rest\",\n",
      "    \"south\",\n",
      "    \"curry king\",\n",
      "    \"pipasha restaurant\",\n",
      "    \"cambridge punter\",\n",
      "    \"saigon city\",\n",
      "    \"bedouin\",\n",
      "    \"pizza express\",\n",
      "    \"ask\",\n",
      "    \"broughton house gallery\",\n",
      "    \"tang chinese\",\n",
      "    \"curry prince\",\n",
      "    \"dontcare\",\n",
      "    \"charlie\",\n",
      "    \"nirala\",\n",
      "    \"nandos\",\n",
      "    \"cotto\",\n",
      "    \"parkside pools\",\n",
      "    \"galleria\",\n",
      "    \"adden\",\n",
      "    \"funky\",\n",
      "    \"maharajah tandoori restaurant\",\n",
      "    \"nil\",\n",
      "    \"the cow pizza kitchen and bar\",\n",
      "    \"bridge\",\n",
      "    \"grafton hotel\",\n",
      "    \"india west\",\n",
      "    \"cow pizza kitchen and bar\",\n",
      "    \"midsummer house restaurant\",\n",
      "    \"cambridge be\",\n",
      "    \"the lucky star\",\n",
      "    \"hakka\",\n",
      "    \"prezzo\",\n",
      "    \"cambridge lodge restaurant\",\n",
      "    \"sala thong\",\n",
      "    \"Kohinoor\",\n",
      "    \"zizzi cambridge\",\n",
      "    \"pizza hut cherry hinton\",\n",
      "    \"4 kings parade city centre\",\n",
      "    \"golden wok|nirala\",\n",
      "    \"nus\",\n",
      "    \"fitzbillies restaurant\",\n",
      "    \"lucky star\",\n",
      "    \"la tasca\",\n",
      "    \"loch fyne\",\n",
      "    \"cityr\",\n",
      "    \"the gardenia\",\n",
      "    \"bangkok city\",\n",
      "    \"chiquito restaurant bar\",\n",
      "    \"anatolia and efes restaurant\",\n",
      "    \"ugly duckling\",\n",
      "    \"cocum\",\n",
      "    \"hk fusion\",\n",
      "    \"stazione restaurant and coffee bar\",\n",
      "    \"restaurant 22\",\n",
      "    \"grafton hotel restaurant\",\n",
      "    \"the maharajah tandoor\",\n",
      "    \"the alex\",\n",
      "    \"thanh binh\",\n",
      "    \"the river bar steakhouse and grill\",\n",
      "    \"india house\",\n",
      "    \"peking restaurant\",\n",
      "    \"hotpot\",\n",
      "    \"kohinoor\",\n",
      "    \"la raza\",\n",
      "    \"da vinci pizzeria\",\n",
      "    \"pizza hut cherry hinton|alimentum\",\n",
      "    \"royal spice\",\n",
      "    \"riverside brasserie\",\n",
      "    \"kitchen and bar\"\n",
      "  ],\n",
      "  \"restaurant-pricerange\": [\n",
      "    \"expensive\",\n",
      "    \"cheap\",\n",
      "    \"moderate\",\n",
      "    \"dontcare\",\n",
      "    \"moderate|cheap\"\n",
      "  ]\n",
      "}\n",
      "First 4 turns:\n",
      "  Turn 0 (User):\n",
      "    Text: I 'm looking for a British restaurant , in the expensive price range .\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: We have several options available . Is there a certain area of town you are looking for ? If not , I recommend Graffiti in the west .\n",
      "    Dialogue Acts: restaurant-request(area, ?); restaurant-recommend(area, west, name, graffiti); restaurant-inform(choice, several)\n",
      "    State: {'taxi': {'leaveAt': '', 'destination': '', 'departure': '', 'arriveBy': ''}, 'restaurant': {'food': 'british', 'pricerange': 'expensive', 'name': 'not mentioned', 'area': 'not mentioned'}, 'hospital': {'department': ''}, 'hotel': {'name': '', 'area': '', 'parking': '', 'pricerange': '', 'stars': '', 'internet': '', 'type': ''}, 'attraction': {'type': '', 'name': '', 'area': ''}, 'train': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}}\n",
      "  Turn 2 (User):\n",
      "    Text: Graffiti sounds good . I will need a table for four on Thursday at 16:45 , please .\n",
      "    Dialogue Acts: booking-book(ref, s3q9j1kj)\n",
      "  Turn 3 (System):\n",
      "    Text: It is all booked , reference number is S3Q9J1KJ\n",
      "    Dialogue Acts: general-bye(none, none); general-welcome(none, none)\n",
      "    State: {'taxi': {'leaveAt': '', 'destination': '', 'departure': '', 'arriveBy': ''}, 'restaurant': {'food': 'british', 'pricerange': 'expensive', 'name': 'graffiti', 'area': 'not mentioned'}, 'hospital': {'department': ''}, 'hotel': {'name': '', 'area': '', 'parking': '', 'pricerange': '', 'stars': '', 'internet': '', 'type': ''}, 'attraction': {'type': '', 'name': '', 'area': ''}, 'train': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}}\n",
      "\n",
      "Dialogue ID: PMUL4781.json\n",
      "Domain: restaurant\n",
      "    Ontology for domain 'restaurant': {\n",
      "  \"restaurant-book day\": [\n",
      "    \"sunday|thursday\",\n",
      "    \"wednesday\",\n",
      "    \"saturday\",\n",
      "    \"sunday\",\n",
      "    \"saturday|thursday\",\n",
      "    \"friday\",\n",
      "    \"dontcare\",\n",
      "    \"monday\",\n",
      "    \"thursday\",\n",
      "    \"tuesday\"\n",
      "  ],\n",
      "  \"restaurant-book people\": [\n",
      "    \"2\",\n",
      "    \"7\",\n",
      "    \"8\",\n",
      "    \"5\",\n",
      "    \"1\",\n",
      "    \"6\",\n",
      "    \"4|7\",\n",
      "    \"3\",\n",
      "    \"4\"\n",
      "  ],\n",
      "  \"restaurant-book time\": [\n",
      "    \"12:00\",\n",
      "    \"13:30\",\n",
      "    \"10:00\",\n",
      "    \"21:00\",\n",
      "    \"7pm\",\n",
      "    \"1345\",\n",
      "    \"19:30\",\n",
      "    \"21:45\",\n",
      "    \"15:00\",\n",
      "    \"01:00\",\n",
      "    \"16:30\",\n",
      "    \"14:15\",\n",
      "    \"03:00\",\n",
      "    \"10:45\",\n",
      "    \"18:30\",\n",
      "    \"15:15\",\n",
      "    \"17:00|16:00\",\n",
      "    \"4pm\",\n",
      "    \"15:30|16:30\",\n",
      "    \"9\",\n",
      "    \"14:00\",\n",
      "    \"17:45\",\n",
      "    \"09:45\",\n",
      "    \"1715\",\n",
      "    \"16:45\",\n",
      "    \"12:45\",\n",
      "    \"1330\",\n",
      "    \"20:45\",\n",
      "    \"11:00\",\n",
      "    \"12:15\",\n",
      "    \"10:30\",\n",
      "    \"16:15\",\n",
      "    \"15:45\",\n",
      "    \"11:30\",\n",
      "    \"20:00\",\n",
      "    \"19:45\",\n",
      "    \"10:48\",\n",
      "    \"13:45\",\n",
      "    \"18:00|12:15\",\n",
      "    \"18:15\",\n",
      "    \"06:30\",\n",
      "    \"20:30\",\n",
      "    \"11:30|12:30\",\n",
      "    \"17:30\",\n",
      "    \"12:30\",\n",
      "    \"09:15\",\n",
      "    \"14:45\",\n",
      "    \"dontcare\",\n",
      "    \"13:00\",\n",
      "    \"14:30\",\n",
      "    \"20:15\",\n",
      "    \"10:15\",\n",
      "    \"09:30\",\n",
      "    \"18:00\",\n",
      "    \"14:40\",\n",
      "    \"09:00\",\n",
      "    \"17:00\",\n",
      "    \"16:00\",\n",
      "    \"17:30|16:30\",\n",
      "    \"13:15\",\n",
      "    \"1430\",\n",
      "    \"1145\",\n",
      "    \"8pm\",\n",
      "    \"08:45\",\n",
      "    \"11:15\",\n",
      "    \"22:00\",\n",
      "    \"18:45\",\n",
      "    \"19:15\",\n",
      "    \"17:15\",\n",
      "    \"15:30\",\n",
      "    \"19:00\",\n",
      "    \"11:45\"\n",
      "  ],\n",
      "  \"restaurant-area\": [\n",
      "    \"north\",\n",
      "    \"east\",\n",
      "    \"dontcare\",\n",
      "    \"south\",\n",
      "    \"centre\",\n",
      "    \"west\",\n",
      "    \"east|south\"\n",
      "  ],\n",
      "  \"restaurant-food\": [\n",
      "    \"north american>indian\",\n",
      "    \"portugese\",\n",
      "    \"polynesian\",\n",
      "    \"austrian\",\n",
      "    \"greek\",\n",
      "    \"asian\",\n",
      "    \"modern global\",\n",
      "    \"korean\",\n",
      "    \"danish\",\n",
      "    \"chinese\",\n",
      "    \"bistro\",\n",
      "    \"afternoon tea\",\n",
      "    \"kosher|british\",\n",
      "    \"tuscan\",\n",
      "    \"indonesian\",\n",
      "    \"spanish|portuguese\",\n",
      "    \"north african\",\n",
      "    \"north indian\",\n",
      "    \"the americas\",\n",
      "    \"italian\",\n",
      "    \"romanian\",\n",
      "    \"welsh\",\n",
      "    \"french\",\n",
      "    \"modern american\",\n",
      "    \"new zealand\",\n",
      "    \"sri lankan\",\n",
      "    \"traditional american\",\n",
      "    \"thai\",\n",
      "    \"fusion\",\n",
      "    \"molecular gastronomy\",\n",
      "    \"brazilian|portuguese\",\n",
      "    \"south african\",\n",
      "    \"european\",\n",
      "    \"kosher\",\n",
      "    \"jamaican>chinese\",\n",
      "    \"modern english\",\n",
      "    \"italian|indian\",\n",
      "    \"mexican\",\n",
      "    \"turkish\",\n",
      "    \"australian|indian\",\n",
      "    \"halal\",\n",
      "    \"german\",\n",
      "    \"dojo noodle bar\",\n",
      "    \"thai and chinese\",\n",
      "    \"international\",\n",
      "    \"australian\",\n",
      "    \"indian|african\",\n",
      "    \"chinese|mexican\",\n",
      "    \"american\",\n",
      "    \"north american\",\n",
      "    \"eastern european\",\n",
      "    \"caribbean>indian\",\n",
      "    \"afghan\",\n",
      "    \"seafood\",\n",
      "    \"moroccan\",\n",
      "    \"russian\",\n",
      "    \"latin american\",\n",
      "    \"creative\",\n",
      "    \"eritrean\",\n",
      "    \"unusual\",\n",
      "    \"caribbean\",\n",
      "    \"spanish\",\n",
      "    \"sushi\",\n",
      "    \"vegetarian\",\n",
      "    \"corsica\",\n",
      "    \"barbeque>modern european\",\n",
      "    \"light bites\",\n",
      "    \"world\",\n",
      "    \"barbeque\",\n",
      "    \"vietnamese\",\n",
      "    \"dontcare\",\n",
      "    \"african\",\n",
      "    \"south indian\",\n",
      "    \"panasian\",\n",
      "    \"persian\",\n",
      "    \"modern eclectic\",\n",
      "    \"gastropub\",\n",
      "    \"japanese\",\n",
      "    \"cuban\",\n",
      "    \"middle eastern\",\n",
      "    \"canapes\",\n",
      "    \"lebanese\",\n",
      "    \"jamaican\",\n",
      "    \"northern european\",\n",
      "    \"basque\",\n",
      "    \"scottish\",\n",
      "    \"christmas\",\n",
      "    \"irish\",\n",
      "    \"indian\",\n",
      "    \"belgian\",\n",
      "    \"british\",\n",
      "    \"hungarian\",\n",
      "    \"catalan\",\n",
      "    \"english\",\n",
      "    \"crossover\",\n",
      "    \"mediterranean\",\n",
      "    \"malaysian\",\n",
      "    \"swiss\",\n",
      "    \"swedish\",\n",
      "    \"polish\",\n",
      "    \"brazilian\",\n",
      "    \"traditional\",\n",
      "    \"steakhouse\",\n",
      "    \"singaporean\",\n",
      "    \"asian oriental\",\n",
      "    \"venetian\",\n",
      "    \"scandinavian\",\n",
      "    \"cantonese\",\n",
      "    \"modern european\"\n",
      "  ],\n",
      "  \"restaurant-name\": [\n",
      "    \"copper kettle\",\n",
      "    \"taj tandoori\",\n",
      "    \"charlie chan\",\n",
      "    \"one seven\",\n",
      "    \"dif\",\n",
      "    \"eraina and michaelhouse cafe\",\n",
      "    \"mahal of cambridge\",\n",
      "    \"restaurant two two\",\n",
      "    \"curry prince|rajmahal\",\n",
      "    \"golden curry\",\n",
      "    \"missing sock\",\n",
      "    \"frankie and bennys\",\n",
      "    \"kymmoy\",\n",
      "    \"cam\",\n",
      "    \"dojo noodle bar|j restaurant\",\n",
      "    \"the bedouin\",\n",
      "    \"restaurant alimentum\",\n",
      "    \"gourmet burger kitchen\",\n",
      "    \"dojo noodle  bar|j restaurant\",\n",
      "    \"la margherita\",\n",
      "    \"golden house\",\n",
      "    \"chiquito\",\n",
      "    \"darrys cookhouse and wine shop\",\n",
      "    \"scudamores punt\",\n",
      "    \"eraina\",\n",
      "    \"the varsity restaurant\",\n",
      "    \"pizza hut fenditton\",\n",
      "    \"saint johns chop house\",\n",
      "    \"curry garden\",\n",
      "    \"sala thong|bangkok city\",\n",
      "    \"jinling noodle bar\",\n",
      "    \"sitar tandoori\",\n",
      "    \"pizza hut\",\n",
      "    \"the Nirala\",\n",
      "    \"city stop restaurant\",\n",
      "    \"de luca cucina and bar riverside brasserie\",\n",
      "    \"rice boat\",\n",
      "    \"wise buddha\",\n",
      "    \"restaurant one seven\",\n",
      "    \"meze bar restaurant\",\n",
      "    \"yipee noodle bar\",\n",
      "    \"little seoul\",\n",
      "    \"rajmahal\",\n",
      "    \"ali baba\",\n",
      "    \"limehouse\",\n",
      "    \"the grafton hotel\",\n",
      "    \"barbakan\",\n",
      "    \"sesame restaurant and bar\",\n",
      "    \"golden wok\",\n",
      "    \"pizza hut city centre\",\n",
      "    \"binh\",\n",
      "    \"hotel du vin and bistro\",\n",
      "    \"lan hong house\",\n",
      "    \"tandoori Palace\",\n",
      "    \"alimentum\",\n",
      "    \"the oak bistro\",\n",
      "    \"anatolia\",\n",
      "    \"ian hong house\",\n",
      "    \"two two and cote\",\n",
      "    \"curry queen\",\n",
      "    \"backstreet bistro\",\n",
      "    \"mahal\",\n",
      "    \"la mimosa\",\n",
      "    \"shanghai family restaurant\",\n",
      "    \"molecular gastronomy\",\n",
      "    \"efes\",\n",
      "    \"cote\",\n",
      "    \"good luck\",\n",
      "    \"cafe uno\",\n",
      "    \"oak bistro\",\n",
      "    \"european\",\n",
      "    \"saffron brasserie\",\n",
      "    \"gardenia\",\n",
      "    \"de luca cucina and bar\",\n",
      "    \"two two\",\n",
      "    \"ashley hotel\",\n",
      "    \"the hotpot\",\n",
      "    \"michaelhouse cafe\",\n",
      "    \"yu garden\",\n",
      "    \"gourmet formal kitchen\",\n",
      "    \"lovel\",\n",
      "    \"efes restaurant\",\n",
      "    \"the peking restaurant: \",\n",
      "    \"river bar steakhouse and grill\",\n",
      "    \"j restaurant\",\n",
      "    \"the slug and lettuce\",\n",
      "    \"yippee noodle bar\",\n",
      "    \"meze bar\",\n",
      "    \"rice house\",\n",
      "    \"clowns cafe\",\n",
      "    \"no\",\n",
      "    \"hobsons house\",\n",
      "    \"dojo noodle bar\",\n",
      "    \"the kohinoor\",\n",
      "    \"royal standard\",\n",
      "    \"bloomsbury restaurant\",\n",
      "    \"graffiti\",\n",
      "    \"el shaddia guesthouse\",\n",
      "    \"worth house\",\n",
      "    \"shiraz\",\n",
      "    \"the golden house\",\n",
      "    \"shanghai\",\n",
      "    \"don pasquale pizzeria\",\n",
      "    \"tandoori palace\",\n",
      "    \"autumn house\",\n",
      "    \"panahar\",\n",
      "    \"the peking\",\n",
      "    \"wagamama\",\n",
      "    \"tandoori\",\n",
      "    \"nusha\",\n",
      "    \"the gandhi\",\n",
      "    \"nandos city centre\",\n",
      "    \"slug and lettuce\",\n",
      "    \"sitar\",\n",
      "    \"alex\",\n",
      "    \"meghna\",\n",
      "    \"cambridge chop house\",\n",
      "    \"the missing sock\",\n",
      "    \"primavera\",\n",
      "    \"the meze bar\",\n",
      "    \"travellers rest\",\n",
      "    \"south\",\n",
      "    \"curry king\",\n",
      "    \"pipasha restaurant\",\n",
      "    \"cambridge punter\",\n",
      "    \"saigon city\",\n",
      "    \"bedouin\",\n",
      "    \"pizza express\",\n",
      "    \"ask\",\n",
      "    \"broughton house gallery\",\n",
      "    \"tang chinese\",\n",
      "    \"curry prince\",\n",
      "    \"dontcare\",\n",
      "    \"charlie\",\n",
      "    \"nirala\",\n",
      "    \"nandos\",\n",
      "    \"cotto\",\n",
      "    \"parkside pools\",\n",
      "    \"galleria\",\n",
      "    \"adden\",\n",
      "    \"funky\",\n",
      "    \"maharajah tandoori restaurant\",\n",
      "    \"nil\",\n",
      "    \"the cow pizza kitchen and bar\",\n",
      "    \"bridge\",\n",
      "    \"grafton hotel\",\n",
      "    \"india west\",\n",
      "    \"cow pizza kitchen and bar\",\n",
      "    \"midsummer house restaurant\",\n",
      "    \"cambridge be\",\n",
      "    \"the lucky star\",\n",
      "    \"hakka\",\n",
      "    \"prezzo\",\n",
      "    \"cambridge lodge restaurant\",\n",
      "    \"sala thong\",\n",
      "    \"Kohinoor\",\n",
      "    \"zizzi cambridge\",\n",
      "    \"pizza hut cherry hinton\",\n",
      "    \"4 kings parade city centre\",\n",
      "    \"golden wok|nirala\",\n",
      "    \"nus\",\n",
      "    \"fitzbillies restaurant\",\n",
      "    \"lucky star\",\n",
      "    \"la tasca\",\n",
      "    \"loch fyne\",\n",
      "    \"cityr\",\n",
      "    \"the gardenia\",\n",
      "    \"bangkok city\",\n",
      "    \"chiquito restaurant bar\",\n",
      "    \"anatolia and efes restaurant\",\n",
      "    \"ugly duckling\",\n",
      "    \"cocum\",\n",
      "    \"hk fusion\",\n",
      "    \"stazione restaurant and coffee bar\",\n",
      "    \"restaurant 22\",\n",
      "    \"grafton hotel restaurant\",\n",
      "    \"the maharajah tandoor\",\n",
      "    \"the alex\",\n",
      "    \"thanh binh\",\n",
      "    \"the river bar steakhouse and grill\",\n",
      "    \"india house\",\n",
      "    \"peking restaurant\",\n",
      "    \"hotpot\",\n",
      "    \"kohinoor\",\n",
      "    \"la raza\",\n",
      "    \"da vinci pizzeria\",\n",
      "    \"pizza hut cherry hinton|alimentum\",\n",
      "    \"royal spice\",\n",
      "    \"riverside brasserie\",\n",
      "    \"kitchen and bar\"\n",
      "  ],\n",
      "  \"restaurant-pricerange\": [\n",
      "    \"expensive\",\n",
      "    \"cheap\",\n",
      "    \"moderate\",\n",
      "    \"dontcare\",\n",
      "    \"moderate|cheap\"\n",
      "  ]\n",
      "}\n",
      "First 4 turns:\n",
      "  Turn 0 (User):\n",
      "    Text: I 'm looking for a cheap restaurant in the center of the town .\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: We have lots of cheap places in the center of town , but I recommend The River Bar Steakhouse and Grill .\n",
      "    Dialogue Acts: restaurant-recommend(name, river bar steakhouse and grill); restaurant-inform(price, cheap, choice, lots, area, center)\n",
      "    State: {'taxi': {'leaveAt': '', 'destination': '', 'departure': '', 'arriveBy': ''}, 'restaurant': {'food': 'not mentioned', 'pricerange': 'cheap', 'name': 'not mentioned', 'area': 'centre'}, 'bus': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}, 'hospital': {'department': ''}, 'hotel': {'name': '', 'area': '', 'parking': '', 'pricerange': '', 'stars': '', 'internet': '', 'type': ''}, 'attraction': {'type': '', 'name': '', 'area': ''}, 'train': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}}\n",
      "  Turn 2 (User):\n",
      "    Text: want to book a table for 5 people at 12:00 on sunday and a confirm number place\n",
      "    Dialogue Acts: booking-book(ref, p5u2xrzd)\n",
      "  Turn 3 (System):\n",
      "    Text: Booking was successful . The table will be reserved for 15 minutes . Reference number : P5U2XRZD .\n",
      "    Dialogue Acts: hotel-request(price, ?, area, ?); hotel-select(type, hotel, type, guesthouse)\n",
      "    State: {'taxi': {'leaveAt': '', 'destination': '', 'departure': '', 'arriveBy': ''}, 'restaurant': {'food': 'not mentioned', 'pricerange': 'cheap', 'name': 'the river bar steakhouse and grill', 'area': 'centre'}, 'bus': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}, 'hospital': {'department': ''}, 'hotel': {'name': '', 'area': '', 'parking': '', 'pricerange': '', 'stars': '', 'internet': '', 'type': ''}, 'attraction': {'type': '', 'name': '', 'area': ''}, 'train': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}}\n",
      "\n",
      " Randomly Checking 2 Dialogues (First 4 Turns) from Processed Test \n",
      "\n",
      "Dialogue ID: PMUL3216.json\n",
      "Domain: hotel\n",
      "    Ontology for domain 'hotel': {\n",
      "  \"hotel-book day\": [\n",
      "    \"saturday\",\n",
      "    \"wednesday\",\n",
      "    \"sunday\",\n",
      "    \"friday\",\n",
      "    \"saturday|tuesday\",\n",
      "    \"wednesday|friday\",\n",
      "    \"dontcare\",\n",
      "    \"monday<thursday\",\n",
      "    \"monday\",\n",
      "    \"thursday\",\n",
      "    \"tuesday\",\n",
      "    \"friday>tuesday\",\n",
      "    \"sunday>monday\"\n",
      "  ],\n",
      "  \"hotel-book people\": [\n",
      "    \"2\",\n",
      "    \"7\",\n",
      "    \"8\",\n",
      "    \"5\",\n",
      "    \"1\",\n",
      "    \"6\",\n",
      "    \"3\",\n",
      "    \"4\"\n",
      "  ],\n",
      "  \"hotel-book stay\": [\n",
      "    \"2\",\n",
      "    \"8\",\n",
      "    \"7\",\n",
      "    \"5\",\n",
      "    \"5|4\",\n",
      "    \"1\",\n",
      "    \"6\",\n",
      "    \"3\",\n",
      "    \"3|1\",\n",
      "    \"4\"\n",
      "  ],\n",
      "  \"hotel-area\": [\n",
      "    \"north\",\n",
      "    \"east\",\n",
      "    \"south\",\n",
      "    \"dontcare\",\n",
      "    \"centre\",\n",
      "    \"west\",\n",
      "    \"west|centre\"\n",
      "  ],\n",
      "  \"hotel-internet\": [\n",
      "    \"free\",\n",
      "    \"dontcare\",\n",
      "    \"no\",\n",
      "    \"yes\"\n",
      "  ],\n",
      "  \"hotel-name\": [\n",
      "    \"a and b guest house\",\n",
      "    \"aylesbray lodge guest house\",\n",
      "    \"aylesbray lodge guest house|rosas bed and breakfast\",\n",
      "    \"la margherit\",\n",
      "    \"leverton house\",\n",
      "    \"bridge guest house|aylesbray lodge guest house\",\n",
      "    \"eraina\",\n",
      "    \"holiday inn\",\n",
      "    \"yes\",\n",
      "    \"huntingdon marriott hotel\",\n",
      "    \"acorn guesthouse\",\n",
      "    \"university arms hotel\",\n",
      "    \"wankworth house\",\n",
      "    \"archway house\",\n",
      "    \"cambridge belfry\",\n",
      "    \"home from home\",\n",
      "    \"the allenbell|autumn house|leverton house\",\n",
      "    \"limehouse\",\n",
      "    \"super 5\",\n",
      "    \"anatolia\",\n",
      "    \"huntingdon marriott hotel|university arms hotel\",\n",
      "    \"bridge guest house\",\n",
      "    \"finches b and b\",\n",
      "    \"a and b guesthouse\",\n",
      "    \"the cambridge belfry\",\n",
      "    \"cote\",\n",
      "    \"sleeperz\",\n",
      "    \"gonville hotel|the lensfield hotel\",\n",
      "    \"doubletree by hilton cambridge\",\n",
      "    \"carolina bed and breakfast\",\n",
      "    \"the alpha-milton|the hamilton lodge\",\n",
      "    \"alexander bed and breakfast|university arms hotel\",\n",
      "    \"ashley hotel\",\n",
      "    \"allenbell\",\n",
      "    \"ashley hotel|lovell lodge\",\n",
      "    \"acorn guest house\",\n",
      "    \"no\",\n",
      "    \"the worth house\",\n",
      "    \"city centre north b and b\",\n",
      "    \"hobsons house\",\n",
      "    \"huntingdon marriott hotel|cambridge belfry\",\n",
      "    \"el shaddia guesthouse\",\n",
      "    \"worth house\",\n",
      "    \"lensfield hotel\",\n",
      "    \"kirkwood\",\n",
      "    \"cambridge belfray\",\n",
      "    \"autumn house\",\n",
      "    \"alexander bed and breakfast|el shaddai\",\n",
      "    \"nusha\",\n",
      "    \"warkworth house\",\n",
      "    \"wandlebury coutn\",\n",
      "    \"lan hon\",\n",
      "    \"rosas bed and breakfast\",\n",
      "    \"city centre b & b|el shaddai\",\n",
      "    \"whale\",\n",
      "    \"belfy hotel\",\n",
      "    \"north b and b\",\n",
      "    \"warkworth house|autumn house\",\n",
      "    \"hamilton lodge\",\n",
      "    \"the ashley hotel|lovell lodge\",\n",
      "    \"dontcare\",\n",
      "    \"express by holiday inn cambridge\",\n",
      "    \"cityroomz\",\n",
      "    \"cherr\",\n",
      "    \"alexander|el shaddai|gonville hotel|university arms hotl\",\n",
      "    \"alpha milton guest house\",\n",
      "    \"city centre north b and b|el shaddai\",\n",
      "    \"holiday inn cambridge\",\n",
      "    \"alpha-milton guest house\",\n",
      "    \"gonville hotel\",\n",
      "    \"kirkwood house\",\n",
      "    \"marriot hotel\",\n",
      "    \"the gonvile hotel\",\n",
      "    \"avalon\",\n",
      "    \"acorn place\",\n",
      "    \"finches bed and breakfast\",\n",
      "    \"arbury lodge guesthouse\",\n",
      "    \"nus\",\n",
      "    \"the acorn guest house\",\n",
      "    \"alesbray lodge guesthouse\",\n",
      "    \"ashely hotel|lovell lodge\",\n",
      "    \"alexander bed and breakfast\",\n",
      "    \"allenbell|autumn house|leverton house\",\n",
      "    \"allenbell|alexander bed and breakfast\",\n",
      "    \"NOT(hamilton lodge)\",\n",
      "    \"sou\",\n",
      "    \"lovell lodge\",\n",
      "    \"wartworth\",\n",
      "    \"ashley hotel|lovell lodge|cityroomz\"\n",
      "  ],\n",
      "  \"hotel-parking\": [\n",
      "    \"dontcare\",\n",
      "    \"free\",\n",
      "    \"no\",\n",
      "    \"yes\"\n",
      "  ],\n",
      "  \"hotel-pricerange\": [\n",
      "    \"expensive\",\n",
      "    \"cheap\",\n",
      "    \"moderate\",\n",
      "    \"cheap>moderate\",\n",
      "    \"dontcare\",\n",
      "    \"cheap|moderate\",\n",
      "    \"moderate|cheap\",\n",
      "    \"$100\"\n",
      "  ],\n",
      "  \"hotel-stars\": [\n",
      "    \"2\",\n",
      "    \"4|5\",\n",
      "    \"0\",\n",
      "    \"5\",\n",
      "    \"1\",\n",
      "    \"dontcare\",\n",
      "    \"3|4\",\n",
      "    \"3\",\n",
      "    \"4\"\n",
      "  ],\n",
      "  \"hotel-type\": [\n",
      "    \"guesthouse\",\n",
      "    \"bed and breakfast\",\n",
      "    \"dontcare\",\n",
      "    \"hotel|guesthouse\",\n",
      "    \"hotel\"\n",
      "  ]\n",
      "}\n",
      "First 4 turns:\n",
      "  Turn 0 (User):\n",
      "    Text: I need help finding a 4 star hotel\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: I have several listed is there a particular area you would like ?\n",
      "    Dialogue Acts: hotel-request(area, ?); hotel-inform(choice, several)\n",
      "    State: {'taxi': {'leaveAt': '', 'destination': '', 'departure': '', 'arriveBy': ''}, 'restaurant': {'food': '', 'pricerange': '', 'name': '', 'area': ''}, 'bus': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}, 'hospital': {'department': ''}, 'hotel': {'name': 'not mentioned', 'area': 'not mentioned', 'parking': 'not mentioned', 'pricerange': 'not mentioned', 'stars': '4', 'internet': 'not mentioned', 'type': 'not mentioned'}, 'attraction': {'type': '', 'name': '', 'area': ''}, 'train': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}}\n",
      "  Turn 2 (User):\n",
      "    Text: no , but I need free parking and wifi .\n",
      "    Dialogue Acts: hotel-request(price, ?); hotel-inform(choice, 19, type, hotels, area, cambridge)\n",
      "  Turn 3 (System):\n",
      "    Text: I have 19 hotels in Cambridge that fit those requirements . Do you have a price range in mind ?\n",
      "    Dialogue Acts: booking-request(people, ?)\n",
      "    State: {'taxi': {'leaveAt': '', 'destination': '', 'departure': '', 'arriveBy': ''}, 'restaurant': {'food': '', 'pricerange': '', 'name': '', 'area': ''}, 'bus': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}, 'hospital': {'department': ''}, 'hotel': {'name': 'not mentioned', 'area': 'not mentioned', 'parking': 'yes', 'pricerange': 'not mentioned', 'stars': '4', 'internet': 'yes', 'type': 'not mentioned'}, 'attraction': {'type': '', 'name': '', 'area': ''}, 'train': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}}\n",
      "\n",
      "Dialogue ID: PMUL2169.json\n",
      "Domain: attraction\n",
      "    Ontology for domain 'attraction': {\n",
      "  \"attraction-area\": [\n",
      "    \"north\",\n",
      "    \"centre|west\",\n",
      "    \"east\",\n",
      "    \"south\",\n",
      "    \"centre\",\n",
      "    \"west\",\n",
      "    \"dontcare\"\n",
      "  ],\n",
      "  \"attraction-name\": [\n",
      "    \"cambridge arts theater\",\n",
      "    \"scudamores punting co\",\n",
      "    \"trinity college\",\n",
      "    \"history of science museum\",\n",
      "    \"old schools\",\n",
      "    \"christs college|saint catharines\",\n",
      "    \"aylesbray lodge guest house\",\n",
      "    \"scott polar\",\n",
      "    \"churchills college\",\n",
      "    \"museum of archaelogy and anthropology\",\n",
      "    \"cambridge artworks\",\n",
      "    \"pembroke college\",\n",
      "    \"milton country park\",\n",
      "    \"regency gallery\",\n",
      "    \"cambridge contemporary art museum\",\n",
      "    \"little saint marys church\",\n",
      "    \"sheeps green and lammas land park fen causeway\",\n",
      "    \"the junction\",\n",
      "    \"college\",\n",
      "    \"peoples portraits exhibition at girton college\",\n",
      "    \"st catharines college\",\n",
      "    \"kings college|hughes hall\",\n",
      "    \"cambridge and country folk museum\",\n",
      "    \"cineworld cinema\",\n",
      "    \"cambridge arts theatre\",\n",
      "    \"great saint marys church\",\n",
      "    \"university arms hotel\",\n",
      "    \"wandlebury country park\",\n",
      "    \"soul tree nightclub\",\n",
      "    \"churchill\",\n",
      "    \"archway house\",\n",
      "    \"salsa\",\n",
      "    \"home from home\",\n",
      "    \"sheeps green\",\n",
      "    \"scudamores punti co|the cambridge punter\",\n",
      "    \"sidney sussex|gonville and caius\",\n",
      "    \"christs church\",\n",
      "    \"riverboat georgina\",\n",
      "    \"cherry hinton village centre\",\n",
      "    \"mumford theatre\",\n",
      "    \"museum\",\n",
      "    \"castle galleries|primavera\",\n",
      "    \"ruskin gallery\",\n",
      "    \"christs college|downing college\",\n",
      "    \"cambridge corn exchange\",\n",
      "    \"lammas land park\",\n",
      "    \"museum of archaelogy\",\n",
      "    \"king hedges learner pool\",\n",
      "    \"whipple museum of the history of science\",\n",
      "    \"camboats\",\n",
      "    \"scudamores punting co|the cambridge punter\",\n",
      "    \"abbey pool and astroturf\",\n",
      "    \"the wandlebury\",\n",
      "    \"abbey pool and astroturf pitch\",\n",
      "    \"the cambridge corn exchange\",\n",
      "    \"cafe uno\",\n",
      "    \"cherry hinton water play\",\n",
      "    \"cafe jello museum\",\n",
      "    \"saint catharines college\",\n",
      "    \"abbey pool\",\n",
      "    \"museum of classical archaeology\",\n",
      "    \"hughes hall\",\n",
      "    \"whale of a time\",\n",
      "    \"kambar\",\n",
      "    \"vue cinema\",\n",
      "    \"the place\",\n",
      "    \"queens college\",\n",
      "    \"scudamore\",\n",
      "    \"ABC Theatre\",\n",
      "    \"museum of archaelogy and anthropology|fitzwilliam museum\",\n",
      "    \"st christs college\",\n",
      "    \"corn cambridge exchange\",\n",
      "    \"holy trinity church\",\n",
      "    \"the fez club\",\n",
      "    \"christ college\",\n",
      "    \"funky fun house\",\n",
      "    \"lynne strover gallery\",\n",
      "    \"city\",\n",
      "    \"cherry hinton hall\",\n",
      "    \"scudamores punting co|cambridge punters\",\n",
      "    \"fitzwilliam museum\",\n",
      "    \"hobsons house\",\n",
      "    \"club salsa\",\n",
      "    \"tenpin\",\n",
      "    \"worth house\",\n",
      "    \"jesus college\",\n",
      "    \"kings hedges learner pool\",\n",
      "    \"cambridge botanic gardens\",\n",
      "    \"camboats|funky fun house\",\n",
      "    \"emmanuel college\",\n",
      "    \"saint barnabas press gallery\",\n",
      "    \"nusha\",\n",
      "    \"clare hall\",\n",
      "    \"soultree\",\n",
      "    \"boat\",\n",
      "    \"cherry hinton water play park\",\n",
      "    \"county folk museum\",\n",
      "    \"botanic gardens\",\n",
      "    \"sidney sussex college\",\n",
      "    \"the man on the moon\",\n",
      "    \"whale of time\",\n",
      "    \"adc\",\n",
      "    \"man on the moon\",\n",
      "    \"primavera\",\n",
      "    \"jesus green\",\n",
      "    \"cambridge punter\",\n",
      "    \"cherry hinton hall and grounds\",\n",
      "    \"the cambridge artworks\",\n",
      "    \"whippple museum\",\n",
      "    \"free\",\n",
      "    \"cherry hinton water park\",\n",
      "    \"broughton house gallery\",\n",
      "    \"williams art and antiques\",\n",
      "    \"cambridge book and print gallery\",\n",
      "    \"cambridge and county folk museum\",\n",
      "    \"all saints church\",\n",
      "    \"dontcare\",\n",
      "    \"cambridge temporary art\",\n",
      "    \"parkside pools\",\n",
      "    \"nusha|tenpin\",\n",
      "    \"gonville hotel\",\n",
      "    \"pizza\",\n",
      "    \"cambridge university botanic gardens\",\n",
      "    \"castle galleries\",\n",
      "    \"scott polar museum\",\n",
      "    \"jesus green outdoor pool\",\n",
      "    \"cinema cinema\",\n",
      "    \"kettles yard\",\n",
      "    \"clare college\",\n",
      "    \"place\",\n",
      "    \"cambridge\",\n",
      "    \"peoples portraits exhibition\",\n",
      "    \"churchhill college\",\n",
      "    \"byard art\",\n",
      "    \"Cambridge university botanic gardens\",\n",
      "    \"bed\",\n",
      "    \"saint johns college\",\n",
      "    \"bangkok city\",\n",
      "    \"gallery\",\n",
      "    \"tenpin|nusha\",\n",
      "    \"trinity street college\",\n",
      "    \"queens\",\n",
      "    \"museum of archaelogy and anthropogy\",\n",
      "    \"kings college\",\n",
      "    \"adc theatre\",\n",
      "    \"magdalene college\",\n",
      "    \"cambridge contemporary art\",\n",
      "    \"contemporary art museum\",\n",
      "    \"caf\\u008e jello galery\",\n",
      "    \"gonville and caius college\",\n",
      "    \"the cambridge arts theatre\",\n",
      "    \"the castle galleries\",\n",
      "    \"thanh\",\n",
      "    \"corpus christi\",\n",
      "    \"cambridge museum of technology\",\n",
      "    \"the churchill college\",\n",
      "    \"kohinoor\",\n",
      "    \"downing college\",\n",
      "    \"cafe jello gallery\",\n",
      "    \"older churches\",\n",
      "    \"school\",\n",
      "    \"ballare\",\n",
      "    \"cambride and country folk museum\",\n",
      "    \"gallery at twelve a high street\"\n",
      "  ],\n",
      "  \"attraction-type\": [\n",
      "    \"sports\",\n",
      "    \"entertainment|cinemas|museums|theatres\",\n",
      "    \"church\",\n",
      "    \"museum\",\n",
      "    \"theatre\",\n",
      "    \"boat\",\n",
      "    \"architecture\",\n",
      "    \"churchills college\",\n",
      "    \"multiple sports|theatre\",\n",
      "    \"park|boat\",\n",
      "    \"theater\",\n",
      "    \"cinema\",\n",
      "    \"camboats\",\n",
      "    \"concert\",\n",
      "    \"night club\",\n",
      "    \"park\",\n",
      "    \"concerthall\",\n",
      "    \"boating\",\n",
      "    \"hiking|historical\",\n",
      "    \"college\",\n",
      "    \"hotel\",\n",
      "    \"gallery\",\n",
      "    \"entertainment\",\n",
      "    \"dontcare\",\n",
      "    \"museum kettles yard\",\n",
      "    \"concerthall|boat\",\n",
      "    \"museum|nightclub\",\n",
      "    \"special\",\n",
      "    \"swimming pool\",\n",
      "    \"gastropub\",\n",
      "    \"outdoor\",\n",
      "    \"pool\",\n",
      "    \"multiple sports\"\n",
      "  ]\n",
      "}\n",
      "First 4 turns:\n",
      "  Turn 0 (User):\n",
      "    Text: I was hoping that you could tell me more about the museum of classical archaeology .\n",
      "    Dialogue Acts: No acts available\n",
      "  Turn 1 (System):\n",
      "    Text: What I can tell you is that it 's located on Sidgwick Avenue and that there is no entrance fee . Would you like the phone number ?\n",
      "    Dialogue Acts: attraction-inform(fee, no entrance fee, addr, sidgwick avenue)\n",
      "    State: {'taxi': {'leaveAt': '', 'destination': '', 'departure': '', 'arriveBy': ''}, 'restaurant': {'food': '', 'pricerange': '', 'name': '', 'area': ''}, 'bus': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}, 'hospital': {'department': ''}, 'hotel': {'name': '', 'area': '', 'parking': '', 'pricerange': '', 'stars': '', 'internet': '', 'type': ''}, 'attraction': {'type': 'museum', 'name': 'museum of classical archaeology', 'area': 'not mentioned'}, 'train': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}}\n",
      "  Turn 2 (User):\n",
      "    Text: No but what exact area is it in ? Center , east , west etc .\n",
      "    Dialogue Acts: general-reqmore(none, none); attraction-inform(area, west)\n",
      "  Turn 3 (System):\n",
      "    Text: It is in the west area . Can I help you with anything else ?\n",
      "    Dialogue Acts: restaurant-request(price, ?); restaurant-inform(choice, 4)\n",
      "    State: {'taxi': {'leaveAt': '', 'destination': '', 'departure': '', 'arriveBy': ''}, 'restaurant': {'food': '', 'pricerange': '', 'name': '', 'area': ''}, 'bus': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}, 'hospital': {'department': ''}, 'hotel': {'name': '', 'area': '', 'parking': '', 'pricerange': '', 'stars': '', 'internet': '', 'type': ''}, 'attraction': {'type': 'museum', 'name': 'museum of classical archaeology', 'area': 'not mentioned'}, 'train': {'leaveAt': '', 'destination': '', 'day': '', 'arriveBy': '', 'departure': ''}}\n"
     ]
    }
   ],
   "source": [
    "# --- Preprocess code ---\n",
    "def normalize_general_text(text):\n",
    "    \"\"\"\n",
    "    Performs general text normalization on a string.\n",
    "    Expands contractions, removes extra spaces, and standardizes punctuation.\n",
    "    \"\"\"\n",
    "    # Use the contractions library to expand contractions\n",
    "    text = contractions.fix(text, slang=False)\n",
    "    \n",
    "    # Add lower letter for text normalization\n",
    "    # text = text.lower()\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Add a space before punctuation to aid tokenization later\n",
    "    text = re.sub(r'([.?!,])', r' \\1', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_time_in_text(text):\n",
    "    def time_repl_hhmm_ampm(match):\n",
    "        try:\n",
    "            parsed_time = dateparser.parse(match.group(0))\n",
    "            return parsed_time.strftime('%H:%M') if parsed_time else match.group(0)\n",
    "        except:\n",
    "            return match.group(0)\n",
    "    text = re.sub(r'\\b(\\d{1,2}:\\d{2})\\s*(am|pm)\\b', time_repl_hhmm_ampm, text, flags=re.IGNORECASE)\n",
    "\n",
    "    def time_repl_hh_ampm(match):\n",
    "        try:\n",
    "            parsed_time = dateparser.parse(match.group(0))\n",
    "            return parsed_time.strftime('%H:%M') if parsed_time else match.group(0)\n",
    "        except:\n",
    "            return match.group(0)\n",
    "    text = re.sub(r'\\b(\\d{1,2})\\s*(am|pm)\\b', time_repl_hh_ampm, text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bnoon\\b', '12:00', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bmidday\\b', '12:00', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bmidnight\\b', '00:00', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "CURRENCY_SYMBOLS_MAP = {\n",
    "    '': 'GBP',\n",
    "    '$': 'USD',\n",
    "    '': 'EUR',\n",
    "    '': 'JPY',\n",
    "    '': 'INR',\n",
    "}\n",
    "\n",
    "CURRENCY_KEYWORDS_MAP = {\n",
    "    'pound': 'GBP', 'pounds': 'GBP', 'quid': 'GBP', 'gbp': 'GBP', 'sterling': 'GBP',\n",
    "    'dollar': 'USD', 'dollars': 'USD', 'buck': 'USD', 'bucks': 'USD', 'usd': 'USD',\n",
    "    'euro': 'EUR', 'euros': 'EUR', 'eur': 'EUR',\n",
    "    'yen': 'JPY', 'jpy': 'JPY',\n",
    "    'rupee': 'INR', 'rupees': 'INR', 'inr': 'INR',\n",
    "}\n",
    "\n",
    "ALL_CURRENCY_KEYWORDS_SORTED = sorted(CURRENCY_KEYWORDS_MAP.keys(), key=len, reverse=True)\n",
    "ALL_CURRENCY_SYMBOLS_SORTED = sorted(CURRENCY_SYMBOLS_MAP.keys(), key=len, reverse=True)\n",
    "\n",
    "NUMBER_WORDS_FOR_REGEX = (\n",
    "    r\"zero|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|\"\n",
    "    r\"thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|\"\n",
    "    r\"thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion\"\n",
    ")\n",
    "\n",
    "COMPLEX_NUMBER_WORDS_PATTERN = rf\"\\b((?:{NUMBER_WORDS_FOR_REGEX})(?:\\s+(?:and\\s+)?(?:{NUMBER_WORDS_FOR_REGEX}))*)\\b\"\n",
    "\n",
    "def normalize_currency_in_text(text):\n",
    "    \"\"\"\n",
    "    Normalizes currency expressions in a string.\n",
    "    Examples: \"twenty pounds\" -> \"20 GBP\", \"20\" -> \"20 GBP\"\n",
    "    \"\"\"\n",
    "    def word_num_keyword_replacer(match):\n",
    "        num_word_str = match.group(1)\n",
    "        currency_key_str = match.group(2).lower()\n",
    "        try:\n",
    "            # Use w2n to convert word to number\n",
    "            num_val = w2n.word_to_num(num_word_str)\n",
    "            currency_code = CURRENCY_KEYWORDS_MAP.get(currency_key_str, CURRENCY_KEYWORDS_MAP.get(currency_key_str.rstrip('s'), currency_key_str.upper()))\n",
    "            return f\"{num_val} {currency_code.upper()}\"\n",
    "        except ValueError:\n",
    "            return match.group(0)\n",
    "\n",
    "    currency_keywords_regex_part = \"|\".join([re.escape(k) for k in ALL_CURRENCY_KEYWORDS_SORTED])\n",
    "    pattern_word_num_then_keyword = rf\"({COMPLEX_NUMBER_WORDS_PATTERN})\\s+({currency_keywords_regex_part})\\b\"\n",
    "    text = re.sub(pattern_word_num_then_keyword, word_num_keyword_replacer, text, flags=re.IGNORECASE)\n",
    "\n",
    "    for symbol in ALL_CURRENCY_SYMBOLS_SORTED:\n",
    "        code = CURRENCY_SYMBOLS_MAP[symbol]\n",
    "        text = re.sub(rf'{re.escape(symbol)}\\s*(\\d+\\.?\\d*)', rf'\\1 {code}', text)\n",
    "        text = re.sub(rf'(\\d+\\.?\\d*)\\s*{re.escape(symbol)}', rf'\\1 {code}', text)\n",
    "\n",
    "    for keyword in ALL_CURRENCY_KEYWORDS_SORTED:\n",
    "        code = CURRENCY_KEYWORDS_MAP[keyword]\n",
    "        text = re.sub(rf'(\\d+\\.?\\d*)\\s+{re.escape(keyword)}\\b', rf'\\1 {code}', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(rf'\\b{re.escape(keyword)}\\s+(\\d+\\.?\\d*)', rf'\\1 {code}', text, flags=re.IGNORECASE)\n",
    "\n",
    "    all_target_codes = set(CURRENCY_SYMBOLS_MAP.values()) | set(CURRENCY_KEYWORDS_MAP.values())\n",
    "    for code_val in all_target_codes:\n",
    "        text = re.sub(rf'\\b{re.escape(code_val)}\\b', code_val.upper(), text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'(\\d)([A-Z]{3}\\b)', r'\\1 \\2', text)\n",
    "    text = re.sub(r'(\\b[A-Z]{3})(\\d)', r'\\1 \\2', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def flatten_dialogue(dialogue_data):\n",
    "    flattened_turns = []\n",
    "    turns = dialogue_data.get(\"log\", [])\n",
    "    for turn_idx, turn in enumerate(turns):\n",
    "        raw_text = turn.get(\"text\", \"\")\n",
    "        # Apply general text normalization first\n",
    "        normalized_text = normalize_general_text(raw_text)\n",
    "        # Apply existing normalizations to the already normalized text\n",
    "        normalized_text = normalize_time_in_text(normalized_text)\n",
    "        normalized_text = normalize_currency_in_text(normalized_text)\n",
    "\n",
    "        is_user_turn = turn_idx % 2 == 0\n",
    "        \n",
    "        # Extract state information from MultiWOZ structure\n",
    "        metadata = turn.get(\"metadata\", {})\n",
    "        dialogue_state = {}\n",
    "        \n",
    "        # Extract state information from metadata\n",
    "        for domain, domain_data in metadata.items():\n",
    "            if isinstance(domain_data, dict) and \"semi\" in domain_data:\n",
    "                # This is the standard MultiWOZ state structure\n",
    "                semi_data = domain_data[\"semi\"]\n",
    "                # Only add if there's actual state data\n",
    "                if semi_data:  \n",
    "                    dialogue_state[domain] = semi_data\n",
    "        \n",
    "        turn_data = {\n",
    "            \"turn_id\": turn_idx,\n",
    "            \"user_utterance\": normalized_text if is_user_turn else \"\",\n",
    "            \"system_response\": \"\" if is_user_turn else normalized_text,\n",
    "            # Use the properly extracted state\n",
    "            \"state\": dialogue_state \n",
    "        }\n",
    "        flattened_turns.append(turn_data)\n",
    "        \n",
    "    return flattened_turns\n",
    "\n",
    "def normalize_dialogue_acts(dialogue_acts, dialogue_data):\n",
    "    normalized_acts = {}\n",
    "    \n",
    "    for dialogue_id, turns_data in dialogue_data.items():\n",
    "        normalized_id = normalize_dialogue_id(dialogue_id)\n",
    "        acts = dialogue_acts.get(normalized_id, {})\n",
    "        turns = turns_data.get(\"log\", [])\n",
    "        normalized_turn_acts = {}\n",
    "        \n",
    "        for turn_idx in range(len(turns)):\n",
    "            turn_key = str(turn_idx)\n",
    "            turn_acts_raw = acts.get(turn_key, {})\n",
    "            processed_acts = {}\n",
    "            \n",
    "            if isinstance(turn_acts_raw, dict):\n",
    "                for act_type, act_values in turn_acts_raw.items():\n",
    "                    # Convert act_type to lowercase\n",
    "                    lower_act_type = act_type.lower()\n",
    "                    if isinstance(act_values, dict):\n",
    "                        # Convert keys and values to lowercase\n",
    "                        processed_acts[lower_act_type] = [\n",
    "                            f\"{k.lower()}:{v.lower()}\" if isinstance(k, str) and isinstance(v, str) else f\"{k}:{v}\" for k, v in act_values.items()\n",
    "                        ]\n",
    "                    elif isinstance(act_values, list):\n",
    "                        flat_values = []\n",
    "                        for item in act_values:\n",
    "                            if isinstance(item, list):\n",
    "                                # Convert items in inner lists to lowercase\n",
    "                                flat_values.extend([str(x).lower() if isinstance(x, str) else str(x) for x in item])\n",
    "                            else:\n",
    "                                # Convert item to lowercase\n",
    "                                flat_values.append(str(item).lower() if isinstance(item, str) else str(item))\n",
    "                        processed_acts[lower_act_type] = flat_values\n",
    "                    else:\n",
    "                        # Convert single string values to lowercase\n",
    "                        processed_acts[lower_act_type] = [str(act_values).lower() if isinstance(act_values, str) else str(act_values)]\n",
    "            \n",
    "            normalized_turn_acts[turn_key] = processed_acts\n",
    "        normalized_acts[dialogue_id] = normalized_turn_acts\n",
    "        \n",
    "    return normalized_acts\n",
    "\n",
    "def process_data(raw_data_subset, dialogue_acts, ontology):\n",
    "    \n",
    "    dialogue_data_map = {d[\"dialogue_id\"]: d[\"dialogue\"] for d in raw_data_subset}\n",
    "    normalized_dialogue_acts = normalize_dialogue_acts(dialogue_acts, dialogue_data_map)\n",
    "    processed_data = []\n",
    "    \n",
    "    for item in raw_data_subset:\n",
    "        dialogue_id = item[\"dialogue_id\"]\n",
    "        dialogue = item[\"dialogue\"]\n",
    "        domain = item[\"domain\"]\n",
    "        domain_ontology = {\n",
    "            slot: values\n",
    "            for slot, values in ontology.items()\n",
    "            if slot.startswith(f\"{domain}-\")\n",
    "        }\n",
    "        \n",
    "        processed_data.append({\n",
    "            \"dialogue_id\": dialogue_id,\n",
    "            \"domain\": domain,\n",
    "            \"dialogue\": flatten_dialogue(dialogue),\n",
    "            \"dialogue_acts\": normalized_dialogue_acts.get(dialogue_id, {}),\n",
    "            \"ontology\": domain_ontology\n",
    "        })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def format_dialogue_acts(acts_dict):\n",
    "    \"\"\"Format dialogue acts for better display.\"\"\"\n",
    "    if not acts_dict:\n",
    "        return \"No acts available\"\n",
    "    \n",
    "    formatted = []\n",
    "    for act_type, slots in acts_dict.items():\n",
    "        slot_strs = []\n",
    "        for slot in slots:\n",
    "            if ':' in slot:\n",
    "                key, value = slot.split(':', 1)\n",
    "                slot_strs.append(f\"{key}={value}\")\n",
    "            else:\n",
    "                slot_strs.append(str(slot))\n",
    "        formatted.append(f\"{act_type}({', '.join(slot_strs)})\")\n",
    "        \n",
    "    return \"; \".join(formatted)\n",
    "\n",
    "def randomly_check_first_dialogue_turns_with_acts_and_ontology(split_name, split_data, num_samples=2, num_turns=4):\n",
    "    \"\"\"\n",
    "    Randomly checks a few dialogues and displays the dialogue text, acts, and ontology.\n",
    "    \"\"\"\n",
    "    print(f\"\\n Randomly Checking {num_samples} Dialogues (First {num_turns} Turns) from {split_name} \")\n",
    "    \n",
    "    if not split_data:\n",
    "        print(f\"No data in {split_name} split.\")\n",
    "        return\n",
    "    \n",
    "    if not isinstance(split_data, (list, tuple)):\n",
    "        split_data = list(split_data)\n",
    "    \n",
    "    if len(split_data) == 0:\n",
    "        print(f\"No data in {split_name} split after conversion.\")\n",
    "        return\n",
    "        \n",
    "    random_dialogues = random.sample(split_data, min(num_samples, len(split_data)))\n",
    "    \n",
    "    for dialogue_item in random_dialogues:\n",
    "        dialogue_id = dialogue_item[\"dialogue_id\"]\n",
    "        \n",
    "        # Check if the 'dialogue' key contains the flattened turns list or the raw object.\n",
    "        raw_dialogue_object = dialogue_item[\"dialogue\"]\n",
    "        \n",
    "        if isinstance(raw_dialogue_object, list):\n",
    "            # This is already processed data\n",
    "            turns = raw_dialogue_object\n",
    "        elif isinstance(raw_dialogue_object, dict):\n",
    "            # This is raw data, so we need to get the 'log' key.\n",
    "            turns = raw_dialogue_object.get(\"log\", [])\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected dialogue format for dialogue ID {dialogue_id}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nDialogue ID: {dialogue_id}\")\n",
    "        print(f\"Domain: {dialogue_item.get('domain', 'unknown')}\")\n",
    "        \n",
    "        # Dialogue acts and ontology\n",
    "        dialogue_acts_for_dialogue = dialogue_item.get(\"dialogue_acts\", {})\n",
    "        domain = dialogue_item.get('domain', 'unknown')\n",
    "        \n",
    "        domain_ontology = {\n",
    "            slot: values\n",
    "            for slot, values in dialogue_item.get(\"ontology\", {}).items()\n",
    "            if slot.startswith(f\"{domain}-\")\n",
    "        }\n",
    "\n",
    "        print(f\"    Ontology for domain '{domain}': {json.dumps(domain_ontology, indent=2)}\")\n",
    "        print(f\"First {min(num_turns, len(turns))} turns:\")\n",
    "        \n",
    "        for i in range(min(num_turns, len(turns))):\n",
    "            turn = turns[i]\n",
    "            turn_idx = turn.get(\"turn_id\") if isinstance(turn, dict) and \"turn_id\" in turn else i\n",
    "            \n",
    "            # Use the text content from the turn dictionary\n",
    "            if \"user_utterance\" in turn and turn[\"user_utterance\"]:\n",
    "                text_content = turn[\"user_utterance\"]\n",
    "                turn_type = \"User\"\n",
    "            elif \"system_response\" in turn and turn[\"system_response\"]:\n",
    "                text_content = turn[\"system_response\"]\n",
    "                turn_type = \"System\"\n",
    "            else:\n",
    "                # Fallback for raw data\n",
    "                text_content = turn.get('text', '')\n",
    "                turn_type = \"User\" if turn_idx % 2 == 0 else \"System\"\n",
    "\n",
    "            print(f\"  Turn {turn_idx} ({turn_type}):\")\n",
    "            \n",
    "            print(f\"    Text: {text_content}\")\n",
    "            \n",
    "            # Get the acts from the processed data's dialogue_acts dictionary\n",
    "            turn_acts_data = dialogue_acts_for_dialogue.get(str(turn_idx), {})\n",
    "            print(f\"    Dialogue Acts: {format_dialogue_acts(turn_acts_data)}\")\n",
    "            \n",
    "            # Only print state if the dictionary is not empty\n",
    "            state_data = turn.get(\"state\", {})\n",
    "            if state_data:\n",
    "                print(f\"    State: {state_data}\")\n",
    "\n",
    "# Call process_data on both datasets\n",
    "print(\"\\nPreprocessing Datasets\")\n",
    "processed_train_data = process_data(train_data_raw, dialogue_acts, ontology)\n",
    "processed_test_data = process_data(test_data_raw, dialogue_acts, ontology)\n",
    "\n",
    "print(f\"Preprocessed train_data size: {len(processed_train_data)}\")\n",
    "print(f\"Preprocessed test_data size: {len(processed_test_data)}\")\n",
    "\n",
    "# Display preprocessed dataset\n",
    "randomly_check_first_dialogue_turns_with_acts_and_ontology(\"Processed Train\", processed_train_data)\n",
    "randomly_check_first_dialogue_turns_with_acts_and_ontology(\"Processed Test\", processed_test_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split features and target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing DeepSeekMoE Dataset \n",
      "Train sequences: 22140\n",
      "Test sequences: 5588\n",
      "\n",
      "Example Sequences \n",
      "Example 1: <bos> Yes I need a cheap restaurant in the Cambridge area on the north side of town . What do you suggest ?...\n",
      "Example 2: thank you I will go to royal spice . <eos>...\n"
     ]
    }
   ],
   "source": [
    "# Special tokens\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "SEP_TOKEN = \"<sep>\"\n",
    "\n",
    "def prepare_deepseek_dataset(dialogues):\n",
    "    \"\"\"\n",
    "    Prepare dataset in DeepSeekMoE format by separating each question and answer\n",
    "    pair in a dialogue.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    for dialogue in dialogues:\n",
    "        for turn in dialogue:\n",
    "            user_utterance = turn.get(\"user_utterance\")\n",
    "            system_response = turn.get(\"system_response\")\n",
    "\n",
    "            if user_utterance:\n",
    "                # Format the user's question with the BOS token\n",
    "                question_text = f\"{BOS_TOKEN} {user_utterance}\"\n",
    "                sequences.append({\"text\": question_text})\n",
    "            \n",
    "            if system_response:\n",
    "                # Format the system's response with the EOS token\n",
    "                answer_text = f\"{system_response} {EOS_TOKEN}\"\n",
    "                sequences.append({\"text\": answer_text})\n",
    "                \n",
    "    return sequences\n",
    "\n",
    "print(\"\\nPreparing DeepSeekMoE Dataset \")\n",
    "train_sequences = prepare_deepseek_dataset([d[\"dialogue\"] for d in processed_train_data])\n",
    "test_sequences = prepare_deepseek_dataset([d[\"dialogue\"] for d in processed_test_data])\n",
    "\n",
    "# Save text for evaluation\n",
    "with open(\"test_evaluation_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for seq in test_sequences:\n",
    "        f.write(seq[\"text\"] + \"\\n\")\n",
    "\n",
    "print(f\"Train sequences: {len(train_sequences)}\")\n",
    "print(f\"Test sequences: {len(test_sequences)}\")\n",
    "\n",
    "# Display examples\n",
    "print(\"\\nExample Sequences \")\n",
    "for i in range(2):\n",
    "    print(f\"Example {i+1}: {train_sequences[i]['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Hugging Face datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 22140\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_list(train_sequences)\n",
    "test_dataset = Dataset.from_list(test_sequences)\n",
    "\n",
    "# Check dataset after covert\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization \n",
      "\n",
      "\n",
      "\n",
      "Vocabulary size: 11480\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a89aa199e14c58aca1739ac9e9dec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df770b2a225a499c8a9425bb5695aa68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Special tokens\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SEP_TOKEN = \"<sep>\"\n",
    "\n",
    "# Max length Configurations\n",
    "max_seq_len = 70\n",
    "vocab_size = 100000\n",
    "\n",
    "# Tokenization\n",
    "print(\"\\nTokenization \")\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Create corpus for tokenizer training\n",
    "corpus_file = \"deepseek_corpus.txt\"\n",
    "with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in train_sequences:\n",
    "        f.write(item[\"text\"] + \"\\n\")\n",
    "\n",
    "# Train tokenizer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    special_tokens=[BOS_TOKEN, EOS_TOKEN, PAD_TOKEN, SEP_TOKEN],\n",
    "    # Number of vocaburary in corpus\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "tokenizer.train([corpus_file], trainer=trainer)\n",
    "os.remove(corpus_file)\n",
    "\n",
    "# Configure tokenizer\n",
    "tokenizer.enable_truncation(max_length=max_seq_len)\n",
    "\n",
    "tokenizer.enable_padding(\n",
    "    direction=\"right\",\n",
    "    pad_id=tokenizer.token_to_id(PAD_TOKEN),\n",
    "    pad_token=PAD_TOKEN\n",
    ")\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "PAD_TOKEN_ID = tokenizer.token_to_id(PAD_TOKEN)\n",
    "\n",
    "# Prepared dataset\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize text sequences\"\"\"\n",
    "    encoded = tokenizer.encode_batch(examples[\"text\"])\n",
    "    return {\n",
    "        \"input_ids\": [e.ids for e in encoded],\n",
    "        \"attention_mask\": [e.attention_mask for e in encoded]\n",
    "    }\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save(\"deepseek_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Remark: max_seq_len check from visualization process in longest sentence***\n",
    "\n",
    "***Remark: Set as large number first then adjust number of word adjust follow print result of Vocabulary size from tokenizer.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the result (train dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Sample 1 \n",
      "Original Text: <bos> Yes I need a cheap restaurant in the Cambridge area on the north side of town . What do you suggest ?\n",
      "Input IDs: [0, 244, 97, 164, 89, 453, 225, 131, 102, 264, 291, 168, 102, 511, 721, 182, 286, 93, 294, 240, 100, 832, 108, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded Text:  Yes I need a cheap restaurant in the Cambridge area on the north side of town . What do you suggest ?\n",
      "\n",
      "--------------------------------------------------\n",
      "Sample 2 \n",
      "Original Text: thank you I will go to royal spice . <eos>\n",
      "Input IDs: [348, 100, 97, 224, 359, 113, 2254, 2644, 93, 87, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded Text:  thank you I will go to royal spice . \n",
      "\n",
      "--------------------------------------------------\n",
      "Sample 3 \n",
      "Original Text: <bos> Okay can you book me a table for Saturday at 19:45 for 3 people ? I would also like the reference number for the booking .\n",
      "Input IDs: [0, 525, 227, 100, 171, 188, 89, 434, 125, 761, 167, 725, 27, 444, 125, 386, 318, 108, 97, 176, 307, 162, 102, 283, 175, 125, 102, 484, 93, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded Text:  Okay can you book me a table for Saturday at 19:45 for 3 people ? I would also like the reference number for the booking .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Visualize a samples of the dataset\n",
    "num_samples = min(3, len(tokenized_train))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample = tokenized_train[i]\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(f\"Sample {i+1} \")\n",
    "    print(f\"Original Text: {sample['text']}\")\n",
    "    print(f\"Input IDs: {sample['input_ids']}\")\n",
    "    print(f\"Attention Mask: {sample['attention_mask']}\")\n",
    "    \n",
    "    # Decoder\n",
    "    decoded_text = tokenizer.decode(sample['input_ids'])\n",
    "    print(f\"Decoded Text: {decoded_text}\")\n",
    "\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the result (test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Sample 1 \n",
      "Original Text: <bos> Yes I need a cheap restaurant in the Cambridge area on the north side of town . What do you suggest ?\n",
      "Input IDs: [0, 244, 97, 164, 89, 453, 225, 131, 102, 264, 291, 168, 102, 511, 721, 182, 286, 93, 294, 240, 100, 832, 108, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded Text:  Yes I need a cheap restaurant in the Cambridge area on the north side of town . What do you suggest ?\n",
      "\n",
      "--------------------------------------------------\n",
      "Sample 2 \n",
      "Original Text: thank you I will go to royal spice . <eos>\n",
      "Input IDs: [348, 100, 97, 224, 359, 113, 2254, 2644, 93, 87, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded Text:  thank you I will go to royal spice . \n",
      "\n",
      "--------------------------------------------------\n",
      "Sample 3 \n",
      "Original Text: <bos> Okay can you book me a table for Saturday at 19:45 for 3 people ? I would also like the reference number for the booking .\n",
      "Input IDs: [0, 525, 227, 100, 171, 188, 89, 434, 125, 761, 167, 725, 27, 444, 125, 386, 318, 108, 97, 176, 307, 162, 102, 283, 175, 125, 102, 484, 93, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded Text:  Okay can you book me a table for Saturday at 19:45 for 3 people ? I would also like the reference number for the booking .\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Visualize a samples of the dataset\n",
    "\n",
    "num_samples = min(3, len(tokenized_test))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    sample = tokenized_train[i]\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(f\"Sample {i+1} \")\n",
    "    print(f\"Original Text: {sample['text']}\")\n",
    "    print(f\"Input IDs: {sample['input_ids']}\")\n",
    "    print(f\"Attention Mask: {sample['attention_mask']}\")\n",
    "    \n",
    "    # Decoder\n",
    "    decoded_text = tokenizer.decode(sample['input_ids'])\n",
    "    print(f\"Decoded Text: {decoded_text}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset: 17712\n",
      "Size of validation dataset: 4428\n"
     ]
    }
   ],
   "source": [
    "# Split into train/validation\n",
    "train_val_split = tokenized_train.train_test_split(test_size=0.2, seed=42)\n",
    "tokenized_train = train_val_split[\"train\"]\n",
    "tokenized_val = train_val_split[\"test\"]\n",
    "\n",
    "# Check the size of each dataset\n",
    "print(f\"Size of training dataset: {len(tokenized_train)}\")\n",
    "print(f\"Size of validation dataset: {len(tokenized_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to TensorFlow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Btch Size for train model\n",
    "batch_size = 2\n",
    "\n",
    "# Convert to TensorFlow datasets\n",
    "def to_tf_dataset(hf_dataset, tokenizer, max_seq_len, pad_token_id, batch_size):\n",
    "    \"\"\"\n",
    "    Convert a Hugging Face dataset to a TensorFlow dataset with proper formatting.\n",
    "    \"\"\"\n",
    "    input_ids = pad_sequences(\n",
    "        hf_dataset[\"input_ids\"],\n",
    "        maxlen=max_seq_len,\n",
    "        padding=\"post\",\n",
    "        truncating=\"post\",\n",
    "        value=pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Create next token prediction targets by shifting the input IDs.\n",
    "    labels = np.roll(input_ids, -1, axis=1)\n",
    "    # Ignore loss for last position padding\n",
    "    labels[:, -1] = pad_token_id\n",
    "    \n",
    "    # Use attention mask to ignore pad tokens during loss calculation\n",
    "    attention_mask = pad_sequences(\n",
    "        hf_dataset[\"attention_mask\"],\n",
    "        maxlen=max_seq_len,\n",
    "        padding=\"post\",\n",
    "        truncating=\"post\",\n",
    "        value=0\n",
    "    )\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        },\n",
    "        labels\n",
    "    ))\n",
    "    return dataset.shuffle(500).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "tf_train_dataset = to_tf_dataset(tokenized_train, tokenizer, max_seq_len, PAD_TOKEN_ID, batch_size)\n",
    "tf_val_dataset = to_tf_dataset(tokenized_val, tokenizer, max_seq_len, PAD_TOKEN_ID, batch_size)\n",
    "tf_test_dataset = to_tf_dataset(tokenized_test, tokenizer, max_seq_len, PAD_TOKEN_ID, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSeekMoE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n",
      "TensorFlow version: 2.15.0\n",
      "Mixed precision policy: mixed_float16\n",
      "GPU available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 12:34:45.309900: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "# Disable XLA (XLA JIT compilation) to prevent CUDA graph errors and ensure stability\n",
    "tf.config.optimizer.set_jit(False)  # Disabled XLA to prevent CUDA graph errors\n",
    "\n",
    "# Configure GPU memory growth to prevent memory allocation issues\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Enable mixed precision training with FP16 for improved performance and reduced memory usage\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Configuration parameters from the model and preprocessing pipeline\n",
    "vocab_size = 12000\n",
    "max_seq_len = 70\n",
    "batch_size = 2\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "SEP_TOKEN = \"<sep>\"\n",
    "\n",
    "# Print TensorFlow version, mixed precision policy, and GPU availability for debugging\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Mixed precision policy: {mixed_precision.global_policy().name}\")\n",
    "print(f\"GPU available: {len(gpus) > 0}\")\n",
    "\n",
    "# DEEPSEEK MOE COMPONENTS\n",
    "class DeepSeekRMSNorm(layers.Layer):\n",
    "    \"\"\"RMS Normalization layer \"\"\"\n",
    "    def __init__(self, hidden_size, eps=1e-6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size  # Dimension of the hidden representations\n",
    "        self.variance_epsilon = eps     # Small constant to avoid division by zero\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize learnable weight parameter for scaling normalized output\n",
    "        self.weight = self.add_weight(\n",
    "            name='weight',\n",
    "            shape=(self.hidden_size,),\n",
    "            initializer='ones',\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        x = tf.cast(x, tf.float32)  # Cast input to float32 for stable computation\n",
    "        \n",
    "        # Compute variance as mean of squared inputs along the last dimension\n",
    "        variance = tf.reduce_mean(tf.square(x), axis=-1, keepdims=True)\n",
    "        # Normalize input using RMS formula with epsilon for numerical stability\n",
    "        x = x * tf.math.rsqrt(variance + self.variance_epsilon)\n",
    "        \n",
    "        # Scale normalized output with learnable weight and cast back to original dtype\n",
    "        return self.weight * tf.cast(x, input_dtype)\n",
    "\n",
    "class RotaryPositionEmbedding(layers.Layer):\n",
    "    \"\"\"Rotary Position Embedding (RoPE) with dynamic NTK scaling for positional encoding\"\"\"\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, scaling_type=None, scaling_factor=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim                              # Dimension of the head embeddings\n",
    "        self.max_position_embeddings = max_position_embeddings  # Maximum sequence length\n",
    "        self.base = base                           # Base value for frequency calculation\n",
    "        self.scaling_type = scaling_type           # Type of scaling (\"linear\" or \"dynamic\")\n",
    "        self.scaling_factor = scaling_factor       # Scaling factor for RoPE\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Compute inverse frequencies for rotary embeddings\n",
    "        inv_freq = 1.0 / (self.base ** (tf.range(0, self.dim, 2, dtype=tf.float32) / tf.cast(self.dim, tf.float32)))\n",
    "        self.inv_freq = tf.constant(inv_freq, dtype=tf.float32)\n",
    "        self._compute_rotary_cache()  # Precompute cosine and sine caches\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def _compute_rotary_cache(self):\n",
    "        \"\"\"Compute cosine and sine caches for rotary embeddings with optional scaling\"\"\"\n",
    "        if self.scaling_type == \"linear\":\n",
    "            # Linear scaling: divide positions by scaling factor\n",
    "            t = tf.range(self.max_position_embeddings, dtype=tf.float32) / self.scaling_factor\n",
    "        elif self.scaling_type == \"dynamic\":\n",
    "            # Dynamic NTK scaling for extended context lengths\n",
    "            if self.max_position_embeddings > self.base:\n",
    "                base = self.base * (\n",
    "                    (self.scaling_factor * self.max_position_embeddings / self.base) - \n",
    "                    (self.scaling_factor - 1)\n",
    "                ) ** (self.dim / (self.dim - 2))\n",
    "                inv_freq = 1.0 / (base ** (tf.range(0, self.dim, 2, dtype=tf.float32) / tf.cast(self.dim, tf.float32)))\n",
    "                self.inv_freq = tf.constant(inv_freq, dtype=tf.float32)\n",
    "            t = tf.range(self.max_position_embeddings, dtype=tf.float32)\n",
    "        else:\n",
    "            # Default case: no scaling\n",
    "            t = tf.range(self.max_position_embeddings, dtype=tf.float32)\n",
    "            \n",
    "        # Compute frequency matrix for rotary embeddings\n",
    "        freqs = tf.einsum('i,j->ij', t, self.inv_freq)\n",
    "        emb = tf.concat([freqs, freqs], axis=-1)  # Concatenate for full embedding dimension\n",
    "        \n",
    "        # Cache cosine and sine values as non-trainable variables\n",
    "        self.cos_cached = tf.Variable(tf.cos(emb), trainable=False, name='cos_cached')\n",
    "        self.sin_cached = tf.Variable(tf.sin(emb), trainable=False, name='sin_cached')\n",
    "        \n",
    "    def call(self, x, position_ids=None, seq_len=None):\n",
    "        # Get input tensor dimensions\n",
    "        batch_size, seq_len_x, num_heads, head_dim = tf.unstack(tf.shape(x))\n",
    "        \n",
    "        # Generate position IDs if not provided\n",
    "        if position_ids is None:\n",
    "            position_ids = tf.range(seq_len_x, dtype=tf.int32)\n",
    "            position_ids = tf.tile(tf.reshape(position_ids, [1, -1]), [batch_size, 1])\n",
    "        \n",
    "        # Gather precomputed cosine and sine values for given position IDs\n",
    "        cos = tf.gather(self.cos_cached, position_ids, axis=0)  # Shape: [batch_size, seq_len_x, head_dim]\n",
    "        sin = tf.gather(self.sin_cached, position_ids, axis=0)  # Shape: [batch_size, seq_len_x, head_dim]\n",
    "        \n",
    "        # Split cosine and sine for application to x1 and x2\n",
    "        cos_1, cos_2 = tf.split(cos, 2, axis=-1)\n",
    "        sin_1, sin_2 = tf.split(sin, 2, axis=-1)\n",
    "        \n",
    "        # Reshape for broadcasting with attention heads\n",
    "        cos_1 = tf.reshape(cos_1, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        sin_1 = tf.reshape(sin_1, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        cos_2 = tf.reshape(cos_2, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        sin_2 = tf.reshape(sin_2, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        \n",
    "        # Repeat to match number of attention heads\n",
    "        cos_1 = tf.repeat(cos_1, num_heads, axis=2)\n",
    "        sin_1 = tf.repeat(sin_1, num_heads, axis=2)\n",
    "        cos_2 = tf.repeat(cos_2, num_heads, axis=2)\n",
    "        sin_2 = tf.repeat(sin_2, num_heads, axis=2)\n",
    "        \n",
    "        # Cast to input dtype for mixed precision compatibility\n",
    "        x_dtype = x.dtype\n",
    "        cos_1 = tf.cast(cos_1, x_dtype)\n",
    "        sin_1 = tf.cast(sin_1, x_dtype)\n",
    "        cos_2 = tf.cast(cos_2, x_dtype)\n",
    "        sin_2 = tf.cast(sin_2, x_dtype)\n",
    "        \n",
    "        # Split input tensor into two parts for rotary application\n",
    "        x1, x2 = tf.split(x, 2, axis=-1)\n",
    "        \n",
    "        # Apply rotary embeddings to compute rotated representations\n",
    "        rotated_x1 = x1 * cos_1 - x2 * sin_1\n",
    "        rotated_x2 = x1 * sin_2 + x2 * cos_2\n",
    "        \n",
    "        # Concatenate rotated parts to restore original shape\n",
    "        return tf.concat([rotated_x1, rotated_x2], axis=-1)\n",
    "\n",
    "class DeepSeekMLP(layers.Layer):\n",
    "    \"\"\"Multi-Layer Perceptron (MLP) with Swish activation\"\"\"\n",
    "    def __init__(self, hidden_size, intermediate_size, hidden_act=\"swish\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size           # Input and output dimension\n",
    "        self.intermediate_size = intermediate_size  # Intermediate layer dimension\n",
    "        self.hidden_act = hidden_act            # Activation function (Swish/SILU)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Define gate projection layer with reduced initializer variance for stability\n",
    "        self.gate_proj = layers.Dense(\n",
    "            self.intermediate_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            use_bias=False\n",
    "        )\n",
    "        # Define up projection layer\n",
    "        self.up_proj = layers.Dense(\n",
    "            self.intermediate_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            use_bias=False\n",
    "        )\n",
    "        # Define down projection layer to return to hidden size\n",
    "        self.down_proj = layers.Dense(\n",
    "            self.hidden_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            use_bias=False\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Apply gate and up projections\n",
    "        gate_output = self.gate_proj(x)\n",
    "        up_output = self.up_proj(x)\n",
    "        # Apply Swish (SiLU) activation to gate output and multiply with up output\n",
    "        activated = tf.nn.silu(gate_output) * up_output\n",
    "        # Project back to hidden size\n",
    "        return self.down_proj(activated)\n",
    "\n",
    "class MoEGate(layers.Layer):\n",
    "    \"\"\"Mixture of Experts (MoE) gating mechanism with load balancing auxiliary loss\"\"\"\n",
    "    def __init__(self, num_experts, num_experts_per_tok, hidden_size, \n",
    "                 scoring_func='softmax', norm_topk_prob=False, \n",
    "                 aux_loss_alpha=0.001, seq_aux=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_experts = num_experts                  # Total number of experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok  # Number of experts per token\n",
    "        self.hidden_size = hidden_size                  # Input dimension\n",
    "        self.scoring_func = scoring_func                # Scoring function for expert selection\n",
    "        self.norm_topk_prob = norm_topk_prob            # Normalize top-k probabilities\n",
    "        self.aux_loss_alpha = aux_loss_alpha            # Weight for auxiliary loss\n",
    "        self.seq_aux = seq_aux                          # Compute auxiliary loss per sequence\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize gating weights with reduced variance for stability\n",
    "        self.weight = self.add_weight(\n",
    "            name='gate_weight',\n",
    "            shape=(self.num_experts, self.hidden_size),\n",
    "            initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            trainable=True\n",
    "        )\n",
    "        # Initialize expert assignment counter for tracking during training\n",
    "        self.expert_counts = tf.Variable(tf.zeros([self.num_experts], dtype=tf.int32), trainable=False)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, hidden_states, training=False):\n",
    "        # Get input dimensions\n",
    "        batch_size, seq_len, h = tf.unstack(tf.shape(hidden_states))\n",
    "        hidden_states_flat = tf.reshape(hidden_states, [-1, h])\n",
    "        # Compute gating logits\n",
    "        logits = tf.matmul(hidden_states_flat, self.weight, transpose_b=True)\n",
    "        \n",
    "        # Clip logits for numerical stability\n",
    "        logits = tf.clip_by_value(logits, -50.0, 50.0)\n",
    "        \n",
    "        # Apply scoring function (softmax) to compute expert scores\n",
    "        if self.scoring_func == 'softmax':\n",
    "            temperature = 0.5                                         # Temperature for scaling logits\n",
    "            scaled_logits = tf.cast(logits, tf.float32) / temperature\n",
    "            scores = tf.nn.softmax(scaled_logits, axis=-1)\n",
    "            scores = tf.cast(scores, hidden_states.dtype)\n",
    "        else:\n",
    "            scores = tf.nn.softmax(tf.cast(logits, tf.float32), axis=-1)\n",
    "            scores = tf.cast(scores, hidden_states.dtype)\n",
    "\n",
    "        # Select top-k experts and their weights\n",
    "        topk_weights, topk_indices = tf.math.top_k(\n",
    "            scores, k=self.num_experts_per_tok, sorted=False\n",
    "        )\n",
    "\n",
    "        # Normalize top-k weights if specified\n",
    "        if self.num_experts_per_tok > 1 and self.norm_topk_prob:\n",
    "            denominator = tf.reduce_sum(topk_weights, axis=-1, keepdims=True) + 1e-20\n",
    "            topk_weights = topk_weights / denominator\n",
    "\n",
    "        # Update expert assignment counts during training\n",
    "        if training:\n",
    "            flat_topk_indices = tf.reshape(topk_indices, [-1])\n",
    "            counts = tf.reduce_sum(tf.one_hot(flat_topk_indices, depth=self.num_experts), axis=0)\n",
    "            self.expert_counts.assign_add(tf.cast(counts, tf.int32))\n",
    "\n",
    "        aux_loss = None\n",
    "        if training and self.aux_loss_alpha > 0.0:\n",
    "            if self.seq_aux:\n",
    "                # Sequence-level auxiliary loss for load balancing\n",
    "                scores_seq = tf.reshape(scores, [batch_size, seq_len, -1])\n",
    "                topk_indices_seq = tf.reshape(topk_indices, [batch_size, -1])\n",
    "                ce = tf.zeros([batch_size, self.num_experts], dtype=tf.float32)\n",
    "                mask = tf.one_hot(topk_indices_seq, depth=self.num_experts)\n",
    "                ce = tf.reduce_sum(mask, axis=1)\n",
    "                seq_len_float = tf.cast(seq_len, tf.float32)\n",
    "                num_experts_per_tok_float = tf.cast(self.num_experts_per_tok, tf.float32)\n",
    "                num_experts_float = tf.cast(self.num_experts, tf.float32)\n",
    "                ce = ce / (seq_len_float * num_experts_per_tok_float / num_experts_float)\n",
    "                scores_seq_mean = tf.reduce_mean(tf.cast(scores_seq, tf.float32), axis=1)\n",
    "                aux_loss = tf.reduce_sum(ce * scores_seq_mean, axis=1)\n",
    "                aux_loss = tf.reduce_mean(aux_loss) * self.aux_loss_alpha\n",
    "            else:\n",
    "                # Token-level auxiliary loss\n",
    "                mask_ce = tf.one_hot(tf.reshape(topk_indices, [-1]), depth=self.num_experts)\n",
    "                ce = tf.reduce_mean(tf.cast(mask_ce, tf.float32), axis=0)\n",
    "                Pi = tf.reduce_mean(tf.cast(scores, tf.float32), axis=0)\n",
    "                fi = ce * self.num_experts\n",
    "                aux_loss = tf.reduce_sum(Pi * fi) * self.aux_loss_alpha\n",
    "\n",
    "            # Add auxiliary loss to the layer's losses\n",
    "            self.add_loss(aux_loss)\n",
    "\n",
    "        return topk_indices, topk_weights, aux_loss\n",
    "\n",
    "class DeepSeekMoE(layers.Layer):\n",
    "    \"\"\"Mixture of Experts (MoE) layer with shared and routed experts\"\"\"\n",
    "    def __init__(self, hidden_size, n_routed_experts, n_shared_experts, \n",
    "                 num_experts_per_tok, moe_intermediate_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size                      # Input and output dimension\n",
    "        self.n_routed_experts = n_routed_experts            # Number of routed experts\n",
    "        self.n_shared_experts = n_shared_experts            # Number of shared experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok      # Experts per token\n",
    "        self.moe_intermediate_size = moe_intermediate_size  # MoE intermediate size\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize list of routed expert MLPs\n",
    "        self.routed_experts = [\n",
    "            DeepSeekMLP(\n",
    "                self.hidden_size, \n",
    "                self.moe_intermediate_size,\n",
    "                name=f\"routed_expert_{i}\"\n",
    "            ) for i in range(self.n_routed_experts)\n",
    "        ]\n",
    "        \n",
    "        # Initialize shared experts if specified\n",
    "        if self.n_shared_experts is not None:\n",
    "            shared_intermediate_size = self.moe_intermediate_size * self.n_shared_experts\n",
    "            self.shared_experts = DeepSeekMLP(\n",
    "                self.hidden_size,\n",
    "                shared_intermediate_size,\n",
    "                name=\"shared_experts\"\n",
    "            )\n",
    "        else:\n",
    "            self.shared_experts = None\n",
    "            \n",
    "        # Initialize MoE gating mechanism\n",
    "        self.gate = MoEGate(\n",
    "            self.n_routed_experts,\n",
    "            self.num_experts_per_tok,\n",
    "            self.hidden_size,\n",
    "            name=\"moe_gate\"\n",
    "        )\n",
    "        \n",
    "        # Build each expert and gate\n",
    "        for expert in self.routed_experts:\n",
    "            expert.build(input_shape)\n",
    "        if self.shared_experts:\n",
    "            self.shared_experts.build(input_shape)\n",
    "        self.gate.build(input_shape)\n",
    "        \n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, hidden_states, training=False):\n",
    "        identity = hidden_states  # Store input for residual connection\n",
    "        orig_shape = tf.shape(hidden_states)\n",
    "        # Get top-k expert indices and weights\n",
    "        topk_idx, topk_weight, aux_loss = self.gate(hidden_states, training=training)\n",
    "        hidden_states_flat = tf.reshape(hidden_states, [-1, self.hidden_size])\n",
    "        \n",
    "        if training:\n",
    "            # Expand inputs for top-k experts during training\n",
    "            hidden_states_expanded = tf.repeat(\n",
    "                hidden_states_flat, self.num_experts_per_tok, axis=0\n",
    "            )\n",
    "            flat_topk_idx = tf.reshape(topk_idx, [-1])\n",
    "            combined_output = tf.zeros_like(hidden_states_expanded)\n",
    "            # Process each expert's input\n",
    "            for i, expert in enumerate(self.routed_experts):\n",
    "                expert_mask = tf.equal(flat_topk_idx, i)\n",
    "                if tf.reduce_any(expert_mask):\n",
    "                    expert_input = tf.boolean_mask(hidden_states_expanded, expert_mask)\n",
    "                    expert_output = expert(expert_input)\n",
    "                    update_indices = tf.where(expert_mask)\n",
    "                    combined_output = tf.tensor_scatter_nd_update(\n",
    "                        combined_output,\n",
    "                        update_indices,\n",
    "                        expert_output\n",
    "                    )\n",
    "                    \n",
    "            # Apply expert weights and reshape output\n",
    "            topk_weight_flat = tf.reshape(topk_weight, [-1, 1])\n",
    "            weighted_output = combined_output * topk_weight_flat\n",
    "            weighted_output = tf.reshape(\n",
    "                weighted_output,\n",
    "                [-1, self.num_experts_per_tok, self.hidden_size]\n",
    "            )\n",
    "            routed_output = tf.reduce_sum(weighted_output, axis=1)\n",
    "            routed_output = tf.reshape(routed_output, orig_shape)\n",
    "        else:\n",
    "            # Use efficient inference path\n",
    "            routed_output = self._moe_infer(hidden_states_flat, topk_idx, topk_weight)\n",
    "            routed_output = tf.reshape(routed_output, orig_shape)\n",
    "            \n",
    "        # Add shared expert output if available\n",
    "        if self.shared_experts is not None:\n",
    "            shared_output = self.shared_experts(identity)\n",
    "            final_output = routed_output + shared_output\n",
    "        else:\n",
    "            final_output = routed_output\n",
    "            \n",
    "        # Apply residual connection\n",
    "        return final_output + identity\n",
    "        \n",
    "    def _moe_infer(self, x, expert_indices, expert_weights):\n",
    "        \"\"\"Efficient inference path for MoE layer\"\"\"\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        expert_cache = tf.zeros_like(x)\n",
    "        # Clip expert indices to valid range\n",
    "        expert_indices = tf.clip_by_value(expert_indices, 0, self.n_routed_experts - 1)\n",
    "        flat_expert_indices = tf.reshape(expert_indices, [-1])\n",
    "        flat_expert_weights = tf.reshape(expert_weights, [-1, 1])\n",
    "        total_tokens = tf.shape(x)[0]\n",
    "        token_indices_base = tf.range(total_tokens, dtype=tf.int64)\n",
    "        token_indices_base = tf.repeat(token_indices_base, self.num_experts_per_tok, axis=0)\n",
    "        \n",
    "        # Process each expert's tokens\n",
    "        for i in range(self.n_routed_experts):\n",
    "            expert_mask = tf.equal(flat_expert_indices, i)\n",
    "            if tf.reduce_any(expert_mask):\n",
    "                token_indices = tf.boolean_mask(token_indices_base, expert_mask)\n",
    "                expert_tokens = tf.gather(x, token_indices)\n",
    "                expert_out = self.routed_experts[i](expert_tokens)\n",
    "                expert_out_weighted = expert_out * tf.gather(flat_expert_weights, tf.where(expert_mask)[:, 0])\n",
    "                expert_cache = tf.tensor_scatter_nd_add(\n",
    "                    expert_cache,\n",
    "                    tf.expand_dims(token_indices, 1),\n",
    "                    expert_out_weighted\n",
    "                )\n",
    "                \n",
    "        return expert_cache\n",
    "\n",
    "class DeepSeekAttention(layers.Layer):\n",
    "    \"\"\"Multi-head attention with Rotary Position Embeddings and Grouped Query Attention (GQA) support\"\"\"\n",
    "    def __init__(self, hidden_size, num_attention_heads, num_key_value_heads=None,\n",
    "                 attention_dropout=0.0, max_position_embeddings=2048,\n",
    "                 rope_theta=10000.0, rope_scaling=None, attention_bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size                                         # Input and output dimension\n",
    "        self.num_attention_heads = num_attention_heads                         # Number of attention heads\n",
    "        self.num_key_value_heads = num_key_value_heads or num_attention_heads  # Number of key/value heads (GQA)\n",
    "        self.attention_dropout = attention_dropout                             # Dropout rate for attention weights\n",
    "        self.max_position_embeddings = max_position_embeddings                 # Maximum sequence length\n",
    "        self.rope_theta = rope_theta                                           # Base period for RoPE\n",
    "        self.rope_scaling = rope_scaling                                       # RoPE scaling configuration\n",
    "        self.attention_bias = attention_bias                                   # Whether to use bias in projections\n",
    "        self.head_dim = hidden_size // num_attention_heads                     # Dimension per head\n",
    "        self.num_key_value_groups = num_attention_heads // self.num_key_value_heads  # Groups for GQA\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize query projection layer\n",
    "        self.q_proj = layers.Dense(\n",
    "            self.num_attention_heads * self.head_dim,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"q_proj\"\n",
    "        )\n",
    "        # Initialize key projection layer\n",
    "        self.k_proj = layers.Dense(\n",
    "            self.num_key_value_heads * self.head_dim,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"k_proj\"\n",
    "        )\n",
    "        # Initialize value projection layer\n",
    "        self.v_proj = layers.Dense(\n",
    "            self.num_key_value_heads * self.head_dim,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"v_proj\"\n",
    "        )\n",
    "        # Initialize output projection layer\n",
    "        self.o_proj = layers.Dense(\n",
    "            self.hidden_size,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"o_proj\"\n",
    "        )\n",
    "        \n",
    "        # Initialize RoPE with specified scaling\n",
    "        scaling_type = self.rope_scaling.get(\"type\") if self.rope_scaling else None\n",
    "        scaling_factor = self.rope_scaling.get(\"factor\", 1.0) if self.rope_scaling else 1.0\n",
    "        self.rotary_emb = RotaryPositionEmbedding(\n",
    "            self.head_dim,\n",
    "            self.max_position_embeddings,\n",
    "            self.rope_theta,\n",
    "            scaling_type,\n",
    "            scaling_factor,\n",
    "            name=\"rotary_emb\"\n",
    "        )\n",
    "        self.dropout_layer = layers.Dropout(self.attention_dropout)\n",
    "        self.rotary_emb.build(input_shape)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, hidden_states, attention_mask=None, position_ids=None, training=False):\n",
    "        # Get input dimensions\n",
    "        batch_size, seq_len, _ = tf.unstack(tf.shape(hidden_states))\n",
    "        \n",
    "        # Project inputs to query, key, and value\n",
    "        q = self.q_proj(hidden_states)\n",
    "        k = self.k_proj(hidden_states)\n",
    "        v = self.v_proj(hidden_states)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = tf.reshape(q, [batch_size, seq_len, self.num_attention_heads, self.head_dim])\n",
    "        k = tf.reshape(k, [batch_size, seq_len, self.num_key_value_heads, self.head_dim])\n",
    "        v = tf.reshape(v, [batch_size, seq_len, self.num_key_value_heads, self.head_dim])\n",
    "        \n",
    "        # Apply rotary embeddings to query and key\n",
    "        q_rotated = self.rotary_emb(q, position_ids=position_ids)\n",
    "        k_rotated = self.rotary_emb(k, position_ids=position_ids)\n",
    "        \n",
    "        # Transpose for attention computation\n",
    "        q_rotated = tf.transpose(q_rotated, [0, 2, 1, 3])\n",
    "        k_rotated = tf.transpose(k_rotated, [0, 2, 1, 3])\n",
    "        v = tf.transpose(v, [0, 2, 1, 3])\n",
    "        \n",
    "        # Repeat key and value for GQA if necessary\n",
    "        if self.num_key_value_groups > 1:\n",
    "            k_rotated = tf.repeat(k_rotated, self.num_key_value_groups, axis=1)\n",
    "            v = tf.repeat(v, self.num_key_value_groups, axis=1)\n",
    "            \n",
    "        # Compute scaled dot-product attention\n",
    "        scale = tf.math.rsqrt(tf.cast(self.head_dim, tf.float32))\n",
    "        scale = tf.cast(scale, q_rotated.dtype)\n",
    "        attn_weights = tf.matmul(q_rotated, k_rotated, transpose_b=True) * scale\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = tf.cast(attention_mask, attn_weights.dtype)\n",
    "            if len(attention_mask.shape) == 2:\n",
    "                attention_mask = tf.expand_dims(tf.expand_dims(attention_mask, 1), 1)\n",
    "            elif len(attention_mask.shape) == 3:\n",
    "                attention_mask = tf.expand_dims(attention_mask, 1)\n",
    "            attn_weights += attention_mask\n",
    "            \n",
    "        # Clip attention weights for numerical stability\n",
    "        attn_weights = tf.clip_by_value(attn_weights, -50.0, 50.0)\n",
    "        attn_weights = tf.cast(attn_weights, tf.float32)\n",
    "        # Apply softmax with max subtraction for stability\n",
    "        attn_weights = attn_weights - tf.reduce_max(attn_weights, axis=-1, keepdims=True)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "        attn_weights = tf.cast(attn_weights, q_rotated.dtype)\n",
    "        \n",
    "        # Apply dropout to attention weights during training\n",
    "        attn_weights = self.dropout_layer(attn_weights, training=training)\n",
    "        # Compute attention output\n",
    "        attn_output = tf.matmul(attn_weights, v)\n",
    "        attn_output = tf.transpose(attn_output, [0, 2, 1, 3])\n",
    "        attn_output = tf.reshape(attn_output, [batch_size, seq_len, self.hidden_size])\n",
    "        \n",
    "        # Project attention output to hidden size\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "class DeepSeekDecoderLayer(layers.Layer):\n",
    "    \"\"\"Transformer decoder layer combining attention and MoE/MLP with residual connections\"\"\"\n",
    "    def __init__(self, config, layer_idx, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        # Initialize self-attention with GQA and RoPE\n",
    "        self.self_attn = DeepSeekAttention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            num_key_value_heads=config.num_key_value_heads,\n",
    "            attention_dropout=config.attention_dropout,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            rope_theta=config.rope_theta,\n",
    "            rope_scaling=config.rope_scaling,\n",
    "            attention_bias=config.attention_bias,\n",
    "            name=\"self_attn\"\n",
    "        )\n",
    "        \n",
    "        # Determine whether to use MoE or dense MLP based on layer index and config\n",
    "        self.use_moe = (\n",
    "            config.n_routed_experts is not None and\n",
    "            layer_idx >= config.first_k_dense_replace and\n",
    "            layer_idx % config.moe_layer_freq == 0\n",
    "        )\n",
    "        \n",
    "        if self.use_moe:\n",
    "            # Initialize MoE layer for routed experts\n",
    "            self.mlp = DeepSeekMoE(\n",
    "                hidden_size=config.hidden_size,\n",
    "                n_routed_experts=config.n_routed_experts,\n",
    "                n_shared_experts=config.n_shared_experts,\n",
    "                num_experts_per_tok=config.num_experts_per_tok,\n",
    "                moe_intermediate_size=config.moe_intermediate_size,\n",
    "                name=\"moe\"\n",
    "            )\n",
    "        else:\n",
    "            # Initialize dense MLP layer\n",
    "            self.mlp = DeepSeekMLP(\n",
    "                hidden_size=config.hidden_size,\n",
    "                intermediate_size=config.intermediate_size,\n",
    "                hidden_act=config.hidden_act,\n",
    "                name=\"mlp\"\n",
    "            )\n",
    "            \n",
    "        # Initialize input and post-attention normalization layers\n",
    "        self.input_layernorm = DeepSeekRMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps, name=\"input_layernorm\"\n",
    "        )\n",
    "        self.post_attention_layernorm = DeepSeekRMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps, name=\"post_attention_layernorm\"\n",
    "        )\n",
    "        \n",
    "    def call(self, hidden_states, attention_mask=None, position_ids=None, training=False):\n",
    "        # Apply input normalization and self-attention with residual connection\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        attn_output = self.self_attn(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            training=training\n",
    "        )\n",
    "        hidden_states = residual + attn_output\n",
    "        \n",
    "        # Apply post-attention normalization and MLP/MoE with residual connection\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        mlp_output = self.mlp(hidden_states, training=training)\n",
    "        hidden_states = residual + mlp_output\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "# MODEL CONFIGURATION \n",
    "\n",
    "class DeepSeekConfig:\n",
    "    \"\"\"\n",
    "    This is the configuration class to store the configuration of a [`DeepseekMoEModel`]. It is used to instantiate an DeepSeek\n",
    "    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
    "    defaults below will yield a configuration for a small, stable, and computationally efficient MoE chatbot model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (`int`, *optional*, defaults to 12000):\n",
    "            Vocabulary size of the Deep model. Defines the number of different tokens that can be represented by the\n",
    "            `inputs_ids` passed when calling [`DeepseekModel`]. (Optimized for chatbot use case).\n",
    "            \n",
    "        hidden_size (`int`, *optional*, defaults to 256):\n",
    "            Dimension of the hidden representations. (Optimized hidden size for stability and performance).\n",
    "            \n",
    "        intermediate_size (`int`, *optional*, defaults to 512):\n",
    "            Dimension of the MLP representations. (Balanced intermediate size for MLP layers).\n",
    "            \n",
    "        moe_intermediate_size (`int`, *optional*, defaults to 256):\n",
    "            Dimension of the MoE representations. (MoE-specific intermediate dimension).\n",
    "            \n",
    "        num_hidden_layers (`int`, *optional*, defaults to 4):\n",
    "            Number of hidden layers in the Transformer decoder. (Optimal layer count for chatbot tasks).\n",
    "            \n",
    "        num_attention_heads (`int`, *optional*, defaults to 4):\n",
    "            Number of attention heads for each attention layer in the Transformer decoder. (Balanced attention heads for computation).\n",
    "        \n",
    "        n_shared_experts (`int`, *optional*, defaults to 1):\n",
    "            Number of shared experts, None means dense model. (Set to 1, a single shared expert for parameter efficiency).\n",
    "        \n",
    "        n_routed_experts (`int`, *optional*, defaults to 4):\n",
    "            Number of routed experts, None means dense model. (Set to 4 for specialization).\n",
    "            \n",
    "        num_experts_per_tok (`int`, *optional*, defaults to 2):\n",
    "            Number of selected experts, None means dense model. (Set to 2 experts per token for balanced load).\n",
    "            \n",
    "        moe_layer_freq (`int`, *optional*, defaults to 2):\n",
    "            The frequency of the MoE layer: one expert layer for every `moe_layer_freq - 1` dense layers.\n",
    "            (MoE every other layer for computational balance).\n",
    "            \n",
    "        first_k_dense_replace (`int`, *optional*, defaults to 1):\n",
    "            - Number of dense layers in shallow layers(embed->dense->dense->...->dense->moe->moe...->lm_head).\n",
    "            - k dense layers (First layer dense for stability).\n",
    "        norm_topk_prob (`bool`, *optional*, defaults to False):\n",
    "            Whether to normalize the weights of the routed experts. (No normalization for expert weights).\n",
    "            \n",
    "        scoring_func (`str`, *optional*, defaults to 'softmax'):\n",
    "            Method of computing expert weights. (Standard softmax for expert selection).\n",
    "            \n",
    "        aux_loss_alpha (`float`, *optional*, defaults to 0.001):\n",
    "            Auxiliary loss weight coefficient. (Default auxiliary loss weight).\n",
    "            \n",
    "        seq_aux = (`bool`, *optional*, defaults to True):\n",
    "            Whether to compute the auxiliary loss for each individual sample. (Sequence-level auxiliary loss).\n",
    "            \n",
    "        num_key_value_heads (`int`, *optional*, defaults to 2):\n",
    "            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
    "            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
    "            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used.\n",
    "            (Set to 2 for GQA and memory efficiency).\n",
    "            \n",
    "        hidden_act (`str` or `function`, *optional*, defaults to \"swish\"):\n",
    "            The non-linear activation function (function or string) in the decoder. (Swish activation for better gradients).\n",
    "        \n",
    "        max_position_embeddings (`int`, *optional*, defaults to 256):\n",
    "            The maximum sequence length that this model might ever be used with. (Sufficient context length for chatbot).\n",
    "        \n",
    "        initializer_range (`float`, *optional*, defaults to 0.001):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices. (Reduced initializer range for stability).\n",
    "        \n",
    "        rms_norm_eps (`float`, *optional*, defaults to 1e-6):\n",
    "            The epsilon used by the rms normalization layers. (Standard RMS norm epsilon).\n",
    "        \n",
    "        use_cache (`bool`, *optional*, defaults to True):\n",
    "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
    "            relevant if `config.is_decoder=True`. (Enable caching for inference).\n",
    "        \n",
    "        pad_token_id (`int`, *optional*):\n",
    "            Padding token id.\n",
    "        bos_token_id (`int`, *optional*, defaults to 1):\n",
    "            Beginning of stream token id.\n",
    "        eos_token_id (`int`, *optional*, defaults to 2):\n",
    "            End of stream token id.\n",
    "        \n",
    "        pretraining_tp (`int`, *optional*, defaults to 1):\n",
    "            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
    "            document](https://huggingface.co/docs/transformers/parallelism) to understand more about it. This value is\n",
    "            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
    "            issue](https://github.com/pytorch/pytorch/issues/76232).\n",
    "        \n",
    "        tie_word_embeddings (`bool`, *optional*, defaults to False):\n",
    "            Whether to tie weight embeddings\n",
    "        \n",
    "        rope_theta (`float`, *optional*, defaults to 10000.0):\n",
    "            The base period of the RoPE embeddings. (Standard RoPE base period).\n",
    "        \n",
    "        rope_scaling (`Dict`, *optional*, defaults to {\"type\": \"linear\", \"factor\": 2.0}):\n",
    "            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
    "            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
    "            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
    "            `max_position_embeddings` to the expected new maximum. (Linear RoPE scaling enabled).\n",
    "        \n",
    "        attention_bias (`bool`, defaults to False, *optional*, defaults to False):\n",
    "            Whether to use a bias in the query, key, value and output projection layers during self-attention. (No attention bias for efficiency).\n",
    "        \n",
    "        attention_dropout (`float`, *optional*, defaults to 0.1):\n",
    "            The dropout ratio for the attention probabilities. (Increased dropout for regularization).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=vocab_size,          # Default vocabulary size optimized for chatbot\n",
    "        hidden_size=256,                # Optimized hidden size for stability and performance\n",
    "        intermediate_size=512,          # Balanced intermediate size for MLP layers\n",
    "        moe_intermediate_size=256,      # MoE-specific intermediate dimension\n",
    "        num_hidden_layers=4,            # Optimal layer count for chatbot tasks\n",
    "        num_attention_heads=4,          # Balanced attention heads for computation\n",
    "        num_key_value_heads=2,          # GQA for memory efficiency\n",
    "        n_shared_experts=1,             # Single shared expert for parameter efficiency\n",
    "        n_routed_experts=4,             # Four routed experts for specialization\n",
    "        num_experts_per_tok=2,          # Two experts per token for balanced load\n",
    "        moe_layer_freq=2,               # MoE every other layer for computational balance\n",
    "        first_k_dense_replace=1,        # First layer dense for stability\n",
    "        norm_topk_prob=False,           # No normalization for expert weights\n",
    "        scoring_func='softmax',         # Standard softmax for expert selection\n",
    "        aux_loss_alpha=0.001,           # Default auxiliary loss weight\n",
    "        seq_aux=True,                   # Sequence-level auxiliary loss\n",
    "        hidden_act=\"swish\",             # Swish activation for better gradients\n",
    "        max_position_embeddings=256,    # Sufficient context length for chatbot\n",
    "        initializer_range=0.001,        # Reduced initializer range for stability\n",
    "        rms_norm_eps=1e-6,              # Standard RMS norm epsilon\n",
    "        use_cache=True,                 # Enable caching for inference\n",
    "        rope_theta=10000.0,             # Standard RoPE base period\n",
    "        rope_scaling={\"type\": \"linear\", \"factor\": 2.0},  # Linear RoPE scaling\n",
    "        attention_bias=False,           # No attention bias for efficiency\n",
    "        attention_dropout=0.1           # Increased dropout for regularization\n",
    "    ):\n",
    "        # Initialize model architecture parameters with optimized values\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.moe_intermediate_size = moe_intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.n_shared_experts = n_shared_experts\n",
    "        self.n_routed_experts = n_routed_experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok\n",
    "        self.moe_layer_freq = moe_layer_freq\n",
    "        self.first_k_dense_replace = first_k_dense_replace\n",
    "        self.norm_topk_prob = norm_topk_prob\n",
    "        self.scoring_func = scoring_func\n",
    "        self.aux_loss_alpha = aux_loss_alpha\n",
    "        self.seq_aux = seq_aux\n",
    "        self.hidden_act = hidden_act\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.initializer_range = initializer_range\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.use_cache = use_cache\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self.attention_bias = attention_bias\n",
    "        self.attention_dropout = attention_dropout\n",
    "\n",
    "class DeepSeekModel(tf.keras.Model):\n",
    "    \"\"\"Core DeepSeek transformer model for chatbot, provide raw hidden states\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.padding_idx = 0\n",
    "        \n",
    "        # Initialize embedding layer for token inputs\n",
    "        self.embed_tokens = layers.Embedding(\n",
    "            config.vocab_size,\n",
    "            config.hidden_size,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomNormal(\n",
    "                stddev=config.initializer_range\n",
    "            ),\n",
    "            name=\"embed_tokens\"\n",
    "        )\n",
    "        \n",
    "        # Initialize decoder layers\n",
    "        self.decoder_layers = [\n",
    "            DeepSeekDecoderLayer(config, layer_idx=i, name=f\"layer_{i}\")\n",
    "            for i in range(config.num_hidden_layers)\n",
    "        ]\n",
    "        \n",
    "        # Initialize final normalization layer\n",
    "        self.norm = DeepSeekRMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps, name=\"norm\"\n",
    "        )\n",
    "        \n",
    "        # Initialize expert assignment counter for MoE layers\n",
    "        self.expert_assignment_counts = tf.Variable(\n",
    "            tf.zeros([config.n_routed_experts], dtype=tf.int32),\n",
    "            trainable=False,\n",
    "            name=\"expert_assignment_counts\"\n",
    "        ) if config.n_routed_experts else None\n",
    "        \n",
    "    def call(self, input_ids, attention_mask=None, position_ids=None, training=False):\n",
    "        # Embed input tokens\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "        \n",
    "        # Create causal mask if not provided\n",
    "        if attention_mask is None:\n",
    "            seq_len = tf.shape(input_ids)[1]\n",
    "            attention_mask = self._create_causal_mask(seq_len)\n",
    "            \n",
    "        # Prepare attention mask for causal attention\n",
    "        attention_mask = tf.cast(attention_mask, hidden_states.dtype)\n",
    "        if len(attention_mask.shape) == 2:\n",
    "            attention_mask = tf.expand_dims(tf.expand_dims(attention_mask, 1), 1)\n",
    "        elif len(attention_mask.shape) == 3:\n",
    "            attention_mask = tf.expand_dims(attention_mask, 1)\n",
    "        attention_mask = (tf.cast(1.0, hidden_states.dtype) - attention_mask) * tf.cast(-1e4, hidden_states.dtype)\n",
    "        \n",
    "        # Process through decoder layers and aggregate expert counts\n",
    "        for layer in self.decoder_layers:\n",
    "            hidden_states = layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                training=training\n",
    "            )\n",
    "            # Update expert assignment counts for MoE layers during training\n",
    "            if training and hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE):\n",
    "                expert_counts = layer.mlp.gate.expert_counts\n",
    "                if self.expert_assignment_counts is not None:\n",
    "                    self.expert_assignment_counts.assign_add(expert_counts)\n",
    "            \n",
    "        # Apply final normalization\n",
    "        return self.norm(hidden_states)\n",
    "        \n",
    "    def _create_causal_mask(self, seq_len):\n",
    "        \"\"\"Create causal attention mask to prevent attending to future tokens\"\"\"\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return tf.cast(mask, tf.float16)\n",
    "\n",
    "class DeepSeekForCausalLM(tf.keras.Model):\n",
    "    \"\"\"Causal language model for chatbot text generation\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.transformer = DeepSeekModel(config, name=\"transformer\")\n",
    "        # Initialize language model head\n",
    "        self.lm_head = layers.Dense(\n",
    "            config.vocab_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                stddev=config.initializer_range\n",
    "            ),\n",
    "            use_bias=False,\n",
    "            name=\"lm_head\"\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, attention_mask=None, position_ids=None, training=False):\n",
    "        # Handle dictionary or tensor inputs\n",
    "        if isinstance(inputs, dict):\n",
    "            input_ids = inputs.get('input_ids')\n",
    "            attention_mask = inputs.get('attention_mask', attention_mask)\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "        \n",
    "        # Process through transformer\n",
    "        hidden_states = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            training=training\n",
    "        )\n",
    "        # Generate logits for token prediction\n",
    "        return self.lm_head(hidden_states)\n",
    "\n",
    "# METRICS AND CALLBACKS\n",
    "\n",
    "class ResourceMonitor(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to monitor system resources (GPU, CPU, memory) during training\"\"\"\n",
    "    def __init__(self, batch_size=2):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.resource_metrics = []  # Store resource usage metrics\n",
    "        self.start_time = time.time()\n",
    "        self.total_tokens_processed = 0\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "        self.epoch_tokens = 0\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # Update token count for throughput calculation\n",
    "        self.total_tokens_processed += self.batch_size * max_seq_len\n",
    "        self.epoch_tokens += self.batch_size * max_seq_len\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        try:\n",
    "            # Collect GPU and system resource usage\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            gpu_memory = sum([gpu.memoryUsed for gpu in gpus]) / len(gpus) if gpus else 0\n",
    "            gpu_usage = sum([gpu.load * 100 for gpu in gpus]) / len(gpus) if gpus else 0\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory_info = psutil.virtual_memory()\n",
    "            system_memory = memory_info.used / (1024 ** 3)\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            tokens_per_second = self.epoch_tokens / epoch_time if epoch_time > 0 else 0\n",
    "            \n",
    "            # Store resource metrics\n",
    "            resource_stats = {\n",
    "                'epoch': epoch + 1,\n",
    "                'avg_gpu_memory_gb': float(gpu_memory / 1024),\n",
    "                'avg_system_memory_gb': float(system_memory),\n",
    "                'avg_gpu_usage_percent': float(gpu_usage),\n",
    "                'avg_cpu_usage_percent': float(cpu_usage),\n",
    "                'tokens_per_second': float(tokens_per_second),\n",
    "                'throughput_tps': float(tokens_per_second)\n",
    "            }\n",
    "            \n",
    "            self.resource_metrics.append(resource_stats)\n",
    "            \n",
    "            # Print resource usage summary\n",
    "            print(f\"Epoch {epoch+1} Resources: GPU Mem: {gpu_memory/1024:.1f}GB, \"\n",
    "                  f\"System Mem: {system_memory:.1f}GB, GPU: {gpu_usage:.1f}%, \"\n",
    "                  f\"CPU: {cpu_usage:.1f}%, TPS: {tokens_per_second:.0f}\")\n",
    "                  \n",
    "        except Exception as e:\n",
    "            print(f\"Resource monitoring error: {e}\")\n",
    "\n",
    "class DeepSeekMetrics(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to track specific metrics, including MaxVIO for expert balancing\"\"\"\n",
    "    def __init__(self, validation_data, tokenizer):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.metrics_history = []\n",
    "        self.start_time = time.time()\n",
    "        self.max_vio_values = []\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Reset expert assignment counts for accurate per-epoch metrics\n",
    "        if hasattr(self.model.transformer, 'expert_assignment_counts') and self.model.transformer.expert_assignment_counts is not None:\n",
    "            self.model.transformer.expert_assignment_counts.assign(tf.zeros([self.model.config.n_routed_experts], dtype=tf.int32))\n",
    "        for layer in self.model.transformer.decoder_layers:\n",
    "            if hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE):\n",
    "                layer.mlp.gate.expert_counts.assign(tf.zeros([layer.mlp.n_routed_experts], dtype=tf.int32))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        \n",
    "        # Estimate FLOPs for computational efficiency tracking\n",
    "        total_params = self.model.count_params()\n",
    "        flops_per_token = total_params * 2\n",
    "        total_flops = flops_per_token * batch_size * max_seq_len\n",
    "        total_gflops = total_flops / 1e9\n",
    "        \n",
    "        logs['gflops'] = float(total_gflops)\n",
    "        \n",
    "        # Calculate MaxVIO for expert load balancing\n",
    "        if self.validation_data:\n",
    "            try:\n",
    "                max_vio = self._calculate_max_vio()\n",
    "                logs['max_vio'] = float(max_vio)\n",
    "                self.max_vio_values.append(max_vio)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Metrics calculation error: {e}\")\n",
    "                logs['max_vio'] = 0.0\n",
    "                \n",
    "        self.metrics_history.append(logs.copy())\n",
    "        \n",
    "        # Print simplified metrics summary\n",
    "        print(f\"Epoch {epoch+1}: Loss={logs.get('loss', 0):.4f}, \"\n",
    "              f\"Val_Loss={logs.get('val_loss', 0):.4f}, \"\n",
    "              f\"GFLOPs={total_gflops:.2f}, \"\n",
    "              f\"MaxVIO={logs.get('max_vio', 0):.4f}\")\n",
    "              \n",
    "    def _calculate_max_vio(self):\n",
    "        \"\"\"Calculate Maximum Violation (MaxVIO) for expert load balancing\"\"\"\n",
    "        total_counts = np.zeros(self.model.config.n_routed_experts, dtype=np.float32)\n",
    "        moe_layers = 0\n",
    "        # Aggregate expert counts across MoE layers\n",
    "        for layer in self.model.transformer.decoder_layers:\n",
    "            if hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE):\n",
    "                counts = layer.mlp.gate.expert_counts.numpy()\n",
    "                total_counts += counts\n",
    "                moe_layers += 1\n",
    "        \n",
    "        # Compute MaxVIO based on aggregated counts\n",
    "        if moe_layers > 0 and np.sum(total_counts) > 0:\n",
    "            normalized_counts = total_counts / (np.sum(total_counts) + 1e-10)\n",
    "            expected_uniform = 1.0 / len(total_counts)\n",
    "            max_vio = np.max(np.abs(normalized_counts - expected_uniform)) / expected_uniform\n",
    "            return max_vio\n",
    "        return 0.0\n",
    "\n",
    "class ComprehensiveModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback for saving model checkpoints based on monitored metric\"\"\"\n",
    "    def __init__(self, filepath, monitor='val_loss', save_best_only=True, mode='min'):\n",
    "        super().__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.save_best_only = save_best_only\n",
    "        self.mode = mode\n",
    "        self.best_value = float('inf') if mode == 'min' else -float('inf')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            return\n",
    "            \n",
    "        current_value = logs.get(self.monitor)\n",
    "        if current_value is None:\n",
    "            return\n",
    "            \n",
    "        # Determine if current model should be saved\n",
    "        if self.mode == 'min':\n",
    "            should_save = current_value < self.best_value\n",
    "        else:\n",
    "            should_save = current_value > self.best_value\n",
    "            \n",
    "        if should_save or not self.save_best_only:\n",
    "            self.best_value = current_value\n",
    "            self.model.save_weights(self.filepath)\n",
    "            print(f\"Model checkpoint saved with {self.monitor}: {current_value:.4f}\")\n",
    "\n",
    "# Measure Perplexity\n",
    "\n",
    "# Function to calculate perplexity on validation set\n",
    "def calculate_perplexity(model, dataset, tokenizer, max_batches=100):          # Sample batch for calculate perplexity\n",
    "    \"\"\"Calculate perplexity on validation dataset to evaluate model performance\"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    PAD_TOKEN_ID = tokenizer.token_to_id(PAD_TOKEN)\n",
    "    for i, batch in enumerate(dataset.take(max_batches)):\n",
    "        inputs, labels = batch\n",
    "        logits = model(inputs, training=False)\n",
    "        # Clip logits for numerical stability\n",
    "        logits = tf.clip_by_value(logits, -50.0, 50.0)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        # Compute per-token loss\n",
    "        per_token_loss = -tf.gather(log_probs, labels, batch_dims=2)\n",
    "        if tf.reduce_any(tf.math.is_nan(per_token_loss)):\n",
    "            continue\n",
    "        # Mask padding tokens\n",
    "        mask = tf.cast(labels != PAD_TOKEN_ID, per_token_loss.dtype)\n",
    "        batch_loss = tf.reduce_sum(per_token_loss * mask)\n",
    "        batch_tokens = tf.reduce_sum(mask)\n",
    "        if tf.math.is_nan(batch_loss) or batch_tokens == 0:\n",
    "            continue\n",
    "        total_loss += batch_loss.numpy()\n",
    "        total_tokens += batch_tokens.numpy()\n",
    "    # Compute perplexity as exponential of average loss\n",
    "    if total_tokens > 0:\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        return math.exp(min(avg_loss, 20.0))\n",
    "    return float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary\n",
      "Model: \"deep_seek_for_causal_lm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (DeepSeekModel  multiple                  3375624   \n",
      " )                                                               \n",
      "                                                                 \n",
      " lm_head (Dense)             multiple                  292096    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3667720 (13.99 MB)\n",
      "Trainable params: 3536640 (13.49 MB)\n",
      "Non-trainable params: 131080 (512.03 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Visualize the model\n",
    "# Create model with specified vocabulary size using direct config\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "# Use the optimized config directly with the same hyperparameters from create_deepseek_chatbot_model\n",
    "config = DeepSeekConfig(vocab_size=vocab_size)\n",
    "model = DeepSeekForCausalLM(config)\n",
    "\n",
    "\n",
    "# Build model\n",
    "if tf_train_dataset:\n",
    "    sample_batch = next(iter(tf_train_dataset.take(1)))\n",
    "    model(sample_batch[0])\n",
    "\n",
    "# Configure optimizer with more conservative settings\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,  # Reduced learning rate\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.95,\n",
    "    weight_decay=0.1,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        ignore_class=PAD_TOKEN_ID,  # Use correct PAD token ID\n",
    "        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
    "    ),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nModel Summary\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "55/55 [==============================] - ETA: 0s - loss: 6.6069 - accuracy: 0.0168Epoch 1: Loss=6.6069, Val_Loss=6.1543, GFLOPs=2.02, MaxVIO=0.0327\n",
      "Epoch 1 Resources: GPU Mem: 14.3GB, System Mem: 9.0GB, GPU: 11.0%, CPU: 29.3%, TPS: 222\n",
      "55/55 [==============================] - 35s 151ms/step - loss: 6.6069 - accuracy: 0.0168 - val_loss: 6.1543 - val_accuracy: 0.0128 - lr: 9.9433e-05 - gflops: 2.0189 - max_vio: 0.0327\n",
      "Calculating final perplexity\n",
      "Final Training Loss: 6.6069\n",
      "Final Validation Loss: 6.1543\n",
      "Final Perplexity: 488.53\n",
      "New best model saved from iteration 2 with perplexity: 488.53\n",
      "Total Training Time 34.64 seconds\n",
      "Average TPS: 222\n",
      "\n",
      "Training Results Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initial Training Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average TPS</th>\n",
       "      <td>222.385694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Training Loss</th>\n",
       "      <td>6.606869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Validation Loss</th>\n",
       "      <td>6.154297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computational Resource Usage</th>\n",
       "      <td>40.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>29.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>9.004910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.274414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPS Estimate (GFLOPS)</th>\n",
       "      <td>2.018874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Validation Perplexity</th>\n",
       "      <td>488.532622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average MaxVIO_global</th>\n",
       "      <td>0.032727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lowest MaxVIO_global</th>\n",
       "      <td>0.032727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highest MaxVIO_global</th>\n",
       "      <td>0.032727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Initial Training Result\n",
       "Average TPS                                   222.385694\n",
       "Final Training Loss                             6.606869\n",
       "Final Validation Loss                           6.154297\n",
       "Computational Resource Usage                   40.300000\n",
       "Average CPU Usage (percent)                    29.300000\n",
       "Average GPU Usage (percent)                    11.000000\n",
       "Average Memory (GB)                             9.004910\n",
       "Average GPU Memory (GB)                        14.274414\n",
       "Average FLOPS Estimate (GFLOPS)                 2.018874\n",
       "Final Validation Perplexity                   488.532622\n",
       "Average MaxVIO_global                           0.032727\n",
       "Lowest MaxVIO_global                            0.032727\n",
       "Highest MaxVIO_global                           0.032727"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect training and results storage\n",
    "results_table = {\n",
    "    'Average TPS': {},\n",
    "    'Final Training Loss': {},\n",
    "    'Final Validation Loss': {},\n",
    "    'Computational Resource Usage': {},\n",
    "    'Average CPU Usage (percent)': {},\n",
    "    'Average GPU Usage (percent)': {},\n",
    "    'Average Memory (GB)': {},\n",
    "    'Average GPU Memory (GB)': {},\n",
    "    'Average FLOPS Estimate (GFLOPS)': {},\n",
    "    'Final Validation Perplexity': {},\n",
    "    'Average MaxVIO_global': {},\n",
    "    'Lowest MaxVIO_global': {},\n",
    "    'Highest MaxVIO_global': {},\n",
    "}\n",
    "\n",
    "best_perplexity = float('inf')\n",
    "BEST_MODEL_PATH = 'best_deepseek_chatbot_model'\n",
    "\n",
    "# Create model with specified vocabulary size using direct config\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "# Use the optimized config directly with the same hyperparameters from create_deepseek_chatbot_model\n",
    "config = DeepSeekConfig(vocab_size=vocab_size)\n",
    "model = DeepSeekForCausalLM(config)\n",
    "\n",
    "# Build model with sample batch\n",
    "if tf_train_dataset:\n",
    "    sample_batch = next(iter(tf_train_dataset.take(1)))\n",
    "    model(sample_batch[0])\n",
    "\n",
    "# Configure optimizer with exponential decay learning rate\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.95,\n",
    "    weight_decay=0.01,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "PAD_TOKEN_ID = tokenizer.token_to_id(PAD_TOKEN)\n",
    "\n",
    "# Compile model with sparse categorical crossentropy loss\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        ignore_class=PAD_TOKEN_ID\n",
    "    ),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define training callbacks\n",
    "metrics_callback = DeepSeekMetrics(tf_val_dataset, tokenizer) if tf_val_dataset else None\n",
    "resource_monitor = ResourceMonitor(batch_size=2)\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=3, \n",
    "        restore_best_weights=True, \n",
    "        monitor='val_loss',\n",
    "        min_delta=0.01\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        factor=0.5, \n",
    "        patience=2, \n",
    "        monitor='val_loss',\n",
    "        min_delta=0.01,\n",
    "        min_lr=1e-6\n",
    "    ),\n",
    "    tf.keras.callbacks.TerminateOnNaN(),\n",
    "]\n",
    "\n",
    "if metrics_callback:\n",
    "    callbacks.append(metrics_callback)\n",
    "if resource_monitor:\n",
    "    callbacks.append(resource_monitor)\n",
    "\n",
    "# Train model on GPU\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Force GPU for training the model\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(\n",
    "        tf_train_dataset,\n",
    "        epochs=100,\n",
    "        validation_data=tf_val_dataset,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "total_training_time = time.time() - start_time\n",
    "\n",
    "# Calculate final metrics\n",
    "print(\"Calculating final perplexity\")\n",
    "final_perplexity = calculate_perplexity(model, tf_val_dataset, tokenizer, max_batches=50) if tf_val_dataset else float('inf')\n",
    "final_training_loss = history.history['loss'][-1] if 'loss' in history.history else float('inf')\n",
    "final_validation_loss = history.history['val_loss'][-1] if 'val_loss' in history.history else float('inf')\n",
    "\n",
    "print(f\"Final Training Loss: {final_training_loss:.4f}\")\n",
    "print(f\"Final Validation Loss: {final_validation_loss:.4f}\")\n",
    "print(f\"Final Perplexity: {final_perplexity:.2f}\")\n",
    "\n",
    "# Save best model based on perplexity\n",
    "if final_perplexity < best_perplexity:\n",
    "    best_perplexity = final_perplexity\n",
    "    best_iteration = iteration + 1\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(BEST_MODEL_PATH):\n",
    "            shutil.rmtree(BEST_MODEL_PATH)\n",
    "        model.save(BEST_MODEL_PATH, save_format='tf')\n",
    "        print(f\"New best model saved from iteration {iteration + 1} with perplexity: {final_perplexity:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving best model: {e}\")\n",
    "\n",
    "# Collect and store resource metrics\n",
    "if resource_monitor.resource_metrics:\n",
    "    avg_gpu_memory = np.mean([m['avg_gpu_memory_gb'] for m in resource_monitor.resource_metrics])\n",
    "    avg_system_memory = np.mean([m['avg_system_memory_gb'] for m in resource_monitor.resource_metrics])\n",
    "    avg_gpu_usage = np.mean([m['avg_gpu_usage_percent'] for m in resource_monitor.resource_metrics])\n",
    "    avg_cpu_usage = np.mean([m['avg_cpu_usage_percent'] for m in resource_monitor.resource_metrics])\n",
    "    avg_tps = np.mean([m['tokens_per_second'] for m in resource_monitor.resource_metrics])\n",
    "else:\n",
    "    avg_gpu_memory = avg_system_memory = avg_gpu_usage = avg_cpu_usage = avg_tps = 0\n",
    "\n",
    "# Calculate FLOPs\n",
    "total_params = model.count_params()\n",
    "batch_size = 2\n",
    "seq_length = max_seq_len\n",
    "flops_per_token = total_params * 2\n",
    "total_flops = flops_per_token * batch_size * seq_length\n",
    "total_gflops = total_flops / 1e9\n",
    "\n",
    "# Collect MaxVIO metrics\n",
    "if metrics_callback and hasattr(metrics_callback, 'max_vio_values'):\n",
    "    max_vio_values = metrics_callback.max_vio_values\n",
    "    if max_vio_values:\n",
    "        avg_max_vio = np.mean(max_vio_values)\n",
    "        min_max_vio = np.min(max_vio_values)\n",
    "        max_max_vio = np.max(max_vio_values)\n",
    "    else:\n",
    "        avg_max_vio = min_max_vio = max_max_vio = 0.0\n",
    "else:\n",
    "    avg_max_vio = min_max_vio = max_max_vio = 0.0\n",
    "\n",
    "# Store results in results table\n",
    "iteration_key = f'Initial Training Result'\n",
    "results_table['Average TPS'][iteration_key] = avg_tps\n",
    "results_table['Final Training Loss'][iteration_key] = final_training_loss\n",
    "results_table['Final Validation Loss'][iteration_key] = final_validation_loss\n",
    "results_table['Computational Resource Usage'][iteration_key] = avg_cpu_usage + avg_gpu_usage\n",
    "results_table['Average CPU Usage (percent)'][iteration_key] = avg_cpu_usage\n",
    "results_table['Average GPU Usage (percent)'][iteration_key] = avg_gpu_usage\n",
    "results_table['Average Memory (GB)'][iteration_key] = avg_system_memory\n",
    "results_table['Average GPU Memory (GB)'][iteration_key] = avg_gpu_memory\n",
    "results_table['Average FLOPS Estimate (GFLOPS)'][iteration_key] = total_gflops\n",
    "results_table['Final Validation Perplexity'][iteration_key] = final_perplexity\n",
    "results_table['Average MaxVIO_global'][iteration_key] = avg_max_vio\n",
    "results_table['Lowest MaxVIO_global'][iteration_key] = min_max_vio\n",
    "results_table['Highest MaxVIO_global'][iteration_key] = max_max_vio\n",
    "\n",
    "print(f\"Total Training Time {total_training_time:.2f} seconds\")\n",
    "print(f\"Average TPS: {avg_tps:.0f}\")\n",
    "\n",
    "# Create and display results DataFrame\n",
    "final_result = pd.DataFrame(results_table)\n",
    "final_result = final_result.T\n",
    "\n",
    "print(\"\\nTraining Results Summary:\")\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n",
      "TensorFlow version: 2.15.0\n",
      "Mixed precision policy: mixed_float16\n",
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# Disable XLA (XLA JIT compilation) to prevent CUDA graph errors and ensure stability\n",
    "tf.config.optimizer.set_jit(False)  # Disabled XLA to prevent CUDA graph errors\n",
    "\n",
    "# Configure GPU memory growth to prevent memory allocation issues\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Enable mixed precision training with FP16 for improved performance and reduced memory usage\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Define global configuration parameters for the model and preprocessing pipeline\n",
    "vocab_size = 12000\n",
    "max_seq_len = 70\n",
    "batch_size = 2\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "SEP_TOKEN = \"<sep>\"\n",
    "\n",
    "# Print TensorFlow version, mixed precision policy, and GPU availability for debugging\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Mixed precision policy: {mixed_precision.global_policy().name}\")\n",
    "print(f\"GPU available: {len(gpus) > 0}\")\n",
    "\n",
    "# DEEPSEEK MOE COMPONENTS\n",
    "\n",
    "class DeepSeekRMSNorm(layers.Layer):\n",
    "    \"\"\"RMS Normalization layer\"\"\"\n",
    "    def __init__(self, hidden_size, eps=1e-6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size  # Dimension of the hidden representations\n",
    "        self.variance_epsilon = eps     # Small constant to avoid division by zero\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize learnable weight parameter for scaling normalized output\n",
    "        self.weight = self.add_weight(\n",
    "            name='weight',\n",
    "            shape=(self.hidden_size,),\n",
    "            initializer='ones',\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        x = tf.cast(x, tf.float32)  # Cast input to float32 for stable computation\n",
    "        \n",
    "        # Compute variance as mean of squared inputs along the last dimension\n",
    "        variance = tf.reduce_mean(tf.square(x), axis=-1, keepdims=True)\n",
    "        # Normalize input using RMS formula with epsilon for numerical stability\n",
    "        x = x * tf.math.rsqrt(variance + self.variance_epsilon)\n",
    "        \n",
    "        # Scale normalized output with learnable weight and cast back to original dtype\n",
    "        return self.weight * tf.cast(x, input_dtype)\n",
    "\n",
    "class RotaryPositionEmbedding(layers.Layer):\n",
    "    \"\"\"Rotary Position Embedding (RoPE) with dynamic NTK scaling for positional encoding\"\"\"\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, scaling_type=None, scaling_factor=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim                                          # Dimension of the head embeddings\n",
    "        self.max_position_embeddings = max_position_embeddings  # Maximum sequence length\n",
    "        self.base = base                                        # Base value for frequency calculation\n",
    "        self.scaling_type = scaling_type                        # Type of scaling (\"linear\" or \"dynamic\")\n",
    "        self.scaling_factor = scaling_factor                    # Scaling factor for RoPE\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Compute inverse frequencies for rotary embeddings\n",
    "        inv_freq = 1.0 / (self.base ** (tf.range(0, self.dim, 2, dtype=tf.float32) / tf.cast(self.dim, tf.float32)))\n",
    "        self.inv_freq = tf.constant(inv_freq, dtype=tf.float32)\n",
    "        self._compute_rotary_cache()                            # Precompute cosine and sine caches\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def _compute_rotary_cache(self):\n",
    "        \"\"\"Compute cosine and sine caches for rotary embeddings with optional scaling\"\"\"\n",
    "        if self.scaling_type == \"linear\":\n",
    "            # Linear scaling: divide positions by scaling factor\n",
    "            t = tf.range(self.max_position_embeddings, dtype=tf.float32) / self.scaling_factor\n",
    "        elif self.scaling_type == \"dynamic\":\n",
    "            # Dynamic NTK scaling for extended context lengths\n",
    "            if self.max_position_embeddings > self.base:\n",
    "                base = self.base * (\n",
    "                    (self.scaling_factor * self.max_position_embeddings / self.base) - \n",
    "                    (self.scaling_factor - 1)\n",
    "                ) ** (self.dim / (self.dim - 2))\n",
    "                inv_freq = 1.0 / (base ** (tf.range(0, self.dim, 2, dtype=tf.float32) / tf.cast(self.dim, tf.float32)))\n",
    "                self.inv_freq = tf.constant(inv_freq, dtype=tf.float32)\n",
    "            t = tf.range(self.max_position_embeddings, dtype=tf.float32)\n",
    "        else:\n",
    "            # Default case: no scaling\n",
    "            t = tf.range(self.max_position_embeddings, dtype=tf.float32)\n",
    "            \n",
    "        # Compute frequency matrix for rotary embeddings\n",
    "        freqs = tf.einsum('i,j->ij', t, self.inv_freq)\n",
    "        emb = tf.concat([freqs, freqs], axis=-1)                # Concatenate for full embedding dimension\n",
    "        \n",
    "        # Cache cosine and sine values as non-trainable variables\n",
    "        self.cos_cached = tf.Variable(tf.cos(emb), trainable=False, name='cos_cached')\n",
    "        self.sin_cached = tf.Variable(tf.sin(emb), trainable=False, name='sin_cached')\n",
    "        \n",
    "    def call(self, x, position_ids=None, seq_len=None):\n",
    "        # Get input tensor dimensions\n",
    "        batch_size, seq_len_x, num_heads, head_dim = tf.unstack(tf.shape(x))\n",
    "        \n",
    "        # Generate position IDs if not provided\n",
    "        if position_ids is None:\n",
    "            position_ids = tf.range(seq_len_x, dtype=tf.int32)\n",
    "            position_ids = tf.tile(tf.reshape(position_ids, [1, -1]), [batch_size, 1])\n",
    "        \n",
    "        # Gather precomputed cosine and sine values for given position IDs\n",
    "        cos = tf.gather(self.cos_cached, position_ids, axis=0)  # Shape: [batch_size, seq_len_x, head_dim]\n",
    "        sin = tf.gather(self.sin_cached, position_ids, axis=0)  # Shape: [batch_size, seq_len_x, head_dim]\n",
    "        \n",
    "        # Split cosine and sine for application to x1 and x2\n",
    "        cos_1, cos_2 = tf.split(cos, 2, axis=-1)\n",
    "        sin_1, sin_2 = tf.split(sin, 2, axis=-1)\n",
    "        \n",
    "        # Reshape for broadcasting with attention heads\n",
    "        cos_1 = tf.reshape(cos_1, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        sin_1 = tf.reshape(sin_1, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        cos_2 = tf.reshape(cos_2, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        sin_2 = tf.reshape(sin_2, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        \n",
    "        # Repeat to match number of attention heads\n",
    "        cos_1 = tf.repeat(cos_1, num_heads, axis=2)\n",
    "        sin_1 = tf.repeat(sin_1, num_heads, axis=2)\n",
    "        cos_2 = tf.repeat(cos_2, num_heads, axis=2)\n",
    "        sin_2 = tf.repeat(sin_2, num_heads, axis=2)\n",
    "        \n",
    "        # Cast to input dtype for mixed precision compatibility\n",
    "        x_dtype = x.dtype\n",
    "        cos_1 = tf.cast(cos_1, x_dtype)\n",
    "        sin_1 = tf.cast(sin_1, x_dtype)\n",
    "        cos_2 = tf.cast(cos_2, x_dtype)\n",
    "        sin_2 = tf.cast(sin_2, x_dtype)\n",
    "        \n",
    "        # Split input tensor into two parts for rotary application\n",
    "        x1, x2 = tf.split(x, 2, axis=-1)\n",
    "        \n",
    "        # Apply rotary embeddings to compute rotated representations\n",
    "        rotated_x1 = x1 * cos_1 - x2 * sin_1\n",
    "        rotated_x2 = x1 * sin_2 + x2 * cos_2\n",
    "        \n",
    "        # Concatenate rotated parts to restore original shape\n",
    "        return tf.concat([rotated_x1, rotated_x2], axis=-1)\n",
    "\n",
    "class DeepSeekMLP(layers.Layer):\n",
    "    \"\"\"Multi-Layer Perceptron (MLP) with Swish activation\"\"\"\n",
    "    def __init__(self, hidden_size, intermediate_size, hidden_act=\"swish\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size              # Input and output dimension\n",
    "        self.intermediate_size = intermediate_size  # Intermediate layer dimension\n",
    "        self.hidden_act = hidden_act                # Activation function (Swish/SILU)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Define gate projection layer with reduced initializer variance for stability\n",
    "        self.gate_proj = layers.Dense(\n",
    "            self.intermediate_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            use_bias=False\n",
    "        )\n",
    "        # Define up projection layer\n",
    "        self.up_proj = layers.Dense(\n",
    "            self.intermediate_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            use_bias=False\n",
    "        )\n",
    "        # Define down projection layer to return to hidden size\n",
    "        self.down_proj = layers.Dense(\n",
    "            self.hidden_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            use_bias=False\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Apply gate and up projections\n",
    "        gate_output = self.gate_proj(x)\n",
    "        up_output = self.up_proj(x)\n",
    "        # Apply Swish (SiLU) activation to gate output and multiply with up output\n",
    "        activated = tf.nn.silu(gate_output) * up_output\n",
    "        # Project back to hidden size\n",
    "        return self.down_proj(activated)\n",
    "\n",
    "class MoEGate(layers.Layer):\n",
    "    \"\"\"Mixture of Experts (MoE) gating mechanism with load balancing auxiliary loss\"\"\"\n",
    "    def __init__(self, num_experts, num_experts_per_tok, hidden_size, \n",
    "                 scoring_func='softmax', norm_topk_prob=False, \n",
    "                 aux_loss_alpha=0.001, seq_aux=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_experts = num_experts                  # Total number of experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok  # Number of experts per token\n",
    "        self.hidden_size = hidden_size                  # Input dimension\n",
    "        self.scoring_func = scoring_func                # Scoring function for expert selection\n",
    "        self.norm_topk_prob = norm_topk_prob            # Normalize top-k probabilities\n",
    "        self.aux_loss_alpha = aux_loss_alpha            # Weight for auxiliary loss\n",
    "        self.seq_aux = seq_aux                          # Compute auxiliary loss per sequence\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize gating weights with reduced variance for stability\n",
    "        self.weight = self.add_weight(\n",
    "            name='gate_weight',\n",
    "            shape=(self.num_experts, self.hidden_size),\n",
    "            initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            trainable=True\n",
    "        )\n",
    "        # Initialize expert assignment counter for tracking during training\n",
    "        self.expert_counts = tf.Variable(tf.zeros([self.num_experts], dtype=tf.int32), trainable=False)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, hidden_states, training=False):\n",
    "        # Get input dimensions\n",
    "        batch_size, seq_len, h = tf.unstack(tf.shape(hidden_states))\n",
    "        hidden_states_flat = tf.reshape(hidden_states, [-1, h])\n",
    "        # Compute gating logits\n",
    "        logits = tf.matmul(hidden_states_flat, self.weight, transpose_b=True)\n",
    "        \n",
    "        # Clip logits for numerical stability\n",
    "        logits = tf.clip_by_value(logits, -50.0, 50.0)\n",
    "        \n",
    "        # Apply scoring function (softmax) to compute expert scores\n",
    "        if self.scoring_func == 'softmax':\n",
    "            temperature = 0.5  # Temperature for scaling logits\n",
    "            scaled_logits = tf.cast(logits, tf.float32) / temperature\n",
    "            scores = tf.nn.softmax(scaled_logits, axis=-1)\n",
    "            scores = tf.cast(scores, hidden_states.dtype)\n",
    "        else:\n",
    "            scores = tf.nn.softmax(tf.cast(logits, tf.float32), axis=-1)\n",
    "            scores = tf.cast(scores, hidden_states.dtype)\n",
    "\n",
    "        # Select top-k experts and their weights\n",
    "        topk_weights, topk_indices = tf.math.top_k(\n",
    "            scores, k=self.num_experts_per_tok, sorted=False\n",
    "        )\n",
    "\n",
    "        # Normalize top-k weights if specified\n",
    "        if self.num_experts_per_tok > 1 and self.norm_topk_prob:\n",
    "            denominator = tf.reduce_sum(topk_weights, axis=-1, keepdims=True) + 1e-20\n",
    "            topk_weights = topk_weights / denominator\n",
    "\n",
    "        # Update expert assignment counts during training\n",
    "        if training:\n",
    "            flat_topk_indices = tf.reshape(topk_indices, [-1])\n",
    "            counts = tf.reduce_sum(tf.one_hot(flat_topk_indices, depth=self.num_experts), axis=0)\n",
    "            self.expert_counts.assign_add(tf.cast(counts, tf.int32))\n",
    "\n",
    "        aux_loss = None\n",
    "        if training and self.aux_loss_alpha > 0.0:\n",
    "            if self.seq_aux:\n",
    "                # Sequence-level auxiliary loss for load balancing\n",
    "                scores_seq = tf.reshape(scores, [batch_size, seq_len, -1])\n",
    "                topk_indices_seq = tf.reshape(topk_indices, [batch_size, -1])\n",
    "                ce = tf.zeros([batch_size, self.num_experts], dtype=tf.float32)\n",
    "                mask = tf.one_hot(topk_indices_seq, depth=self.num_experts)\n",
    "                ce = tf.reduce_sum(mask, axis=1)\n",
    "                seq_len_float = tf.cast(seq_len, tf.float32)\n",
    "                num_experts_per_tok_float = tf.cast(self.num_experts_per_tok, tf.float32)\n",
    "                num_experts_float = tf.cast(self.num_experts, tf.float32)\n",
    "                ce = ce / (seq_len_float * num_experts_per_tok_float / num_experts_float)\n",
    "                scores_seq_mean = tf.reduce_mean(tf.cast(scores_seq, tf.float32), axis=1)\n",
    "                aux_loss = tf.reduce_sum(ce * scores_seq_mean, axis=1)\n",
    "                aux_loss = tf.reduce_mean(aux_loss) * self.aux_loss_alpha\n",
    "            else:\n",
    "                # Token-level auxiliary loss\n",
    "                mask_ce = tf.one_hot(tf.reshape(topk_indices, [-1]), depth=self.num_experts)\n",
    "                ce = tf.reduce_mean(tf.cast(mask_ce, tf.float32), axis=0)\n",
    "                Pi = tf.reduce_mean(tf.cast(scores, tf.float32), axis=0)\n",
    "                fi = ce * self.num_experts\n",
    "                aux_loss = tf.reduce_sum(Pi * fi) * self.aux_loss_alpha\n",
    "\n",
    "            # Add auxiliary loss to the layer's losses\n",
    "            self.add_loss(aux_loss)\n",
    "\n",
    "        return topk_indices, topk_weights, aux_loss\n",
    "\n",
    "class DeepSeekMoE(layers.Layer):\n",
    "    \"\"\"Mixture of Experts (MoE) layer with shared and routed experts\"\"\"\n",
    "    def __init__(self, hidden_size, n_routed_experts, n_shared_experts, \n",
    "                 num_experts_per_tok, moe_intermediate_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size                      # Input and output dimension\n",
    "        self.n_routed_experts = n_routed_experts            # Number of routed experts\n",
    "        self.n_shared_experts = n_shared_experts            # Number of shared experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok      # Experts per token\n",
    "        self.moe_intermediate_size = moe_intermediate_size  # MoE intermediate size\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize list of routed expert MLPs\n",
    "        self.routed_experts = [\n",
    "            DeepSeekMLP(\n",
    "                self.hidden_size, \n",
    "                self.moe_intermediate_size,\n",
    "                name=f\"routed_expert_{i}\"\n",
    "            ) for i in range(self.n_routed_experts)\n",
    "        ]\n",
    "        \n",
    "        # Initialize shared experts if specified\n",
    "        if self.n_shared_experts is not None:\n",
    "            shared_intermediate_size = self.moe_intermediate_size * self.n_shared_experts\n",
    "            self.shared_experts = DeepSeekMLP(\n",
    "                self.hidden_size,\n",
    "                shared_intermediate_size,\n",
    "                name=\"shared_experts\"\n",
    "            )\n",
    "        else:\n",
    "            self.shared_experts = None\n",
    "            \n",
    "        # Initialize MoE gating mechanism\n",
    "        self.gate = MoEGate(\n",
    "            self.n_routed_experts,\n",
    "            self.num_experts_per_tok,\n",
    "            self.hidden_size,\n",
    "            name=\"moe_gate\"\n",
    "        )\n",
    "        \n",
    "        # Build each expert and gate\n",
    "        for expert in self.routed_experts:\n",
    "            expert.build(input_shape)\n",
    "        if self.shared_experts:\n",
    "            self.shared_experts.build(input_shape)\n",
    "        self.gate.build(input_shape)\n",
    "        \n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, hidden_states, training=False):\n",
    "        identity = hidden_states  # Store input for residual connection\n",
    "        orig_shape = tf.shape(hidden_states)\n",
    "        # Get top-k expert indices and weights\n",
    "        topk_idx, topk_weight, aux_loss = self.gate(hidden_states, training=training)\n",
    "        hidden_states_flat = tf.reshape(hidden_states, [-1, self.hidden_size])\n",
    "        \n",
    "        if training:\n",
    "            # Expand inputs for top-k experts during training\n",
    "            hidden_states_expanded = tf.repeat(\n",
    "                hidden_states_flat, self.num_experts_per_tok, axis=0\n",
    "            )\n",
    "            flat_topk_idx = tf.reshape(topk_idx, [-1])\n",
    "            combined_output = tf.zeros_like(hidden_states_expanded)\n",
    "            # Process each expert's input\n",
    "            for i, expert in enumerate(self.routed_experts):\n",
    "                expert_mask = tf.equal(flat_topk_idx, i)\n",
    "                if tf.reduce_any(expert_mask):\n",
    "                    expert_input = tf.boolean_mask(hidden_states_expanded, expert_mask)\n",
    "                    expert_output = expert(expert_input)\n",
    "                    update_indices = tf.where(expert_mask)\n",
    "                    combined_output = tf.tensor_scatter_nd_update(\n",
    "                        combined_output,\n",
    "                        update_indices,\n",
    "                        expert_output\n",
    "                    )\n",
    "                    \n",
    "            # Apply expert weights and reshape output\n",
    "            topk_weight_flat = tf.reshape(topk_weight, [-1, 1])\n",
    "            weighted_output = combined_output * topk_weight_flat\n",
    "            weighted_output = tf.reshape(\n",
    "                weighted_output,\n",
    "                [-1, self.num_experts_per_tok, self.hidden_size]\n",
    "            )\n",
    "            routed_output = tf.reduce_sum(weighted_output, axis=1)\n",
    "            routed_output = tf.reshape(routed_output, orig_shape)\n",
    "        else:\n",
    "            # Use efficient inference path\n",
    "            routed_output = self._moe_infer(hidden_states_flat, topk_idx, topk_weight)\n",
    "            routed_output = tf.reshape(routed_output, orig_shape)\n",
    "            \n",
    "        # Add shared expert output if available\n",
    "        if self.shared_experts is not None:\n",
    "            shared_output = self.shared_experts(identity)\n",
    "            final_output = routed_output + shared_output\n",
    "        else:\n",
    "            final_output = routed_output\n",
    "            \n",
    "        # Apply residual connection\n",
    "        return final_output + identity\n",
    "        \n",
    "    def _moe_infer(self, x, expert_indices, expert_weights):\n",
    "        \"\"\"Efficient inference path for MoE layer\"\"\"\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        expert_cache = tf.zeros_like(x)\n",
    "        # Clip expert indices to valid range\n",
    "        expert_indices = tf.clip_by_value(expert_indices, 0, self.n_routed_experts - 1)\n",
    "        flat_expert_indices = tf.reshape(expert_indices, [-1])\n",
    "        flat_expert_weights = tf.reshape(expert_weights, [-1, 1])\n",
    "        total_tokens = tf.shape(x)[0]\n",
    "        token_indices_base = tf.range(total_tokens, dtype=tf.int64)\n",
    "        token_indices_base = tf.repeat(token_indices_base, self.num_experts_per_tok, axis=0)\n",
    "        \n",
    "        # Process each expert's tokens\n",
    "        for i in range(self.n_routed_experts):\n",
    "            expert_mask = tf.equal(flat_expert_indices, i)\n",
    "            if tf.reduce_any(expert_mask):\n",
    "                token_indices = tf.boolean_mask(token_indices_base, expert_mask)\n",
    "                expert_tokens = tf.gather(x, token_indices)\n",
    "                expert_out = self.routed_experts[i](expert_tokens)\n",
    "                expert_out_weighted = expert_out * tf.gather(flat_expert_weights, tf.where(expert_mask)[:, 0])\n",
    "                expert_cache = tf.tensor_scatter_nd_add(\n",
    "                    expert_cache,\n",
    "                    tf.expand_dims(token_indices, 1),\n",
    "                    expert_out_weighted\n",
    "                )\n",
    "                \n",
    "        return expert_cache\n",
    "\n",
    "class DeepSeekAttention(layers.Layer):\n",
    "    \"\"\"Multi-head attention with Rotary Position Embeddings and Grouped Query Attention (GQA) support\"\"\"\n",
    "    def __init__(self, hidden_size, num_attention_heads, num_key_value_heads=None,\n",
    "                 attention_dropout=0.0, max_position_embeddings=2048,\n",
    "                 rope_theta=10000.0, rope_scaling=None, attention_bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size                                         # Input and output dimension\n",
    "        self.num_attention_heads = num_attention_heads                         # Number of attention heads\n",
    "        self.num_key_value_heads = num_key_value_heads or num_attention_heads  # Number of key/value heads (GQA)\n",
    "        self.attention_dropout = attention_dropout                             # Dropout rate for attention weights\n",
    "        self.max_position_embeddings = max_position_embeddings                 # Maximum sequence length\n",
    "        self.rope_theta = rope_theta                                           # Base period for RoPE\n",
    "        self.rope_scaling = rope_scaling                                       # RoPE scaling configuration\n",
    "        self.attention_bias = attention_bias                                   # Whether to use bias in projections\n",
    "        self.head_dim = hidden_size // num_attention_heads                     # Dimension per head\n",
    "        self.num_key_value_groups = num_attention_heads // self.num_key_value_heads  # Groups for GQA\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize query projection layer\n",
    "        self.q_proj = layers.Dense(\n",
    "            self.num_attention_heads * self.head_dim,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"q_proj\"\n",
    "        )\n",
    "        # Initialize key projection layer\n",
    "        self.k_proj = layers.Dense(\n",
    "            self.num_key_value_heads * self.head_dim,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"k_proj\"\n",
    "        )\n",
    "        # Initialize value projection layer\n",
    "        self.v_proj = layers.Dense(\n",
    "            self.num_key_value_heads * self.head_dim,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"v_proj\"\n",
    "        )\n",
    "        # Initialize output projection layer\n",
    "        self.o_proj = layers.Dense(\n",
    "            self.hidden_size,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"o_proj\"\n",
    "        )\n",
    "        \n",
    "        # Initialize RoPE with specified scaling\n",
    "        scaling_type = self.rope_scaling.get(\"type\") if self.rope_scaling else None\n",
    "        scaling_factor = self.rope_scaling.get(\"factor\", 1.0) if self.rope_scaling else 1.0\n",
    "        self.rotary_emb = RotaryPositionEmbedding(\n",
    "            self.head_dim,\n",
    "            self.max_position_embeddings,\n",
    "            self.rope_theta,\n",
    "            scaling_type,\n",
    "            scaling_factor,\n",
    "            name=\"rotary_emb\"\n",
    "        )\n",
    "        self.dropout_layer = layers.Dropout(self.attention_dropout)\n",
    "        self.rotary_emb.build(input_shape)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, hidden_states, attention_mask=None, position_ids=None, training=False):\n",
    "        # Get input dimensions\n",
    "        batch_size, seq_len, _ = tf.unstack(tf.shape(hidden_states))\n",
    "        \n",
    "        # Project inputs to query, key, and value\n",
    "        q = self.q_proj(hidden_states)\n",
    "        k = self.k_proj(hidden_states)\n",
    "        v = self.v_proj(hidden_states)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = tf.reshape(q, [batch_size, seq_len, self.num_attention_heads, self.head_dim])\n",
    "        k = tf.reshape(k, [batch_size, seq_len, self.num_key_value_heads, self.head_dim])\n",
    "        v = tf.reshape(v, [batch_size, seq_len, self.num_key_value_heads, self.head_dim])\n",
    "        \n",
    "        # Apply rotary embeddings to query and key\n",
    "        q_rotated = self.rotary_emb(q, position_ids=position_ids)\n",
    "        k_rotated = self.rotary_emb(k, position_ids=position_ids)\n",
    "        \n",
    "        # Transpose for attention computation\n",
    "        q_rotated = tf.transpose(q_rotated, [0, 2, 1, 3])\n",
    "        k_rotated = tf.transpose(k_rotated, [0, 2, 1, 3])\n",
    "        v = tf.transpose(v, [0, 2, 1, 3])\n",
    "        \n",
    "        # Repeat key and value for GQA if necessary\n",
    "        if self.num_key_value_groups > 1:\n",
    "            k_rotated = tf.repeat(k_rotated, self.num_key_value_groups, axis=1)\n",
    "            v = tf.repeat(v, self.num_key_value_groups, axis=1)\n",
    "            \n",
    "        # Compute scaled dot-product attention\n",
    "        scale = tf.math.rsqrt(tf.cast(self.head_dim, tf.float32))\n",
    "        scale = tf.cast(scale, q_rotated.dtype)\n",
    "        attn_weights = tf.matmul(q_rotated, k_rotated, transpose_b=True) * scale\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = tf.cast(attention_mask, attn_weights.dtype)\n",
    "            if len(attention_mask.shape) == 2:\n",
    "                attention_mask = tf.expand_dims(tf.expand_dims(attention_mask, 1), 1)\n",
    "            elif len(attention_mask.shape) == 3:\n",
    "                attention_mask = tf.expand_dims(attention_mask, 1)\n",
    "            attn_weights += attention_mask\n",
    "            \n",
    "        # Clip attention weights for numerical stability\n",
    "        attn_weights = tf.clip_by_value(attn_weights, -50.0, 50.0)\n",
    "        attn_weights = tf.cast(attn_weights, tf.float32)\n",
    "        # Apply softmax with max subtraction for stability\n",
    "        attn_weights = attn_weights - tf.reduce_max(attn_weights, axis=-1, keepdims=True)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "        attn_weights = tf.cast(attn_weights, q_rotated.dtype)\n",
    "        \n",
    "        # Apply dropout to attention weights during training\n",
    "        attn_weights = self.dropout_layer(attn_weights, training=training)\n",
    "        # Compute attention output\n",
    "        attn_output = tf.matmul(attn_weights, v)\n",
    "        attn_output = tf.transpose(attn_output, [0, 2, 1, 3])\n",
    "        attn_output = tf.reshape(attn_output, [batch_size, seq_len, self.hidden_size])\n",
    "        \n",
    "        # Project attention output to hidden size\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "class DeepSeekDecoderLayer(layers.Layer):\n",
    "    \"\"\"Transformer decoder layer combining attention and MoE/MLP with residual connections\"\"\"\n",
    "    def __init__(self, config, layer_idx, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        # Initialize self-attention with GQA and RoPE\n",
    "        self.self_attn = DeepSeekAttention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            num_key_value_heads=config.num_key_value_heads,\n",
    "            attention_dropout=config.attention_dropout,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            rope_theta=config.rope_theta,\n",
    "            rope_scaling=config.rope_scaling,\n",
    "            attention_bias=config.attention_bias,\n",
    "            name=\"self_attn\"\n",
    "        )\n",
    "        \n",
    "        # Determine whether to use MoE or dense MLP based on layer index and config\n",
    "        self.use_moe = (\n",
    "            config.n_routed_experts is not None and\n",
    "            layer_idx >= config.first_k_dense_replace and\n",
    "            layer_idx % config.moe_layer_freq == 0\n",
    "        )\n",
    "        \n",
    "        if self.use_moe:\n",
    "            # Initialize MoE layer for routed experts\n",
    "            self.mlp = DeepSeekMoE(\n",
    "                hidden_size=config.hidden_size,\n",
    "                n_routed_experts=config.n_routed_experts,\n",
    "                n_shared_experts=config.n_shared_experts,\n",
    "                num_experts_per_tok=config.num_experts_per_tok,\n",
    "                moe_intermediate_size=config.moe_intermediate_size,\n",
    "                name=\"moe\"\n",
    "            )\n",
    "        else:\n",
    "            # Initialize dense MLP layer\n",
    "            self.mlp = DeepSeekMLP(\n",
    "                hidden_size=config.hidden_size,\n",
    "                intermediate_size=config.intermediate_size,\n",
    "                hidden_act=config.hidden_act,\n",
    "                name=\"mlp\"\n",
    "            )\n",
    "            \n",
    "        # Initialize input and post-attention normalization layers\n",
    "        self.input_layernorm = DeepSeekRMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps, name=\"input_layernorm\"\n",
    "        )\n",
    "        self.post_attention_layernorm = DeepSeekRMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps, name=\"post_attention_layernorm\"\n",
    "        )\n",
    "        \n",
    "    def call(self, hidden_states, attention_mask=None, position_ids=None, training=False):\n",
    "        # Apply input normalization and self-attention with residual connection\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        attn_output = self.self_attn(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            training=training\n",
    "        )\n",
    "        hidden_states = residual + attn_output\n",
    "        \n",
    "        # Apply post-attention normalization and MLP/MoE with residual connection\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        mlp_output = self.mlp(hidden_states, training=training)\n",
    "        hidden_states = residual + mlp_output\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "# MODEL CONFIGURATION\n",
    "\n",
    "class DeepSeekConfig:\n",
    "    \"\"\"Configuration class for DeepSeek model parameters with optimized chatbot hyperparameters\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=vocab_size,  # Default vocabulary size for chatbot\n",
    "        hidden_size=256,        # Optimized hidden size for stability and performance\n",
    "        intermediate_size=512,  # Balanced intermediate size for MLP layers\n",
    "        moe_intermediate_size=256,  # MoE-specific intermediate dimension\n",
    "        num_hidden_layers=4,    # Optimal layer count for chatbot tasks\n",
    "        num_attention_heads=4,  # Balanced attention heads for computation\n",
    "        num_key_value_heads=2,  # GQA for memory efficiency\n",
    "        n_shared_experts=1,     # Single shared expert for parameter efficiency\n",
    "        n_routed_experts=4,     # Four routed experts for specialization\n",
    "        num_experts_per_tok=2,  # Two experts per token for balanced load\n",
    "        moe_layer_freq=2,       # MoE every other layer for computational balance\n",
    "        first_k_dense_replace=1,  # First layer dense for stability\n",
    "        norm_topk_prob=False,   # No normalization for expert weights\n",
    "        scoring_func='softmax', # Standard softmax for expert selection\n",
    "        aux_loss_alpha=0.001,   # Default auxiliary loss weight\n",
    "        seq_aux=True,           # Sequence-level auxiliary loss\n",
    "        hidden_act=\"swish\",     # Swish activation for better gradients\n",
    "        max_position_embeddings=256,  # Sufficient context length for chatbot\n",
    "        initializer_range=0.001,  # Reduced initializer range for stability\n",
    "        rms_norm_eps=1e-6,      # Standard RMS norm epsilon\n",
    "        use_cache=True,         # Enable caching for inference\n",
    "        rope_theta=10000.0,     # Standard RoPE base period\n",
    "        rope_scaling={\"type\": \"linear\", \"factor\": 2.0},  # Linear RoPE scaling\n",
    "        attention_bias=False,   # No attention bias for efficiency\n",
    "        attention_dropout=0.1   # Increased dropout for regularization\n",
    "    ):\n",
    "        # Initialize model architecture parameters with optimized values\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.moe_intermediate_size = moe_intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.n_shared_experts = n_shared_experts\n",
    "        self.n_routed_experts = n_routed_experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok\n",
    "        self.moe_layer_freq = moe_layer_freq\n",
    "        self.first_k_dense_replace = first_k_dense_replace\n",
    "        self.norm_topk_prob = norm_topk_prob\n",
    "        self.scoring_func = scoring_func\n",
    "        self.aux_loss_alpha = aux_loss_alpha\n",
    "        self.seq_aux = seq_aux\n",
    "        self.hidden_act = hidden_act\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.initializer_range = initializer_range\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.use_cache = use_cache\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self.attention_bias = attention_bias\n",
    "        self.attention_dropout = attention_dropout\n",
    "\n",
    "class DeepSeekModel(tf.keras.Model):\n",
    "    \"\"\"Core DeepSeek transformer model for chatbot, provide raw hidden states\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.padding_idx = 0\n",
    "        \n",
    "        # Initialize embedding layer for token inputs\n",
    "        self.embed_tokens = layers.Embedding(\n",
    "            config.vocab_size,\n",
    "            config.hidden_size,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomNormal(\n",
    "                stddev=config.initializer_range\n",
    "            ),\n",
    "            name=\"embed_tokens\"\n",
    "        )\n",
    "        \n",
    "        # Initialize decoder layers\n",
    "        self.decoder_layers = [\n",
    "            DeepSeekDecoderLayer(config, layer_idx=i, name=f\"layer_{i}\")\n",
    "            for i in range(config.num_hidden_layers)\n",
    "        ]\n",
    "        \n",
    "        # Initialize final normalization layer\n",
    "        self.norm = DeepSeekRMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps, name=\"norm\"\n",
    "        )\n",
    "        \n",
    "        # Initialize expert assignment counter for MoE layers\n",
    "        self.expert_assignment_counts = tf.Variable(\n",
    "            tf.zeros([config.n_routed_experts], dtype=tf.int32),\n",
    "            trainable=False,\n",
    "            name=\"expert_assignment_counts\"\n",
    "        ) if config.n_routed_experts else None\n",
    "        \n",
    "    def call(self, input_ids, attention_mask=None, position_ids=None, training=False):\n",
    "        # Embed input tokens\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "        \n",
    "        # Create causal mask if not provided\n",
    "        if attention_mask is None:\n",
    "            seq_len = tf.shape(input_ids)[1]\n",
    "            attention_mask = self._create_causal_mask(seq_len)\n",
    "            \n",
    "        # Prepare attention mask for causal attention\n",
    "        attention_mask = tf.cast(attention_mask, hidden_states.dtype)\n",
    "        if len(attention_mask.shape) == 2:\n",
    "            attention_mask = tf.expand_dims(tf.expand_dims(attention_mask, 1), 1)\n",
    "        elif len(attention_mask.shape) == 3:\n",
    "            attention_mask = tf.expand_dims(attention_mask, 1)\n",
    "        attention_mask = (tf.cast(1.0, hidden_states.dtype) - attention_mask) * tf.cast(-1e4, hidden_states.dtype)\n",
    "        \n",
    "        # Process through decoder layers and aggregate expert counts\n",
    "        for layer in self.decoder_layers:\n",
    "            hidden_states = layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                training=training\n",
    "            )\n",
    "            # Update expert assignment counts for MoE layers during training\n",
    "            if training and hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE):\n",
    "                expert_counts = layer.mlp.gate.expert_counts\n",
    "                if self.expert_assignment_counts is not None:\n",
    "                    self.expert_assignment_counts.assign_add(expert_counts)\n",
    "            \n",
    "        # Apply final normalization\n",
    "        return self.norm(hidden_states)\n",
    "        \n",
    "    def _create_causal_mask(self, seq_len):\n",
    "        \"\"\"Create causal attention mask to prevent attending to future tokens\"\"\"\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return tf.cast(mask, tf.float16)\n",
    "\n",
    "class DeepSeekForCausalLM(tf.keras.Model):\n",
    "    \"\"\"Causal language model for chatbot text generation\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.transformer = DeepSeekModel(config, name=\"transformer\")\n",
    "        # Initialize language model head\n",
    "        self.lm_head = layers.Dense(\n",
    "            config.vocab_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                stddev=config.initializer_range\n",
    "            ),\n",
    "            use_bias=False,\n",
    "            name=\"lm_head\"\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, attention_mask=None, position_ids=None, training=False):\n",
    "        # Handle dictionary or tensor inputs\n",
    "        if isinstance(inputs, dict):\n",
    "            input_ids = inputs.get('input_ids')\n",
    "            attention_mask = inputs.get('attention_mask', attention_mask)\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "        \n",
    "        # Process through transformer\n",
    "        hidden_states = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            training=training\n",
    "        )\n",
    "        # Generate logits for token prediction\n",
    "        return self.lm_head(hidden_states)\n",
    "\n",
    "# TRAINING METRICS AND CALLBACKS \n",
    "class ResourceMonitor(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to monitor system resources (GPU, CPU, memory) during training\"\"\"\n",
    "    def __init__(self, batch_size=2):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.resource_metrics = []  # Store resource usage metrics\n",
    "        self.start_time = time.time()\n",
    "        self.total_tokens_processed = 0\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "        self.epoch_tokens = 0\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # Update token count for throughput calculation\n",
    "        self.total_tokens_processed += self.batch_size * max_seq_len\n",
    "        self.epoch_tokens += self.batch_size * max_seq_len\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        try:\n",
    "            # Collect GPU and system resource usage\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            gpu_memory = sum([gpu.memoryUsed for gpu in gpus]) / len(gpus) if gpus else 0\n",
    "            gpu_usage = sum([gpu.load * 100 for gpu in gpus]) / len(gpus) if gpus else 0\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory_info = psutil.virtual_memory()\n",
    "            system_memory = memory_info.used / (1024 ** 3)\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            tokens_per_second = self.epoch_tokens / epoch_time if epoch_time > 0 else 0\n",
    "            \n",
    "            # Store resource metrics\n",
    "            resource_stats = {\n",
    "                'epoch': epoch + 1,\n",
    "                'avg_gpu_memory_gb': float(gpu_memory / 1024),\n",
    "                'avg_system_memory_gb': float(system_memory),\n",
    "                'avg_gpu_usage_percent': float(gpu_usage),\n",
    "                'avg_cpu_usage_percent': float(cpu_usage),\n",
    "                'tokens_per_second': float(tokens_per_second),\n",
    "                'throughput_tps': float(tokens_per_second)\n",
    "            }\n",
    "            \n",
    "            self.resource_metrics.append(resource_stats)\n",
    "            \n",
    "            # Print resource usage summary\n",
    "            print(f\"Epoch {epoch+1} Resources: GPU Mem: {gpu_memory/1024:.1f}GB, \"\n",
    "                  f\"System Mem: {system_memory:.1f}GB, GPU: {gpu_usage:.1f}%, \"\n",
    "                  f\"CPU: {cpu_usage:.1f}%, TPS: {tokens_per_second:.0f}\")\n",
    "                  \n",
    "        except Exception as e:\n",
    "            print(f\"Resource monitoring error: {e}\")\n",
    "\n",
    "class DeepSeekMetrics(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to track specific metrics, including MaxVIO for expert balancing\"\"\"\n",
    "    def __init__(self, validation_data, tokenizer):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.metrics_history = []\n",
    "        self.start_time = time.time()\n",
    "        self.max_vio_values = []\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Reset expert assignment counts for accurate per-epoch metrics\n",
    "        if hasattr(self.model.transformer, 'expert_assignment_counts') and self.model.transformer.expert_assignment_counts is not None:\n",
    "            self.model.transformer.expert_assignment_counts.assign(tf.zeros([self.model.config.n_routed_experts], dtype=tf.int32))\n",
    "        for layer in self.model.transformer.decoder_layers:\n",
    "            if hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE):\n",
    "                layer.mlp.gate.expert_counts.assign(tf.zeros([layer.mlp.n_routed_experts], dtype=tf.int32))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        \n",
    "        # Estimate FLOPs for computational efficiency tracking\n",
    "        total_params = self.model.count_params()\n",
    "        flops_per_token = total_params * 2\n",
    "        total_flops = flops_per_token * batch_size * max_seq_len\n",
    "        total_gflops = total_flops / 1e9\n",
    "        \n",
    "        logs['gflops'] = float(total_gflops)\n",
    "        \n",
    "        # Calculate MaxVIO for expert load balancing if validation data is available\n",
    "        if self.validation_data:\n",
    "            try:\n",
    "                max_vio = self._calculate_max_vio()\n",
    "                logs['max_vio'] = float(max_vio)\n",
    "                self.max_vio_values.append(max_vio)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Metrics calculation error: {e}\")\n",
    "                logs['max_vio'] = 0.0\n",
    "                \n",
    "        self.metrics_history.append(logs.copy())\n",
    "        \n",
    "        # Print simplified metrics summary\n",
    "        print(f\"Epoch {epoch+1}: Loss={logs.get('loss', 0):.4f}, \"\n",
    "              f\"Val_Loss={logs.get('val_loss', 0):.4f}, \"\n",
    "              f\"GFLOPs={total_gflops:.2f}, \"\n",
    "              f\"MaxVIO={logs.get('max_vio', 0):.4f}\")\n",
    "              \n",
    "    def _calculate_max_vio(self):\n",
    "        \"\"\"Calculate Maximum Violation (MaxVIO) for expert load balancing\"\"\"\n",
    "        total_counts = np.zeros(self.model.config.n_routed_experts, dtype=np.float32)\n",
    "        moe_layers = 0\n",
    "        # Aggregate expert counts across MoE layers\n",
    "        for layer in self.model.transformer.decoder_layers:\n",
    "            if hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE):\n",
    "                counts = layer.mlp.gate.expert_counts.numpy()\n",
    "                total_counts += counts\n",
    "                moe_layers += 1\n",
    "        \n",
    "        # Compute MaxVIO based on aggregated counts\n",
    "        if moe_layers > 0 and np.sum(total_counts) > 0:\n",
    "            normalized_counts = total_counts / (np.sum(total_counts) + 1e-10)\n",
    "            expected_uniform = 1.0 / len(total_counts)\n",
    "            max_vio = np.max(np.abs(normalized_counts - expected_uniform)) / expected_uniform\n",
    "            return max_vio\n",
    "        return 0.0\n",
    "\n",
    "class ComprehensiveModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback for saving model checkpoints based on monitored metric\"\"\"\n",
    "    def __init__(self, filepath, monitor='val_loss', save_best_only=True, mode='min'):\n",
    "        super().__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.save_best_only = save_best_only\n",
    "        self.mode = mode\n",
    "        self.best_value = float('inf') if mode == 'min' else -float('inf')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            return\n",
    "            \n",
    "        current_value = logs.get(self.monitor)\n",
    "        if current_value is None:\n",
    "            return\n",
    "            \n",
    "        # Determine if current model should be saved\n",
    "        if self.mode == 'min':\n",
    "            should_save = current_value < self.best_value\n",
    "        else:\n",
    "            should_save = current_value > self.best_value\n",
    "            \n",
    "        if should_save or not self.save_best_only:\n",
    "            self.best_value = current_value\n",
    "            self.model.save_weights(self.filepath)\n",
    "            print(f\"Model checkpoint saved with {self.monitor}: {current_value:.4f}\")\n",
    "\n",
    "# Function to calculate perplexity on validation set\n",
    "def calculate_perplexity(model, dataset, tokenizer, max_batches=50):\n",
    "    \"\"\"Calculate perplexity on validation dataset to evaluate model performance\"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    PAD_TOKEN_ID = tokenizer.token_to_id(PAD_TOKEN)\n",
    "    for i, batch in enumerate(dataset.take(max_batches)):\n",
    "        inputs, labels = batch\n",
    "        logits = model(inputs, training=False)\n",
    "        # Clip logits for numerical stability\n",
    "        logits = tf.clip_by_value(logits, -50.0, 50.0)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        # Compute per-token loss\n",
    "        per_token_loss = -tf.gather(log_probs, labels, batch_dims=2)\n",
    "        if tf.reduce_any(tf.math.is_nan(per_token_loss)):\n",
    "            continue\n",
    "        # Mask padding tokens\n",
    "        mask = tf.cast(labels != PAD_TOKEN_ID, per_token_loss.dtype)\n",
    "        batch_loss = tf.reduce_sum(per_token_loss * mask)\n",
    "        batch_tokens = tf.reduce_sum(mask)\n",
    "        if tf.math.is_nan(batch_loss) or batch_tokens == 0:\n",
    "            continue\n",
    "        total_loss += batch_loss.numpy()\n",
    "        total_tokens += batch_tokens.numpy()\n",
    "    # Compute perplexity as exponential of average loss\n",
    "    if total_tokens > 0:\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        return math.exp(min(avg_loss, 20.0))\n",
    "    return float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define suggest hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def suggest_routing_hyperparameters(trial):\n",
    "    \"\"\"\n",
    "    Suggest hyperparameters related ONLY to routing algorithm (Top-K).\n",
    "    Other parameters (hidden_size, num_attention_heads, etc.) are kept fixed.\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    \n",
    "    n_shared_experts = trial.suggest_int('n_shared_experts', 1, 4)\n",
    "    params['n_shared_experts'] = n_shared_experts\n",
    "    \n",
    "    n_routed_experts = trial.suggest_int('n_routed_experts', 2, 8)\n",
    "    params['n_routed_experts'] = n_routed_experts\n",
    "    \n",
    "    # Ensure num_experts_per_tok doesn't exceed n_routed_experts\n",
    "    max_experts_per_tok = min(4, n_routed_experts)\n",
    "    num_experts_per_tok = trial.suggest_int('num_experts_per_tok', 1, max_experts_per_tok)\n",
    "    params['num_experts_per_tok'] = num_experts_per_tok\n",
    "    \n",
    "    # MoE layer frequency constraints\n",
    "    # Using fixed num_hidden_layers from default config for constraint calculation\n",
    "    num_hidden_layers = 4                              # Fixed from default config\n",
    "    max_moe_freq = max(2, num_hidden_layers // 2)\n",
    "    moe_layer_freq = trial.suggest_int('moe_layer_freq', 1, max_moe_freq)\n",
    "    params['moe_layer_freq'] = moe_layer_freq\n",
    "    \n",
    "    # First K dense replace constraints\n",
    "    max_first_k = max(1, num_hidden_layers // 2)\n",
    "    first_k_dense_replace = trial.suggest_int('first_k_dense_replace', 0, max_first_k)\n",
    "    params['first_k_dense_replace'] = first_k_dense_replace\n",
    "    \n",
    "    # MoE gating and balancing parameters - core routing algorithm parameters\n",
    "    norm_topk_prob = trial.suggest_categorical('norm_topk_prob', [True, False])\n",
    "    params['norm_topk_prob'] = norm_topk_prob\n",
    "    \n",
    "    scoring_func = trial.suggest_categorical('scoring_func', ['softmax', 'sigmoid'])\n",
    "    params['scoring_func'] = scoring_func\n",
    "    \n",
    "    aux_loss_alpha = trial.suggest_float('aux_loss_alpha', 1e-4, 0.1, log=True)\n",
    "    params['aux_loss_alpha'] = aux_loss_alpha\n",
    "    \n",
    "    seq_aux = trial.suggest_categorical('seq_aux', [True, False])\n",
    "    params['seq_aux'] = seq_aux\n",
    "    \n",
    "    # Keep all non-routing parameters fixed at default values\n",
    "    # These are not tuned to maintain model architecture stability\n",
    "    params['hidden_size'] = 256  \n",
    "    params['intermediate_size'] = 512  \n",
    "    params['moe_intermediate_size'] = 256 \n",
    "    params['num_hidden_layers'] = 4  \n",
    "    params['num_attention_heads'] = 4  \n",
    "    params['num_key_value_heads'] = 2 \n",
    "    params['hidden_act'] = 'swish' \n",
    "    params['max_position_embeddings'] = 256 \n",
    "    params['initializer_range'] = 0.001  \n",
    "    params['rms_norm_eps'] = 1e-6 \n",
    "    params['use_cache'] = True  \n",
    "    params['rope_theta'] = 10000.0  \n",
    "    params['rope_scaling_type'] = 'linear'  \n",
    "    params['rope_scaling_factor'] = 2.0  \n",
    "    params['attention_bias'] = False  \n",
    "    params['attention_dropout'] = 0.1  \n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-17 12:38:39,704] Using an existing study with name 'deepseek_routing_tuning_V1' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1: Loss=8.7774, Val_Loss=7.9827, GFLOPs=2.75, MaxVIO=0.0660\n",
      "Epoch 1 Resources: GPU Mem: 14.3GB, System Mem: 6.1GB, GPU: 27.0%, CPU: 13.1%, TPS: 311\n",
      "Calculating final metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-17 12:39:09,428] Trial 36 finished with values: [7.982700824737549, 0.06597402691841125] and parameters: {'n_shared_experts': 2, 'n_routed_experts': 6, 'num_experts_per_tok': 3, 'moe_layer_freq': 2, 'first_k_dense_replace': 2, 'norm_topk_prob': False, 'scoring_func': 'softmax', 'aux_loss_alpha': 0.0003182236318183972, 'seq_aux': False}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Loss: 8.7774\n",
      "Final Validation Loss: 7.9827\n",
      "Final Perplexity: 2921.71\n",
      "Final MaxVIO: 0.0660\n",
      "Minimum MaxVIO: 0.0660\n",
      "Average MaxVIO: 0.0660\n",
      "Starting training...\n",
      "Epoch 1: Loss=8.7912, Val_Loss=8.0240, GFLOPs=2.80, MaxVIO=0.1166\n",
      "Epoch 1 Resources: GPU Mem: 14.3GB, System Mem: 6.4GB, GPU: 23.0%, CPU: 24.1%, TPS: 327\n",
      "Calculating final metrics...\n",
      "Final Training Loss: 8.7912\n",
      "Final Validation Loss: 8.0240\n",
      "Final Perplexity: 3074.14\n",
      "Final MaxVIO: 0.1166\n",
      "Minimum MaxVIO: 0.1166\n",
      "Average MaxVIO: 0.1166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-17 12:39:38,215] Trial 37 finished with values: [8.023995399475098, 0.11662334203720093] and parameters: {'n_shared_experts': 3, 'n_routed_experts': 6, 'num_experts_per_tok': 1, 'moe_layer_freq': 2, 'first_k_dense_replace': 2, 'norm_topk_prob': False, 'scoring_func': 'softmax', 'aux_loss_alpha': 0.0003786647150736817, 'seq_aux': False}.\n"
     ]
    }
   ],
   "source": [
    "def multi_objective(trial):\n",
    "    \"\"\"\n",
    "    Multi-objective optimization function focusing ONLY on routing parameters.\n",
    "    This prevents dimensional mismatches and maintains stable hyperparameter model architecture.\n",
    "    \"\"\"\n",
    "    # Get validated parameters with constraints\n",
    "    params = suggest_routing_hyperparameters(trial)\n",
    "    \n",
    "    # Create rope_scaling dictionary with fixed parameters\n",
    "    rope_scaling = {\n",
    "        \"type\": params['rope_scaling_type'], \n",
    "        \"factor\": params['rope_scaling_factor']\n",
    "    }\n",
    "    \n",
    "    # Create config with fixed architecture parameters and tuned routing parameters\n",
    "    config = DeepSeekConfig(\n",
    "        # Fixed parameters\n",
    "        vocab_size=vocab_size,  \n",
    "        hidden_size=params['hidden_size'],  \n",
    "        intermediate_size=params['intermediate_size'], \n",
    "        moe_intermediate_size=params['moe_intermediate_size'],  \n",
    "        num_hidden_layers=params['num_hidden_layers'], \n",
    "        num_attention_heads=params['num_attention_heads'], \n",
    "        num_key_value_heads=params['num_key_value_heads'],  \n",
    "        hidden_act=params['hidden_act'],  # Fixed\n",
    "        max_position_embeddings=params['max_position_embeddings'],  \n",
    "        initializer_range=params['initializer_range'],  \n",
    "        rms_norm_eps=params['rms_norm_eps'], \n",
    "        use_cache=params['use_cache'],  \n",
    "        rope_theta=params['rope_theta'],  \n",
    "        rope_scaling=rope_scaling, \n",
    "        attention_bias=params['attention_bias'], \n",
    "        attention_dropout=params['attention_dropout'],\n",
    "        \n",
    "        # Tuned Routing Parameters\n",
    "        n_shared_experts=params['n_shared_experts'],  \n",
    "        n_routed_experts=params['n_routed_experts'],\n",
    "        num_experts_per_tok=params['num_experts_per_tok'], \n",
    "        moe_layer_freq=params['moe_layer_freq'], \n",
    "        first_k_dense_replace=params['first_k_dense_replace'], \n",
    "        norm_topk_prob=params['norm_topk_prob'],  \n",
    "        scoring_func=params['scoring_func'],  \n",
    "        aux_loss_alpha=params['aux_loss_alpha'], \n",
    "        seq_aux=params['seq_aux'],  \n",
    "    )\n",
    "    \n",
    "    # Clear TensorFlow session and memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    # Create model with the tuned config using DeepSeekForCausalLM directly\n",
    "    model = DeepSeekForCausalLM(config)\n",
    "\n",
    "    # Build model with sample batch\n",
    "    sample_batch = next(iter(tf_train_dataset.take(1)))\n",
    "    model(sample_batch[0])\n",
    "\n",
    "    # Configure optimizer with exponential decay learning rate\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-4,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=lr_schedule,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.95,\n",
    "        weight_decay=0.01,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "\n",
    "    # Get PAD_TOKEN_ID \n",
    "    PAD_TOKEN_ID = tokenizer.token_to_id(PAD_TOKEN)\n",
    "\n",
    "    # Compile model \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, \n",
    "            ignore_class=PAD_TOKEN_ID\n",
    "        ),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Define training callbacks\n",
    "    metrics_callback = DeepSeekMetrics(tf_val_dataset, tokenizer)\n",
    "    resource_monitor = ResourceMonitor(batch_size=2)\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            patience=3,\n",
    "            restore_best_weights=True, \n",
    "            monitor='val_loss',\n",
    "            min_delta=0.01\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=0.5, \n",
    "            patience=2,\n",
    "            monitor='val_loss',\n",
    "            min_delta=0.01,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        tf.keras.callbacks.TerminateOnNaN(),\n",
    "        metrics_callback,\n",
    "        resource_monitor\n",
    "    ]\n",
    "\n",
    "    # Train model on GPU\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Force GPU for training the model\n",
    "    with tf.device('/GPU:0'):\n",
    "        history = model.fit(\n",
    "            tf_train_dataset,\n",
    "            epochs=100,                         # Number of epoch for each hyperparameter tuning iteration\n",
    "            validation_data=tf_val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0,\n",
    "        )\n",
    "            \n",
    "    total_training_time = time.time() - start_time\n",
    "\n",
    "    # Calculate final metrics\n",
    "    print(\"Calculating final metrics...\")\n",
    "    final_perplexity = calculate_perplexity(model, tf_val_dataset, tokenizer, max_batches=100)\n",
    "            \n",
    "    final_training_loss = history.history['loss'][-1]\n",
    "    final_validation_loss = history.history['val_loss'][-1]\n",
    "\n",
    "    # Calculate MaxVIO metrics - this is our second objective\n",
    "    max_vio_values = metrics_callback.max_vio_values\n",
    "    final_max_vio = max_vio_values[-1]\n",
    "    min_max_vio = min(max_vio_values)\n",
    "    avg_max_vio = np.mean(max_vio_values)\n",
    "\n",
    "    print(f\"Final Training Loss: {final_training_loss:.4f}\")\n",
    "    print(f\"Final Validation Loss: {final_validation_loss:.4f}\")\n",
    "    print(f\"Final Perplexity: {final_perplexity:.2f}\")\n",
    "    print(f\"Final MaxVIO: {final_max_vio:.4f}\")\n",
    "    print(f\"Minimum MaxVIO: {min_max_vio:.4f}\")\n",
    "    print(f\"Average MaxVIO: {avg_max_vio:.4f}\")\n",
    "\n",
    "    # Store additional metrics in trial user attributes for analysis\n",
    "    trial.set_user_attr(\"final_training_loss\", float(final_training_loss))\n",
    "    trial.set_user_attr(\"final_perplexity\", float(final_perplexity))\n",
    "    trial.set_user_attr(\"min_max_vio\", float(min_max_vio))\n",
    "    trial.set_user_attr(\"avg_max_vio\", float(avg_max_vio))\n",
    "    trial.set_user_attr(\"training_time\", total_training_time)\n",
    "\n",
    "    # Return both validation loss and final MaxVIO as objectives to minimize\n",
    "    return final_validation_loss, final_max_vio\n",
    "\n",
    "# Create multi-objective study for Pareto front analysis\n",
    "multi_study = optuna.create_study(\n",
    "    study_name=\"deepseek_routing_tuning_V1\",\n",
    "    directions=['minimize', 'minimize'],                    # Minimize both validation loss and MaxVIO\n",
    "    storage='sqlite:///deepseek_routing_tuning_study_V1.db',# Save hyperparameter tuning for continue tuning\n",
    "    load_if_exists=True                                     # Set to False if firstime\n",
    ")\n",
    "\n",
    "# Tuned Hyperparameters\n",
    "multi_study.optimize(multi_objective, n_trials=20)  # Number of iteration to do hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pareto Front Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pareto Front Analysis...\n",
      "Found 2 Pareto-optimal solutions before filtering...\n",
      "Found 1 Pareto-optimal solutions after filtering out zero MaxVIO...\n",
      "Solution 1: Val Loss = 0.5756, MaxVIO = 0.0481\n",
      "    Routing params: {'n_routed_experts': 6, 'num_experts_per_tok': 2, 'n_shared_experts': 3, 'aux_loss_alpha': 0.0022997166087842954, 'moe_layer_freq': 1, 'scoring_func': 'softmax', 'norm_topk_prob': True}\n",
      "\n",
      "------Best Solution for Validation Loss------\n",
      "Lowest Validation Loss: 0.5756\n",
      "Corresponding MaxVIO: 0.0481\n",
      "Parameters: {'n_shared_experts': 3, 'n_routed_experts': 6, 'num_experts_per_tok': 2, 'moe_layer_freq': 1, 'first_k_dense_replace': 1, 'norm_topk_prob': True, 'scoring_func': 'softmax', 'aux_loss_alpha': 0.0022997166087842954, 'seq_aux': False}\n",
      "\n",
      "-------Best Solution for MaxVIO------\n",
      "Lowest MaxVIO: 0.0481\n",
      "Corresponding Validation Loss: 0.5756\n",
      "Parameters: {'n_shared_experts': 3, 'n_routed_experts': 6, 'num_experts_per_tok': 2, 'moe_layer_freq': 1, 'first_k_dense_replace': 1, 'norm_topk_prob': True, 'scoring_func': 'softmax', 'aux_loss_alpha': 0.0022997166087842954, 'seq_aux': False}\n",
      "\n",
      "Finding Best Balanced Solution\n",
      "Evaluating 1 solutions for balanced scoring (excluding zero MaxVIO)\n",
      "Best Balanced Solution (after filtering zero MaxVIO):\n",
      "\n",
      "  Validation Loss: 0.5756\n",
      "  MaxVIO: 0.0481\n",
      "  Balanced Score: 0.1663\n",
      "  Routing Parameters: {'n_shared_experts': 3, 'n_routed_experts': 6, 'num_experts_per_tok': 2, 'moe_layer_freq': 1, 'first_k_dense_replace': 1, 'norm_topk_prob': True, 'scoring_func': 'softmax', 'aux_loss_alpha': 0.0022997166087842954, 'seq_aux': False}\n",
      "\n",
      "Final Best Balanced Routing Selected\n",
      "Validation Loss: 0.5756\n",
      "MaxVIO: 0.0481\n",
      "Optimized Routing Hyperparameters:\n",
      "  n_routed_experts: 6\n",
      "  n_shared_experts: 3\n",
      "  num_experts_per_tok: 2\n",
      "  moe_layer_freq: 1\n",
      "  first_k_dense_replace: 1\n",
      "  scoring_func: softmax\n",
      "  norm_topk_prob: True\n",
      "  aux_loss_alpha: 0.0022997166087842954\n",
      "  seq_aux: False\n",
      "\n",
      "Hyperparameter Tuning Complete\n"
     ]
    }
   ],
   "source": [
    "def analyze_pareto_front(study):\n",
    "    \"\"\"Use Pareto front for multi-objective optimization with zero MaxVIO filtering.\n",
    "       Multi-objective are lowest validation loss and lowest MaxViO .\n",
    "       By, that MaxViO must not be 0 (ideal = errors) and 1 (model cannot achieve spare model).\n",
    "    \"\"\"\n",
    "    print(\"\\nPareto Front Analysis...\")\n",
    "    \n",
    "    # Get Pareto front trials\n",
    "    pareto_trials = study.best_trials\n",
    "    \n",
    "    # Filter out trials with zero MaxVIO to avoid unrealistic solutions\n",
    "    filtered_trials = [trial for trial in pareto_trials \n",
    "                      if trial.values and len(trial.values) >= 2 \n",
    "                      and trial.values[1] > 0.0]                       # Exclude zero MaxVIO\n",
    "    \n",
    "    print(f\"Found {len(pareto_trials)} Pareto-optimal solutions before filtering...\")\n",
    "    print(f\"Found {len(filtered_trials)} Pareto-optimal solutions after filtering out zero MaxVIO...\")\n",
    "    \n",
    "    # Find the solution with lowest validation loss (excluding zero MaxVIO)\n",
    "    lowest_val_loss_trial = None\n",
    "    lowest_val_loss = float('inf')\n",
    "    \n",
    "    # Find the solution with lowest MaxVIO (excluding zero MaxVIO)\n",
    "    lowest_maxvio_trial = None\n",
    "    lowest_maxvio = float('inf')\n",
    "    \n",
    "    for trial in filtered_trials:\n",
    "        if trial.values and len(trial.values) >= 2:\n",
    "            val_loss = trial.values[0]\n",
    "            max_vio = trial.values[1]\n",
    "            \n",
    "            # Track lowest validation loss\n",
    "            if val_loss < lowest_val_loss:\n",
    "                lowest_val_loss = val_loss\n",
    "                lowest_val_loss_trial = trial\n",
    "                \n",
    "            # Track lowest MaxVIO\n",
    "            if max_vio < lowest_maxvio:\n",
    "                lowest_maxvio = max_vio\n",
    "                lowest_maxvio_trial = trial\n",
    "    \n",
    "    # Display all filtered Pareto-optimal solutions\n",
    "    for i, trial in enumerate(filtered_trials):\n",
    "        val_loss = trial.values[0]\n",
    "        max_vio = trial.values[1]\n",
    "        print(f\"Solution {i+1}: Val Loss = {val_loss:.4f}, MaxVIO = {max_vio:.4f}\")\n",
    "        \n",
    "        # Display key routing parameters for this solution\n",
    "        key_params = {\n",
    "            'n_routed_experts': trial.params.get('n_routed_experts'),\n",
    "            'num_experts_per_tok': trial.params.get('num_experts_per_tok'),\n",
    "            'n_shared_experts': trial.params.get('n_shared_experts'),\n",
    "            'aux_loss_alpha': trial.params.get('aux_loss_alpha'),\n",
    "            'moe_layer_freq': trial.params.get('moe_layer_freq'),\n",
    "            'scoring_func': trial.params.get('scoring_func'),\n",
    "            'norm_topk_prob': trial.params.get('norm_topk_prob')\n",
    "        }\n",
    "        print(f\"    Routing params: {key_params}\")\n",
    "    \n",
    "    # Display the best solutions for each objective\n",
    "    if lowest_val_loss_trial:\n",
    "        print(f\"\\n------Best Solution for Validation Loss------\")\n",
    "        print(f\"Lowest Validation Loss: {lowest_val_loss:.4f}\")\n",
    "        print(f\"Corresponding MaxVIO: {lowest_val_loss_trial.values[1]:.4f}\")\n",
    "        print(f\"Parameters: {lowest_val_loss_trial.params}\")\n",
    "    \n",
    "    if lowest_maxvio_trial:\n",
    "        print(f\"\\n-------Best Solution for MaxVIO------\")\n",
    "        print(f\"Lowest MaxVIO: {lowest_maxvio:.4f}\")\n",
    "        print(f\"Corresponding Validation Loss: {lowest_maxvio_trial.values[0]:.4f}\")\n",
    "        print(f\"Parameters: {lowest_maxvio_trial.params}\")\n",
    "    \n",
    "    return filtered_trials, lowest_val_loss_trial, lowest_maxvio_trial\n",
    "\n",
    "def find_best_balanced_solution(study):\n",
    "    \"\"\"\n",
    "    Find the best balanced solution that considers both validation loss and MaxVIO.\n",
    "    This identifies solutions that are good on both objectives rather than extreme on one obejective.\n",
    "    Filters out zero MaxVIO solutions before calculating balanced scores.\n",
    "    \"\"\"\n",
    "    print(\"\\nFinding Best Balanced Solution\")\n",
    "    \n",
    "    # Get filtered trials excluding zero MaxVIO\n",
    "    filtered_trials = [trial for trial in study.best_trials \n",
    "                      if trial.values and len(trial.values) >= 2 \n",
    "                      and trial.values[1] > 0.0]\n",
    "        \n",
    "    print(f\"Evaluating {len(filtered_trials)} solutions for balanced scoring (excluding zero MaxVIO)\")\n",
    "        \n",
    "    # Calculate balanced scores for each filtered Pareto-optimal solution\n",
    "    balanced_scores = []\n",
    "    for trial in filtered_trials:\n",
    "        val_loss = trial.values[0]\n",
    "        max_vio = trial.values[1]\n",
    "        \n",
    "        # Use weighted geometric mean as balanced score\n",
    "        # This penalizes extreme values in both objectives\n",
    "        if val_loss > 0 and max_vio > 0:\n",
    "            # Geometric mean gives equal importance to both objectives\n",
    "            balanced_score = (val_loss * max_vio) ** 0.5\n",
    "            balanced_scores.append((balanced_score, trial, val_loss, max_vio))\n",
    "        else:\n",
    "            # Skip invalid values\n",
    "            continue\n",
    "    \n",
    "    # Find the solution with the best (lowest) balanced score\n",
    "    best_balanced_score, best_balanced_trial, best_val_loss, best_max_vio = min(\n",
    "        balanced_scores, key=lambda x: x[0]\n",
    "    )\n",
    "    \n",
    "    print(f\"Best Balanced Solution (after filtering zero MaxVIO):\")\n",
    "    print(f\"\\n  Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"  MaxVIO: {best_max_vio:.4f}\")\n",
    "    print(f\"  Balanced Score: {best_balanced_score:.4f}\")\n",
    "    print(f\"  Routing Parameters: {best_balanced_trial.params}\")\n",
    "    \n",
    "    # Show the range of balanced scores for context\n",
    "    if len(balanced_scores) > 1:\n",
    "        scores_only = [score[0] for score in balanced_scores]\n",
    "        print(f\"  Balanced Score Range: {min(scores_only):.4f} - {max(scores_only):.4f}\")\n",
    "    \n",
    "    return best_balanced_trial\n",
    "\n",
    "# Analyze results\n",
    "analyze_pareto_front(multi_study)\n",
    "best_balanced_trial = find_best_balanced_solution(multi_study)\n",
    "\n",
    "# Display final results summary\n",
    "print(f\"\\nFinal Best Balanced Routing Selected\")\n",
    "print(f\"Validation Loss: {best_balanced_trial.values[0]:.4f}\")\n",
    "print(f\"MaxVIO: {best_balanced_trial.values[1]:.4f}\")\n",
    "\n",
    "print(f\"Optimized Routing Hyperparameters:\")\n",
    "routing_params = {\n",
    "    'n_routed_experts': best_balanced_trial.params.get('n_routed_experts'),\n",
    "    'n_shared_experts': best_balanced_trial.params.get('n_shared_experts'),\n",
    "    'num_experts_per_tok': best_balanced_trial.params.get('num_experts_per_tok'),\n",
    "    'moe_layer_freq': best_balanced_trial.params.get('moe_layer_freq'),\n",
    "    'first_k_dense_replace': best_balanced_trial.params.get('first_k_dense_replace'),\n",
    "    'scoring_func': best_balanced_trial.params.get('scoring_func'),\n",
    "    'norm_topk_prob': best_balanced_trial.params.get('norm_topk_prob'),\n",
    "    'aux_loss_alpha': best_balanced_trial.params.get('aux_loss_alpha'),\n",
    "    'seq_aux': best_balanced_trial.params.get('seq_aux')\n",
    "}\n",
    "for key, value in routing_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nHyperparameter Tuning Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and experiment with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n",
      "TensorFlow version: 2.15.0\n",
      "Mixed precision policy: mixed_float16\n",
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# Disable XLA (XLA JIT compilation) to prevent CUDA graph errors and ensure stability\n",
    "tf.config.optimizer.set_jit(False)  # Disabled XLA to prevent CUDA graph errors\n",
    "\n",
    "# Configure GPU memory growth to prevent memory allocation issues\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Enable mixed precision training with FP16 for improved performance and reduced memory usage\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Configuration parameters from the model and preprocessing pipeline\n",
    "vocab_size = 12000\n",
    "max_seq_len = 70\n",
    "batch_size = 2\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "SEP_TOKEN = \"<sep>\"\n",
    "\n",
    "# Print TensorFlow version, mixed precision policy, and GPU availability for debugging\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Mixed precision policy: {mixed_precision.global_policy().name}\")\n",
    "print(f\"GPU available: {len(gpus) > 0}\")\n",
    "\n",
    "# DEEPSEEK MOE COMPONENTS\n",
    "\n",
    "class DeepSeekRMSNorm(layers.Layer):\n",
    "    \"\"\"RMS Normalization layer \"\"\"\n",
    "    def __init__(self, hidden_size, eps=1e-6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size  # Dimension of the hidden representations\n",
    "        self.variance_epsilon = eps     # Small constant to avoid division by zero\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize learnable weight parameter for scaling normalized output\n",
    "        self.weight = self.add_weight(\n",
    "            name='weight',\n",
    "            shape=(self.hidden_size,),\n",
    "            initializer='ones',\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        x = tf.cast(x, tf.float32)  # Cast input to float32 for stable computation\n",
    "        \n",
    "        # Compute variance as mean of squared inputs along the last dimension\n",
    "        variance = tf.reduce_mean(tf.square(x), axis=-1, keepdims=True)\n",
    "        # Normalize input using RMS formula with epsilon for numerical stability\n",
    "        x = x * tf.math.rsqrt(variance + self.variance_epsilon)\n",
    "        \n",
    "        # Scale normalized output with learnable weight and cast back to original dtype\n",
    "        return self.weight * tf.cast(x, input_dtype)\n",
    "\n",
    "class RotaryPositionEmbedding(layers.Layer):\n",
    "    \"\"\"Rotary Position Embedding (RoPE) with dynamic NTK scaling for positional encoding\"\"\"\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, scaling_type=None, scaling_factor=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim                              # Dimension of the head embeddings\n",
    "        self.max_position_embeddings = max_position_embeddings  # Maximum sequence length\n",
    "        self.base = base                           # Base value for frequency calculation\n",
    "        self.scaling_type = scaling_type           # Type of scaling (\"linear\" or \"dynamic\")\n",
    "        self.scaling_factor = scaling_factor       # Scaling factor for RoPE\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Compute inverse frequencies for rotary embeddings\n",
    "        inv_freq = 1.0 / (self.base ** (tf.range(0, self.dim, 2, dtype=tf.float32) / tf.cast(self.dim, tf.float32)))\n",
    "        self.inv_freq = tf.constant(inv_freq, dtype=tf.float32)\n",
    "        self._compute_rotary_cache()  # Precompute cosine and sine caches\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def _compute_rotary_cache(self):\n",
    "        \"\"\"Compute cosine and sine caches for rotary embeddings with optional scaling\"\"\"\n",
    "        if self.scaling_type == \"linear\":\n",
    "            # Linear scaling: divide positions by scaling factor\n",
    "            t = tf.range(self.max_position_embeddings, dtype=tf.float32) / self.scaling_factor\n",
    "        elif self.scaling_type == \"dynamic\":\n",
    "            # Dynamic NTK scaling for extended context lengths\n",
    "            if self.max_position_embeddings > self.base:\n",
    "                base = self.base * (\n",
    "                    (self.scaling_factor * self.max_position_embeddings / self.base) - \n",
    "                    (self.scaling_factor - 1)\n",
    "                ) ** (self.dim / (self.dim - 2))\n",
    "                inv_freq = 1.0 / (base ** (tf.range(0, self.dim, 2, dtype=tf.float32) / tf.cast(self.dim, tf.float32)))\n",
    "                self.inv_freq = tf.constant(inv_freq, dtype=tf.float32)\n",
    "            t = tf.range(self.max_position_embeddings, dtype=tf.float32)\n",
    "        else:\n",
    "            # Default case: no scaling\n",
    "            t = tf.range(self.max_position_embeddings, dtype=tf.float32)\n",
    "            \n",
    "        # Compute frequency matrix for rotary embeddings\n",
    "        freqs = tf.einsum('i,j->ij', t, self.inv_freq)\n",
    "        emb = tf.concat([freqs, freqs], axis=-1)  # Concatenate for full embedding dimension\n",
    "        \n",
    "        # Cache cosine and sine values as non-trainable variables\n",
    "        self.cos_cached = tf.Variable(tf.cos(emb), trainable=False, name='cos_cached')\n",
    "        self.sin_cached = tf.Variable(tf.sin(emb), trainable=False, name='sin_cached')\n",
    "        \n",
    "    def call(self, x, position_ids=None, seq_len=None):\n",
    "        # Get input tensor dimensions\n",
    "        batch_size, seq_len_x, num_heads, head_dim = tf.unstack(tf.shape(x))\n",
    "        \n",
    "        # Generate position IDs if not provided\n",
    "        if position_ids is None:\n",
    "            position_ids = tf.range(seq_len_x, dtype=tf.int32)\n",
    "            position_ids = tf.tile(tf.reshape(position_ids, [1, -1]), [batch_size, 1])\n",
    "        \n",
    "        # Gather precomputed cosine and sine values for given position IDs\n",
    "        cos = tf.gather(self.cos_cached, position_ids, axis=0)  # Shape: [batch_size, seq_len_x, head_dim]\n",
    "        sin = tf.gather(self.sin_cached, position_ids, axis=0)  # Shape: [batch_size, seq_len_x, head_dim]\n",
    "        \n",
    "        # Split cosine and sine for application to x1 and x2\n",
    "        cos_1, cos_2 = tf.split(cos, 2, axis=-1)\n",
    "        sin_1, sin_2 = tf.split(sin, 2, axis=-1)\n",
    "        \n",
    "        # Reshape for broadcasting with attention heads\n",
    "        cos_1 = tf.reshape(cos_1, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        sin_1 = tf.reshape(sin_1, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        cos_2 = tf.reshape(cos_2, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        sin_2 = tf.reshape(sin_2, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        \n",
    "        # Repeat to match number of attention heads\n",
    "        cos_1 = tf.repeat(cos_1, num_heads, axis=2)\n",
    "        sin_1 = tf.repeat(sin_1, num_heads, axis=2)\n",
    "        cos_2 = tf.repeat(cos_2, num_heads, axis=2)\n",
    "        sin_2 = tf.repeat(sin_2, num_heads, axis=2)\n",
    "        \n",
    "        # Cast to input dtype for mixed precision compatibility\n",
    "        x_dtype = x.dtype\n",
    "        cos_1 = tf.cast(cos_1, x_dtype)\n",
    "        sin_1 = tf.cast(sin_1, x_dtype)\n",
    "        cos_2 = tf.cast(cos_2, x_dtype)\n",
    "        sin_2 = tf.cast(sin_2, x_dtype)\n",
    "        \n",
    "        # Split input tensor into two parts for rotary application\n",
    "        x1, x2 = tf.split(x, 2, axis=-1)\n",
    "        \n",
    "        # Apply rotary embeddings to compute rotated representations\n",
    "        rotated_x1 = x1 * cos_1 - x2 * sin_1\n",
    "        rotated_x2 = x1 * sin_2 + x2 * cos_2\n",
    "        \n",
    "        # Concatenate rotated parts to restore original shape\n",
    "        return tf.concat([rotated_x1, rotated_x2], axis=-1)\n",
    "\n",
    "class DeepSeekMLP(layers.Layer):\n",
    "    \"\"\"Multi-Layer Perceptron (MLP) with Swish activation\"\"\"\n",
    "    def __init__(self, hidden_size, intermediate_size, hidden_act=\"swish\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size           # Input and output dimension\n",
    "        self.intermediate_size = intermediate_size  # Intermediate layer dimension\n",
    "        self.hidden_act = hidden_act            # Activation function (Swish/SILU)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Define gate projection layer with reduced initializer variance for stability\n",
    "        self.gate_proj = layers.Dense(\n",
    "            self.intermediate_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            use_bias=False\n",
    "        )\n",
    "        # Define up projection layer\n",
    "        self.up_proj = layers.Dense(\n",
    "            self.intermediate_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            use_bias=False\n",
    "        )\n",
    "        # Define down projection layer to return to hidden size\n",
    "        self.down_proj = layers.Dense(\n",
    "            self.hidden_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            use_bias=False\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Apply gate and up projections\n",
    "        gate_output = self.gate_proj(x)\n",
    "        up_output = self.up_proj(x)\n",
    "        # Apply Swish (SiLU) activation to gate output and multiply with up output\n",
    "        activated = tf.nn.silu(gate_output) * up_output\n",
    "        # Project back to hidden size\n",
    "        return self.down_proj(activated)\n",
    "\n",
    "class MoEGate(layers.Layer):\n",
    "    \"\"\"Mixture of Experts (MoE) gating mechanism with load balancing auxiliary loss\"\"\"\n",
    "    def __init__(self, num_experts, num_experts_per_tok, hidden_size, \n",
    "                 scoring_func='softmax', norm_topk_prob=False, \n",
    "                 aux_loss_alpha=0.001, seq_aux=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_experts = num_experts                  # Total number of experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok  # Number of experts per token\n",
    "        self.hidden_size = hidden_size                  # Input dimension\n",
    "        self.scoring_func = scoring_func                # Scoring function for expert selection\n",
    "        self.norm_topk_prob = norm_topk_prob            # Normalize top-k probabilities\n",
    "        self.aux_loss_alpha = aux_loss_alpha            # Weight for auxiliary loss\n",
    "        self.seq_aux = seq_aux                          # Compute auxiliary loss per sequence\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize gating weights with reduced variance for stability\n",
    "        self.weight = self.add_weight(\n",
    "            name='gate_weight',\n",
    "            shape=(self.num_experts, self.hidden_size),\n",
    "            initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            trainable=True\n",
    "        )\n",
    "        # Initialize expert assignment counter for tracking during training\n",
    "        self.expert_counts = tf.Variable(tf.zeros([self.num_experts], dtype=tf.int32), trainable=False)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, hidden_states, training=False):\n",
    "        # Get input dimensions\n",
    "        batch_size, seq_len, h = tf.unstack(tf.shape(hidden_states))\n",
    "        hidden_states_flat = tf.reshape(hidden_states, [-1, h])\n",
    "        # Compute gating logits\n",
    "        logits = tf.matmul(hidden_states_flat, self.weight, transpose_b=True)\n",
    "        \n",
    "        # Clip logits for numerical stability\n",
    "        logits = tf.clip_by_value(logits, -50.0, 50.0)\n",
    "        \n",
    "        # Apply scoring function (softmax) to compute expert scores\n",
    "        if self.scoring_func == 'softmax':\n",
    "            temperature = 0.5                                         # Temperature for scaling logits\n",
    "            scaled_logits = tf.cast(logits, tf.float32) / temperature\n",
    "            scores = tf.nn.softmax(scaled_logits, axis=-1)\n",
    "            scores = tf.cast(scores, hidden_states.dtype)\n",
    "        else:\n",
    "            scores = tf.nn.softmax(tf.cast(logits, tf.float32), axis=-1)\n",
    "            scores = tf.cast(scores, hidden_states.dtype)\n",
    "\n",
    "        # Select top-k experts and their weights\n",
    "        topk_weights, topk_indices = tf.math.top_k(\n",
    "            scores, k=self.num_experts_per_tok, sorted=False\n",
    "        )\n",
    "\n",
    "        # Normalize top-k weights if specified\n",
    "        if self.num_experts_per_tok > 1 and self.norm_topk_prob:\n",
    "            denominator = tf.reduce_sum(topk_weights, axis=-1, keepdims=True) + 1e-20\n",
    "            topk_weights = topk_weights / denominator\n",
    "\n",
    "        # Update expert assignment counts during training\n",
    "        if training:\n",
    "            flat_topk_indices = tf.reshape(topk_indices, [-1])\n",
    "            counts = tf.reduce_sum(tf.one_hot(flat_topk_indices, depth=self.num_experts), axis=0)\n",
    "            self.expert_counts.assign_add(tf.cast(counts, tf.int32))\n",
    "\n",
    "        aux_loss = None\n",
    "        if training and self.aux_loss_alpha > 0.0:\n",
    "            if self.seq_aux:\n",
    "                # Sequence-level auxiliary loss for load balancing\n",
    "                scores_seq = tf.reshape(scores, [batch_size, seq_len, -1])\n",
    "                topk_indices_seq = tf.reshape(topk_indices, [batch_size, -1])\n",
    "                ce = tf.zeros([batch_size, self.num_experts], dtype=tf.float32)\n",
    "                mask = tf.one_hot(topk_indices_seq, depth=self.num_experts)\n",
    "                ce = tf.reduce_sum(mask, axis=1)\n",
    "                seq_len_float = tf.cast(seq_len, tf.float32)\n",
    "                num_experts_per_tok_float = tf.cast(self.num_experts_per_tok, tf.float32)\n",
    "                num_experts_float = tf.cast(self.num_experts, tf.float32)\n",
    "                ce = ce / (seq_len_float * num_experts_per_tok_float / num_experts_float)\n",
    "                scores_seq_mean = tf.reduce_mean(tf.cast(scores_seq, tf.float32), axis=1)\n",
    "                aux_loss = tf.reduce_sum(ce * scores_seq_mean, axis=1)\n",
    "                aux_loss = tf.reduce_mean(aux_loss) * self.aux_loss_alpha\n",
    "            else:\n",
    "                # Token-level auxiliary loss\n",
    "                mask_ce = tf.one_hot(tf.reshape(topk_indices, [-1]), depth=self.num_experts)\n",
    "                ce = tf.reduce_mean(tf.cast(mask_ce, tf.float32), axis=0)\n",
    "                Pi = tf.reduce_mean(tf.cast(scores, tf.float32), axis=0)\n",
    "                fi = ce * self.num_experts\n",
    "                aux_loss = tf.reduce_sum(Pi * fi) * self.aux_loss_alpha\n",
    "\n",
    "            # Add auxiliary loss to the layer's losses\n",
    "            self.add_loss(aux_loss)\n",
    "\n",
    "        return topk_indices, topk_weights, aux_loss\n",
    "\n",
    "class DeepSeekMoE(layers.Layer):\n",
    "    \"\"\"Mixture of Experts (MoE) layer with shared and routed experts\"\"\"\n",
    "    def __init__(self, hidden_size, n_routed_experts, n_shared_experts, \n",
    "                 num_experts_per_tok, moe_intermediate_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size                      # Input and output dimension\n",
    "        self.n_routed_experts = n_routed_experts            # Number of routed experts\n",
    "        self.n_shared_experts = n_shared_experts            # Number of shared experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok      # Experts per token\n",
    "        self.moe_intermediate_size = moe_intermediate_size  # MoE intermediate size\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize list of routed expert MLPs\n",
    "        self.routed_experts = [\n",
    "            DeepSeekMLP(\n",
    "                self.hidden_size, \n",
    "                self.moe_intermediate_size,\n",
    "                name=f\"routed_expert_{i}\"\n",
    "            ) for i in range(self.n_routed_experts)\n",
    "        ]\n",
    "        \n",
    "        # Initialize shared experts if specified\n",
    "        if self.n_shared_experts is not None:\n",
    "            shared_intermediate_size = self.moe_intermediate_size * self.n_shared_experts\n",
    "            self.shared_experts = DeepSeekMLP(\n",
    "                self.hidden_size,\n",
    "                shared_intermediate_size,\n",
    "                name=\"shared_experts\"\n",
    "            )\n",
    "        else:\n",
    "            self.shared_experts = None\n",
    "            \n",
    "        # Initialize MoE gating mechanism\n",
    "        self.gate = MoEGate(\n",
    "            self.n_routed_experts,\n",
    "            self.num_experts_per_tok,\n",
    "            self.hidden_size,\n",
    "            name=\"moe_gate\"\n",
    "        )\n",
    "        \n",
    "        # Build each expert and gate\n",
    "        for expert in self.routed_experts:\n",
    "            expert.build(input_shape)\n",
    "        if self.shared_experts:\n",
    "            self.shared_experts.build(input_shape)\n",
    "        self.gate.build(input_shape)\n",
    "        \n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, hidden_states, training=False):\n",
    "        identity = hidden_states  # Store input for residual connection\n",
    "        orig_shape = tf.shape(hidden_states)\n",
    "        # Get top-k expert indices and weights\n",
    "        topk_idx, topk_weight, aux_loss = self.gate(hidden_states, training=training)\n",
    "        hidden_states_flat = tf.reshape(hidden_states, [-1, self.hidden_size])\n",
    "        \n",
    "        if training:\n",
    "            # Expand inputs for top-k experts during training\n",
    "            hidden_states_expanded = tf.repeat(\n",
    "                hidden_states_flat, self.num_experts_per_tok, axis=0\n",
    "            )\n",
    "            flat_topk_idx = tf.reshape(topk_idx, [-1])\n",
    "            combined_output = tf.zeros_like(hidden_states_expanded)\n",
    "            # Process each expert's input\n",
    "            for i, expert in enumerate(self.routed_experts):\n",
    "                expert_mask = tf.equal(flat_topk_idx, i)\n",
    "                if tf.reduce_any(expert_mask):\n",
    "                    expert_input = tf.boolean_mask(hidden_states_expanded, expert_mask)\n",
    "                    expert_output = expert(expert_input)\n",
    "                    update_indices = tf.where(expert_mask)\n",
    "                    combined_output = tf.tensor_scatter_nd_update(\n",
    "                        combined_output,\n",
    "                        update_indices,\n",
    "                        expert_output\n",
    "                    )\n",
    "                    \n",
    "            # Apply expert weights and reshape output\n",
    "            topk_weight_flat = tf.reshape(topk_weight, [-1, 1])\n",
    "            weighted_output = combined_output * topk_weight_flat\n",
    "            weighted_output = tf.reshape(\n",
    "                weighted_output,\n",
    "                [-1, self.num_experts_per_tok, self.hidden_size]\n",
    "            )\n",
    "            routed_output = tf.reduce_sum(weighted_output, axis=1)\n",
    "            routed_output = tf.reshape(routed_output, orig_shape)\n",
    "        else:\n",
    "            # Use efficient inference path\n",
    "            routed_output = self._moe_infer(hidden_states_flat, topk_idx, topk_weight)\n",
    "            routed_output = tf.reshape(routed_output, orig_shape)\n",
    "            \n",
    "        # Add shared expert output if available\n",
    "        if self.shared_experts is not None:\n",
    "            shared_output = self.shared_experts(identity)\n",
    "            final_output = routed_output + shared_output\n",
    "        else:\n",
    "            final_output = routed_output\n",
    "            \n",
    "        # Apply residual connection\n",
    "        return final_output + identity\n",
    "        \n",
    "    def _moe_infer(self, x, expert_indices, expert_weights):\n",
    "        \"\"\"Efficient inference path for MoE layer\"\"\"\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        expert_cache = tf.zeros_like(x)\n",
    "        # Clip expert indices to valid range\n",
    "        expert_indices = tf.clip_by_value(expert_indices, 0, self.n_routed_experts - 1)\n",
    "        flat_expert_indices = tf.reshape(expert_indices, [-1])\n",
    "        flat_expert_weights = tf.reshape(expert_weights, [-1, 1])\n",
    "        total_tokens = tf.shape(x)[0]\n",
    "        token_indices_base = tf.range(total_tokens, dtype=tf.int64)\n",
    "        token_indices_base = tf.repeat(token_indices_base, self.num_experts_per_tok, axis=0)\n",
    "        \n",
    "        # Process each expert's tokens\n",
    "        for i in range(self.n_routed_experts):\n",
    "            expert_mask = tf.equal(flat_expert_indices, i)\n",
    "            if tf.reduce_any(expert_mask):\n",
    "                token_indices = tf.boolean_mask(token_indices_base, expert_mask)\n",
    "                expert_tokens = tf.gather(x, token_indices)\n",
    "                expert_out = self.routed_experts[i](expert_tokens)\n",
    "                expert_out_weighted = expert_out * tf.gather(flat_expert_weights, tf.where(expert_mask)[:, 0])\n",
    "                expert_cache = tf.tensor_scatter_nd_add(\n",
    "                    expert_cache,\n",
    "                    tf.expand_dims(token_indices, 1),\n",
    "                    expert_out_weighted\n",
    "                )\n",
    "                \n",
    "        return expert_cache\n",
    "\n",
    "class DeepSeekAttention(layers.Layer):\n",
    "    \"\"\"Multi-head attention with Rotary Position Embeddings and Grouped Query Attention (GQA) support\"\"\"\n",
    "    def __init__(self, hidden_size, num_attention_heads, num_key_value_heads=None,\n",
    "                 attention_dropout=0.0, max_position_embeddings=2048,\n",
    "                 rope_theta=10000.0, rope_scaling=None, attention_bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size                                         # Input and output dimension\n",
    "        self.num_attention_heads = num_attention_heads                         # Number of attention heads\n",
    "        self.num_key_value_heads = num_key_value_heads or num_attention_heads  # Number of key/value heads (GQA)\n",
    "        self.attention_dropout = attention_dropout                             # Dropout rate for attention weights\n",
    "        self.max_position_embeddings = max_position_embeddings                 # Maximum sequence length\n",
    "        self.rope_theta = rope_theta                                           # Base period for RoPE\n",
    "        self.rope_scaling = rope_scaling                                       # RoPE scaling configuration\n",
    "        self.attention_bias = attention_bias                                   # Whether to use bias in projections\n",
    "        self.head_dim = hidden_size // num_attention_heads                     # Dimension per head\n",
    "        self.num_key_value_groups = num_attention_heads // self.num_key_value_heads  # Groups for GQA\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize query projection layer\n",
    "        self.q_proj = layers.Dense(\n",
    "            self.num_attention_heads * self.head_dim,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"q_proj\"\n",
    "        )\n",
    "        # Initialize key projection layer\n",
    "        self.k_proj = layers.Dense(\n",
    "            self.num_key_value_heads * self.head_dim,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"k_proj\"\n",
    "        )\n",
    "        # Initialize value projection layer\n",
    "        self.v_proj = layers.Dense(\n",
    "            self.num_key_value_heads * self.head_dim,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"v_proj\"\n",
    "        )\n",
    "        # Initialize output projection layer\n",
    "        self.o_proj = layers.Dense(\n",
    "            self.hidden_size,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"o_proj\"\n",
    "        )\n",
    "        \n",
    "        # Initialize RoPE with specified scaling\n",
    "        scaling_type = self.rope_scaling.get(\"type\") if self.rope_scaling else None\n",
    "        scaling_factor = self.rope_scaling.get(\"factor\", 1.0) if self.rope_scaling else 1.0\n",
    "        self.rotary_emb = RotaryPositionEmbedding(\n",
    "            self.head_dim,\n",
    "            self.max_position_embeddings,\n",
    "            self.rope_theta,\n",
    "            scaling_type,\n",
    "            scaling_factor,\n",
    "            name=\"rotary_emb\"\n",
    "        )\n",
    "        self.dropout_layer = layers.Dropout(self.attention_dropout)\n",
    "        self.rotary_emb.build(input_shape)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, hidden_states, attention_mask=None, position_ids=None, training=False):\n",
    "        # Get input dimensions\n",
    "        batch_size, seq_len, _ = tf.unstack(tf.shape(hidden_states))\n",
    "        \n",
    "        # Project inputs to query, key, and value\n",
    "        q = self.q_proj(hidden_states)\n",
    "        k = self.k_proj(hidden_states)\n",
    "        v = self.v_proj(hidden_states)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = tf.reshape(q, [batch_size, seq_len, self.num_attention_heads, self.head_dim])\n",
    "        k = tf.reshape(k, [batch_size, seq_len, self.num_key_value_heads, self.head_dim])\n",
    "        v = tf.reshape(v, [batch_size, seq_len, self.num_key_value_heads, self.head_dim])\n",
    "        \n",
    "        # Apply rotary embeddings to query and key\n",
    "        q_rotated = self.rotary_emb(q, position_ids=position_ids)\n",
    "        k_rotated = self.rotary_emb(k, position_ids=position_ids)\n",
    "        \n",
    "        # Transpose for attention computation\n",
    "        q_rotated = tf.transpose(q_rotated, [0, 2, 1, 3])\n",
    "        k_rotated = tf.transpose(k_rotated, [0, 2, 1, 3])\n",
    "        v = tf.transpose(v, [0, 2, 1, 3])\n",
    "        \n",
    "        # Repeat key and value for GQA if necessary\n",
    "        if self.num_key_value_groups > 1:\n",
    "            k_rotated = tf.repeat(k_rotated, self.num_key_value_groups, axis=1)\n",
    "            v = tf.repeat(v, self.num_key_value_groups, axis=1)\n",
    "            \n",
    "        # Compute scaled dot-product attention\n",
    "        scale = tf.math.rsqrt(tf.cast(self.head_dim, tf.float32))\n",
    "        scale = tf.cast(scale, q_rotated.dtype)\n",
    "        attn_weights = tf.matmul(q_rotated, k_rotated, transpose_b=True) * scale\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = tf.cast(attention_mask, attn_weights.dtype)\n",
    "            if len(attention_mask.shape) == 2:\n",
    "                attention_mask = tf.expand_dims(tf.expand_dims(attention_mask, 1), 1)\n",
    "            elif len(attention_mask.shape) == 3:\n",
    "                attention_mask = tf.expand_dims(attention_mask, 1)\n",
    "            attn_weights += attention_mask\n",
    "            \n",
    "        # Clip attention weights for numerical stability\n",
    "        attn_weights = tf.clip_by_value(attn_weights, -50.0, 50.0)\n",
    "        attn_weights = tf.cast(attn_weights, tf.float32)\n",
    "        # Apply softmax with max subtraction for stability\n",
    "        attn_weights = attn_weights - tf.reduce_max(attn_weights, axis=-1, keepdims=True)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "        attn_weights = tf.cast(attn_weights, q_rotated.dtype)\n",
    "        \n",
    "        # Apply dropout to attention weights during training\n",
    "        attn_weights = self.dropout_layer(attn_weights, training=training)\n",
    "        # Compute attention output\n",
    "        attn_output = tf.matmul(attn_weights, v)\n",
    "        attn_output = tf.transpose(attn_output, [0, 2, 1, 3])\n",
    "        attn_output = tf.reshape(attn_output, [batch_size, seq_len, self.hidden_size])\n",
    "        \n",
    "        # Project attention output to hidden size\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "class DeepSeekDecoderLayer(layers.Layer):\n",
    "    \"\"\"Transformer decoder layer combining attention and MoE/MLP with residual connections\"\"\"\n",
    "    def __init__(self, config, layer_idx, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        # Initialize self-attention with GQA and RoPE\n",
    "        self.self_attn = DeepSeekAttention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            num_key_value_heads=config.num_key_value_heads,\n",
    "            attention_dropout=config.attention_dropout,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            rope_theta=config.rope_theta,\n",
    "            rope_scaling=config.rope_scaling,\n",
    "            attention_bias=config.attention_bias,\n",
    "            name=\"self_attn\"\n",
    "        )\n",
    "        \n",
    "        # Determine whether to use MoE or dense MLP based on layer index and config\n",
    "        self.use_moe = (\n",
    "            config.n_routed_experts is not None and\n",
    "            layer_idx >= config.first_k_dense_replace and\n",
    "            layer_idx % config.moe_layer_freq == 0\n",
    "        )\n",
    "        \n",
    "        if self.use_moe:\n",
    "            # Initialize MoE layer for routed experts\n",
    "            self.mlp = DeepSeekMoE(\n",
    "                hidden_size=config.hidden_size,\n",
    "                n_routed_experts=config.n_routed_experts,\n",
    "                n_shared_experts=config.n_shared_experts,\n",
    "                num_experts_per_tok=config.num_experts_per_tok,\n",
    "                moe_intermediate_size=config.moe_intermediate_size,\n",
    "                name=\"moe\"\n",
    "            )\n",
    "        else:\n",
    "            # Initialize dense MLP layer\n",
    "            self.mlp = DeepSeekMLP(\n",
    "                hidden_size=config.hidden_size,\n",
    "                intermediate_size=config.intermediate_size,\n",
    "                hidden_act=config.hidden_act,\n",
    "                name=\"mlp\"\n",
    "            )\n",
    "            \n",
    "        # Initialize input and post-attention normalization layers\n",
    "        self.input_layernorm = DeepSeekRMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps, name=\"input_layernorm\"\n",
    "        )\n",
    "        self.post_attention_layernorm = DeepSeekRMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps, name=\"post_attention_layernorm\"\n",
    "        )\n",
    "        \n",
    "    def call(self, hidden_states, attention_mask=None, position_ids=None, training=False):\n",
    "        # Apply input normalization and self-attention with residual connection\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        attn_output = self.self_attn(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            training=training\n",
    "        )\n",
    "        hidden_states = residual + attn_output\n",
    "        \n",
    "        # Apply post-attention normalization and MLP/MoE with residual connection\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        mlp_output = self.mlp(hidden_states, training=training)\n",
    "        hidden_states = residual + mlp_output\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "# MODEL CONFIGURATION \n",
    "\n",
    "class DeepSeekConfig:\n",
    "    \"\"\"\n",
    "    This is the configuration class to store the configuration of a [`DeepseekMoEModel`]. It is used to instantiate an DeepSeek\n",
    "    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
    "    defaults below will yield a configuration for a small, stable, and computationally efficient MoE chatbot model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (`int`, *optional*, defaults to 12000):\n",
    "            Vocabulary size of the Deep model. Defines the number of different tokens that can be represented by the\n",
    "            `inputs_ids` passed when calling [`DeepseekModel`]. (Optimized for chatbot use case).\n",
    "            \n",
    "        hidden_size (`int`, *optional*, defaults to 256):\n",
    "            Dimension of the hidden representations. (Optimized hidden size for stability and performance).\n",
    "            \n",
    "        intermediate_size (`int`, *optional*, defaults to 512):\n",
    "            Dimension of the MLP representations. (Balanced intermediate size for MLP layers).\n",
    "            \n",
    "        moe_intermediate_size (`int`, *optional*, defaults to 256):\n",
    "            Dimension of the MoE representations. (MoE-specific intermediate dimension).\n",
    "            \n",
    "        num_hidden_layers (`int`, *optional*, defaults to 4):\n",
    "            Number of hidden layers in the Transformer decoder. (Optimal layer count for chatbot tasks).\n",
    "            \n",
    "        num_attention_heads (`int`, *optional*, defaults to 4):\n",
    "            Number of attention heads for each attention layer in the Transformer decoder. (Balanced attention heads for computation).\n",
    "        \n",
    "        n_shared_experts (`int`, *optional*, defaults to 1):\n",
    "            Number of shared experts, None means dense model. (Set to 1, a single shared expert for parameter efficiency).\n",
    "        \n",
    "        n_routed_experts (`int`, *optional*, defaults to 4):\n",
    "            Number of routed experts, None means dense model. (Set to 4 for specialization).\n",
    "            \n",
    "        num_experts_per_tok (`int`, *optional*, defaults to 2):\n",
    "            Number of selected experts, None means dense model. (Set to 2 experts per token for balanced load).\n",
    "            \n",
    "        moe_layer_freq (`int`, *optional*, defaults to 2):\n",
    "            The frequency of the MoE layer: one expert layer for every `moe_layer_freq - 1` dense layers.\n",
    "            (MoE every other layer for computational balance).\n",
    "            \n",
    "        first_k_dense_replace (`int`, *optional*, defaults to 1):\n",
    "            - Number of dense layers in shallow layers(embed->dense->dense->...->dense->moe->moe...->lm_head).\n",
    "            - k dense layers (First layer dense for stability).\n",
    "        norm_topk_prob (`bool`, *optional*, defaults to False):\n",
    "            Whether to normalize the weights of the routed experts. (No normalization for expert weights).\n",
    "            \n",
    "        scoring_func (`str`, *optional*, defaults to 'softmax'):\n",
    "            Method of computing expert weights. (Standard softmax for expert selection).\n",
    "            \n",
    "        aux_loss_alpha (`float`, *optional*, defaults to 0.001):\n",
    "            Auxiliary loss weight coefficient. (Default auxiliary loss weight).\n",
    "            \n",
    "        seq_aux = (`bool`, *optional*, defaults to True):\n",
    "            Whether to compute the auxiliary loss for each individual sample. (Sequence-level auxiliary loss).\n",
    "            \n",
    "        num_key_value_heads (`int`, *optional*, defaults to 2):\n",
    "            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
    "            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
    "            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used.\n",
    "            (Set to 2 for GQA and memory efficiency).\n",
    "            \n",
    "        hidden_act (`str` or `function`, *optional*, defaults to \"swish\"):\n",
    "            The non-linear activation function (function or string) in the decoder. (Swish activation for better gradients).\n",
    "        \n",
    "        max_position_embeddings (`int`, *optional*, defaults to 256):\n",
    "            The maximum sequence length that this model might ever be used with. (Sufficient context length for chatbot).\n",
    "        \n",
    "        initializer_range (`float`, *optional*, defaults to 0.001):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices. (Reduced initializer range for stability).\n",
    "        \n",
    "        rms_norm_eps (`float`, *optional*, defaults to 1e-6):\n",
    "            The epsilon used by the rms normalization layers. (Standard RMS norm epsilon).\n",
    "        \n",
    "        use_cache (`bool`, *optional*, defaults to True):\n",
    "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
    "            relevant if `config.is_decoder=True`. (Enable caching for inference).\n",
    "        \n",
    "        pad_token_id (`int`, *optional*):\n",
    "            Padding token id.\n",
    "        bos_token_id (`int`, *optional*, defaults to 1):\n",
    "            Beginning of stream token id.\n",
    "        eos_token_id (`int`, *optional*, defaults to 2):\n",
    "            End of stream token id.\n",
    "        \n",
    "        pretraining_tp (`int`, *optional*, defaults to 1):\n",
    "            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
    "            document](https://huggingface.co/docs/transformers/parallelism) to understand more about it. This value is\n",
    "            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
    "            issue](https://github.com/pytorch/pytorch/issues/76232).\n",
    "        \n",
    "        tie_word_embeddings (`bool`, *optional*, defaults to False):\n",
    "            Whether to tie weight embeddings\n",
    "        \n",
    "        rope_theta (`float`, *optional*, defaults to 10000.0):\n",
    "            The base period of the RoPE embeddings. (Standard RoPE base period).\n",
    "        \n",
    "        rope_scaling (`Dict`, *optional*, defaults to {\"type\": \"linear\", \"factor\": 2.0}):\n",
    "            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
    "            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
    "            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
    "            `max_position_embeddings` to the expected new maximum. (Linear RoPE scaling enabled).\n",
    "        \n",
    "        attention_bias (`bool`, defaults to False, *optional*, defaults to False):\n",
    "            Whether to use a bias in the query, key, value and output projection layers during self-attention. (No attention bias for efficiency).\n",
    "        \n",
    "        attention_dropout (`float`, *optional*, defaults to 0.1):\n",
    "            The dropout ratio for the attention probabilities. (Increased dropout for regularization).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=vocab_size,      # Default vocabulary size optimized for chatbot\n",
    "        hidden_size=256,            # Optimized hidden size for stability and performance\n",
    "        intermediate_size=512,      # Balanced intermediate size for MLP layers\n",
    "        moe_intermediate_size=256,  # MoE-specific intermediate dimension\n",
    "        num_hidden_layers=4,        # Optimal layer count for chatbot tasks\n",
    "        num_attention_heads=4,      # Balanced attention heads for computation\n",
    "        num_key_value_heads=2,      # GQA for memory efficiency\n",
    "        n_shared_experts=3,         # Three shared experts for parameter efficiency (REVISED)\n",
    "        n_routed_experts=6,         # Six routed experts for specialization     (REVISED)\n",
    "        num_experts_per_tok=2,      # Two experts per token for balanced load\n",
    "        moe_layer_freq=1,           # MoE every layer for computational balance (REVISED)\n",
    "        first_k_dense_replace=1,    # First layer dense for stability\n",
    "        norm_topk_prob=True,        # Normalization for expert weights          (REVISED)\n",
    "        scoring_func='softmax',     # Standard softmax for expert selection\n",
    "        aux_loss_alpha=0.0022997166087842954,  # Auxiliary loss weight          (REVISED)\n",
    "        seq_aux=False,              # Token-level auxiliary loss                (REVISED)\n",
    "        hidden_act=\"swish\",         # Swish activation for better gradients\n",
    "        max_position_embeddings=256,  # Sufficient context length for chatbot\n",
    "        initializer_range=0.001,    # Reduced initializer range for stability\n",
    "        rms_norm_eps=1e-6,          # Standard RMS norm epsilon\n",
    "        use_cache=True,             # Enable caching for inference\n",
    "        rope_theta=10000.0,         # Standard RoPE base period\n",
    "        rope_scaling={\"type\": \"linear\", \"factor\": 2.0},  # Linear RoPE scaling\n",
    "        attention_bias=False,       # No attention bias for efficiency\n",
    "        attention_dropout=0.1       # Increased dropout for regularization\n",
    "    ):\n",
    "        # Initialize model architecture parameters with optimized values\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.moe_intermediate_size = moe_intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.n_shared_experts = n_shared_experts\n",
    "        self.n_routed_experts = n_routed_experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok\n",
    "        self.moe_layer_freq = moe_layer_freq\n",
    "        self.first_k_dense_replace = first_k_dense_replace\n",
    "        self.norm_topk_prob = norm_topk_prob\n",
    "        self.scoring_func = scoring_func\n",
    "        self.aux_loss_alpha = aux_loss_alpha\n",
    "        self.seq_aux = seq_aux\n",
    "        self.hidden_act = hidden_act\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.initializer_range = initializer_range\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.use_cache = use_cache\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self.attention_bias = attention_bias\n",
    "        self.attention_dropout = attention_dropout\n",
    "\n",
    "class DeepSeekModel(tf.keras.Model):\n",
    "    \"\"\"Core DeepSeek transformer model for chatbot, provide raw hidden states\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.padding_idx = 0\n",
    "        \n",
    "        # Initialize embedding layer for token inputs\n",
    "        self.embed_tokens = layers.Embedding(\n",
    "            config.vocab_size,\n",
    "            config.hidden_size,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomNormal(\n",
    "                stddev=config.initializer_range\n",
    "            ),\n",
    "            name=\"embed_tokens\"\n",
    "        )\n",
    "        \n",
    "        # Initialize decoder layers\n",
    "        self.decoder_layers = [\n",
    "            DeepSeekDecoderLayer(config, layer_idx=i, name=f\"layer_{i}\")\n",
    "            for i in range(config.num_hidden_layers)\n",
    "        ]\n",
    "        \n",
    "        # Initialize final normalization layer\n",
    "        self.norm = DeepSeekRMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps, name=\"norm\"\n",
    "        )\n",
    "        \n",
    "        # Initialize expert assignment counter for MoE layers\n",
    "        self.expert_assignment_counts = tf.Variable(\n",
    "            tf.zeros([config.n_routed_experts], dtype=tf.int32),\n",
    "            trainable=False,\n",
    "            name=\"expert_assignment_counts\"\n",
    "        ) if config.n_routed_experts else None\n",
    "        \n",
    "    def call(self, input_ids, attention_mask=None, position_ids=None, training=False):\n",
    "        # Embed input tokens\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "        \n",
    "        # Create causal mask if not provided\n",
    "        if attention_mask is None:\n",
    "            seq_len = tf.shape(input_ids)[1]\n",
    "            attention_mask = self._create_causal_mask(seq_len)\n",
    "            \n",
    "        # Prepare attention mask for causal attention\n",
    "        attention_mask = tf.cast(attention_mask, hidden_states.dtype)\n",
    "        if len(attention_mask.shape) == 2:\n",
    "            attention_mask = tf.expand_dims(tf.expand_dims(attention_mask, 1), 1)\n",
    "        elif len(attention_mask.shape) == 3:\n",
    "            attention_mask = tf.expand_dims(attention_mask, 1)\n",
    "        attention_mask = (tf.cast(1.0, hidden_states.dtype) - attention_mask) * tf.cast(-1e4, hidden_states.dtype)\n",
    "        \n",
    "        # Process through decoder layers and aggregate expert counts\n",
    "        for layer in self.decoder_layers:\n",
    "            hidden_states = layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                training=training\n",
    "            )\n",
    "            # Update expert assignment counts for MoE layers during training\n",
    "            if training and hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE):\n",
    "                expert_counts = layer.mlp.gate.expert_counts\n",
    "                if self.expert_assignment_counts is not None:\n",
    "                    self.expert_assignment_counts.assign_add(expert_counts)\n",
    "            \n",
    "        # Apply final normalization\n",
    "        return self.norm(hidden_states)\n",
    "        \n",
    "    def _create_causal_mask(self, seq_len):\n",
    "        \"\"\"Create causal attention mask to prevent attending to future tokens\"\"\"\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return tf.cast(mask, tf.float16)\n",
    "\n",
    "class DeepSeekForCausalLM(tf.keras.Model):\n",
    "    \"\"\"Causal language model for chatbot text generation\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.transformer = DeepSeekModel(config, name=\"transformer\")\n",
    "        # Initialize language model head\n",
    "        self.lm_head = layers.Dense(\n",
    "            config.vocab_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                stddev=config.initializer_range\n",
    "            ),\n",
    "            use_bias=False,\n",
    "            name=\"lm_head\"\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, attention_mask=None, position_ids=None, training=False):\n",
    "        # Handle dictionary or tensor inputs\n",
    "        if isinstance(inputs, dict):\n",
    "            input_ids = inputs.get('input_ids')\n",
    "            attention_mask = inputs.get('attention_mask', attention_mask)\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "        \n",
    "        # Process through transformer\n",
    "        hidden_states = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            training=training\n",
    "        )\n",
    "        # Generate logits for token prediction\n",
    "        return self.lm_head(hidden_states)\n",
    "\n",
    "# METRICS AND CALLBACKS\n",
    "class ResourceMonitor(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to monitor system resources (GPU, CPU, memory) during training\"\"\"\n",
    "    def __init__(self, batch_size=2):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.resource_metrics = []  # Store resource usage metrics\n",
    "        self.start_time = time.time()\n",
    "        self.total_tokens_processed = 0\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "        self.epoch_tokens = 0\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # Update token count for throughput calculation\n",
    "        self.total_tokens_processed += self.batch_size * max_seq_len\n",
    "        self.epoch_tokens += self.batch_size * max_seq_len\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        try:\n",
    "            # Collect GPU and system resource usage\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            gpu_memory = sum([gpu.memoryUsed for gpu in gpus]) / len(gpus) if gpus else 0\n",
    "            gpu_usage = sum([gpu.load * 100 for gpu in gpus]) / len(gpus) if gpus else 0\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory_info = psutil.virtual_memory()\n",
    "            system_memory = memory_info.used / (1024 ** 3)\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            tokens_per_second = self.epoch_tokens / epoch_time if epoch_time > 0 else 0\n",
    "            \n",
    "            # Store resource metrics\n",
    "            resource_stats = {\n",
    "                'epoch': epoch + 1,\n",
    "                'avg_gpu_memory_gb': float(gpu_memory / 1024),\n",
    "                'avg_system_memory_gb': float(system_memory),\n",
    "                'avg_gpu_usage_percent': float(gpu_usage),\n",
    "                'avg_cpu_usage_percent': float(cpu_usage),\n",
    "                'tokens_per_second': float(tokens_per_second),\n",
    "                'throughput_tps': float(tokens_per_second)\n",
    "            }\n",
    "            \n",
    "            self.resource_metrics.append(resource_stats)\n",
    "            \n",
    "            # Print resource usage summary\n",
    "            print(f\"Epoch {epoch+1} Resources: GPU Mem: {gpu_memory/1024:.1f}GB, \"\n",
    "                  f\"System Mem: {system_memory:.1f}GB, GPU: {gpu_usage:.1f}%, \"\n",
    "                  f\"CPU: {cpu_usage:.1f}%, TPS: {tokens_per_second:.0f}\")\n",
    "                  \n",
    "        except Exception as e:\n",
    "            print(f\"Resource monitoring error: {e}\")\n",
    "\n",
    "class DeepSeekMetrics(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to track specific metrics, including MaxVIO for expert balancing\"\"\"\n",
    "    def __init__(self, validation_data, tokenizer):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.metrics_history = []\n",
    "        self.start_time = time.time()\n",
    "        self.max_vio_values = []\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Reset expert assignment counts for accurate per-epoch metrics\n",
    "        if hasattr(self.model.transformer, 'expert_assignment_counts') and self.model.transformer.expert_assignment_counts is not None:\n",
    "            self.model.transformer.expert_assignment_counts.assign(tf.zeros([self.model.config.n_routed_experts], dtype=tf.int32))\n",
    "        for layer in self.model.transformer.decoder_layers:\n",
    "            if hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE):\n",
    "                layer.mlp.gate.expert_counts.assign(tf.zeros([layer.mlp.n_routed_experts], dtype=tf.int32))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        \n",
    "        # Estimate FLOPs for computational efficiency tracking\n",
    "        total_params = self.model.count_params()\n",
    "        flops_per_token = total_params * 2\n",
    "        total_flops = flops_per_token * batch_size * max_seq_len\n",
    "        total_gflops = total_flops / 1e9\n",
    "        \n",
    "        logs['gflops'] = float(total_gflops)\n",
    "        \n",
    "        # Calculate MaxVIO for expert load balancing\n",
    "        if self.validation_data:\n",
    "            try:\n",
    "                max_vio = self._calculate_max_vio()\n",
    "                logs['max_vio'] = float(max_vio)\n",
    "                self.max_vio_values.append(max_vio)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Metrics calculation error: {e}\")\n",
    "                logs['max_vio'] = 0.0\n",
    "                \n",
    "        self.metrics_history.append(logs.copy())\n",
    "        \n",
    "        # Print simplified metrics summary\n",
    "        print(f\"Epoch {epoch+1}: Loss={logs.get('loss', 0):.4f}, \"\n",
    "              f\"Val_Loss={logs.get('val_loss', 0):.4f}, \"\n",
    "              f\"GFLOPs={total_gflops:.2f}, \"\n",
    "              f\"MaxVIO={logs.get('max_vio', 0):.4f}\")\n",
    "              \n",
    "    def _calculate_max_vio(self):\n",
    "        \"\"\"Calculate Maximum Violation (MaxVIO) for expert load balancing\"\"\"\n",
    "        total_counts = np.zeros(self.model.config.n_routed_experts, dtype=np.float32)\n",
    "        moe_layers = 0\n",
    "        # Aggregate expert counts across MoE layers\n",
    "        for layer in self.model.transformer.decoder_layers:\n",
    "            if hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE):\n",
    "                counts = layer.mlp.gate.expert_counts.numpy()\n",
    "                total_counts += counts\n",
    "                moe_layers += 1\n",
    "        \n",
    "        # Compute MaxVIO based on aggregated counts\n",
    "        if moe_layers > 0 and np.sum(total_counts) > 0:\n",
    "            normalized_counts = total_counts / (np.sum(total_counts) + 1e-10)\n",
    "            expected_uniform = 1.0 / len(total_counts)\n",
    "            max_vio = np.max(np.abs(normalized_counts - expected_uniform)) / expected_uniform\n",
    "            return max_vio\n",
    "        return 0.0\n",
    "\n",
    "class ComprehensiveModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback for saving model checkpoints based on monitored metric\"\"\"\n",
    "    def __init__(self, filepath, monitor='val_loss', save_best_only=True, mode='min'):\n",
    "        super().__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.save_best_only = save_best_only\n",
    "        self.mode = mode\n",
    "        self.best_value = float('inf') if mode == 'min' else -float('inf')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            return\n",
    "            \n",
    "        current_value = logs.get(self.monitor)\n",
    "        if current_value is None:\n",
    "            return\n",
    "            \n",
    "        # Determine if current model should be saved\n",
    "        if self.mode == 'min':\n",
    "            should_save = current_value < self.best_value\n",
    "        else:\n",
    "            should_save = current_value > self.best_value\n",
    "            \n",
    "        if should_save or not self.save_best_only:\n",
    "            self.best_value = current_value\n",
    "            self.model.save_weights(self.filepath)\n",
    "            print(f\"Model checkpoint saved with {self.monitor}: {current_value:.4f}\")\n",
    "\n",
    "\n",
    "# Function to calculate perplexity on validation set\n",
    "def calculate_perplexity(model, dataset, tokenizer, max_batches=100):          # Sample batch for calculate perplexity\n",
    "    \"\"\"Calculate perplexity on validation dataset to evaluate model performance\"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    PAD_TOKEN_ID = tokenizer.token_to_id(PAD_TOKEN)\n",
    "    \n",
    "    for i, batch in enumerate(dataset.take(max_batches)):\n",
    "        inputs, labels = batch\n",
    "        logits = model(inputs, training=False)\n",
    "        # Clip logits for numerical stability\n",
    "        logits = tf.clip_by_value(logits, -50.0, 50.0)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        # Compute per-token loss\n",
    "        per_token_loss = -tf.gather(log_probs, labels, batch_dims=2)\n",
    "        if tf.reduce_any(tf.math.is_nan(per_token_loss)):\n",
    "            continue\n",
    "        # Mask padding tokens\n",
    "        mask = tf.cast(labels != PAD_TOKEN_ID, per_token_loss.dtype)\n",
    "        batch_loss = tf.reduce_sum(per_token_loss * mask)\n",
    "        batch_tokens = tf.reduce_sum(mask)\n",
    "        if tf.math.is_nan(batch_loss) or batch_tokens == 0:\n",
    "            continue\n",
    "        total_loss += batch_loss.numpy()\n",
    "        total_tokens += batch_tokens.numpy()\n",
    "        \n",
    "    # Compute perplexity as exponential of average loss\n",
    "    if total_tokens > 0:\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        return math.exp(min(avg_loss, 20.0))\n",
    "    return float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary\n",
      "Model: \"deep_seek_for_causal_lm_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (DeepSeekModel  multiple                  6918168   \n",
      " )                                                               \n",
      "                                                                 \n",
      " lm_head (Dense)             multiple                  292096    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7210264 (27.50 MB)\n",
      "Trainable params: 7079168 (27.00 MB)\n",
      "Non-trainable params: 131096 (512.09 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Visualize the model\n",
    "# Create model with specified vocabulary size using direct config\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "# Use the optimized config directly with the same hyperparameters from create_deepseek_chatbot_model\n",
    "config = DeepSeekConfig(vocab_size=vocab_size)\n",
    "model = DeepSeekForCausalLM(config)\n",
    "\n",
    "\n",
    "# Build model\n",
    "if tf_train_dataset:\n",
    "    sample_batch = next(iter(tf_train_dataset.take(1)))\n",
    "    model(sample_batch[0])\n",
    "\n",
    "# Configure optimizer with more conservative settings\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,  # Reduced learning rate\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.95,\n",
    "    weight_decay=0.1,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        ignore_class=PAD_TOKEN_ID,  # Use correct PAD token ID\n",
    "        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
    "    ),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nModel Summary\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training Iteration 1\n",
      "Starting training...\n",
      "55/55 [==============================] - ETA: 0s - loss: 6.6434 - accuracy: 0.0187Epoch 1: Loss=6.6434, Val_Loss=6.1649, GFLOPs=2.02, MaxVIO=0.0149\n",
      "Epoch 1 Resources: GPU Mem: 14.3GB, System Mem: 6.9GB, GPU: 10.0%, CPU: 28.1%, TPS: 201\n",
      "55/55 [==============================] - 38s 172ms/step - loss: 6.6434 - accuracy: 0.0187 - val_loss: 6.1649 - val_accuracy: 0.0209 - lr: 9.9433e-05 - gflops: 2.0189 - max_vio: 0.0149\n",
      "Calculating final perplexity\n",
      "Final Training Loss: 6.6434\n",
      "Final Validation Loss: 6.1649\n",
      "Final Perplexity: 479.32\n",
      "New best model saved from iteration 1 with perplexity: 479.32\n",
      "Result 1 completed in 38.27 seconds\n",
      "Average TPS: 201\n",
      "\n",
      "Starting Training Iteration 2\n",
      "Starting training...\n",
      "55/55 [==============================] - ETA: 0s - loss: 6.6407 - accuracy: 0.0208Epoch 1: Loss=6.6407, Val_Loss=6.1621, GFLOPs=2.02, MaxVIO=0.0332\n",
      "Epoch 1 Resources: GPU Mem: 14.3GB, System Mem: 7.8GB, GPU: 15.0%, CPU: 22.7%, TPS: 213\n",
      "55/55 [==============================] - 36s 158ms/step - loss: 6.6407 - accuracy: 0.0208 - val_loss: 6.1621 - val_accuracy: 0.0112 - lr: 9.9433e-05 - gflops: 2.0189 - max_vio: 0.0332\n",
      "Calculating final perplexity\n",
      "Final Training Loss: 6.6407\n",
      "Final Validation Loss: 6.1621\n",
      "Final Perplexity: 483.97\n",
      "Result 2 completed in 36.20 seconds\n",
      "Average TPS: 213\n",
      "\n",
      "Training Results Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Iteration 2</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Average TPS</th>\n",
       "      <td>201.341918</td>\n",
       "      <td>212.828149</td>\n",
       "      <td>207.085033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Training Loss</th>\n",
       "      <td>6.643372</td>\n",
       "      <td>6.640678</td>\n",
       "      <td>6.642025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Validation Loss</th>\n",
       "      <td>6.164899</td>\n",
       "      <td>6.162109</td>\n",
       "      <td>6.163504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computational Resource Usage</th>\n",
       "      <td>38.100000</td>\n",
       "      <td>37.700000</td>\n",
       "      <td>37.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>28.100000</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>25.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>6.876156</td>\n",
       "      <td>7.781719</td>\n",
       "      <td>7.328938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.272461</td>\n",
       "      <td>14.272461</td>\n",
       "      <td>14.272461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPS Estimate (GFLOPS)</th>\n",
       "      <td>2.018874</td>\n",
       "      <td>2.018874</td>\n",
       "      <td>2.018874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final Validation Perplexity</th>\n",
       "      <td>479.324046</td>\n",
       "      <td>483.965125</td>\n",
       "      <td>481.644585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average MaxVIO_global</th>\n",
       "      <td>0.014935</td>\n",
       "      <td>0.033247</td>\n",
       "      <td>0.024091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lowest MaxVIO_global</th>\n",
       "      <td>0.014935</td>\n",
       "      <td>0.033247</td>\n",
       "      <td>0.024091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highest MaxVIO_global</th>\n",
       "      <td>0.014935</td>\n",
       "      <td>0.033247</td>\n",
       "      <td>0.024091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Iteration 1  Iteration 2     Average\n",
       "Average TPS                       201.341918   212.828149  207.085033\n",
       "Final Training Loss                 6.643372     6.640678    6.642025\n",
       "Final Validation Loss               6.164899     6.162109    6.163504\n",
       "Computational Resource Usage       38.100000    37.700000   37.900000\n",
       "Average CPU Usage (percent)        28.100000    22.700000   25.400000\n",
       "Average GPU Usage (percent)        10.000000    15.000000   12.500000\n",
       "Average Memory (GB)                 6.876156     7.781719    7.328938\n",
       "Average GPU Memory (GB)            14.272461    14.272461   14.272461\n",
       "Average FLOPS Estimate (GFLOPS)     2.018874     2.018874    2.018874\n",
       "Final Validation Perplexity       479.324046   483.965125  481.644585\n",
       "Average MaxVIO_global               0.014935     0.033247    0.024091\n",
       "Lowest MaxVIO_global                0.014935     0.033247    0.024091\n",
       "Highest MaxVIO_global               0.014935     0.033247    0.024091"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect training and results storage\n",
    "results_table = {\n",
    "    'Average TPS': {},\n",
    "    'Final Training Loss': {},\n",
    "    'Final Validation Loss': {},\n",
    "    'Computational Resource Usage': {},\n",
    "    'Average CPU Usage (percent)': {},\n",
    "    'Average GPU Usage (percent)': {},\n",
    "    'Average Memory (GB)': {},\n",
    "    'Average GPU Memory (GB)': {},\n",
    "    'Average FLOPS Estimate (GFLOPS)': {},\n",
    "    'Final Validation Perplexity': {},\n",
    "    'Average MaxVIO_global': {},\n",
    "    'Lowest MaxVIO_global': {},\n",
    "    'Highest MaxVIO_global': {},\n",
    "}\n",
    "\n",
    "best_perplexity = float('inf')\n",
    "# best_iteration = 0\n",
    "BEST_MODEL_PATH = 'best_deepseek_chatbot_model'\n",
    "\n",
    "# number training model iteration\n",
    "num_iteration = 5\n",
    "\n",
    "for iteration in range(num_iteration):\n",
    "    print(f\"\\nStarting Training Iteration {iteration + 1}\")\n",
    "    # Clear TensorFlow session and memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    # Create model with specified vocabulary size using direct config\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    # Use the optimized config directly with the same hyperparameters from create_deepseek_chatbot_model\n",
    "    config = DeepSeekConfig(vocab_size=vocab_size)\n",
    "    model = DeepSeekForCausalLM(config)\n",
    "\n",
    "    # Build model with sample batch\n",
    "    if tf_train_dataset:\n",
    "        sample_batch = next(iter(tf_train_dataset.take(1)))\n",
    "        model(sample_batch[0])\n",
    "\n",
    "    # Configure optimizer with exponential decay learning rate\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-4,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=lr_schedule,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.95,\n",
    "        weight_decay=0.01,\n",
    "        clipnorm=1.0\n",
    "    )\n",
    "\n",
    "    PAD_TOKEN_ID = tokenizer.token_to_id(PAD_TOKEN)\n",
    "\n",
    "    # Compile model with sparse categorical crossentropy loss\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, \n",
    "            ignore_class=PAD_TOKEN_ID\n",
    "        ),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Define training callbacks\n",
    "    metrics_callback = DeepSeekMetrics(tf_val_dataset, tokenizer) if tf_val_dataset else None\n",
    "    resource_monitor = ResourceMonitor(batch_size=2)\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            patience=3, \n",
    "            restore_best_weights=True, \n",
    "            monitor='val_loss',\n",
    "            min_delta=0.01\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            factor=0.5, \n",
    "            patience=2, \n",
    "            monitor='val_loss',\n",
    "            min_delta=0.01,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        tf.keras.callbacks.TerminateOnNaN(),\n",
    "    ]\n",
    "\n",
    "    if metrics_callback:\n",
    "        callbacks.append(metrics_callback)\n",
    "    if resource_monitor:\n",
    "        callbacks.append(resource_monitor)\n",
    "\n",
    "    # Train model on GPU\n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Force GPU for training the model\n",
    "    with tf.device('/GPU:0'):\n",
    "        history = model.fit(\n",
    "            tf_train_dataset,\n",
    "            epochs=100,\n",
    "            validation_data=tf_val_dataset,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "    total_training_time = time.time() - start_time\n",
    "\n",
    "    # Calculate final metrics\n",
    "    print(\"Calculating final perplexity\")\n",
    "    final_perplexity = calculate_perplexity(model, tf_val_dataset, tokenizer, max_batches=50) if tf_val_dataset else float('inf')\n",
    "    final_training_loss = history.history['loss'][-1] if 'loss' in history.history else float('inf')\n",
    "    final_validation_loss = history.history['val_loss'][-1] if 'val_loss' in history.history else float('inf')\n",
    "\n",
    "    print(f\"Final Training Loss: {final_training_loss:.4f}\")\n",
    "    print(f\"Final Validation Loss: {final_validation_loss:.4f}\")\n",
    "    print(f\"Final Perplexity: {final_perplexity:.2f}\")\n",
    "\n",
    "    # Save best model based on perplexity\n",
    "    if final_perplexity < best_perplexity:\n",
    "        best_perplexity = final_perplexity\n",
    "        best_iteration = iteration + 1\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(BEST_MODEL_PATH):\n",
    "                shutil.rmtree(BEST_MODEL_PATH)\n",
    "            model.save(BEST_MODEL_PATH, save_format='tf')\n",
    "            print(f\"New best model saved from iteration {iteration + 1} with perplexity: {final_perplexity:.2f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving best model: {e}\")\n",
    "\n",
    "    # Collect and store resource metrics\n",
    "    if resource_monitor.resource_metrics:\n",
    "        avg_gpu_memory = np.mean([m['avg_gpu_memory_gb'] for m in resource_monitor.resource_metrics])\n",
    "        avg_system_memory = np.mean([m['avg_system_memory_gb'] for m in resource_monitor.resource_metrics])\n",
    "        avg_gpu_usage = np.mean([m['avg_gpu_usage_percent'] for m in resource_monitor.resource_metrics])\n",
    "        avg_cpu_usage = np.mean([m['avg_cpu_usage_percent'] for m in resource_monitor.resource_metrics])\n",
    "        avg_tps = np.mean([m['tokens_per_second'] for m in resource_monitor.resource_metrics])\n",
    "    else:\n",
    "        avg_gpu_memory = avg_system_memory = avg_gpu_usage = avg_cpu_usage = avg_tps = 0\n",
    "\n",
    "    # Calculate FLOPs\n",
    "    total_params = model.count_params()\n",
    "    batch_size = 2\n",
    "    seq_length = max_seq_len\n",
    "    flops_per_token = total_params * 2\n",
    "    total_flops = flops_per_token * batch_size * seq_length\n",
    "    total_gflops = total_flops / 1e9\n",
    "\n",
    "    # Collect MaxVIO metrics\n",
    "    if metrics_callback and hasattr(metrics_callback, 'max_vio_values'):\n",
    "        max_vio_values = metrics_callback.max_vio_values\n",
    "        if max_vio_values:\n",
    "            avg_max_vio = np.mean(max_vio_values)\n",
    "            min_max_vio = np.min(max_vio_values)\n",
    "            max_max_vio = np.max(max_vio_values)\n",
    "        else:\n",
    "            avg_max_vio = min_max_vio = max_max_vio = 0.0\n",
    "    else:\n",
    "        avg_max_vio = min_max_vio = max_max_vio = 0.0\n",
    "\n",
    "    # Store results in results table\n",
    "    iteration_key = f'Iteration {iteration + 1}'\n",
    "    results_table['Average TPS'][iteration_key] = avg_tps\n",
    "    results_table['Final Training Loss'][iteration_key] = final_training_loss\n",
    "    results_table['Final Validation Loss'][iteration_key] = final_validation_loss\n",
    "    results_table['Computational Resource Usage'][iteration_key] = avg_cpu_usage + avg_gpu_usage\n",
    "    results_table['Average CPU Usage (percent)'][iteration_key] = avg_cpu_usage\n",
    "    results_table['Average GPU Usage (percent)'][iteration_key] = avg_gpu_usage\n",
    "    results_table['Average Memory (GB)'][iteration_key] = avg_system_memory\n",
    "    results_table['Average GPU Memory (GB)'][iteration_key] = avg_gpu_memory\n",
    "    results_table['Average FLOPS Estimate (GFLOPS)'][iteration_key] = total_gflops\n",
    "    results_table['Final Validation Perplexity'][iteration_key] = final_perplexity\n",
    "    results_table['Average MaxVIO_global'][iteration_key] = avg_max_vio\n",
    "    results_table['Lowest MaxVIO_global'][iteration_key] = min_max_vio\n",
    "    results_table['Highest MaxVIO_global'][iteration_key] = max_max_vio\n",
    "\n",
    "    print(f\"Result {iteration + 1} completed in {total_training_time:.2f} seconds\")\n",
    "    print(f\"Average TPS: {avg_tps:.0f}\")\n",
    "\n",
    "# Calculate average metrics across iterations\n",
    "for key in results_table:\n",
    "    values = [results_table[key][f'Iteration {i+1}'] for i in range(num_iteration) if f'Iteration {i+1}' in results_table[key]]\n",
    "    results_table[key]['Average'] = np.mean(values) if values else 0\n",
    "\n",
    "# Create and display results DataFrame\n",
    "final_result = pd.DataFrame(results_table)\n",
    "final_result = final_result.T\n",
    "\n",
    "# Save final model weights\n",
    "model.save_weights('final_deepseek_chatbot_weights.h5')\n",
    "\n",
    "print(\"\\nTraining Results Summary:\")\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n",
      "TensorFlow version: 2.15.0\n",
      "Mixed precision policy: mixed_float16\n",
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# Disable XLA (XLA JIT compilation) to prevent CUDA graph errors and ensure stability\n",
    "tf.config.optimizer.set_jit(False)  # Disabled XLA to prevent CUDA graph errors\n",
    "\n",
    "# Configure GPU memory growth to prevent memory allocation issues\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Enable mixed precision training with FP16 for improved performance and reduced memory usage\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Configuration parameters from the model and preprocessing pipeline\n",
    "vocab_size = 12000\n",
    "max_seq_len = 70\n",
    "batch_size = 2\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "SEP_TOKEN = \"<sep>\"\n",
    "\n",
    "# Print TensorFlow version, mixed precision policy, and GPU availability for debugging\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Mixed precision policy: {mixed_precision.global_policy().name}\")\n",
    "print(f\"GPU available: {len(gpus) > 0}\")\n",
    "\n",
    "# DEEPSEEK MOE COMPONENTS\n",
    "\n",
    "class DeepSeekRMSNorm(layers.Layer):\n",
    "    \"\"\"RMS Normalization layer \"\"\"\n",
    "    def __init__(self, hidden_size, eps=1e-6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size  # Dimension of the hidden representations\n",
    "        self.variance_epsilon = eps     # Small constant to avoid division by zero\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize learnable weight parameter for scaling normalized output\n",
    "        self.weight = self.add_weight(\n",
    "            name='weight',\n",
    "            shape=(self.hidden_size,),\n",
    "            initializer='ones',\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        x = tf.cast(x, tf.float32)  # Cast input to float32 for stable computation\n",
    "        \n",
    "        # Compute variance as mean of squared inputs along the last dimension\n",
    "        variance = tf.reduce_mean(tf.square(x), axis=-1, keepdims=True)\n",
    "        # Normalize input using RMS formula with epsilon for numerical stability\n",
    "        x = x * tf.math.rsqrt(variance + self.variance_epsilon)\n",
    "        \n",
    "        # Scale normalized output with learnable weight and cast back to original dtype\n",
    "        return self.weight * tf.cast(x, input_dtype)\n",
    "\n",
    "class RotaryPositionEmbedding(layers.Layer):\n",
    "    \"\"\"Rotary Position Embedding (RoPE) with dynamic NTK scaling for positional encoding\"\"\"\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, scaling_type=None, scaling_factor=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim                              # Dimension of the head embeddings\n",
    "        self.max_position_embeddings = max_position_embeddings  # Maximum sequence length\n",
    "        self.base = base                           # Base value for frequency calculation\n",
    "        self.scaling_type = scaling_type           # Type of scaling (\"linear\" or \"dynamic\")\n",
    "        self.scaling_factor = scaling_factor       # Scaling factor for RoPE\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Compute inverse frequencies for rotary embeddings\n",
    "        inv_freq = 1.0 / (self.base ** (tf.range(0, self.dim, 2, dtype=tf.float32) / tf.cast(self.dim, tf.float32)))\n",
    "        self.inv_freq = tf.constant(inv_freq, dtype=tf.float32)\n",
    "        self._compute_rotary_cache()  # Precompute cosine and sine caches\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def _compute_rotary_cache(self):\n",
    "        \"\"\"Compute cosine and sine caches for rotary embeddings with optional scaling\"\"\"\n",
    "        if self.scaling_type == \"linear\":\n",
    "            # Linear scaling: divide positions by scaling factor\n",
    "            t = tf.range(self.max_position_embeddings, dtype=tf.float32) / self.scaling_factor\n",
    "        elif self.scaling_type == \"dynamic\":\n",
    "            # Dynamic NTK scaling for extended context lengths\n",
    "            if self.max_position_embeddings > self.base:\n",
    "                base = self.base * (\n",
    "                    (self.scaling_factor * self.max_position_embeddings / self.base) - \n",
    "                    (self.scaling_factor - 1)\n",
    "                ) ** (self.dim / (self.dim - 2))\n",
    "                inv_freq = 1.0 / (base ** (tf.range(0, self.dim, 2, dtype=tf.float32) / tf.cast(self.dim, tf.float32)))\n",
    "                self.inv_freq = tf.constant(inv_freq, dtype=tf.float32)\n",
    "            t = tf.range(self.max_position_embeddings, dtype=tf.float32)\n",
    "        else:\n",
    "            # Default case: no scaling\n",
    "            t = tf.range(self.max_position_embeddings, dtype=tf.float32)\n",
    "            \n",
    "        # Compute frequency matrix for rotary embeddings\n",
    "        freqs = tf.einsum('i,j->ij', t, self.inv_freq)\n",
    "        emb = tf.concat([freqs, freqs], axis=-1)  # Concatenate for full embedding dimension\n",
    "        \n",
    "        # Cache cosine and sine values as non-trainable variables\n",
    "        self.cos_cached = tf.Variable(tf.cos(emb), trainable=False, name='cos_cached')\n",
    "        self.sin_cached = tf.Variable(tf.sin(emb), trainable=False, name='sin_cached')\n",
    "        \n",
    "    def call(self, x, position_ids=None, seq_len=None):\n",
    "        # Get input tensor dimensions\n",
    "        batch_size, seq_len_x, num_heads, head_dim = tf.unstack(tf.shape(x))\n",
    "        \n",
    "        # Generate position IDs if not provided\n",
    "        if position_ids is None:\n",
    "            position_ids = tf.range(seq_len_x, dtype=tf.int32)\n",
    "            position_ids = tf.tile(tf.reshape(position_ids, [1, -1]), [batch_size, 1])\n",
    "        \n",
    "        # Gather precomputed cosine and sine values for given position IDs\n",
    "        cos = tf.gather(self.cos_cached, position_ids, axis=0)  # Shape: [batch_size, seq_len_x, head_dim]\n",
    "        sin = tf.gather(self.sin_cached, position_ids, axis=0)  # Shape: [batch_size, seq_len_x, head_dim]\n",
    "        \n",
    "        # Split cosine and sine for application to x1 and x2\n",
    "        cos_1, cos_2 = tf.split(cos, 2, axis=-1)\n",
    "        sin_1, sin_2 = tf.split(sin, 2, axis=-1)\n",
    "        \n",
    "        # Reshape for broadcasting with attention heads\n",
    "        cos_1 = tf.reshape(cos_1, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        sin_1 = tf.reshape(sin_1, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        cos_2 = tf.reshape(cos_2, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        sin_2 = tf.reshape(sin_2, [batch_size, seq_len_x, 1, head_dim // 2])\n",
    "        \n",
    "        # Repeat to match number of attention heads\n",
    "        cos_1 = tf.repeat(cos_1, num_heads, axis=2)\n",
    "        sin_1 = tf.repeat(sin_1, num_heads, axis=2)\n",
    "        cos_2 = tf.repeat(cos_2, num_heads, axis=2)\n",
    "        sin_2 = tf.repeat(sin_2, num_heads, axis=2)\n",
    "        \n",
    "        # Cast to input dtype for mixed precision compatibility\n",
    "        x_dtype = x.dtype\n",
    "        cos_1 = tf.cast(cos_1, x_dtype)\n",
    "        sin_1 = tf.cast(sin_1, x_dtype)\n",
    "        cos_2 = tf.cast(cos_2, x_dtype)\n",
    "        sin_2 = tf.cast(sin_2, x_dtype)\n",
    "        \n",
    "        # Split input tensor into two parts for rotary application\n",
    "        x1, x2 = tf.split(x, 2, axis=-1)\n",
    "        \n",
    "        # Apply rotary embeddings to compute rotated representations\n",
    "        rotated_x1 = x1 * cos_1 - x2 * sin_1\n",
    "        rotated_x2 = x1 * sin_2 + x2 * cos_2\n",
    "        \n",
    "        # Concatenate rotated parts to restore original shape\n",
    "        return tf.concat([rotated_x1, rotated_x2], axis=-1)\n",
    "\n",
    "class DeepSeekMLP(layers.Layer):\n",
    "    \"\"\"Multi-Layer Perceptron (MLP) with Swish activation\"\"\"\n",
    "    def __init__(self, hidden_size, intermediate_size, hidden_act=\"swish\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size           # Input and output dimension\n",
    "        self.intermediate_size = intermediate_size  # Intermediate layer dimension\n",
    "        self.hidden_act = hidden_act            # Activation function (Swish/SILU)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Define gate projection layer with reduced initializer variance for stability\n",
    "        self.gate_proj = layers.Dense(\n",
    "            self.intermediate_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            use_bias=False\n",
    "        )\n",
    "        # Define up projection layer\n",
    "        self.up_proj = layers.Dense(\n",
    "            self.intermediate_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            use_bias=False\n",
    "        )\n",
    "        # Define down projection layer to return to hidden size\n",
    "        self.down_proj = layers.Dense(\n",
    "            self.hidden_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            use_bias=False\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Apply gate and up projections\n",
    "        gate_output = self.gate_proj(x)\n",
    "        up_output = self.up_proj(x)\n",
    "        # Apply Swish (SiLU) activation to gate output and multiply with up output\n",
    "        activated = tf.nn.silu(gate_output) * up_output\n",
    "        # Project back to hidden size\n",
    "        return self.down_proj(activated)\n",
    "\n",
    "class MoEGate(layers.Layer):\n",
    "    \"\"\"Mixture of Experts (MoE) gating mechanism with load balancing auxiliary loss\"\"\"\n",
    "    def __init__(self, num_experts, num_experts_per_tok, hidden_size, \n",
    "                 scoring_func='softmax', norm_topk_prob=False, \n",
    "                 aux_loss_alpha=0.001, seq_aux=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_experts = num_experts                  # Total number of experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok  # Number of experts per token\n",
    "        self.hidden_size = hidden_size                  # Input dimension\n",
    "        self.scoring_func = scoring_func                # Scoring function for expert selection\n",
    "        self.norm_topk_prob = norm_topk_prob            # Normalize top-k probabilities\n",
    "        self.aux_loss_alpha = aux_loss_alpha            # Weight for auxiliary loss\n",
    "        self.seq_aux = seq_aux                          # Compute auxiliary loss per sequence\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize gating weights with reduced variance for stability\n",
    "        self.weight = self.add_weight(\n",
    "            name='gate_weight',\n",
    "            shape=(self.num_experts, self.hidden_size),\n",
    "            initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            trainable=True\n",
    "        )\n",
    "        # Initialize expert assignment counter for tracking during training\n",
    "        self.expert_counts = tf.Variable(tf.zeros([self.num_experts], dtype=tf.int32), trainable=False)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, hidden_states, training=False):\n",
    "        # Get input dimensions\n",
    "        batch_size, seq_len, h = tf.unstack(tf.shape(hidden_states))\n",
    "        hidden_states_flat = tf.reshape(hidden_states, [-1, h])\n",
    "        # Compute gating logits\n",
    "        logits = tf.matmul(hidden_states_flat, self.weight, transpose_b=True)\n",
    "        \n",
    "        # Clip logits for numerical stability\n",
    "        logits = tf.clip_by_value(logits, -50.0, 50.0)\n",
    "        \n",
    "        # Apply scoring function (softmax) to compute expert scores\n",
    "        if self.scoring_func == 'softmax':\n",
    "            temperature = 0.5                                         # Temperature for scaling logits\n",
    "            scaled_logits = tf.cast(logits, tf.float32) / temperature\n",
    "            scores = tf.nn.softmax(scaled_logits, axis=-1)\n",
    "            scores = tf.cast(scores, hidden_states.dtype)\n",
    "        else:\n",
    "            scores = tf.nn.softmax(tf.cast(logits, tf.float32), axis=-1)\n",
    "            scores = tf.cast(scores, hidden_states.dtype)\n",
    "\n",
    "        # Select top-k experts and their weights\n",
    "        topk_weights, topk_indices = tf.math.top_k(\n",
    "            scores, k=self.num_experts_per_tok, sorted=False\n",
    "        )\n",
    "\n",
    "        # Normalize top-k weights if specified\n",
    "        if self.num_experts_per_tok > 1 and self.norm_topk_prob:\n",
    "            denominator = tf.reduce_sum(topk_weights, axis=-1, keepdims=True) + 1e-20\n",
    "            topk_weights = topk_weights / denominator\n",
    "\n",
    "        # Update expert assignment counts during training\n",
    "        if training:\n",
    "            flat_topk_indices = tf.reshape(topk_indices, [-1])\n",
    "            counts = tf.reduce_sum(tf.one_hot(flat_topk_indices, depth=self.num_experts), axis=0)\n",
    "            self.expert_counts.assign_add(tf.cast(counts, tf.int32))\n",
    "\n",
    "        aux_loss = None\n",
    "        if training and self.aux_loss_alpha > 0.0:\n",
    "            if self.seq_aux:\n",
    "                # Sequence-level auxiliary loss for load balancing\n",
    "                scores_seq = tf.reshape(scores, [batch_size, seq_len, -1])\n",
    "                topk_indices_seq = tf.reshape(topk_indices, [batch_size, -1])\n",
    "                ce = tf.zeros([batch_size, self.num_experts], dtype=tf.float32)\n",
    "                mask = tf.one_hot(topk_indices_seq, depth=self.num_experts)\n",
    "                ce = tf.reduce_sum(mask, axis=1)\n",
    "                seq_len_float = tf.cast(seq_len, tf.float32)\n",
    "                num_experts_per_tok_float = tf.cast(self.num_experts_per_tok, tf.float32)\n",
    "                num_experts_float = tf.cast(self.num_experts, tf.float32)\n",
    "                ce = ce / (seq_len_float * num_experts_per_tok_float / num_experts_float)\n",
    "                scores_seq_mean = tf.reduce_mean(tf.cast(scores_seq, tf.float32), axis=1)\n",
    "                aux_loss = tf.reduce_sum(ce * scores_seq_mean, axis=1)\n",
    "                aux_loss = tf.reduce_mean(aux_loss) * self.aux_loss_alpha\n",
    "            else:\n",
    "                # Token-level auxiliary loss\n",
    "                mask_ce = tf.one_hot(tf.reshape(topk_indices, [-1]), depth=self.num_experts)\n",
    "                ce = tf.reduce_mean(tf.cast(mask_ce, tf.float32), axis=0)\n",
    "                Pi = tf.reduce_mean(tf.cast(scores, tf.float32), axis=0)\n",
    "                fi = ce * self.num_experts\n",
    "                aux_loss = tf.reduce_sum(Pi * fi) * self.aux_loss_alpha\n",
    "\n",
    "            # Add auxiliary loss to the layer's losses\n",
    "            self.add_loss(aux_loss)\n",
    "\n",
    "        return topk_indices, topk_weights, aux_loss\n",
    "\n",
    "class DeepSeekMoE(layers.Layer):\n",
    "    \"\"\"Mixture of Experts (MoE) layer with shared and routed experts\"\"\"\n",
    "    def __init__(self, hidden_size, n_routed_experts, n_shared_experts, \n",
    "                 num_experts_per_tok, moe_intermediate_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size                      # Input and output dimension\n",
    "        self.n_routed_experts = n_routed_experts            # Number of routed experts\n",
    "        self.n_shared_experts = n_shared_experts            # Number of shared experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok      # Experts per token\n",
    "        self.moe_intermediate_size = moe_intermediate_size  # MoE intermediate size\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize list of routed expert MLPs\n",
    "        self.routed_experts = [\n",
    "            DeepSeekMLP(\n",
    "                self.hidden_size, \n",
    "                self.moe_intermediate_size,\n",
    "                name=f\"routed_expert_{i}\"\n",
    "            ) for i in range(self.n_routed_experts)\n",
    "        ]\n",
    "        \n",
    "        # Initialize shared experts if specified\n",
    "        if self.n_shared_experts is not None:\n",
    "            shared_intermediate_size = self.moe_intermediate_size * self.n_shared_experts\n",
    "            self.shared_experts = DeepSeekMLP(\n",
    "                self.hidden_size,\n",
    "                shared_intermediate_size,\n",
    "                name=\"shared_experts\"\n",
    "            )\n",
    "        else:\n",
    "            self.shared_experts = None\n",
    "            \n",
    "        # Initialize MoE gating mechanism\n",
    "        self.gate = MoEGate(\n",
    "            self.n_routed_experts,\n",
    "            self.num_experts_per_tok,\n",
    "            self.hidden_size,\n",
    "            name=\"moe_gate\"\n",
    "        )\n",
    "        \n",
    "        # Build each expert and gate\n",
    "        for expert in self.routed_experts:\n",
    "            expert.build(input_shape)\n",
    "        if self.shared_experts:\n",
    "            self.shared_experts.build(input_shape)\n",
    "        self.gate.build(input_shape)\n",
    "        \n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, hidden_states, training=False):\n",
    "        identity = hidden_states  # Store input for residual connection\n",
    "        orig_shape = tf.shape(hidden_states)\n",
    "        # Get top-k expert indices and weights\n",
    "        topk_idx, topk_weight, aux_loss = self.gate(hidden_states, training=training)\n",
    "        hidden_states_flat = tf.reshape(hidden_states, [-1, self.hidden_size])\n",
    "        \n",
    "        if training:\n",
    "            # Expand inputs for top-k experts during training\n",
    "            hidden_states_expanded = tf.repeat(\n",
    "                hidden_states_flat, self.num_experts_per_tok, axis=0\n",
    "            )\n",
    "            flat_topk_idx = tf.reshape(topk_idx, [-1])\n",
    "            combined_output = tf.zeros_like(hidden_states_expanded)\n",
    "            # Process each expert's input\n",
    "            for i, expert in enumerate(self.routed_experts):\n",
    "                expert_mask = tf.equal(flat_topk_idx, i)\n",
    "                if tf.reduce_any(expert_mask):\n",
    "                    expert_input = tf.boolean_mask(hidden_states_expanded, expert_mask)\n",
    "                    expert_output = expert(expert_input)\n",
    "                    update_indices = tf.where(expert_mask)\n",
    "                    combined_output = tf.tensor_scatter_nd_update(\n",
    "                        combined_output,\n",
    "                        update_indices,\n",
    "                        expert_output\n",
    "                    )\n",
    "                    \n",
    "            # Apply expert weights and reshape output\n",
    "            topk_weight_flat = tf.reshape(topk_weight, [-1, 1])\n",
    "            weighted_output = combined_output * topk_weight_flat\n",
    "            weighted_output = tf.reshape(\n",
    "                weighted_output,\n",
    "                [-1, self.num_experts_per_tok, self.hidden_size]\n",
    "            )\n",
    "            routed_output = tf.reduce_sum(weighted_output, axis=1)\n",
    "            routed_output = tf.reshape(routed_output, orig_shape)\n",
    "        else:\n",
    "            # Use efficient inference path\n",
    "            routed_output = self._moe_infer(hidden_states_flat, topk_idx, topk_weight)\n",
    "            routed_output = tf.reshape(routed_output, orig_shape)\n",
    "            \n",
    "        # Add shared expert output if available\n",
    "        if self.shared_experts is not None:\n",
    "            shared_output = self.shared_experts(identity)\n",
    "            final_output = routed_output + shared_output\n",
    "        else:\n",
    "            final_output = routed_output\n",
    "            \n",
    "        # Apply residual connection\n",
    "        return final_output + identity\n",
    "        \n",
    "    def _moe_infer(self, x, expert_indices, expert_weights):\n",
    "        \"\"\"Efficient inference path for MoE layer\"\"\"\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        expert_cache = tf.zeros_like(x)\n",
    "        # Clip expert indices to valid range\n",
    "        expert_indices = tf.clip_by_value(expert_indices, 0, self.n_routed_experts - 1)\n",
    "        flat_expert_indices = tf.reshape(expert_indices, [-1])\n",
    "        flat_expert_weights = tf.reshape(expert_weights, [-1, 1])\n",
    "        total_tokens = tf.shape(x)[0]\n",
    "        token_indices_base = tf.range(total_tokens, dtype=tf.int64)\n",
    "        token_indices_base = tf.repeat(token_indices_base, self.num_experts_per_tok, axis=0)\n",
    "        \n",
    "        # Process each expert's tokens\n",
    "        for i in range(self.n_routed_experts):\n",
    "            expert_mask = tf.equal(flat_expert_indices, i)\n",
    "            if tf.reduce_any(expert_mask):\n",
    "                token_indices = tf.boolean_mask(token_indices_base, expert_mask)\n",
    "                expert_tokens = tf.gather(x, token_indices)\n",
    "                expert_out = self.routed_experts[i](expert_tokens)\n",
    "                expert_out_weighted = expert_out * tf.gather(flat_expert_weights, tf.where(expert_mask)[:, 0])\n",
    "                expert_cache = tf.tensor_scatter_nd_add(\n",
    "                    expert_cache,\n",
    "                    tf.expand_dims(token_indices, 1),\n",
    "                    expert_out_weighted\n",
    "                )\n",
    "                \n",
    "        return expert_cache\n",
    "\n",
    "class DeepSeekAttention(layers.Layer):\n",
    "    \"\"\"Multi-head attention with Rotary Position Embeddings and Grouped Query Attention (GQA) support\"\"\"\n",
    "    def __init__(self, hidden_size, num_attention_heads, num_key_value_heads=None,\n",
    "                 attention_dropout=0.0, max_position_embeddings=2048,\n",
    "                 rope_theta=10000.0, rope_scaling=None, attention_bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size                                         # Input and output dimension\n",
    "        self.num_attention_heads = num_attention_heads                         # Number of attention heads\n",
    "        self.num_key_value_heads = num_key_value_heads or num_attention_heads  # Number of key/value heads (GQA)\n",
    "        self.attention_dropout = attention_dropout                             # Dropout rate for attention weights\n",
    "        self.max_position_embeddings = max_position_embeddings                 # Maximum sequence length\n",
    "        self.rope_theta = rope_theta                                           # Base period for RoPE\n",
    "        self.rope_scaling = rope_scaling                                       # RoPE scaling configuration\n",
    "        self.attention_bias = attention_bias                                   # Whether to use bias in projections\n",
    "        self.head_dim = hidden_size // num_attention_heads                     # Dimension per head\n",
    "        self.num_key_value_groups = num_attention_heads // self.num_key_value_heads  # Groups for GQA\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Initialize query projection layer\n",
    "        self.q_proj = layers.Dense(\n",
    "            self.num_attention_heads * self.head_dim,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"q_proj\"\n",
    "        )\n",
    "        # Initialize key projection layer\n",
    "        self.k_proj = layers.Dense(\n",
    "            self.num_key_value_heads * self.head_dim,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"k_proj\"\n",
    "        )\n",
    "        # Initialize value projection layer\n",
    "        self.v_proj = layers.Dense(\n",
    "            self.num_key_value_heads * self.head_dim,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"v_proj\"\n",
    "        )\n",
    "        # Initialize output projection layer\n",
    "        self.o_proj = layers.Dense(\n",
    "            self.hidden_size,\n",
    "            use_bias=self.attention_bias,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.001),\n",
    "            name=\"o_proj\"\n",
    "        )\n",
    "        \n",
    "        # Initialize RoPE with specified scaling\n",
    "        scaling_type = self.rope_scaling.get(\"type\") if self.rope_scaling else None\n",
    "        scaling_factor = self.rope_scaling.get(\"factor\", 1.0) if self.rope_scaling else 1.0\n",
    "        self.rotary_emb = RotaryPositionEmbedding(\n",
    "            self.head_dim,\n",
    "            self.max_position_embeddings,\n",
    "            self.rope_theta,\n",
    "            scaling_type,\n",
    "            scaling_factor,\n",
    "            name=\"rotary_emb\"\n",
    "        )\n",
    "        self.dropout_layer = layers.Dropout(self.attention_dropout)\n",
    "        self.rotary_emb.build(input_shape)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, hidden_states, attention_mask=None, position_ids=None, training=False):\n",
    "        # Get input dimensions\n",
    "        batch_size, seq_len, _ = tf.unstack(tf.shape(hidden_states))\n",
    "        \n",
    "        # Project inputs to query, key, and value\n",
    "        q = self.q_proj(hidden_states)\n",
    "        k = self.k_proj(hidden_states)\n",
    "        v = self.v_proj(hidden_states)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = tf.reshape(q, [batch_size, seq_len, self.num_attention_heads, self.head_dim])\n",
    "        k = tf.reshape(k, [batch_size, seq_len, self.num_key_value_heads, self.head_dim])\n",
    "        v = tf.reshape(v, [batch_size, seq_len, self.num_key_value_heads, self.head_dim])\n",
    "        \n",
    "        # Apply rotary embeddings to query and key\n",
    "        q_rotated = self.rotary_emb(q, position_ids=position_ids)\n",
    "        k_rotated = self.rotary_emb(k, position_ids=position_ids)\n",
    "        \n",
    "        # Transpose for attention computation\n",
    "        q_rotated = tf.transpose(q_rotated, [0, 2, 1, 3])\n",
    "        k_rotated = tf.transpose(k_rotated, [0, 2, 1, 3])\n",
    "        v = tf.transpose(v, [0, 2, 1, 3])\n",
    "        \n",
    "        # Repeat key and value for GQA if necessary\n",
    "        if self.num_key_value_groups > 1:\n",
    "            k_rotated = tf.repeat(k_rotated, self.num_key_value_groups, axis=1)\n",
    "            v = tf.repeat(v, self.num_key_value_groups, axis=1)\n",
    "            \n",
    "        # Compute scaled dot-product attention\n",
    "        scale = tf.math.rsqrt(tf.cast(self.head_dim, tf.float32))\n",
    "        scale = tf.cast(scale, q_rotated.dtype)\n",
    "        attn_weights = tf.matmul(q_rotated, k_rotated, transpose_b=True) * scale\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = tf.cast(attention_mask, attn_weights.dtype)\n",
    "            if len(attention_mask.shape) == 2:\n",
    "                attention_mask = tf.expand_dims(tf.expand_dims(attention_mask, 1), 1)\n",
    "            elif len(attention_mask.shape) == 3:\n",
    "                attention_mask = tf.expand_dims(attention_mask, 1)\n",
    "            attn_weights += attention_mask\n",
    "            \n",
    "        # Clip attention weights for numerical stability\n",
    "        attn_weights = tf.clip_by_value(attn_weights, -50.0, 50.0)\n",
    "        attn_weights = tf.cast(attn_weights, tf.float32)\n",
    "        # Apply softmax with max subtraction for stability\n",
    "        attn_weights = attn_weights - tf.reduce_max(attn_weights, axis=-1, keepdims=True)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "        attn_weights = tf.cast(attn_weights, q_rotated.dtype)\n",
    "        \n",
    "        # Apply dropout to attention weights during training\n",
    "        attn_weights = self.dropout_layer(attn_weights, training=training)\n",
    "        # Compute attention output\n",
    "        attn_output = tf.matmul(attn_weights, v)\n",
    "        attn_output = tf.transpose(attn_output, [0, 2, 1, 3])\n",
    "        attn_output = tf.reshape(attn_output, [batch_size, seq_len, self.hidden_size])\n",
    "        \n",
    "        # Project attention output to hidden size\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "class DeepSeekDecoderLayer(layers.Layer):\n",
    "    \"\"\"Transformer decoder layer combining attention and MoE/MLP with residual connections\"\"\"\n",
    "    def __init__(self, config, layer_idx, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        # Initialize self-attention with GQA and RoPE\n",
    "        self.self_attn = DeepSeekAttention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_attention_heads=config.num_attention_heads,\n",
    "            num_key_value_heads=config.num_key_value_heads,\n",
    "            attention_dropout=config.attention_dropout,\n",
    "            max_position_embeddings=config.max_position_embeddings,\n",
    "            rope_theta=config.rope_theta,\n",
    "            rope_scaling=config.rope_scaling,\n",
    "            attention_bias=config.attention_bias,\n",
    "            name=\"self_attn\"\n",
    "        )\n",
    "        \n",
    "        # Determine whether to use MoE or dense MLP based on layer index and config\n",
    "        self.use_moe = (\n",
    "            config.n_routed_experts is not None and\n",
    "            layer_idx >= config.first_k_dense_replace and\n",
    "            layer_idx % config.moe_layer_freq == 0\n",
    "        )\n",
    "        \n",
    "        if self.use_moe:\n",
    "            # Initialize MoE layer for routed experts\n",
    "            self.mlp = DeepSeekMoE(\n",
    "                hidden_size=config.hidden_size,\n",
    "                n_routed_experts=config.n_routed_experts,\n",
    "                n_shared_experts=config.n_shared_experts,\n",
    "                num_experts_per_tok=config.num_experts_per_tok,\n",
    "                moe_intermediate_size=config.moe_intermediate_size,\n",
    "                name=\"moe\"\n",
    "            )\n",
    "        else:\n",
    "            # Initialize dense MLP layer\n",
    "            self.mlp = DeepSeekMLP(\n",
    "                hidden_size=config.hidden_size,\n",
    "                intermediate_size=config.intermediate_size,\n",
    "                hidden_act=config.hidden_act,\n",
    "                name=\"mlp\"\n",
    "            )\n",
    "            \n",
    "        # Initialize input and post-attention normalization layers\n",
    "        self.input_layernorm = DeepSeekRMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps, name=\"input_layernorm\"\n",
    "        )\n",
    "        self.post_attention_layernorm = DeepSeekRMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps, name=\"post_attention_layernorm\"\n",
    "        )\n",
    "        \n",
    "    def call(self, hidden_states, attention_mask=None, position_ids=None, training=False):\n",
    "        # Apply input normalization and self-attention with residual connection\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        attn_output = self.self_attn(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            training=training\n",
    "        )\n",
    "        hidden_states = residual + attn_output\n",
    "        \n",
    "        # Apply post-attention normalization and MLP/MoE with residual connection\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        mlp_output = self.mlp(hidden_states, training=training)\n",
    "        hidden_states = residual + mlp_output\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "# MODEL CONFIGURATION \n",
    "\n",
    "class DeepSeekConfig:\n",
    "    \"\"\"\n",
    "    This is the configuration class to store the configuration of a [`DeepseekMoEModel`]. It is used to instantiate an DeepSeek\n",
    "    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
    "    defaults below will yield a configuration for a small, stable, and computationally efficient MoE chatbot model.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (`int`, *optional*, defaults to 12000):\n",
    "            Vocabulary size of the Deep model. Defines the number of different tokens that can be represented by the\n",
    "            `inputs_ids` passed when calling [`DeepseekModel`]. (Optimized for chatbot use case).\n",
    "            \n",
    "        hidden_size (`int`, *optional*, defaults to 256):\n",
    "            Dimension of the hidden representations. (Optimized hidden size for stability and performance).\n",
    "            \n",
    "        intermediate_size (`int`, *optional*, defaults to 512):\n",
    "            Dimension of the MLP representations. (Balanced intermediate size for MLP layers).\n",
    "            \n",
    "        moe_intermediate_size (`int`, *optional*, defaults to 256):\n",
    "            Dimension of the MoE representations. (MoE-specific intermediate dimension).\n",
    "            \n",
    "        num_hidden_layers (`int`, *optional*, defaults to 4):\n",
    "            Number of hidden layers in the Transformer decoder. (Optimal layer count for chatbot tasks).\n",
    "            \n",
    "        num_attention_heads (`int`, *optional*, defaults to 4):\n",
    "            Number of attention heads for each attention layer in the Transformer decoder. (Balanced attention heads for computation).\n",
    "        \n",
    "        n_shared_experts (`int`, *optional*, defaults to 1):\n",
    "            Number of shared experts, None means dense model. (Set to 1, a single shared expert for parameter efficiency).\n",
    "        \n",
    "        n_routed_experts (`int`, *optional*, defaults to 4):\n",
    "            Number of routed experts, None means dense model. (Set to 4 for specialization).\n",
    "            \n",
    "        num_experts_per_tok (`int`, *optional*, defaults to 2):\n",
    "            Number of selected experts, None means dense model. (Set to 2 experts per token for balanced load).\n",
    "            \n",
    "        moe_layer_freq (`int`, *optional*, defaults to 2):\n",
    "            The frequency of the MoE layer: one expert layer for every `moe_layer_freq - 1` dense layers.\n",
    "            (MoE every other layer for computational balance).\n",
    "            \n",
    "        first_k_dense_replace (`int`, *optional*, defaults to 1):\n",
    "            - Number of dense layers in shallow layers(embed->dense->dense->...->dense->moe->moe...->lm_head).\n",
    "            - k dense layers (First layer dense for stability).\n",
    "        norm_topk_prob (`bool`, *optional*, defaults to False):\n",
    "            Whether to normalize the weights of the routed experts. (No normalization for expert weights).\n",
    "            \n",
    "        scoring_func (`str`, *optional*, defaults to 'softmax'):\n",
    "            Method of computing expert weights. (Standard softmax for expert selection).\n",
    "            \n",
    "        aux_loss_alpha (`float`, *optional*, defaults to 0.001):\n",
    "            Auxiliary loss weight coefficient. (Default auxiliary loss weight).\n",
    "            \n",
    "        seq_aux = (`bool`, *optional*, defaults to True):\n",
    "            Whether to compute the auxiliary loss for each individual sample. (Sequence-level auxiliary loss).\n",
    "            \n",
    "        num_key_value_heads (`int`, *optional*, defaults to 2):\n",
    "            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
    "            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
    "            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used.\n",
    "            (Set to 2 for GQA and memory efficiency).\n",
    "            \n",
    "        hidden_act (`str` or `function`, *optional*, defaults to \"swish\"):\n",
    "            The non-linear activation function (function or string) in the decoder. (Swish activation for better gradients).\n",
    "        \n",
    "        max_position_embeddings (`int`, *optional*, defaults to 256):\n",
    "            The maximum sequence length that this model might ever be used with. (Sufficient context length for chatbot).\n",
    "        \n",
    "        initializer_range (`float`, *optional*, defaults to 0.001):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices. (Reduced initializer range for stability).\n",
    "        \n",
    "        rms_norm_eps (`float`, *optional*, defaults to 1e-6):\n",
    "            The epsilon used by the rms normalization layers. (Standard RMS norm epsilon).\n",
    "        \n",
    "        use_cache (`bool`, *optional*, defaults to True):\n",
    "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
    "            relevant if `config.is_decoder=True`. (Enable caching for inference).\n",
    "        \n",
    "        pad_token_id (`int`, *optional*):\n",
    "            Padding token id.\n",
    "        bos_token_id (`int`, *optional*, defaults to 1):\n",
    "            Beginning of stream token id.\n",
    "        eos_token_id (`int`, *optional*, defaults to 2):\n",
    "            End of stream token id.\n",
    "        \n",
    "        pretraining_tp (`int`, *optional*, defaults to 1):\n",
    "            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
    "            document](https://huggingface.co/docs/transformers/parallelism) to understand more about it. This value is\n",
    "            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
    "            issue](https://github.com/pytorch/pytorch/issues/76232).\n",
    "        \n",
    "        tie_word_embeddings (`bool`, *optional*, defaults to False):\n",
    "            Whether to tie weight embeddings\n",
    "        \n",
    "        rope_theta (`float`, *optional*, defaults to 10000.0):\n",
    "            The base period of the RoPE embeddings. (Standard RoPE base period).\n",
    "        \n",
    "        rope_scaling (`Dict`, *optional*, defaults to {\"type\": \"linear\", \"factor\": 2.0}):\n",
    "            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
    "            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
    "            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
    "            `max_position_embeddings` to the expected new maximum. (Linear RoPE scaling enabled).\n",
    "        \n",
    "        attention_bias (`bool`, defaults to False, *optional*, defaults to False):\n",
    "            Whether to use a bias in the query, key, value and output projection layers during self-attention. (No attention bias for efficiency).\n",
    "        \n",
    "        attention_dropout (`float`, *optional*, defaults to 0.1):\n",
    "            The dropout ratio for the attention probabilities. (Increased dropout for regularization).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=vocab_size,      # Default vocabulary size optimized for chatbot\n",
    "        hidden_size=256,            # Optimized hidden size for stability and performance\n",
    "        intermediate_size=512,      # Balanced intermediate size for MLP layers\n",
    "        moe_intermediate_size=256,  # MoE-specific intermediate dimension\n",
    "        num_hidden_layers=4,        # Optimal layer count for chatbot tasks\n",
    "        num_attention_heads=4,      # Balanced attention heads for computation\n",
    "        num_key_value_heads=2,      # GQA for memory efficiency\n",
    "        n_shared_experts=3,         # Three shared experts for parameter efficiency (REVISED)\n",
    "        n_routed_experts=6,         # Six routed experts for specialization     (REVISED)\n",
    "        num_experts_per_tok=2,      # Two experts per token for balanced load\n",
    "        moe_layer_freq=1,           # MoE every layer for computational balance (REVISED)\n",
    "        first_k_dense_replace=1,    # First layer dense for stability\n",
    "        norm_topk_prob=True,        # Normalization for expert weights          (REVISED)\n",
    "        scoring_func='softmax',     # Standard softmax for expert selection\n",
    "        aux_loss_alpha=0.0022997166087842954,  # Auxiliary loss weight          (REVISED)\n",
    "        seq_aux=False,              # Token-level auxiliary loss                (REVISED)\n",
    "        hidden_act=\"swish\",         # Swish activation for better gradients\n",
    "        max_position_embeddings=256,  # Sufficient context length for chatbot\n",
    "        initializer_range=0.001,    # Reduced initializer range for stability\n",
    "        rms_norm_eps=1e-6,          # Standard RMS norm epsilon\n",
    "        use_cache=True,             # Enable caching for inference\n",
    "        rope_theta=10000.0,         # Standard RoPE base period\n",
    "        rope_scaling={\"type\": \"linear\", \"factor\": 2.0},  # Linear RoPE scaling\n",
    "        attention_bias=False,       # No attention bias for efficiency\n",
    "        attention_dropout=0.1       # Increased dropout for regularization\n",
    "    ):\n",
    "        # Initialize model architecture parameters with optimized values\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.moe_intermediate_size = moe_intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.n_shared_experts = n_shared_experts\n",
    "        self.n_routed_experts = n_routed_experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok\n",
    "        self.moe_layer_freq = moe_layer_freq\n",
    "        self.first_k_dense_replace = first_k_dense_replace\n",
    "        self.norm_topk_prob = norm_topk_prob\n",
    "        self.scoring_func = scoring_func\n",
    "        self.aux_loss_alpha = aux_loss_alpha\n",
    "        self.seq_aux = seq_aux\n",
    "        self.hidden_act = hidden_act\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.initializer_range = initializer_range\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.use_cache = use_cache\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self.attention_bias = attention_bias\n",
    "        self.attention_dropout = attention_dropout\n",
    "\n",
    "class DeepSeekModel(tf.keras.Model):\n",
    "    \"\"\"Core DeepSeek transformer model for chatbot, provide raw hidden states\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.padding_idx = 0\n",
    "        \n",
    "        # Initialize embedding layer for token inputs\n",
    "        self.embed_tokens = layers.Embedding(\n",
    "            config.vocab_size,\n",
    "            config.hidden_size,\n",
    "            embeddings_initializer=tf.keras.initializers.RandomNormal(\n",
    "                stddev=config.initializer_range\n",
    "            ),\n",
    "            name=\"embed_tokens\"\n",
    "        )\n",
    "        \n",
    "        # Initialize decoder layers\n",
    "        self.decoder_layers = [\n",
    "            DeepSeekDecoderLayer(config, layer_idx=i, name=f\"layer_{i}\")\n",
    "            for i in range(config.num_hidden_layers)\n",
    "        ]\n",
    "        \n",
    "        # Initialize final normalization layer\n",
    "        self.norm = DeepSeekRMSNorm(\n",
    "            config.hidden_size, eps=config.rms_norm_eps, name=\"norm\"\n",
    "        )\n",
    "        \n",
    "        # Initialize expert assignment counter for MoE layers\n",
    "        self.expert_assignment_counts = tf.Variable(\n",
    "            tf.zeros([config.n_routed_experts], dtype=tf.int32),\n",
    "            trainable=False,\n",
    "            name=\"expert_assignment_counts\"\n",
    "        ) if config.n_routed_experts else None\n",
    "        \n",
    "    def call(self, input_ids, attention_mask=None, position_ids=None, training=False):\n",
    "        # Embed input tokens\n",
    "        hidden_states = self.embed_tokens(input_ids)\n",
    "        \n",
    "        # Create causal mask if not provided\n",
    "        if attention_mask is None:\n",
    "            seq_len = tf.shape(input_ids)[1]\n",
    "            attention_mask = self._create_causal_mask(seq_len)\n",
    "            \n",
    "        # Prepare attention mask for causal attention\n",
    "        attention_mask = tf.cast(attention_mask, hidden_states.dtype)\n",
    "        if len(attention_mask.shape) == 2:\n",
    "            attention_mask = tf.expand_dims(tf.expand_dims(attention_mask, 1), 1)\n",
    "        elif len(attention_mask.shape) == 3:\n",
    "            attention_mask = tf.expand_dims(attention_mask, 1)\n",
    "        attention_mask = (tf.cast(1.0, hidden_states.dtype) - attention_mask) * tf.cast(-1e4, hidden_states.dtype)\n",
    "        \n",
    "        # Process through decoder layers and aggregate expert counts\n",
    "        for layer in self.decoder_layers:\n",
    "            hidden_states = layer(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                training=training\n",
    "            )\n",
    "            # Update expert assignment counts for MoE layers during training\n",
    "            if training and hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE):\n",
    "                expert_counts = layer.mlp.gate.expert_counts\n",
    "                if self.expert_assignment_counts is not None:\n",
    "                    self.expert_assignment_counts.assign_add(expert_counts)\n",
    "            \n",
    "        # Apply final normalization\n",
    "        return self.norm(hidden_states)\n",
    "        \n",
    "    def _create_causal_mask(self, seq_len):\n",
    "        \"\"\"Create causal attention mask to prevent attending to future tokens\"\"\"\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return tf.cast(mask, tf.float16)\n",
    "\n",
    "class DeepSeekForCausalLM(tf.keras.Model):\n",
    "    \"\"\"Causal language model for chatbot text generation\"\"\"\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.transformer = DeepSeekModel(config, name=\"transformer\")\n",
    "        # Initialize language model head\n",
    "        self.lm_head = layers.Dense(\n",
    "            config.vocab_size,\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(\n",
    "                stddev=config.initializer_range\n",
    "            ),\n",
    "            use_bias=False,\n",
    "            name=\"lm_head\"\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, attention_mask=None, position_ids=None, training=False):\n",
    "        # Handle dictionary or tensor inputs\n",
    "        if isinstance(inputs, dict):\n",
    "            input_ids = inputs.get('input_ids')\n",
    "            attention_mask = inputs.get('attention_mask', attention_mask)\n",
    "        else:\n",
    "            input_ids = inputs\n",
    "        \n",
    "        # Process through transformer\n",
    "        hidden_states = self.transformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            training=training\n",
    "        )\n",
    "        # Generate logits for token prediction\n",
    "        return self.lm_head(hidden_states)\n",
    "\n",
    "# METRICS AND CALLBACKS\n",
    "\n",
    "class ResourceMonitor(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to monitor system resources (GPU, CPU, memory) during training\"\"\"\n",
    "    def __init__(self, batch_size=2):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.resource_metrics = []  # Store resource usage metrics\n",
    "        self.start_time = time.time()\n",
    "        self.total_tokens_processed = 0\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "        self.epoch_tokens = 0\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # Update token count for throughput calculation\n",
    "        self.total_tokens_processed += self.batch_size * max_seq_len\n",
    "        self.epoch_tokens += self.batch_size * max_seq_len\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        try:\n",
    "            # Collect GPU and system resource usage\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            gpu_memory = sum([gpu.memoryUsed for gpu in gpus]) / len(gpus) if gpus else 0\n",
    "            gpu_usage = sum([gpu.load * 100 for gpu in gpus]) / len(gpus) if gpus else 0\n",
    "            cpu_usage = psutil.cpu_percent()\n",
    "            memory_info = psutil.virtual_memory()\n",
    "            system_memory = memory_info.used / (1024 ** 3)\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            tokens_per_second = self.epoch_tokens / epoch_time if epoch_time > 0 else 0\n",
    "            \n",
    "            # Store resource metrics\n",
    "            resource_stats = {\n",
    "                'epoch': epoch + 1,\n",
    "                'avg_gpu_memory_gb': float(gpu_memory / 1024),\n",
    "                'avg_system_memory_gb': float(system_memory),\n",
    "                'avg_gpu_usage_percent': float(gpu_usage),\n",
    "                'avg_cpu_usage_percent': float(cpu_usage),\n",
    "                'tokens_per_second': float(tokens_per_second),\n",
    "                'throughput_tps': float(tokens_per_second)\n",
    "            }\n",
    "            \n",
    "            self.resource_metrics.append(resource_stats)\n",
    "            \n",
    "            # Print resource usage summary\n",
    "            print(f\"Epoch {epoch+1} Resources: GPU Mem: {gpu_memory/1024:.1f}GB, \"\n",
    "                  f\"System Mem: {system_memory:.1f}GB, GPU: {gpu_usage:.1f}%, \"\n",
    "                  f\"CPU: {cpu_usage:.1f}%, TPS: {tokens_per_second:.0f}\")\n",
    "                  \n",
    "        except Exception as e:\n",
    "            print(f\"Resource monitoring error: {e}\")\n",
    "\n",
    "class DeepSeekMetrics(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback to track specific metrics, including MaxVIO for expert balancing\"\"\"\n",
    "    def __init__(self, validation_data, tokenizer):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.metrics_history = []\n",
    "        self.start_time = time.time()\n",
    "        self.max_vio_values = []\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Reset expert assignment counts for accurate per-epoch metrics\n",
    "        if hasattr(self.model.transformer, 'expert_assignment_counts') and self.model.transformer.expert_assignment_counts is not None:\n",
    "            self.model.transformer.expert_assignment_counts.assign(tf.zeros([self.model.config.n_routed_experts], dtype=tf.int32))\n",
    "        for layer in self.model.transformer.decoder_layers:\n",
    "            if hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE):\n",
    "                layer.mlp.gate.expert_counts.assign(tf.zeros([layer.mlp.n_routed_experts], dtype=tf.int32))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        \n",
    "        # Estimate FLOPs for computational efficiency tracking\n",
    "        total_params = self.model.count_params()\n",
    "        flops_per_token = total_params * 2\n",
    "        total_flops = flops_per_token * batch_size * max_seq_len\n",
    "        total_gflops = total_flops / 1e9\n",
    "        \n",
    "        logs['gflops'] = float(total_gflops)\n",
    "        \n",
    "        # Calculate MaxVIO for expert load balancing\n",
    "        if self.validation_data:\n",
    "            try:\n",
    "                max_vio = self._calculate_max_vio()\n",
    "                logs['max_vio'] = float(max_vio)\n",
    "                self.max_vio_values.append(max_vio)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Metrics calculation error: {e}\")\n",
    "                logs['max_vio'] = 0.0\n",
    "                \n",
    "        self.metrics_history.append(logs.copy())\n",
    "        \n",
    "        # Print simplified metrics summary\n",
    "        print(f\"Epoch {epoch+1}: Loss={logs.get('loss', 0):.4f}, \"\n",
    "              f\"Val_Loss={logs.get('val_loss', 0):.4f}, \"\n",
    "              f\"GFLOPs={total_gflops:.2f}, \"\n",
    "              f\"MaxVIO={logs.get('max_vio', 0):.4f}\")\n",
    "              \n",
    "    def _calculate_max_vio(self):\n",
    "        \"\"\"Calculate Maximum Violation (MaxVIO) for expert load balancing\"\"\"\n",
    "        total_counts = np.zeros(self.model.config.n_routed_experts, dtype=np.float32)\n",
    "        moe_layers = 0\n",
    "        # Aggregate expert counts across MoE layers\n",
    "        for layer in self.model.transformer.decoder_layers:\n",
    "            if hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE):\n",
    "                counts = layer.mlp.gate.expert_counts.numpy()\n",
    "                total_counts += counts\n",
    "                moe_layers += 1\n",
    "        \n",
    "        # Compute MaxVIO based on aggregated counts\n",
    "        if moe_layers > 0 and np.sum(total_counts) > 0:\n",
    "            normalized_counts = total_counts / (np.sum(total_counts) + 1e-10)\n",
    "            expected_uniform = 1.0 / len(total_counts)\n",
    "            max_vio = np.max(np.abs(normalized_counts - expected_uniform)) / expected_uniform\n",
    "            return max_vio\n",
    "        return 0.0\n",
    "\n",
    "class ComprehensiveModelCheckpoint(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Callback for saving model checkpoints based on monitored metric\"\"\"\n",
    "    def __init__(self, filepath, monitor='val_loss', save_best_only=True, mode='min'):\n",
    "        super().__init__()\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.save_best_only = save_best_only\n",
    "        self.mode = mode\n",
    "        self.best_value = float('inf') if mode == 'min' else -float('inf')\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            return\n",
    "            \n",
    "        current_value = logs.get(self.monitor)\n",
    "        if current_value is None:\n",
    "            return\n",
    "            \n",
    "        # Determine if current model should be saved\n",
    "        if self.mode == 'min':\n",
    "            should_save = current_value < self.best_value\n",
    "        else:\n",
    "            should_save = current_value > self.best_value\n",
    "            \n",
    "        if should_save or not self.save_best_only:\n",
    "            self.best_value = current_value\n",
    "            self.model.save_weights(self.filepath)\n",
    "            print(f\"Model checkpoint saved with {self.monitor}: {current_value:.4f}\")\n",
    "\n",
    "\n",
    "# Function to calculate perplexity on validation set\n",
    "def calculate_perplexity(model, dataset, tokenizer, max_batches=None):          # Sample batch for calculate perplexity\n",
    "    \"\"\"Calculate perplexity on validation dataset to evaluate model performance\"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    PAD_TOKEN_ID = tokenizer.token_to_id(PAD_TOKEN)\n",
    "    \n",
    "    for i, batch in enumerate(dataset.take(max_batches)):\n",
    "        inputs, labels = batch\n",
    "        logits = model(inputs, training=False)\n",
    "        # Clip logits for numerical stability\n",
    "        logits = tf.clip_by_value(logits, -50.0, 50.0)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        # Compute per-token loss\n",
    "        per_token_loss = -tf.gather(log_probs, labels, batch_dims=2)\n",
    "        if tf.reduce_any(tf.math.is_nan(per_token_loss)):\n",
    "            continue\n",
    "        # Mask padding tokens\n",
    "        mask = tf.cast(labels != PAD_TOKEN_ID, per_token_loss.dtype)\n",
    "        batch_loss = tf.reduce_sum(per_token_loss * mask)\n",
    "        batch_tokens = tf.reduce_sum(mask)\n",
    "        if tf.math.is_nan(batch_loss) or batch_tokens == 0:\n",
    "            continue\n",
    "        total_loss += batch_loss.numpy()\n",
    "        total_tokens += batch_tokens.numpy()\n",
    "        \n",
    "    # Compute perplexity as exponential of average loss\n",
    "    if total_tokens > 0:\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        return math.exp(min(avg_loss, 20.0))\n",
    "    return float('inf')\n",
    "\n",
    "\n",
    "# Define custom objects for model loading\n",
    "custom_objects = {\n",
    "    'DeepSeekRMSNorm': DeepSeekRMSNorm,\n",
    "    'RotaryPositionEmbedding': RotaryPositionEmbedding,\n",
    "    'DeepSeekMLP': DeepSeekMLP,\n",
    "    'MoEGate': MoEGate,\n",
    "    'DeepSeekMoE': DeepSeekMoE,\n",
    "    'DeepSeekAttention': DeepSeekAttention,\n",
    "    'DeepSeekDecoderLayer': DeepSeekDecoderLayer,\n",
    "    'DeepSeekModel': DeepSeekModel,\n",
    "    'DeepSeekForCausalLM': DeepSeekForCausalLM\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary\n",
      "Model: \"deep_seek_for_causal_lm_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (DeepSeekModel  multiple                  6918168   \n",
      " )                                                               \n",
      "                                                                 \n",
      " lm_head (Dense)             multiple                  292096    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7210264 (27.50 MB)\n",
      "Trainable params: 7079168 (27.00 MB)\n",
      "Non-trainable params: 131096 (512.09 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Visualize the model\n",
    "# Create model with specified vocabulary size using direct config\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "# Use the optimized config directly with the same hyperparameters from create_deepseek_chatbot_model\n",
    "config = DeepSeekConfig(vocab_size=vocab_size)\n",
    "model = DeepSeekForCausalLM(config)\n",
    "\n",
    "# Build model\n",
    "if tf_train_dataset:\n",
    "    sample_batch = next(iter(tf_train_dataset.take(1)))\n",
    "    model(sample_batch[0])\n",
    "\n",
    "# Configure optimizer with more conservative settings\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,  # Reduced learning rate\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.95,\n",
    "    weight_decay=0.1,\n",
    "    clipnorm=1.0\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, \n",
    "        ignore_class=PAD_TOKEN_ID,  # Use correct PAD token ID\n",
    "        reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
    "    ),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nModel Summary\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "SEP_TOKEN = \"<sep>\"\n",
    "\n",
    "def calculate_perplexity(model, dataset, tokenizer, max_batches=None):\n",
    "    \"\"\"\n",
    "    Calculate perplexity on validation dataset to evaluate model performance,\n",
    "    using the manual Negative Log-Likelihood method as specified in the model training.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0  # Initialize total loss\n",
    "    total_tokens = 0  # Initialize total token count\n",
    "    # Load special token IDs with fallback to 0 if not found\n",
    "    special_token_ids = [\n",
    "        tokenizer.token_to_id(token) or 0\n",
    "        for token in [PAD_TOKEN, BOS_TOKEN, EOS_TOKEN, SEP_TOKEN]\n",
    "    ]\n",
    "    \n",
    "    # Use entire dataset if max_batches is None for calculate perplexity\n",
    "    dataset_iter = dataset.take(max_batches) if max_batches else dataset\n",
    "\n",
    "    for batch in dataset_iter:\n",
    "        # Handle different batch formats\n",
    "        # Access dictionary from tuple\n",
    "        batch_dict = batch[0]  \n",
    "        inputs = {\n",
    "            'input_ids': batch_dict['input_ids'],\n",
    "            'attention_mask': batch_dict.get('attention_mask', tf.ones_like(batch_dict['input_ids'], dtype=tf.int32))\n",
    "        }\n",
    "        labels = batch_dict.get('labels', batch_dict['input_ids'])  # Use input_ids as fallback\n",
    "\n",
    "        # Get model predictions (logits)\n",
    "        # model(inputs, training=False) returns (batch_size, sequence_length, vocab_size) logits\n",
    "        logits = model(inputs, training=False)\n",
    "\n",
    "        # Shift logits and labels for next token prediction (standard LM objective)\n",
    "        logits = logits[:, :-1, :]\n",
    "        labels = labels[:, 1:]\n",
    "        \n",
    "        # Clip logits for numerical stability (from prototype)\n",
    "        logits = tf.clip_by_value(logits, -50.0, 50.0)  # Use training's clipping range\n",
    "        \n",
    "        # Calculate log probabilities\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Compute per-token loss (Negative Log Likelihood)\n",
    "        # tf.gather extracts the log_prob corresponding to the true label for each token\n",
    "        per_token_loss = -tf.gather(log_probs, labels, batch_dims=2)\n",
    "        \n",
    "        # Mask special tokens (<pad>, <bos>, <eos>, <sep>)\n",
    "        mask = tf.cast(tf.reduce_all([labels != token_id for token_id in special_token_ids], axis=0), per_token_loss.dtype)\n",
    "        \n",
    "        # Sum masked losses and count tokens\n",
    "        batch_loss = tf.reduce_sum(per_token_loss * mask)\n",
    "        batch_tokens = tf.reduce_sum(mask)\n",
    "        \n",
    "        # Skip invalid batches (NaN loss or zero tokens)\n",
    "        valid_batch = tf.logical_and(tf.logical_not(tf.math.is_nan(batch_loss)), batch_tokens > 0)\n",
    "        total_loss += tf.where(valid_batch, batch_loss.numpy(), 0.0)\n",
    "        total_tokens += tf.where(valid_batch, batch_tokens.numpy(), 0.0)\n",
    "        \n",
    "    # Compute perplexity as exponential of average loss\n",
    "    return math.exp(min(total_loss / total_tokens, 20.0)) if total_tokens > 0 else float('inf')  # Use training's capping\n",
    "\n",
    "def calculate_rouge_scores(generated_texts, reference_texts):\n",
    "    \"\"\"Calculate ROUGE scores using the rouge_score library\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for gen, ref in zip(generated_texts, reference_texts):\n",
    "        # Skip invalid pairs\n",
    "        scores = scorer.score(ref, gen) if isinstance(gen, str) and isinstance(ref, str) else {'rouge1': {'fmeasure': 0}, 'rouge2': {'fmeasure': 0}, 'rougeL': {'fmeasure': 0}}\n",
    "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': np.mean(rouge1_scores),\n",
    "        'rouge2': np.mean(rouge2_scores),\n",
    "        'rougeL': np.mean(rougeL_scores)\n",
    "    }\n",
    "\n",
    "def calculate_bleu_score(generated_texts, reference_texts):\n",
    "    \"\"\"Calculate BLEU score using NLTK\"\"\"\n",
    "    gen_tokens = []\n",
    "    ref_tokens = []\n",
    "    for gen, ref in zip(generated_texts, reference_texts):\n",
    "        gen_tok = nltk.word_tokenize(gen.lower()) if isinstance(gen, str) else []\n",
    "        ref_tok = nltk.word_tokenize(ref.lower()) if isinstance(ref, str) else []\n",
    "        gen_tokens.append(gen_tok)\n",
    "        ref_tokens.append([ref_tok] if ref_tok else [[]])\n",
    "    \n",
    "    # Skip if no valid tokens\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    return corpus_bleu(ref_tokens, gen_tokens, smoothing_function=smoothing) if gen_tokens and ref_tokens else 0.0\n",
    "\n",
    "def calculate_meteor_score(generated_texts, reference_texts):\n",
    "    \"\"\"Calculate METEOR score using NLTK\"\"\"\n",
    "    meteor_scores = []\n",
    "    for gen, ref in zip(generated_texts, reference_texts):\n",
    "        gen_tokens = nltk.word_tokenize(gen.lower()) if isinstance(gen, str) else []\n",
    "        ref_tokens = nltk.word_tokenize(ref.lower()) if isinstance(ref, str) else []\n",
    "        score = meteor_score([ref_tokens], gen_tokens) if gen_tokens and ref_tokens else 0.0\n",
    "        meteor_scores.append(score)\n",
    "    \n",
    "    return np.mean(meteor_scores)\n",
    "\n",
    "def nucleus_sampling_decode(model, input_ids, tokenizer, top_p=0.9, max_length=50):\n",
    "    \"\"\"Nucleus sampling (top-p sampling) decoding strategy for text generation\"\"\"\n",
    "    generated = input_ids\n",
    "    eos_token_id = tokenizer.token_to_id(EOS_TOKEN)\n",
    "    max_input_length = 70\n",
    "    \n",
    "    # Truncate input if too long\n",
    "    generated = generated[:, :max_input_length] if tf.shape(generated)[1] > max_input_length else generated\n",
    "        \n",
    "    for _ in range(max_length):\n",
    "        # Use last max_input_length tokens\n",
    "        model_input_seq = generated[:, -max_input_length:] if tf.shape(generated)[1] > max_input_length else generated\n",
    "            \n",
    "        attention_mask = tf.ones_like(model_input_seq, dtype=tf.int32)\n",
    "        inputs = {'input_ids': model_input_seq, 'attention_mask': attention_mask}\n",
    "        logits = model(inputs, training=False)\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        sorted_logits = tf.sort(next_token_logits, direction='DESCENDING')\n",
    "        sorted_indices = tf.argsort(next_token_logits, direction='DESCENDING')\n",
    "        cumulative_probs = tf.cumsum(tf.nn.softmax(tf.cast(sorted_logits, tf.float32), axis=-1), axis=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove = tf.concat([\n",
    "            tf.zeros_like(sorted_indices_to_remove[:, :1]),\n",
    "            sorted_indices_to_remove[:, :-1]\n",
    "        ], axis=1)\n",
    "        indices_to_remove = tf.gather(sorted_indices_to_remove, tf.argsort(sorted_indices), batch_dims=1)\n",
    "        next_token_logits = tf.where(\n",
    "            indices_to_remove,\n",
    "            tf.constant(-1e9, dtype=next_token_logits.dtype),\n",
    "            next_token_logits\n",
    "        )\n",
    "        \n",
    "        next_token = tf.random.categorical(next_token_logits, num_samples=1, dtype=tf.int32)\n",
    "        next_token = tf.expand_dims(tf.squeeze(next_token, axis=0), axis=0)\n",
    "        \n",
    "        generated = tf.concat([generated, next_token], axis=1)\n",
    "        \n",
    "        # Stop if EOS token is generated\n",
    "        if next_token.numpy()[0, 0] == eos_token_id:\n",
    "            break\n",
    "            \n",
    "    return generated\n",
    "\n",
    "def generate_text_batch(model, input_batch, tokenizer, strategy='nucleus', **kwargs):\n",
    "    \"\"\"Generate text for a batch of inputs using specified strategy\"\"\"\n",
    "    generated_texts = [\n",
    "        tokenizer.decode(nucleus_sampling_decode(model, tf.expand_dims(input_batch[i], 0), tokenizer, **kwargs)[0].numpy(), skip_special_tokens=True)\n",
    "        for i in range(input_batch.shape[0])\n",
    "    ]\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "def get_reference_texts(dataset, tokenizer, num_samples=None):\n",
    "    \"\"\"Extract reference texts from dataset for evaluation\"\"\"\n",
    "    reference_texts = []\n",
    "    \n",
    "    # Use entire dataset\n",
    "    dataset_iter = dataset.take(num_samples) if num_samples else dataset\n",
    "    \n",
    "    for batch in dataset_iter:\n",
    "        input_ids = batch[0]['input_ids']\n",
    "        reference_texts.extend([\n",
    "            tokenizer.decode(input_ids[i].numpy(), skip_special_tokens=True).strip()\n",
    "            for i in range(input_ids.shape[0])\n",
    "        ])\n",
    "        if num_samples and len(reference_texts) >= num_samples:\n",
    "            return reference_texts[:num_samples]\n",
    "        \n",
    "    return reference_texts\n",
    "\n",
    "def calculate_gflops(model, batch_size=2, seq_length=max_seq_len):\n",
    "    \"\"\"Calculate estimated GFLOPS for the model\"\"\"\n",
    "    total_params = model.count_params()\n",
    "    flops_per_token = total_params * 2\n",
    "    total_flops = flops_per_token * batch_size * seq_length\n",
    "    \n",
    "    return total_flops / 1e9\n",
    "\n",
    "# AUXILIARY-LOSS-FREE LOAD BALANCING METRICS\n",
    "def calculate_aux_free_max_vio(model, dataset, num_batches=None):\n",
    "    \"\"\"\n",
    "    Calculate MaxVIO (Maximum Violation) according to auxiliary-loss-free load balancing strategy.\n",
    "    Based on \"AUXILIARY-LOSS-FREE LOAD BALANCING STRATEGY FOR MIXTURE-OF-EXPERTS\" research paper.\n",
    "    This implementation focuses on measuring the imbalance in expert utilization without auxiliary loss.\n",
    "    \"\"\"\n",
    "    # Ensure the model has the expected structure before proceeding\n",
    "    model.transformer.decoder_layers\n",
    "    \n",
    "    # Collect expert routing statistics across all MoE layers\n",
    "    expert_utilization = {}\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Use entire dataset if num_batches is None\n",
    "    dataset_iter = dataset.take(num_batches) if num_batches else dataset\n",
    "    \n",
    "    for batch in dataset_iter:\n",
    "        # Get input data based on batch type\n",
    "        input_ids = batch[0]['input_ids']\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = tf.ones_like(input_ids, dtype=tf.int32)\n",
    "        \n",
    "        # Call the model with the expected dictionary format\n",
    "        inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "        \n",
    "        # Process batch through model to collect expert statistics\n",
    "        _ = model(inputs, training=False)\n",
    "        \n",
    "        # Collect expert counts from all MoE layers\n",
    "        for layer_idx, layer in enumerate(model.transformer.decoder_layers):\n",
    "            expert_counts = (\n",
    "                layer.mlp.gate.expert_counts.numpy()\n",
    "                if hasattr(layer, 'mlp') and isinstance(layer.mlp, DeepSeekMoE)\n",
    "                else np.zeros(0)\n",
    "            )\n",
    "            if expert_counts.size:\n",
    "                layer_key = f'layer_{layer_idx}'\n",
    "                expert_utilization[layer_key] = expert_utilization.get(layer_key, np.zeros_like(expert_counts)) + expert_counts\n",
    "                total_tokens += np.sum(expert_counts)\n",
    "    \n",
    "    # Calculate MaxVIO across all layers and experts\n",
    "    max_vio_values = [\n",
    "        np.max(np.abs(counts / np.sum(counts) - 1.0 / len(counts))) / (1.0 / len(counts))\n",
    "        for counts in expert_utilization.values() if np.sum(counts) > 0\n",
    "    ]\n",
    "    \n",
    "    # Return average MaxVIO across all layers\n",
    "    return np.mean(max_vio_values) if max_vio_values else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Experiment Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully\n",
      "\n",
      "------------------------------------------------------------\n",
      "Starting Evaluation Iteration 1\n",
      "------------------------------------------------------------\n",
      "Iteration 1: Model loaded successfully.\n",
      "Loaded reference texts from entire test dataset\n",
      "Loaded 162 reference texts from test dataset\n",
      "Calculating MaxVIO (Global)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86325a68a5864d558284ca99ebc58f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration 1 Strategies:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating nucleus generation strategy on entire test dataset...\n",
      "Processing 81 batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ea85db0b874cff9ec449711dbef8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "nucleus batches:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  nucleus: ROUGE-L=0.0661, BLEU=0.0065, METEOR=0.1160\n",
      "\n",
      "Calculating perplexity...\n",
      "\n",
      "Iteration 1 Results:\n",
      "  - Strategy: nucleus\n",
      "  - Perplexity: 624.84\n",
      "  - ROUGE-L: 0.0661\n",
      "  - BLEU: 0.0065\n",
      "  - METEOR: 0.1160\n",
      "  - Average TPS: 53.60\n",
      "  - GPU Memory: 14.27 GB\n",
      "  - GFLOPS: 2.02\n",
      "  - MaxVIO (Global): 0.05307359\n",
      "\n",
      "Completed Evaluation Iteration 1\n",
      "\n",
      "------------------------------------------------------------\n",
      "Starting Evaluation Iteration 2\n",
      "------------------------------------------------------------\n",
      "Iteration 2: Model loaded successfully.\n",
      "Loaded reference texts from entire test dataset\n",
      "Loaded 162 reference texts from test dataset\n",
      "Calculating MaxVIO (Global)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963b0d00e3bc4e21848d7aa0da5c2da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration 2 Strategies:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating nucleus generation strategy on entire test dataset...\n",
      "Processing 81 batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97566e5ba14844e1a3e98d09366b6f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "nucleus batches:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  nucleus: ROUGE-L=0.0649, BLEU=0.0045, METEOR=0.1115\n",
      "\n",
      "Calculating perplexity...\n",
      "\n",
      "Iteration 2 Results:\n",
      "  - Strategy: nucleus\n",
      "  - Perplexity: 619.98\n",
      "  - ROUGE-L: 0.0649\n",
      "  - BLEU: 0.0045\n",
      "  - METEOR: 0.1115\n",
      "  - Average TPS: 55.69\n",
      "  - GPU Memory: 14.27 GB\n",
      "  - GFLOPS: 2.02\n",
      "  - MaxVIO (Global): 0.05307359\n",
      "\n",
      "Completed Evaluation Iteration 2\n",
      "\n",
      "Final Evaluation Results Summary\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration 1</th>\n",
       "      <th>Iteration 2</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Computational Resource Usage</th>\n",
       "      <td>41.400000</td>\n",
       "      <td>39.400000</td>\n",
       "      <td>41.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average CPU Usage (percent)</th>\n",
       "      <td>29.400000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>29.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Usage (percent)</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Memory (GB)</th>\n",
       "      <td>8.144314</td>\n",
       "      <td>8.515434</td>\n",
       "      <td>8.144314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average GPU Memory (GB)</th>\n",
       "      <td>14.272461</td>\n",
       "      <td>14.272461</td>\n",
       "      <td>14.272461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average FLOPS Estimate (GFLOPS)</th>\n",
       "      <td>2.018874</td>\n",
       "      <td>2.018874</td>\n",
       "      <td>2.018874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Validation Perplexity</th>\n",
       "      <td>624.842739</td>\n",
       "      <td>619.980174</td>\n",
       "      <td>624.842739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average MaxVIO (Global)</th>\n",
       "      <td>0.053074</td>\n",
       "      <td>0.053074</td>\n",
       "      <td>0.053074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average ROUGE-1</th>\n",
       "      <td>0.085332</td>\n",
       "      <td>0.081101</td>\n",
       "      <td>0.085332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average ROUGE-2</th>\n",
       "      <td>0.009024</td>\n",
       "      <td>0.007523</td>\n",
       "      <td>0.009024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average ROUGE-L</th>\n",
       "      <td>0.066066</td>\n",
       "      <td>0.064900</td>\n",
       "      <td>0.066066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average BLEU</th>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.004472</td>\n",
       "      <td>0.006522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average METEOR</th>\n",
       "      <td>0.116034</td>\n",
       "      <td>0.111503</td>\n",
       "      <td>0.116034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average TTFT (s)</th>\n",
       "      <td>2.547544</td>\n",
       "      <td>2.451104</td>\n",
       "      <td>2.547544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average TPOT (s/token)</th>\n",
       "      <td>0.036393</td>\n",
       "      <td>0.035016</td>\n",
       "      <td>0.036393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average TPS</th>\n",
       "      <td>53.600483</td>\n",
       "      <td>55.686530</td>\n",
       "      <td>53.600483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Iteration 1  Iteration 2     Average\n",
       "Computational Resource Usage       41.400000    39.400000   41.400000\n",
       "Average CPU Usage (percent)        29.400000    27.400000   29.400000\n",
       "Average GPU Usage (percent)        12.000000    12.000000   12.000000\n",
       "Average Memory (GB)                 8.144314     8.515434    8.144314\n",
       "Average GPU Memory (GB)            14.272461    14.272461   14.272461\n",
       "Average FLOPS Estimate (GFLOPS)     2.018874     2.018874    2.018874\n",
       "Average Validation Perplexity     624.842739   619.980174  624.842739\n",
       "Average MaxVIO (Global)             0.053074     0.053074    0.053074\n",
       "Average ROUGE-1                     0.085332     0.081101    0.085332\n",
       "Average ROUGE-2                     0.009024     0.007523    0.009024\n",
       "Average ROUGE-L                     0.066066     0.064900    0.066066\n",
       "Average BLEU                        0.006522     0.004472    0.006522\n",
       "Average METEOR                      0.116034     0.111503    0.116034\n",
       "Average TTFT (s)                    2.547544     2.451104    2.547544\n",
       "Average TPOT (s/token)              0.036393     0.035016    0.036393\n",
       "Average TPS                        53.600483    55.686530   53.600483"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAIN EVALUATION CODE\n",
    "\n",
    "# Initialize results table with comprehensive metrics\n",
    "results_table = {\n",
    "    'Computational Resource Usage': {},\n",
    "    'Average CPU Usage (percent)': {},\n",
    "    'Average GPU Usage (percent)': {},\n",
    "    'Average Memory (GB)': {},\n",
    "    'Average GPU Memory (GB)': {},\n",
    "    'Average FLOPS Estimate (GFLOPS)': {},\n",
    "    'Average Validation Perplexity': {},\n",
    "    'Average MaxVIO (Global)': {},\n",
    "    'Average ROUGE-1': {},\n",
    "    'Average ROUGE-2': {},\n",
    "    'Average ROUGE-L': {},\n",
    "    'Average BLEU': {},\n",
    "    'Average METEOR': {},\n",
    "    'Average TTFT (s)': {},\n",
    "    'Average TPOT (s/token)': {},\n",
    "    'Average TPS': {}\n",
    "}\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"deepseek_tokenizer.json\")\n",
    "print(\"Tokenizer loaded successfully\")\n",
    "\n",
    "# Define text generation strategies to evaluate (NUCLEUS SAMPLING)\n",
    "generation_strategies = [\n",
    "    ('nucleus', {'top_p': 0.9, 'max_length': 50})\n",
    "]\n",
    "\n",
    "# Initialize global MaxVIO list for all iterations \n",
    "global_max_vio_values = []\n",
    "\n",
    "# Number of evaluation iteration\n",
    "num_iteration = 5\n",
    "\n",
    "# Evaluate model for 2 iterations\n",
    "for iteration in range(num_iteration):\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Starting Evaluation Iteration {iteration + 1}\")\n",
    "    print(f\"{'-'*60}\", flush=True)  # Force flush for immediate output\n",
    "\n",
    "    # Clear memory before each iteration to ensure consistent evaluation\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    model = tf.keras.models.load_model(\n",
    "        'best_deepseek_chatbot_model',\n",
    "        custom_objects=custom_objects,\n",
    "        compile=False\n",
    "    )\n",
    "    \n",
    "    # Set config for loaded model to fix AttributeError\n",
    "    model.config = DeepSeekConfig(vocab_size=tokenizer.get_vocab_size())\n",
    "    print(f\"Iteration {iteration + 1}: Model loaded successfully.\", flush=True)\n",
    "    \n",
    "    # Build model\n",
    "    sample_batch = next(iter(tf_test_dataset.take(1)))\n",
    "    model(sample_batch[0])\n",
    "\n",
    "    # Initialize metrics for this iteration\n",
    "    start_time = time.time()\n",
    "    all_generated_texts = []\n",
    "    all_reference_texts = []\n",
    "    resource_metrics = []\n",
    "    inference_times = []\n",
    "    strategy_results = {}\n",
    "\n",
    "    # Get reference texts from entire test dataset\n",
    "    print(\"Loaded reference texts from entire test dataset\", flush=True)\n",
    "    reference_texts = get_reference_texts(tf_test_dataset, tokenizer, num_samples=None)\n",
    "    print(f\"Loaded {len(reference_texts)} reference texts from test dataset\", flush=True)\n",
    "\n",
    "    # Calculate auxiliary-loss-free MaxVIO metrics on entire test dataset\n",
    "    print(\"Calculating MaxVIO (Global)...\", flush=True)\n",
    "    aux_free_max_vio = calculate_aux_free_max_vio(model, tf_test_dataset, num_batches=None)\n",
    "    #imbalance_metrics = calculate_expert_load_imbalance(model, tf_test_dataset, num_batches=None)\n",
    "\n",
    "    # Store MaxVIO values for global statistics\n",
    "    global_max_vio_values.append(aux_free_max_vio)\n",
    "\n",
    "    # Evaluate each generation strategy on entire test dataset with progress bar\n",
    "    for strategy_name, strategy_kwargs in tqdm(generation_strategies, desc=f\"Iteration {iteration + 1} Strategies\", file=sys.stdout):\n",
    "        print(f\"\\nEvaluating {strategy_name} generation strategy on entire test dataset...\", flush=True)\n",
    "        strategy_generated_texts = []\n",
    "        strategy_inference_times = []\n",
    "\n",
    "        batch_count = 0\n",
    "        total_batches = sum(1 for _ in tf_test_dataset)\n",
    "        print(f\"Processing {total_batches} batches...\", flush=True)\n",
    "\n",
    "        # Wrap batch processing with tqdm for progress visualization, ensure output to stdout\n",
    "        for batch in tqdm(tf_test_dataset, total=total_batches, desc=f\"{strategy_name} batches\", file=sys.stdout):\n",
    "            # Extract input_ids from batch\n",
    "            input_batch = batch[0]['input_ids']\n",
    "\n",
    "            # Generate text with current strategy\n",
    "            strategy_start_time = time.time()\n",
    "            generated_texts = generate_text_batch(\n",
    "                model, input_batch, tokenizer, \n",
    "                strategy=strategy_name, \n",
    "                **strategy_kwargs\n",
    "            )\n",
    "            strategy_inference_time = time.time() - strategy_start_time\n",
    "\n",
    "            strategy_generated_texts.extend(generated_texts)\n",
    "            strategy_inference_times.append(strategy_inference_time)\n",
    "            batch_count += 1\n",
    "\n",
    "        # Calculate metrics for this strategy\n",
    "        # Use all available texts for evaluation\n",
    "        min_len = len(strategy_generated_texts)\n",
    "        strategy_generated_subset = strategy_generated_texts[:min_len]\n",
    "        strategy_reference_subset = reference_texts[:min_len]\n",
    "\n",
    "        rouge_scores = calculate_rouge_scores(strategy_generated_subset, strategy_reference_subset)\n",
    "        bleu_score = calculate_bleu_score(strategy_generated_subset, strategy_reference_subset)\n",
    "        meteor_score_val = calculate_meteor_score(strategy_generated_subset, strategy_reference_subset)\n",
    "        avg_inference_time = np.mean(strategy_inference_times)\n",
    "\n",
    "        strategy_results[strategy_name] = {\n",
    "            'rouge1': rouge_scores['rouge1'],\n",
    "            'rouge2': rouge_scores['rouge2'],\n",
    "            'rougeL': rouge_scores['rougeL'],\n",
    "            'bleu': bleu_score,\n",
    "            'meteor': meteor_score_val,\n",
    "            'avg_inference_time': avg_inference_time,\n",
    "            'num_generated': len(strategy_generated_subset)\n",
    "        }\n",
    "\n",
    "        print(f\"  {strategy_name}: ROUGE-L={rouge_scores['rougeL']:.4f}, BLEU={bleu_score:.4f}, METEOR={meteor_score_val:.4f}\", flush=True)\n",
    "\n",
    "        # Collect best strategy texts for overall metrics\n",
    "        all_generated_texts = strategy_generated_texts\n",
    "\n",
    "    # Calculate perplexity on entire test dataset\n",
    "    print(\"\\nCalculating perplexity...\", flush=True)\n",
    "    perplexity = calculate_perplexity(model, tf_test_dataset, tokenizer, max_batches=None)\n",
    "\n",
    "    # Collect resource usage metrics\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu_memory = sum([gpu.memoryUsed for gpu in gpus]) / len(gpus)\n",
    "    gpu_memory_gb = gpu_memory / 1024\n",
    "    gpu_usage = sum([gpu.load * 100 for gpu in gpus]) / len(gpus)\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    system_memory = psutil.virtual_memory().used / (1024 ** 3)\n",
    "\n",
    "    resource_metrics.append({\n",
    "        'gpu_memory_gb': float(gpu_memory_gb),\n",
    "        'system_memory_gb': float(system_memory),\n",
    "        'gpu_usage_percent': float(gpu_usage),\n",
    "        'cpu_usage_percent': float(cpu_usage)\n",
    "    })\n",
    "\n",
    "    # Calculate overall metrics (using best strategy)\n",
    "    best_strategy = max(strategy_results.items(), \n",
    "                       key=lambda x: x[1]['rougeL'] + x[1]['bleu'] + x[1]['meteor'])\n",
    "\n",
    "    best_metrics = best_strategy[1]\n",
    "    total_time = time.time() - start_time\n",
    "    total_tokens_generated = len(all_generated_texts) * max_seq_len\n",
    "    avg_tps = total_tokens_generated / total_time\n",
    "\n",
    "    # Store results for this iteration\n",
    "    iteration_key = f'Iteration {iteration + 1}'\n",
    "\n",
    "    # Calculate resource usage averages\n",
    "    avg_gpu_memory = np.mean([m['gpu_memory_gb'] for m in resource_metrics])\n",
    "    avg_system_memory = np.mean([m['system_memory_gb'] for m in resource_metrics])\n",
    "    avg_gpu_usage = np.mean([m['gpu_usage_percent'] for m in resource_metrics])\n",
    "    avg_cpu_usage = np.mean([m['cpu_usage_percent'] for m in resource_metrics])\n",
    "\n",
    "    total_gflops = calculate_gflops(model)\n",
    "\n",
    "    results_table['Computational Resource Usage'][iteration_key] = avg_cpu_usage + avg_gpu_usage\n",
    "    results_table['Average CPU Usage (percent)'][iteration_key] = avg_cpu_usage\n",
    "    results_table['Average GPU Usage (percent)'][iteration_key] = avg_gpu_usage\n",
    "    results_table['Average Memory (GB)'][iteration_key] = avg_system_memory\n",
    "    results_table['Average GPU Memory (GB)'][iteration_key] = avg_gpu_memory\n",
    "    results_table['Average FLOPS Estimate (GFLOPS)'][iteration_key] = total_gflops\n",
    "    results_table['Average Validation Perplexity'][iteration_key] = perplexity\n",
    "    results_table['Average MaxVIO (Global)'][iteration_key] = aux_free_max_vio\n",
    "    results_table['Average ROUGE-1'][iteration_key] = best_metrics['rouge1']\n",
    "    results_table['Average ROUGE-2'][iteration_key] = best_metrics['rouge2']\n",
    "    results_table['Average ROUGE-L'][iteration_key] = best_metrics['rougeL']\n",
    "    results_table['Average BLEU'][iteration_key] = best_metrics['bleu']\n",
    "    results_table['Average METEOR'][iteration_key] = best_metrics['meteor']\n",
    "    results_table['Average TTFT (s)'][iteration_key] = best_metrics['avg_inference_time']\n",
    "    results_table['Average TPOT (s/token)'][iteration_key] = best_metrics['avg_inference_time'] / max_seq_len\n",
    "    results_table['Average TPS'][iteration_key] = avg_tps\n",
    "\n",
    "    print(f\"\\nIteration {iteration + 1} Results:\", flush=True)\n",
    "    print(f\"  - Strategy: {best_strategy[0]}\", flush=True)\n",
    "    print(f\"  - Perplexity: {perplexity:.2f}\", flush=True)\n",
    "    print(f\"  - ROUGE-L: {best_metrics['rougeL']:.4f}\", flush=True)\n",
    "    print(f\"  - BLEU: {best_metrics['bleu']:.4f}\", flush=True)\n",
    "    print(f\"  - METEOR: {best_metrics['meteor']:.4f}\", flush=True)\n",
    "    print(f\"  - Average TPS: {avg_tps:.2f}\", flush=True)\n",
    "    print(f\"  - GPU Memory: {avg_gpu_memory:.2f} GB\", flush=True)\n",
    "    print(f\"  - GFLOPS: {total_gflops:.2f}\", flush=True)\n",
    "    print(f\"  - MaxVIO (Global): {aux_free_max_vio:.8f}\", flush=True)\n",
    "\n",
    "    # Clean up after each iteration\n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"\\nCompleted Evaluation Iteration {iteration + 1}\", flush=True)\n",
    "\n",
    "for key in results_table:\n",
    "    values = [results_table[key][f'Iteration {i+1}'] for i in range(num_iteration)]\n",
    "    results_table[key]['Average'] = np.mean(values)\n",
    "\n",
    "# Create and display final results DataFrame\n",
    "final_result = pd.DataFrame(results_table)\n",
    "final_result = final_result.T\n",
    "\n",
    "print(\"\\nFinal Evaluation Results Summary\", flush=True)\n",
    "final_result"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
